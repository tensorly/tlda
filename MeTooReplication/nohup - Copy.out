[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
text length: 
11314
2098
53845
got data
('sklearn LDA', 435.2334303855896)
('gensim LDA', 1757.1910638809204)
Centering time: 0.07810115814208984
PCA fit: 3.5400211811065674
PCA Transform: 0.32713818550109863
total iterations: 15001
TLDA fit: 1049.6193809509277
PCA Reverse Transform: 0.0008721351623535156
weights shape:
(20,)
fac2 shape: 
(2098, 20)
decenter with new strategy:
[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan]
Decentering: 0.001956462860107422
decenter with old strategy:
[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan]
Smoothing and Normalization: 0.003101348876953125
[('centering', 0.07810115814208984), ('PCA fit', 3.5400211811065674), ('PCA transform', 0.32713818550109863), ('TLDA fit', 1049.6193809509277), ('unwhiten factors', 0.0008721351623535156), (' decentering', 0.001956462860107422), (' smoothing and normalization', 0.003101348876953125)]
(20, 2098)
(2098, 2098)
(20, 2098)
(20, 2097)
(20, 2098)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
(20,)
sklearn
[-3.9006910805211765, -3.25998431710567, -6.2676231258830954, -2.412165963866162, -4.834859224164658, -10.107096824591201, -15.810319821126493, -3.565884037656619, -5.9126457798464855, -4.989537406949047, -7.862170069416057, -6.475094690746831, -1.3297091711763729, -0.31810370236794694, 0.7969059721578338, -3.977298773177754, -17.255923451340355, -5.700140085518155, -2.9053468238220046, -4.523783975433301] [0.053395978235163025, 0.13543035633494335, 0.11800106702176755, 0.2870411146490104, 0.09649155775648137, 0.08502215662209237, 0.007400613772651623, 0.11327370281605985, 0.26778826476282147, 0.09203888632278552, 0.06456565504680853, 0.24243729518799548, 0.013508153737003765, -0.024992382837344937, 0.11487891333783744, 0.07488544528131229, 0.43388732357943666, 0.08937119842868717, 0.22827054389624016, 0.017164547674707792] -5.530573617627577 0.12549301958132303
gensim
[-5.47455634398315, 5.080060907255776, -4.943716799598496, -5.590501845087855, -11.360634829545537, -3.6599452124466616, -9.678021781129514, -17.88979660150637, -1.406321097807853, -2.0316503393009606, -4.887216660535277, 0.47487063027463866, -2.1696876185528633, -8.033609899306615, -9.332981125071507, -2.4839184779541044, -3.166276358564835, -11.630163050210752, 1.5746665308608248, 0.4359444946602144] [0.002079774870809794, 6.106226635438361e-18, 0.008320185072045808, -3.885780586188048e-18, 0.02071392828122793, 0.008964222165143945, 0.030773607274720227, 0.11146590545358354, 0.03047313941694464, -1.1102230246251566e-18, 0.05028495862447995, -0.0025793504075168104, 0.011246586270796048, -0.006923517784604305, 0.011339892717092187, 0.0010809144871003317, 2.498001805406602e-18, 0.11782364481706609, 0.0032466408623629595, 1.6653345369377347e-18] -4.808672773877545 0.01991552660606262
tlda
[3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927, 3.1017979876961927] [0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297, 0.01433840430299297] 3.101797987696193 0.014338404302992972
new version
Centering time: 0.07998108863830566
PCA fit: 1.221947193145752
PCA Transform: 0.3160707950592041
trigger at iteration 3591
total iterations: 10001
TLDA fit: 235.66913747787476
PCA Reverse Transform: 0.0008177757263183594
weights shape:
(2,)
fac2 shape: 
(1000, 2)
final fac2: 
[[ 0.00000000e+00  0.00000000e+00]
 [ 5.45760263e-05 -1.81316370e-04]
 [ 3.70422816e-17  6.02669992e-17]
 ...
 [ 0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00]]
decenter with new strategy:
[0. 0.]
Decentering: 0.0019352436065673828
decenter with old strategy:
[0. 0.]
Smoothing and Normalization: 0.0036668777465820312
Fit RMSE new decenter: 0.04205718496302459
Fit RMSE: 0.04235730278727757
 Test Against Ground Truth
[0.56522809]
[0.08918859]
[0.16931932]
[0.56420171]
M1: 0.00016689300537109375
Traceback (most recent call last):
  File "generate_tables.py", line 384, in <module>
    main()
  File "generate_tables.py", line 349, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15()
  File "generate_tables.py", line 189, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
new version
Centering time: 0.08474183082580566
PCA fit: 1.273925542831421
PCA Transform: 0.32754993438720703
total iterations: 10001
TLDA fit: 196.44146966934204
PCA Reverse Transform: 0.0008361339569091797
weights shape:
(2,)
fac2 shape: 
(1000, 2)
final fac2: 
[[ 0.00000000e+00  0.00000000e+00]
 [-1.08072195e-03  4.98724567e-03]
 [ 1.68341823e-17  6.59902012e-17]
 ...
 [ 0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00]]
decenter with new strategy:
[0. 0.]
Decentering: 0.0019578933715820312
decenter with old strategy:
[0. 0.]
Smoothing and Normalization: 0.0036895275115966797
Fit RMSE new decenter: 0.03981825428666733
Fit RMSE: 0.039717432067033265
 Test Against Ground Truth
[0.98929452]
[0.99437271]
[0.99055994]
[0.99436867]
M1: 0.00017333030700683594
Traceback (most recent call last):
  File "generate_tables.py", line 384, in <module>
    main()
  File "generate_tables.py", line 349, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15()
  File "generate_tables.py", line 189, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
new version
Centering time: 0.07881665229797363
PCA fit: 1.2218153476715088
PCA Transform: 0.32085108757019043
total iterations: 10001
TLDA fit: 473.54072666168213
PCA Reverse Transform: 0.0008111000061035156
weights shape:
(2,)
fac2 shape: 
(1000, 2)
final fac2: 
[[ 0.00000000e+00  0.00000000e+00]
 [-1.79899622e-03 -3.20056470e-03]
 [ 1.26349604e-17  2.05334042e-17]
 ...
 [ 0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00]]
decenter with new strategy:
[0. 0.]
Decentering: 0.0019080638885498047
decenter with old strategy:
[0. 0.]
Smoothing and Normalization: 0.003606081008911133
Fit RMSE new decenter: 0.04216099326041247
Fit RMSE: 0.04209736409905049
 Test Against Ground Truth
[0.03528166]
[0.99706391]
[0.03600547]
[0.99707652]
M1: 0.00017452239990234375
Traceback (most recent call last):
  File "generate_tables.py", line 384, in <module>
    main()
  File "generate_tables.py", line 349, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15()
  File "generate_tables.py", line 189, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
new version
Centering time: 0.08075642585754395
PCA fit: 1.2361116409301758
PCA Transform: 0.31426429748535156
total iterations: 10001
TLDA fit: 996.1096315383911
PCA Reverse Transform: 0.0008306503295898438
weights shape:
(2,)
fac2 shape: 
(1000, 2)
final fac2: 
[[ 0.00000000e+00  0.00000000e+00]
 [ 5.03871341e-04 -2.28162063e-03]
 [ 1.37097232e-17  2.23171034e-17]
 ...
 [ 0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00]]
decenter with new strategy:
[0. 0.]
Decentering: 0.0019404888153076172
decenter with old strategy:
[0. 0.]
Smoothing and Normalization: 0.0036737918853759766
Fit RMSE new decenter: 0.04134454894216544
Fit RMSE: 0.04209601119279786
 Test Against Ground Truth
[0.03635004]
[0.99792792]
[0.11614232]
[0.99885934]
M1: 0.00017595291137695312
Traceback (most recent call last):
  File "generate_tables.py", line 385, in <module>
    main()
  File "generate_tables.py", line 350, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15()
  File "generate_tables.py", line 190, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
new version
Centering time: 0.0805211067199707
PCA fit: 1.2504222393035889
PCA Transform: 0.31756162643432617
total iterations: 10001
TLDA fit: 192.25402975082397
PCA Reverse Transform: 0.0008265972137451172
weights shape:
(2,)
fac2 shape: 
(1000, 2)
final fac2: 
[[ 0.00000000e+00  0.00000000e+00]
 [ 5.06058026e-04 -2.27464229e-03]
 [ 1.25923920e-17  1.99987210e-17]
 ...
 [ 0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00]]
decenter with new strategy:
[0. 0.]
Decentering: 0.0019423961639404297
decenter with old strategy:
[0. 0.]
Smoothing and Normalization: 0.003741025924682617
Fit RMSE new decenter: 0.04134414574208838
Fit RMSE: 0.04209650076719447
 Test Against Ground Truth
[0.03524434]
[0.99674278]
[0.11611675]
[0.99836776]
M1: 0.00015735626220703125
Traceback (most recent call last):
  File "generate_tables.py", line 385, in <module>
    main()
  File "generate_tables.py", line 350, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15()
  File "generate_tables.py", line 190, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
new version
Centering time: 0.08123469352722168
PCA fit: 1.2318415641784668
PCA Transform: 0.31244421005249023
total iterations: 15001
TLDA fit: 288.37556314468384
PCA Reverse Transform: 0.0008423328399658203
weights shape:
(2,)
fac2 shape: 
(1000, 2)
final fac2: 
[[0.00000000e+00 0.00000000e+00]
 [2.38111721e-03 7.75064205e-04]
 [3.69098868e-17 6.00556632e-17]
 ...
 [0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00]]
decenter with new strategy:
[0. 0.]
Decentering: 0.0019643306732177734
decenter with old strategy:
[0. 0.]
Smoothing and Normalization: 0.003710031509399414
Fit RMSE new decenter: 0.04199003227868067
Fit RMSE: 0.04236135033148677
 Test Against Ground Truth
[0.09592723]
[0.55957568]
[0.70018]
[0.42380037]
M1: 0.00016021728515625
Traceback (most recent call last):
  File "generate_tables.py", line 393, in <module>
    main()
  File "generate_tables.py", line 358, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15()
  File "generate_tables.py", line 198, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
new version
Centering time: 0.07857251167297363
PCA fit: 1.2242622375488281
PCA Transform: 0.30860304832458496
total iterations: 15001
TLDA fit: 711.4520874023438
PCA Reverse Transform: 0.0008213520050048828
weights shape:
(2,)
fac2 shape: 
(1000, 2)
final fac2: 
[[0.00000000e+00 0.00000000e+00]
 [2.37887132e-03 7.74344249e-04]
 [3.69360483e-17 6.00231164e-17]
 ...
 [0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00]]
decenter with new strategy:
[0. 0.]
Decentering: 0.0019321441650390625
decenter with old strategy:
[0. 0.]
Smoothing and Normalization: 0.0036382675170898438
Fit RMSE new decenter: 0.04199301038181573
Fit RMSE: 0.04233164650361369
 Test Against Ground Truth
[0.09523535]
[0.56000583]
[0.70067025]
[0.42305382]
M1: 0.0001690387725830078
Traceback (most recent call last):
  File "generate_tables.py", line 393, in <module>
    main()
  File "generate_tables.py", line 358, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15()
  File "generate_tables.py", line 198, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
new version
Centering time: 0.07727956771850586
PCA fit: 1.2037849426269531
PCA Transform: 0.30588388442993164
total iterations: 15001
TLDA fit: 714.6488628387451
PCA Reverse Transform: 0.0008189678192138672
weights shape:
(2,)
fac2 shape: 
(1000, 2)
final fac2: 
[[0.00000000e+00 0.00000000e+00]
 [2.33136716e-03 6.96951978e-04]
 [3.72334010e-17 6.05073022e-17]
 ...
 [0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00]]
decenter with new strategy:
[0. 0.]
Decentering: 0.0019276142120361328
decenter with old strategy:
[0. 0.]
Smoothing and Normalization: 0.0036737918853759766
Fit RMSE new decenter: 0.04205557294463977
Fit RMSE: 0.04218998679039379
 Test Against Ground Truth
[0.08351228]
[0.57225225]
[0.71368033]
[0.38324165]
M1: 0.00015354156494140625
Traceback (most recent call last):
  File "generate_tables.py", line 393, in <module>
    main()
  File "generate_tables.py", line 358, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15()
  File "generate_tables.py", line 198, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
new version
Centering time: 0.0829627513885498
PCA fit: 1.270944356918335
PCA Transform: 0.3227412700653076
total iterations: 15001
TLDA fit: 717.4149534702301
PCA Reverse Transform: 0.0008649826049804688
weights shape:
(2,)
fac2 shape: 
(1000, 2)
final fac2: 
[[ 0.00000000e+00  0.00000000e+00]
 [ 6.21024016e-05 -2.98757416e-03]
 [ 1.54582386e-17  2.51096709e-17]
 ...
 [ 0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00]]
decenter with new strategy:
[0. 0.]
Decentering: 0.0019664764404296875
decenter with old strategy:
[0. 0.]
Smoothing and Normalization: 0.0037927627563476562
Fit RMSE new decenter: 0.042119263693271305
Fit RMSE: 0.04209780218974594
 Test Against Ground Truth
[0.03527115]
[0.99705561]
[0.04013321]
[0.9983467]
M1: 0.00016355514526367188
Traceback (most recent call last):
  File "generate_tables.py", line 393, in <module>
    main()
  File "generate_tables.py", line 358, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15()
  File "generate_tables.py", line 198, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
new version
Vocab: 500
num_tweets: 20000
density: 15
Traceback (most recent call last):
  File "generate_tables.py", line 436, in <module>
    main()
  File "generate_tables.py", line 383, in main
    create_data(vocab=vocab, seed=seed_arr[j])
  File "generate_tables.py", line 65, in create_data
    x, mu, _, alpha_0 = test_util.get_mu(num_tops, vocab, num_tweets, density, seed)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/test_util.py", line 64, in get_mu
    if w[0][k] == 1:
KeyboardInterrupt
new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.08294200897216797
PCA fit: 0.7209906578063965
PCA Transform: 0.3290574550628662
[[0.10098005 0.5849903 ]
 [0.9711602  0.13986723]]
total iterations: 2362
TLDA fit: 941.4430859088898
PCA Reverse Transform: 0.0008215904235839844
weights shape:
(2,)
fac2 shape: 
(500, 2)
final fac2: 
[[-2.45434959e-005 -3.98639725e-004]
 [-2.67042292e-016 -3.27131370e-016]
 [ 2.44102094e-004  2.50347703e-004]
 [-2.68838274e-017 -3.29327340e-017]
 [ 1.63305434e-017  2.00049432e-017]
 [ 3.24784293e-018  3.97861306e-018]
 [-7.99356517e-019 -9.79213081e-019]
 [ 5.28433152e-019  6.47331502e-019]
 [-5.37843252e-020 -6.58858891e-020]
 [ 6.54194024e-001  5.95725321e-001]
 [-4.42922237e-021 -5.42580488e-021]
 [-3.50859159e-021 -4.29803062e-021]
 [ 2.99664765e-022  3.67089842e-022]
 [-8.81698559e-023 -1.08008222e-022]
 [ 6.37576567e-024  7.81032370e-024]
 [-1.58288511e-025 -1.93903693e-025]
 [-4.10156493e-027 -5.02442396e-027]
 [-1.18509831e-004 -7.84963344e-004]
 [-9.80043596e-030 -1.20055506e-029]
 [-5.03045655e-030 -6.16231776e-030]
 [-8.54224705e-032 -1.04642670e-031]
 [ 1.56293432e-004  1.08011722e-004]
 [-2.62857508e-035 -3.22000891e-035]
 [ 1.76322875e+000  1.79593955e+000]
 [-7.91281596e-039 -9.69321289e-039]
 [ 1.69044854e-041  2.07080231e-041]
 [ 7.57425665e-043  9.27847717e-043]
 [-9.57851704e-044 -1.17336995e-043]
 [ 2.90723397e-046  3.56136652e-046]
 [-1.56037454e-004 -1.33859325e-003]
 [-1.76245072e-050 -2.15900511e-050]
 [ 7.27062265e-053  8.90652500e-053]
 [-1.57904971e-055 -1.93433856e-055]
 [ 1.70090799e-057  2.08361515e-057]
 [ 3.72846602e-060  4.56737716e-060]
 [ 4.83470601e-062  5.92252302e-062]
 [-1.31025989e-004 -8.97654921e-004]
 [-5.98618502e-067 -7.33308674e-067]
 [-1.70389056e-002 -1.18836926e-001]
 [-3.34456443e-072 -4.09709707e-072]
 [ 1.21128784e-074  1.48382965e-074]
 [ 3.33391899e-078  4.08405639e-078]
 [-1.40436075e-081 -1.72034428e-081]
 [ 2.19601154e-084  2.69011783e-084]
 [-6.41490003e-087 -7.85826336e-087]
 [ 1.08094333e-003  1.05295249e-003]
 [-7.91652031e-094 -9.69775073e-094]
 [ 1.44277856e-096  1.76740617e-096]
 [-1.27029918e-100 -1.55611863e-100]
 [ 7.95049400e-104  9.73936855e-104]
 [-1.75098637e-106 -2.14496125e-106]
 [-2.08913413e-003 -1.99656136e-002]
 [-4.59221045e-113 -5.62546554e-113]
 [ 3.55564489e-005  3.66027573e-005]
 [ 6.14339486e-119  7.52566906e-119]
 [ 5.76300183e-005  4.27809709e-005]
 [-7.38428162e-126 -9.04575744e-126]
 [ 5.62084008e-129  6.88553857e-129]
 [ 1.20130412e-004  1.19344081e-004]
 [ 1.09203427e-136  1.33774383e-136]
 [ 4.41069886e-140  5.40311354e-140]
 [-1.51544567e-142 -1.85642350e-142]
 [-4.11656302e-147 -5.04279664e-147]
 [-3.24319426e-150 -3.97291844e-150]
 [-8.33139086e-155 -1.02059679e-154]
 [-2.85338218e-158 -3.49539798e-158]
 [-6.94334276e-005 -6.34439731e-004]
 [ 7.31158924e-166  8.95670915e-166]
 [-2.22960252e-169 -2.73126685e-169]
 [-3.05925174e-174 -3.74758853e-174]
 [-1.33645433e-179 -1.63715881e-179]
 [ 8.28880203e-183  1.01537965e-182]
 [ 6.85102158e-188  8.39251299e-188]
 [-3.11041653e-192 -3.81026550e-192]
 [ 9.52681421e-197  1.16703635e-196]
 [-7.29571848e-202 -8.93726744e-202]
 [ 1.49042002e-205  1.82576704e-205]
 [ 8.05994809e-212  9.87345000e-212]
 [-8.86924961e-216 -1.08648457e-215]
 [ 5.28527930e-221  6.47447605e-221]
 [-1.02860109e-228 -1.26003807e-228]
 [-2.90923086e-233 -3.56381271e-233]
 [ 8.64473195e-241  1.05898112e-240]
 [-1.18139401e-245 -1.44720965e-245]
 [ 2.06612561e-253  2.53100735e-253]
 [ 2.69024540e-002  2.55217528e-002]
 [-2.70937647e-271 -3.31899074e-271]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.39453664e-004  1.36061105e-004]
 [ 1.24129320e-002  1.14228989e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-1.66997438e-004 -1.86663125e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-2.42510077e-003 -1.93896770e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-3.50900304e-005 -2.09886693e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 8.89227705e-004  7.06836596e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [-3.53241426e-004 -3.06141847e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.77052028e-005  2.69849377e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.79342983e-004  1.50155231e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-1.55864161e-005 -1.92948818e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [-1.39509873e-004 -9.28910288e-004]
 [-3.57390404e-005 -2.24590333e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [-4.67849522e-006 -4.74565540e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-1.77854104e-003 -1.41330295e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 7.16049306e-004  6.61588493e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.51643998e-001  1.56435573e-001]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-9.67363129e-003 -7.22198642e-002]
 [ 2.00131138e-004  1.96483101e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 3.90053127e-005  3.38735788e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-2.58577501e-002 -2.19725251e-001]
 [-9.91183419e-003 -6.98551530e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-2.11212857e-002 -1.42370885e-001]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 3.32461271e-005  2.68185845e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-6.35396768e-005 -4.11639189e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [-8.89893371e-004 -8.21124568e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.03247542e-003  9.10131307e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-7.72067144e-007 -7.90002592e-006]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-3.86805074e-003 -3.05315919e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 7.49494791e-003  7.05340731e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-5.12556261e-004 -3.56256558e-003]
 [ 1.12958310e-002  1.06664002e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-1.37847864e-005 -1.00337145e-004]
 [ 2.87342633e-003  2.72719912e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.05140897e-004  1.60896201e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-1.04334906e-003 -8.75389699e-003]
 [-1.16381119e-006 -8.37989592e-006]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-1.68875230e-006 -9.02293369e-006]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.59302170e-002  1.49040575e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [-6.94476212e-005 -4.81464145e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.50747055e-004  1.22079425e-004]
 [ 6.52715884e-001  5.63630143e-001]
 [-2.28733341e-005 -6.33037541e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 8.75364320e-005  8.63703526e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-4.01021210e-006 -1.88209608e-005]
 [ 2.82833226e-001  2.65624324e-001]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-4.94673448e-004 -4.10395084e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-1.05656322e-004 -8.45714438e-004]
 [ 2.64531242e-002  2.46444874e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-8.20130442e-005 -6.70712777e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-1.40059891e-006 -8.66995651e-006]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.59795461e-005  2.48710198e-005]
 [ 1.02252061e-004  1.11351093e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-8.42846291e-004 -6.73495247e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-1.75846533e-005 -7.02207618e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.03766744e-003  1.00689352e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 5.07773394e-003  4.86420798e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.31071321e-004  1.32746754e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.03463282e-001  9.65694041e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 6.25815271e-002  5.81927210e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-4.35329416e-003 -3.48813010e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [-4.87563406e-005 -4.83934303e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-8.07225699e-005 -7.31719876e-004]
 [-4.23774746e-006 -3.30081673e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 3.76486080e-005  3.91656500e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 5.53083963e-005 -1.75644249e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-2.60998747e-002 -2.05577899e-001]
 [-5.46131528e-003 -4.22888059e-002]
 [ 1.04534265e-004  1.14146749e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-1.69990465e-005 -2.92038503e-004]
 [-1.61536288e-002 -1.14810820e-001]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-1.34638977e-006 -8.60355782e-006]
 [ 0.00000000e+000  0.00000000e+000]]new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.08407998085021973
PCA fit: 0.7699217796325684
PCA Transform: 0.31940460205078125
[[0.13266198 0.15758531]
 [0.9222406  0.80088586]]
total iterations: 1172
TLDA fit: 458.1817443370819
PCA Reverse Transform: 0.0008704662322998047
weights shape:
(2,)
fac2 shape: 
(500, 2)
final fac2: 
[[ 0.00000000e+000  0.00000000e+000]
 [ 8.48387960e-018  1.30597907e-017]
 [ 4.54002906e-017  6.98885878e-017]
 [ 3.60070889e-006 -1.42491768e-004]
 [ 1.31748085e-017  2.02810852e-017]
 [ 1.29640673e-018  1.99566736e-018]
 [ 3.27080398e-019  5.03502227e-019]
 [ 1.24238429e-019  1.91250609e-019]
 [ 5.43612422e-021  8.36828091e-021]
 [ 2.14023402e-020  3.29464132e-020]
 [ 1.20176080e-021  1.84997096e-021]
 [ 1.06977030e-021  1.64678693e-021]
 [ 2.13218810e-022  3.28225556e-022]
 [ 4.03387755e-004  3.69310713e-004]
 [-9.32756858e-027 -1.43586903e-026]
 [ 3.01609414e-026  4.64292439e-026]
 [ 3.70999140e-002  3.58237115e-002]
 [-1.10924250e-026 -1.70755236e-026]
 [ 1.86550235e-026  2.87172852e-026]
 [ 3.27874042e-027  5.04724754e-027]
 [ 6.00888337e-027  9.24999131e-027]
 [ 4.20938878e-027  6.47987606e-027]
 [-9.80681674e-028 -1.50964871e-027]
 [ 6.43980805e-027  9.91335074e-027]
 [ 1.16775560e-027  1.79762755e-027]
 [ 5.42221284e-028  8.34689982e-028]
 [ 3.69751952e-004  3.61942540e-004]
 [-3.75608794e-028 -5.78206479e-028]
 [ 1.98070592e-003 -1.96594451e-002]
 [ 1.69385022e-028  2.60747935e-028]
 [ 1.82856890e-001  1.78988172e-001]
 [-2.78892398e-027 -4.29322999e-027]
 [-1.09743371e-028 -1.68937842e-028]
 [-1.04855108e-027 -1.61412482e-027]
 [-7.79140397e-028 -1.19939884e-027]
 [ 2.43738845e-027  3.75208169e-027]
 [-8.40228923e-028 -1.29343621e-027]
 [-3.12701629e-027 -4.81368596e-027]
 [-1.34734284e-027 -2.07408329e-027]
 [ 2.70934579e-028  4.17072860e-028]
 [ 1.33992346e-027  2.06265962e-027]
 [-2.62315039e-027 -4.03803968e-027]
 [-2.20740872e-027 -3.39805566e-027]
 [-4.76941603e-027 -7.34197290e-027]
 [-1.22987895e-027 -1.89325795e-027]
 [ 5.60495730e-028  8.62819433e-028]
 [ 2.78522106e-005  2.80718487e-005]
 [-9.08053741e-028 -1.39784544e-027]
 [ 2.07101550e-027  3.18809206e-027]
 [ 1.35289209e-027  2.08262296e-027]
 [ 4.48568230e-027  6.90519717e-027]
 [ 6.13834572e-028  9.44927868e-028]
 [ 2.42217171e-005 -1.55158482e-004]
 [-3.99723031e-028 -6.15327795e-028]
 [-9.32773872e-028 -1.43589942e-027]
 [-2.88940643e-027 -4.44791092e-027]
 [ 1.07743060e-027  1.65858182e-027]
 [-1.14039865e-027 -1.75551390e-027]
 [ 6.90955697e-028  1.06364770e-027]
 [-1.02983825e-027 -1.58531883e-027]
 [-2.40414257e-027 -3.70090319e-027]
 [-2.30030486e-028 -3.54105712e-028]
 [ 2.65512461e-002  2.53881981e-002]
 [ 2.53429681e-027  3.90126078e-027]
 [ 4.98140653e-028  7.66831547e-028]
 [ 2.70744283e-029  4.16779294e-029]
 [ 1.04705004e-002 -2.56458063e-001]
 [ 1.48406336e-030  2.28457412e-030]
 [-2.66332020e-029 -4.09987967e-029]
 [-9.19277276e-029 -1.41512295e-028]
 [ 5.59890545e-029  8.61887789e-029]
 [ 5.70141323e-029  8.77667825e-029]
 [ 3.10991860e-029  4.78736422e-029]
 [ 6.97037501e-002  6.68579235e-002]
 [-1.48213454e-030 -2.28159007e-030]
 [-1.12587080e-030 -1.73314877e-030]
 [ 9.68382691e-029  1.49071502e-028]
 [-7.73195397e-030 -1.19024617e-029]
 [ 1.69990267e-001  1.64895612e-001]
 [-1.14869410e-029 -1.76828463e-029]
 [ 1.83242125e-002  1.75347421e-002]
 [ 3.22513776e-029  4.96473220e-029]
 [ 1.42365173e-003 -1.11463724e-002]
 [ 9.78058916e-002  9.42782648e-002]
 [-5.67336290e-030 -8.73350508e-030]
 [ 3.23132495e-029  4.97425599e-029]
 [ 4.29980930e-029  6.61906600e-029]
 [ 5.01885352e-030  7.72596227e-030]
 [ 1.69986309e-030  2.61674621e-030]
 [ 1.41794754e-005 -7.77917640e-006]
 [ 4.71987895e-030  7.26571721e-030]
 [ 5.33828943e-030  8.21768803e-030]
 [-2.39733687e-030 -3.69042048e-030]
 [-1.60596757e-029 -2.47220450e-029]
 [ 7.65670438e-030  1.17866165e-029]
 [ 7.61514818e-030  1.17226414e-029]
 [ 2.44459458e-030  3.76317063e-030]
 [-2.61912493e-029 -4.03184311e-029]
 [-6.28111169e-030 -9.66905405e-030]
 [ 7.81339108e-031  1.20277187e-030]
 [ 2.99778170e-029  4.61474315e-029]
 [ 9.75152624e-003 -2.30326462e-001]
 [-3.47729551e-029 -5.35290187e-029]
 [ 9.80731767e-031  1.50972794e-030]
 [ 4.20307383e-030  6.47017436e-030]
 [-5.28916792e-030 -8.14206758e-030]
 [ 7.41965007e-032  1.14218190e-031]
 [-1.17488376e-029 -1.80860078e-029]
 [-1.63682143e-030 -2.51970298e-030]
 [ 6.43251592e-030  9.90212880e-030]
 [ 8.49799164e-003  7.78205245e-003]
 [-3.71773605e-030 -5.72303070e-030]
 [ 2.43735742e-030  3.75201792e-030]
 [ 3.69315841e-030  5.68519539e-030]
 [-2.39421247e-031 -3.68562571e-031]
 [-1.61741079e-029 -2.48981948e-029]
 [ 5.18530692e-004  4.28132660e-004]
 [-3.22302783e-030 -4.96148481e-030]
 [-6.88470258e-031 -1.05982135e-030]
 [ 9.09368249e-032  1.39986843e-031]
 [-4.41045688e-032 -6.78937114e-032]
 [ 8.29593384e-032  1.27706134e-031]
 [ 3.93311867e-002 -7.41804502e-002]
 [ 2.64149818e-031  4.06628492e-031]
 [-3.97930625e-032 -6.12568328e-032]
 [ 5.83314416e-032  8.97945383e-032]
 [ 1.02960015e-003 -7.97808636e-003]
 [-2.40056834e-032 -3.69540380e-032]
 [ 3.44428438e-033  5.30211760e-033]
 [ 6.51466803e-036  1.00285694e-035]
 [ 1.63449639e-035  2.51611571e-035]
 [ 8.76243572e-035  1.34887563e-034]
 [ 3.22333438e-035  4.96195408e-035]
 [ 3.55208797e-004  3.39555078e-004]
 [ 1.24761267e-034  1.92055622e-034]
 [ 1.51660104e-035  2.33461843e-035]
 [-1.10816518e-035 -1.70590458e-035]
 [-4.36690044e-035 -6.72233173e-035]
 [-2.89362855e-035 -4.45440426e-035]
 [ 2.81337264e-036  4.33080824e-036]
 [-1.53113476e-035 -2.35700565e-035]
 [ 2.00200458e-001  1.91371690e-001]
 [-1.38894065e-035 -2.13811622e-035]
 [-1.89332346e-036 -2.91457083e-036]
 [ 7.10907382e-035  1.09436043e-034]
 [-7.67969546e-035 -1.18220040e-034]
 [ 4.81952761e-004  4.60645563e-004]
 [ 5.44619460e-036  8.38378028e-036]
 [-1.51665432e-035 -2.33470763e-035]
 [ 1.04197407e-035  1.60400247e-035]
 [-3.54934682e-007 -1.53498441e-005]
 [ 1.78691865e-035  2.75075999e-035]
 [-4.95413316e-036 -7.62634292e-036]
 [-1.30981973e-035 -2.01631583e-035]
 [-4.34174778e-035 -6.68361892e-035]
 [ 3.90301572e-004 -4.28431851e-003]
 [ 4.18044217e-007 -1.41599331e-005]
 [-4.56600379e-035 -7.02883952e-035]
 [-3.43192907e-035 -5.28306756e-035]
 [-1.53386869e-035 -2.36121441e-035]
 [ 8.54074939e-002  8.29493541e-002]
 [-1.65320589e-035 -2.54492034e-035]
 [ 1.93368514e-035  2.97668522e-035]
 [ 1.78064333e-035  2.74109347e-035]
 [ 4.53834375e-006 -7.81720388e-006]
 [ 3.05898122e-035  4.70894736e-035]
 [ 1.95964717e-035  3.01665454e-035]
 [ 3.67612061e-035  5.65896783e-035]
 [-4.83540760e-035 -7.44354872e-035]
 [ 1.24610521e-005 -2.52280459e-005]
 [-1.27108866e-035 -1.95669598e-035]
 [ 4.11822792e-036  6.33956459e-036]
 [ 1.29500089e-035  1.99350284e-035]
 [-7.96972195e-037 -1.22684933e-036]
 [ 2.99802666e-035  4.61510723e-035]
 [ 1.18544062e-035  1.82484831e-035]
 [-2.28065341e-035 -3.51081289e-035]
 [ 1.61072406e-035  2.47950888e-035]
 [-2.56467332e-035 -3.94801857e-035]
 [-2.74225058e-002 -4.87457580e-001]
 [-1.75380477e-035 -2.69977483e-035]
 [ 4.22239848e-035  6.49989793e-035]
 [-4.68333674e-035 -7.20945427e-035]
 [ 4.84400739e-035  7.45679176e-035]
 [ 5.66344657e-004 -3.45078894e-003]
 [ 8.51118023e-037  1.31019690e-036]
 [-7.39502639e-036 -1.13837746e-035]
 [-1.86302480e-035 -2.86790567e-035]
 [ 3.13510609e-036  4.82613616e-036]
 [ 1.09010944e-002 -1.41305204e-001]
 [ 7.19611967e-003 -5.01199188e-002]
 [ 1.16857578e-004  1.20675011e-004]
 [ 2.00418550e-035  3.08521273e-035]
 [-1.08599693e-036 -1.67176615e-036]
 [-4.66841477e-040 -7.18589696e-040]
 [ 3.64381441e-037  5.60922943e-037]
 [-8.59082208e-038 -1.32245635e-037]
 [ 1.16869068e-037  1.79906973e-037]
 [ 4.55838200e-003 -4.30186033e-002]
 [ 1.48780461e-037  2.29030095e-037]
 [-5.59250512e-038 -8.60902330e-038]
 [ 1.83409755e-037  2.82338103e-037]
 [-6.74560109e-038 -1.03840796e-037]
 [ 1.00821084e-037  1.55202393e-037]
 [-5.00402924e-038 -7.70313331e-038]
 [-5.87615756e-038 -9.04566645e-038]
 [ 1.56578873e-038  2.41035197e-038]
 [ 6.53477529e-002  6.25653265e-002]
 [ 3.07622790e-038  4.73550204e-038]
 [ 2.44172600e-005  2.27841328e-005]
 [ 1.71639025e-002  1.70068675e-002]
 [-1.17812230e-037 -1.81358595e-037]
 [-1.26734267e-038 -1.95092516e-038]
 [-1.06842189e-037 -1.64471205e-037]
 [ 2.41987590e-037  3.72512006e-037]
 [ 1.64848316e-037  2.53764847e-037]
 [-3.43524892e-037 -5.28816624e-037]
 [ 1.36084415e-003 -1.00439775e-002]
 [-1.01762511e-037 -1.56651414e-037]
 [ 7.36445954e-038  1.13367247e-037]
 [ 7.07227203e-039  1.08869552e-038]
 [ 4.16101065e-038  6.40543192e-038]
 [ 1.33592947e-038  2.05650998e-038]
 [-1.34671633e-037 -2.07311441e-037]
 [ 2.42250287e-038  3.72915224e-038]
 [-3.65705863e-037 -5.62962251e-037]
 [ 1.63722246e-037  2.52031529e-037]
 [ 6.04853112e-003 -2.90888436e-001]
 [-2.11650327e-006 -4.76685063e-005]
 [-6.39908094e-039 -9.85064018e-039]
 [-5.46325429e-039 -8.41004938e-039]
 [ 5.33602585e-007 -1.39820440e-005]
 [ 3.05761379e-041  4.70685078e-041]
 [ 4.52037998e-042  6.95859890e-042]
 [ 1.98427494e-004 -1.76702865e-003]
 [ 2.44534527e-005  2.28398477e-005]
 [ 1.66058904e-042  2.55628348e-042]
 [-3.10128329e-044 -4.77405699e-044]
 [ 2.47724761e-044  3.81343470e-044]
 [-1.34942316e-044 -2.07728683e-044]
 [-4.08094814e-044 -6.28215070e-044]
 [-2.46397109e-044 -3.79300143e-044]
 [ 1.60636429e-005 -1.08503041e-004]
 [-5.54260531e-045 -8.53220723e-045]
 [-1.43733670e-044 -2.21261636e-044]
 [-1.10671984e-044 -1.70366753e-044]
 [-2.77905732e-046 -4.27803894e-046]
 [-4.30056408e-046 -6.62022158e-046]
 [-2.19041064e-047 -3.37188617e-047]
 [ 9.29621583e-048  1.43104590e-047]
 [-4.41025292e-048 -6.78908316e-048]
 [ 1.75405706e-001  1.70019717e-001]
 [ 3.01079737e-048  4.63477959e-048]
 [ 8.31791544e-004 -6.81704699e-003]
 [ 3.38596442e-048  5.21229812e-048]
 [ 3.38118694e-048  5.20493232e-048]
 [ 5.04013598e-048  7.75870501e-048]
 [-1.34331202e-051 -2.06787571e-051]
 [ 7.09786048e-051  1.09263388e-050]
 [ 5.85103519e-002  5.61552899e-002]
 [-4.93997269e-052 -7.60452901e-052]
 [ 1.72259452e-003 -1.37060910e-002]
 [-5.64758448e-051 -8.69380114e-051]
 [ 7.57048859e-051  1.16538910e-050]
 [-5.16384271e-051 -7.94914207e-051]
 [-4.49810220e-051 -6.92430666e-051]
 [ 5.43629130e-052  8.36855971e-052]
 [-1.84302536e-050 -2.83712526e-050]
 [ 9.32781922e-051  1.43590976e-050]
 [-5.44255722e-051 -8.37819320e-051]
 [ 2.78701052e-004 -2.82773375e-003]
 [ 6.02412715e-051  9.27345137e-051]
 [ 5.79432006e-003 -5.39654260e-002]
 [ 9.48188489e-051  1.45962708e-050]
 [-2.77693578e-051 -4.27477289e-051]
 [-2.41449626e-050 -3.71683464e-050]
 [-8.46615168e-051 -1.30326592e-050]
 [-1.15394241e-051 -1.77636138e-051]
 [ 1.25002135e-050  1.92426392e-050]
 [ 1.99622916e-050  3.07296231e-050]
 [ 1.45929935e-050  2.24642217e-050]
 [ 6.08063801e-003 -5.81433523e-002]
 [-2.57713752e-051 -3.96720692e-051]
 [-2.96265711e-051 -4.56067081e-051]
 [ 9.48643836e-053  1.46034079e-052]
 [ 1.64629714e-051  2.53428451e-051]
 [-1.04652404e-050 -1.61100236e-050]
 [ 2.22231446e-051  3.42099448e-051]
 [ 6.59976986e-051  1.01595854e-050]
 [ 1.17837748e-051  1.81397512e-051]
 [ 1.63986222e-002  1.51182451e-002]
 [ 1.14116042e-050  1.75668477e-050]
 [-1.82549110e-006 -6.20239890e-005]
 [-3.13451142e-051 -4.82521742e-051]
 [ 2.88337915e-051  4.43862947e-051]
 [ 5.48610154e-003 -5.19381074e-002]
 [ 3.79119095e-051  5.83610242e-051]
 [ 2.12030145e-051  3.26395812e-051]
 [ 5.43849409e-051  8.37193036e-051]
 [ 9.25072510e-052  1.42404258e-051]
 [ 4.81647620e-006 -9.62098366e-005]
 [-8.04193855e-051 -1.23796273e-050]
 [ 7.31785179e-051  1.12649822e-050]
 [-4.07717795e-051 -6.27634031e-051]
 [-7.91800968e-052 -1.21888685e-051]
 [-1.10919521e-050 -1.70747826e-050]
 [-1.73670452e-050 -2.67345614e-050]
 [ 2.76902152e-051  4.26258493e-051]
 [ 1.89329999e-004  2.02631099e-004]
 [-6.79424428e-051 -1.04589527e-050]
 [-2.84686301e-052 -4.38226368e-052]
 [ 2.27229301e-051  3.49793133e-051]
 [ 4.32311861e-003 -4.18560053e-002]
 [-7.15952660e-052 -1.10212393e-051]
 [ 5.55920529e-052  8.55771420e-052]
 [ 1.49202416e-003 -1.21661844e-002]
 [ 1.27375644e-050  1.96080205e-050]
 [-1.25849325e-051 -1.93730054e-051]
 [ 4.05302011e-004 -3.18057420e-003]
 [-1.06144376e-050 -1.63397095e-050]
 [-9.66919370e-051 -1.48846120e-050]
 [-4.75860785e-066 -7.32533483e-066]
 [ 3.86105370e-067  5.94361182e-067]
 [ 6.11625595e-066  9.41526741e-066]
 [-4.85000718e-066 -7.46602789e-066]
 [ 6.04370894e-066  9.30359262e-066]
 [ 4.30674029e-067  6.62972165e-067]
 [ 2.72342850e-066  4.19240227e-066]
 [ 6.65956546e-002  6.37905691e-002]
 [ 6.76954251e-066  1.04209356e-065]
 [ 5.21776738e-066  8.03215503e-066]
 [ 1.35642020e-002  1.28274612e-002]
 [-5.04255153e-067 -7.76242058e-067]
 [-8.03636889e-066 -1.23710681e-065]
 [ 1.19671780e-066  1.84221078e-066]
 [-1.39796574e-067 -2.15199533e-067]
 [-1.15669455e-066 -1.78059749e-066]
 [ 1.02187405e-002 -1.30912538e-001]
 [-4.22897992e-066 -6.51002588e-066]
 [ 2.63766581e-066  4.06038862e-066]
 [-1.77097294e-066 -2.72620608e-066]
 [ 5.09033903e-066  7.83598790e-066]
 [ 2.45430379e-004  1.85367330e-004]
 [ 2.62489203e-066  4.04072746e-066]
 [-1.88043103e-066 -2.89470185e-066]
 [-7.52177783e-066 -1.15789151e-065]
 [-1.99070551e-067 -3.06441511e-067]
 [ 2.09606353e-066  3.22664752e-066]
 [-4.36041997e-067 -6.71239288e-067]
 [-2.00584853e-067 -3.08773805e-067]
 [ 9.89622872e-066  1.52341014e-065]
 [ 3.12831884e-066  4.81568085e-066]
 [-1.43463855e-066 -2.20846154e-066]
 [ 6.07665569e-005 -3.06150355e-004]
 [ 3.53743479e-066  5.44547388e-066]
 [-1.02427236e-066 -1.57674997e-066]
 [ 4.56965046e-004 -5.54361627e-003]
 [ 5.08277837e-066  7.82434670e-066]
 [-1.16551516e-066 -1.79417578e-066]
 [-6.88066973e-067 -1.05919743e-066]
 [ 1.60876895e-067  2.47655421e-067]
 [-1.19932504e-066 -1.84622534e-066]
 [-3.06702947e-067 -4.72134354e-067]
 [-3.97555211e-066 -6.11990455e-066]
 [ 5.21674208e-066  8.03058034e-066]
 [-1.01096751e-067 -1.55620416e-067]
 [ 7.74586291e-067  1.19238172e-066]
 [ 1.43777004e-003  1.35468246e-003]
 [ 1.23460637e-066  1.90053282e-066]
 [ 6.88850747e-067  1.06040304e-066]
 [ 2.41921760e-066  3.72410812e-066]
 [ 6.88619938e-068  1.05999719e-067]
 [ 4.69936867e-066  7.23414389e-066]
 [-3.28903318e-066 -5.06308855e-066]
 [ 2.66301458e-001  2.59137886e-001]
 [ 4.92822633e-005  6.10609098e-005]
 [ 2.05049825e-004  1.67616320e-004]
 [-6.34064119e-067 -9.76070000e-067]
 [ 2.87217026e-067  4.42138278e-067]
 [-3.44233471e-067 -5.29907706e-067]
 [-3.37306720e-067 -5.19244896e-067]
 [-3.11678623e-006 -1.96013979e-005]
 [ 1.66921545e-003  1.54812837e-003]
 [ 1.21417195e-003 -9.32234030e-003]
 [-9.87206572e-067 -1.51969355e-066]
 [-3.16460847e-080 -4.87154457e-080]
 [ 2.87510632e-080  4.42589157e-080]
 [-8.45008152e-080 -1.30079164e-079]
 [-1.04327384e-079 -1.60599887e-079]
 [ 1.73556422e-080  2.67169934e-080]
 [-1.53045989e-080 -2.35596160e-080]
 [-1.87237464e-080 -2.88230319e-080]
 [ 5.03786417e-004  4.64649207e-004]
 [-5.02515559e-080 -7.73564468e-080]
 [-9.54546968e-080 -1.46941440e-079]
 [-1.16379506e-079 -1.79152630e-079]
 [ 1.47268227e-001  1.43255536e-001]
 [-1.67374665e-079 -2.57653717e-079]
 [ 5.79548167e-081  8.92146568e-081]
 [-5.78774533e-080 -8.90956233e-080]
 [-1.26974064e-079 -1.95461823e-079]
 [ 7.95980737e-081  1.22531947e-080]
 [ 1.19531227e-079  1.84004430e-079]
 [-1.54291294e-080 -2.37513452e-080]
 [ 8.31407300e-001  6.87896643e-001]
 [-2.92407312e-080 -4.50127031e-080]
 [ 2.36445095e-080  3.63979678e-080]
 [-7.56028329e-080 -1.16381728e-079]
 [ 9.25459284e-006 -1.53605432e-005]
 [ 1.02700032e-079  1.58094766e-079]
 [ 8.69100058e-080  1.33787798e-079]
 [-6.42103389e-080 -9.88443284e-080]
 [ 7.84790872e-002  7.49484887e-002]
 [-2.14100105e-079 -3.29582033e-079]
 [-9.73082381e-080 -1.49794733e-079]
 [ 3.88261084e-080  5.97682748e-080]
 [-3.67995486e-080 -5.66486046e-080]
 [ 1.09960221e-079  1.69271045e-079]
 [-7.26194290e-081 -1.11789085e-080]
 [ 4.46550444e-005  5.39378492e-005]
 [ 2.49162216e-005 -2.57713635e-004]
 [ 1.43562283e-079  2.20997383e-079]
 [-2.46903747e-079 -3.80079648e-079]
 [ 9.34016169e-081  1.43780926e-080]
 [-2.50356875e-079 -3.85395322e-079]
 [-6.64955726e-080 -1.02362159e-079]
 [-1.43976320e-082 -2.21628078e-082]
 [ 3.38781197e-005  7.74122373e-006]
 [-7.38309395e-080 -1.13654151e-079]
 [ 5.31233453e-080  8.17771164e-080]
 [ 1.66549840e-003 -1.33794869e-002]
 [-8.98557153e-082 -1.38322089e-081]
 [ 8.43299470e-081  1.29816025e-080]
 [-1.47752471e-080 -2.27447684e-080]
 [-6.79298174e-080 -1.04570038e-079]
 [-2.26792265e-079 -3.49120365e-079]
 [ 2.11714793e-080  3.25910395e-080]
 [ 2.02274413e-080  3.11377830e-080]
 [-9.03875649e-080 -1.39141172e-079]
 [ 3.02318753e-080  4.65384692e-080]
 [ 4.61056717e-080  7.09743444e-080]
 [ 2.97472649e-080  4.57924347e-080]
 [ 1.06997685e+000  1.17656412e+000]
 [-2.52744350e-082 -3.89075565e-082]
 [-2.33617550e-080 -3.59627120e-080]
 [ 2.25406510e-004 -1.05934156e-003]
 [ 4.78497251e-081  7.36590941e-081]
 [ 1.82599557e-080  2.81090717e-080]
 [-4.62350699e-081 -7.11734810e-081]
 [-3.11696166e-098 -4.79820047e-098]
 [-3.45920743e-099 -5.32505026e-099]
 [ 1.26820380e-098  1.95225235e-098]
 [-5.96902785e-099 -9.18862765e-099]
 [ 1.16852954e-005  3.18475559e-006]
 [-3.79642437e-102 -5.84414978e-102]
 [-4.83446016e-104 -7.44208572e-104]
 [-7.10321330e-122 -1.09345723e-121]
 [ 3.16340444e-121  4.86969461e-121]
 [-4.01715503e-122 -6.18394165e-122]
 [ 1.43301271e-121  2.20595621e-121]
 [ 2.38452118e-121  3.67069242e-121]
 [ 3.48107183e-123  5.35870454e-123]
 [-8.96752054e-138 -1.38044524e-137]
 [-4.47898999e-137 -6.89488389e-137]
 [ 2.50840873e-137  3.86140206e-137]
 [ 4.16526230e-139  6.41192172e-139]
 [ 2.07553451e-002  2.03593766e-002]
 [ 4.20248342e-003 -4.02504929e-002]
 [ 6.93670810e-155  1.06782525e-154]
 [ 3.63369722e-154  5.59365296e-154]
 [-8.40187376e-155 -1.29337181e-154]
 [ 7.30426883e-154  1.12440768e-153]
 [ 1.46238794e-170  2.25117532e-170]
 [ 1.91126310e-170  2.94216577e-170]
 [ 3.97717866e-171  6.12240302e-171]
 [ 2.38615191e-169  3.67320324e-169]
 [-2.51243487e-186 -3.86760144e-186]
 [-5.74501538e-187 -8.84378467e-187]
 [ 9.52321169e-187  1.46598720e-186]
 [ 1.47210136e-187  2.26612830e-187]
 [ 5.80090001e-204  8.92981671e-204]
 [-1.61605878e-203 -2.48773475e-203]
 [-1.77527626e-203 -2.73283137e-203]
 [ 8.39879757e-205  1.29289633e-204]
 [-3.90571797e-220 -6.01240045e-220]
 [ 1.07873256e-004  1.21648098e-004]
 [ 1.26479903e-220  1.94701135e-220]
 [-9.41425185e-222 -1.44921493e-221]
 [ 1.91459029e-237  2.94728887e-237]
 [ 4.16144137e-002  3.95017060e-002]
 [ 1.50537049e-004 -9.82149526e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 8.99145071e-002  8.52835188e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]]new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.08175539970397949
PCA fit: 0.8008718490600586
PCA Transform: 0.3148648738861084
[[0.35941696 0.6039684 ]
 [0.6658672  0.8587709 ]]
total iterations: 31
TLDA fit: 11.98563838005066
PCA Reverse Transform: 0.0008275508880615234
final fac2: 
[[ 0.00000000e+000  0.00000000e+000]
 [ 4.17613363e-004 -6.91869015e-003]
 [-1.24949590e-016 -1.88422901e-016]
 [ 2.36925931e-018  3.57282408e-018]
 [-2.17164639e-018 -3.27482545e-018]
 [ 1.80439519e-005 -6.35924530e-004]
 [ 6.27495744e-020  9.46258584e-020]
 [ 1.97042940e-022  2.97139184e-022]
 [-5.56075953e-022 -8.38558108e-022]
 [ 4.59407921e-023  6.92783485e-023]
 [-1.05344516e-023 -1.58858691e-023]
 [ 4.69777011e-003  4.62919564e-003]
 [ 2.24368693e-027  3.38346243e-027]
 [ 1.08953636e-027  1.64301217e-027]
 [ 2.57835792e-029  3.88814254e-029]
 [ 1.03279594e-028  1.55744812e-028]
 [ 1.59557817e-003  1.50312909e-003]
 [ 4.82099128e-002  4.53423872e-002]
 [-1.21712326e-028 -1.83541171e-028]
 [-1.21781414e-028 -1.83645442e-028]
 [-7.56034270e-029 -1.14009336e-028]
 [ 5.24640771e-002  4.85971687e-002]
 [ 2.14852109e-028  3.23995322e-028]
 [-7.31838084e-030 -1.10360674e-029]
 [-1.08928814e-029 -1.64263222e-029]
 [ 1.64293476e-029  2.47752791e-029]
 [ 1.49771029e-029  2.25853628e-029]
 [ 9.29625666e-029  1.40187084e-028]
 [ 2.44428509e-028  3.68596424e-028]
 [-6.45951847e-028 -9.74090769e-028]
 [ 1.51460931e-028  2.28401982e-028]
 [ 2.17292388e-028  3.27675357e-028]
 [-2.00635379e-028 -3.02556730e-028]
 [-3.27828294e-028 -4.94362777e-028]
 [-3.05155784e-031 -4.60202281e-031]
 [ 2.44937311e-003 -2.25990824e-001]
 [-1.67527334e-028 -2.52629901e-028]
 [ 1.05013564e-029  1.58359624e-029]
 [ 1.57510289e-003 -2.87638673e-002]
 [ 1.27080081e-028  1.91635853e-028]
 [-6.31250888e-030 -9.51900003e-030]
 [-9.40282883e-029 -1.41793897e-028]
 [ 6.12204596e-028  9.23200117e-028]
 [-1.27445646e-028 -1.92186970e-028]
 [ 4.57108547e-006 -2.13253328e-005]
 [-9.13627436e-030 -1.37774361e-029]
 [ 5.22140821e-030  7.87383655e-030]
 [ 1.47975101e-003 -2.57048583e-002]
 [-5.61251671e-029 -8.46362833e-029]
 [ 1.01809899e-028  1.53528530e-028]
 [ 1.26088662e-029  1.90140749e-029]
 [ 2.11779172e-029  3.19361619e-029]
 [ 6.68413666e-030  1.00796271e-029]
 [-3.79385638e-030 -5.72110974e-030]
 [ 2.06375690e-003 -4.19528114e-002]
 [-1.96907944e-029 -2.96936245e-029]
 [-4.01890790e-029 -6.06049118e-029]
 [ 3.74934422e-030  5.65398855e-030]
 [ 2.54889042e-029  3.84370646e-029]
 [ 1.76887568e-004 -2.75263416e-003]
 [-7.94165928e-029 -1.19759646e-028]
 [-1.46994941e-029 -2.21667011e-029]
 [ 2.01686930e-030  3.04142457e-030]
 [ 1.24871218e-003 -3.58733005e-002]
 [ 7.24802797e-030  1.09299716e-029]
 [-1.60441558e-030 -2.41944766e-030]
 [ 1.32129925e-030  1.99251023e-030]
 [ 2.11697134e-031  3.19242178e-031]
 [ 1.83936336e-006 -1.13355013e-005]
 [ 4.08241496e-030  6.15625246e-030]
 [ 1.51816101e-004 -2.86098785e-003]
 [-2.00965975e-031 -3.03055364e-031]
 [ 7.21231728e-031  1.08761230e-030]
 [ 1.80677157e-005 -4.94796218e-004]
 [ 2.17422401e-031  3.27871351e-031]
 [-2.19096047e-030 -3.30395305e-030]
 [-2.72886082e-006 -1.82243510e-005]
 [ 4.54739529e-031  6.85743976e-031]
 [ 8.47982779e-033  1.27877582e-032]
 [-8.68400888e-031 -1.30954183e-030]
 [ 1.76104296e-030  2.65564234e-030]
 [ 2.61288278e-003 -2.65885067e-001]
 [ 5.13296944e-005 -1.31941079e-003]
 [ 4.17037187e-005 -1.23516200e-003]
 [-4.43490974e-032 -6.68781221e-032]
 [ 6.23330217e-005  5.16700374e-005]
 [-1.62561643e-030 -2.45141754e-030]
 [ 2.52208861e-031  3.80328456e-031]
 [ 1.16237889e-030  1.75285779e-030]
 [ 3.07311397e-006 -9.47498773e-006]
 [ 3.95745819e-031  5.96781862e-031]
 [ 8.31391293e-007 -1.28555161e-005]
 [-1.66277487e-031 -2.50745037e-031]
 [ 3.26009863e-031  4.91620185e-031]
 [ 4.40296730e-031  6.63964302e-031]
 [-9.71684156e-032 -1.46529171e-031]
 [-3.15081384e-030 -4.75140471e-030]
 [-4.81166722e-031 -7.25595882e-031]
 [-7.34690119e-031 -1.10790729e-030]
 [-3.27536154e-033 -4.93922545e-033]
 [ 1.20845553e-005 -1.36978313e-004]
 [-4.16720205e-031 -6.28410957e-031]
 [ 6.86282334e-031  1.03490974e-030]
 [-9.61267298e-032 -1.44958298e-031]
 [-4.90002558e-031 -7.38919672e-031]
 [-4.22656464e-030 -6.37362867e-030]
 [ 3.39432508e-032  5.11860282e-032]
 [-2.41273721e-031 -3.63838730e-031]
 [ 2.30082542e-030  3.46962818e-030]
 [ 6.68606954e-032  1.00825353e-031]
 [ 7.49048549e-031  1.12955978e-030]
 [ 1.87554354e-032  2.82830464e-032]
 [ 2.49681663e-032  3.76518054e-032]
 [-2.78131572e-032 -4.19418670e-032]
 [ 5.97717121e-032  9.01352692e-032]
 [ 3.52965035e-005 -6.80454061e-004]
 [-5.48053519e-033 -8.26459383e-033]
 [ 1.61390322e-033  2.43375394e-033]
 [ 1.40858655e-032  2.12413837e-032]
 [-2.42561782e-032 -3.65781251e-032]
 [ 5.87384826e-033  8.85772537e-033]
 [ 9.24313863e-034  1.39385728e-033]
 [-1.15945536e-033 -1.74845367e-033]
 [-5.40102136e-033 -8.14469830e-033]
 [ 5.12539679e-033  7.72906044e-033]
 [-2.30892355e-033 -3.48184319e-033]
 [-4.31029735e-033 -6.49990064e-033]
 [-2.07730114e-033 -3.13255700e-033]
 [ 1.07786057e-006 -1.53576329e-004]
 [-1.67563006e-037 -2.52684278e-037]
 [-1.12602090e-038 -1.69805948e-038]
 [ 1.14484228e-039  1.72640396e-039]
 [ 8.23638970e-038  1.24204114e-037]
 [ 5.48157308e-038  8.26615775e-038]
 [ 6.91909499e-038  1.04339279e-037]
 [-3.21156544e-039 -4.84323391e-039]
 [ 5.76062483e-039  8.68697581e-039]
 [ 4.21437788e-037  6.35524926e-037]
 [ 2.37576475e-005  2.17171225e-005]
 [ 8.09155104e-039  1.22019949e-038]
 [ 2.23312491e-003  2.19646938e-003]
 [-1.19195235e-037 -1.79745295e-037]
 [ 3.87425371e-038  5.84234623e-038]
 [-3.30359218e-038 -4.98178724e-038]
 [ 4.56505614e-038  6.88405624e-038]
 [ 1.02722531e-039  1.54905278e-039]
 [-9.70448902e-038 -1.46342725e-037]
 [-8.35223366e-038 -1.25951032e-037]
 [-2.51617489e-038 -3.79436647e-038]
 [-1.90356539e-037 -2.87056326e-037]
 [ 5.95654381e-006 -8.97823127e-005]
 [-5.30008680e-039 -7.99252115e-039]
 [-6.87584896e-038 -1.03687112e-037]
 [-8.82967435e-038 -1.33150833e-037]
 [ 3.43764565e-038  5.18393574e-038]
 [-1.63724377e-037 -2.46894974e-037]
 [-5.64248161e-039 -8.50870669e-039]
 [ 1.08422319e-004 -2.54547577e-003]
 [ 2.55233894e-005 -3.83867271e-006]
 [-2.66835694e-037 -4.02386055e-037]
 [ 1.59905380e+000  1.71198054e+000]
 [ 8.23389741e-005  9.59480765e-005]
 [ 7.85839792e-038  1.18504236e-037]
 [ 7.82374386e-038  1.17981231e-037]
 [ 2.13038323e-005 -5.32243953e-004]
 [-2.07745040e-004 -2.90187809e-001]
 [ 5.51688155e-038  8.31945250e-038]
 [ 2.43564204e-038  3.67292162e-038]
 [-2.35147338e-037 -3.54600331e-037]
 [-4.30099986e-038 -6.48587395e-038]
 [-1.07881900e-037 -1.62684906e-037]
 [ 2.15326835e-039  3.24704752e-039]
 [ 1.46749404e-037  2.21297037e-037]
 [-7.21360115e-038 -1.08780565e-037]
 [-2.73549362e-038 -4.12509328e-038]
 [-6.33663336e-038 -9.55558973e-038]
 [ 3.66110306e-004  2.84016246e-004]
 [-1.66713942e-037 -2.51403387e-037]
 [-3.45891969e-038 -5.21602467e-038]
 [-7.26600507e-006 -8.15033311e-005]
 [ 2.57676486e-038  3.88572154e-038]
 [ 7.62650851e-039  1.15006485e-038]
 [-1.64957624e-037 -2.48754849e-037]
 [ 1.82042861e-037  2.74519154e-037]
 [ 2.17924934e-037  3.28629072e-037]
 [-2.70355572e-038 -4.07695080e-038]
 [ 6.68330604e-038  1.00783755e-037]
 [-3.98706529e-038 -6.01246290e-038]
 [-6.05772784e-038 -9.13500364e-038]
 [-1.24309601e-037 -1.87457859e-037]
 [-1.37944362e-037 -2.08019000e-037]
 [ 8.58450177e-038  1.29453582e-037]
 [ 2.15192243e-037  3.24508263e-037]
 [-1.04872405e-005 -4.40331802e-005]
 [ 7.41402294e-041  1.11803521e-040]
 [ 3.63519535e-041  5.48181538e-041]
 [ 4.38017019e-039  6.60526193e-039]
 [-3.04440040e-040 -4.59093164e-040]
 [ 1.61712637e-004 -3.87603918e-003]
 [-4.61768321e-006 -3.51819301e-005]
 [ 1.86699367e-006 -2.54030836e-005]
 [ 3.43228782e-039  5.17586161e-039]
 [-1.15240748e-040 -1.73781123e-040]
 [ 1.18510097e-039  1.78712292e-039]
 [ 1.03708710e-006 -8.16772450e-004]
 [-9.54552457e-041 -1.43948124e-040]
 [ 1.17156950e-040  1.76671936e-040]
 [-3.66326017e-039 -5.52416700e-039]
 [ 1.44579721e-039  2.18025162e-039]
 [ 1.46159846e-041  2.20407968e-041]
 [-8.01389724e-041 -1.20847170e-040]
 [ 2.34963470e-040  3.54323339e-040]
 [ 1.60576709e-040  2.42147189e-040]
 [ 1.90819507e-004 -3.40886875e-003]
 [ 1.14845408e-039  1.73185993e-039]
 [ 6.15917748e-004 -1.07959860e-002]
 [ 1.53194738e-039  2.31016432e-039]
 [ 5.90718193e-040  8.90799630e-040]
 [ 3.59425832e-002  3.33194838e-002]
 [ 1.30197168e-041  1.96336723e-041]
 [-2.06407699e-040 -3.11261090e-040]
 [ 3.46979347e-040  5.23242243e-040]
 [-2.47167196e-040 -3.72725842e-040]
 [-2.92668995e-040 -4.41342619e-040]
 [-9.55753847e-040 -1.44127056e-039]
 [-9.29428653e-042 -1.40157040e-041]
 [-4.94717445e-042 -7.46032168e-042]
 [-2.06887389e-040 -3.11984349e-040]
 [-1.43893400e-005 -1.91010010e-004]
 [-1.00497110e-040 -1.51548832e-040]
 [ 3.90320023e-040  5.88599381e-040]
 [ 4.20417968e-041  6.33984545e-041]
 [ 2.93095723e-003 -7.64825727e-002]
 [-6.02226235e-041 -9.08152278e-041]
 [ 1.72218956e-041  2.59704630e-041]
 [ 9.65342957e-004  9.61906772e-004]
 [-2.98085707e-041 -4.49510979e-041]
 [-2.33539477e-042 -3.52175683e-042]
 [ 1.28918890e-041  1.94408708e-041]
 [-3.14566443e-006 -1.88528879e-005]
 [-5.92220920e-043 -8.93064767e-043]
 [-2.19227458e-043 -3.30593260e-043]
 [ 3.39921734e-044  5.12599446e-044]
 [-6.49232613e-045 -9.79037483e-045]
 [-9.12629510e-007 -1.14250227e-004]
 [-6.10638929e-045 -9.20839104e-045]
 [ 3.32320787e-045  5.01137131e-045]
 [ 3.36779837e-045  5.07861344e-045]
 [ 1.83356546e-046  2.76500002e-046]
 [-6.72703497e-046 -1.01443185e-045]
 [ 2.16907998e-047  3.27096266e-047]
 [ 3.97505542e-046  5.99435377e-046]
 [-1.08310289e-047 -1.63331123e-047]
 [-3.08022363e-047 -4.64495564e-047]
 [ 2.21174842e-047  3.33529989e-047]
 [-2.46071852e-048 -3.71074409e-048]
 [ 1.31401139e-047  1.98151892e-047]
 [ 1.36413254e-003 -2.68245291e-002]
 [ 3.95178845e-054  5.95927369e-054]
 [ 1.15350801e-054  1.73948199e-054]
 [-5.61682008e-054 -8.47012090e-054]
 [ 5.17776285e-054  7.80802454e-054]
 [-3.41398440e-054 -5.14826707e-054]
 [-1.82910163e-054 -2.75826977e-054]
 [ 1.61890327e-053  2.44129178e-053]
 [ 2.16655669e-054  3.26715364e-054]
 [-2.18215485e-054 -3.29067800e-054]
 [ 7.76662217e-054  1.17120089e-053]
 [ 2.42174853e-054  3.65197941e-054]
 [ 8.75732249e-005  8.97320654e-005]
 [ 1.15010464e-054  1.73435079e-054]
 [ 3.24009120e-054  4.88603113e-054]
 [-1.83329555e-054 -2.76459960e-054]
 [ 1.02283374e-004 -2.06090939e-003]
 [ 1.80898966e-054  2.72794310e-054]
 [-1.85566420e-054 -2.79832459e-054]
 [ 7.70864326e-054  1.16245879e-053]
 [-2.03132346e-054 -3.06322448e-054]
 [-9.91149611e-055 -1.49465198e-054]
 [-1.34285558e-054 -2.02501688e-054]
 [-1.27654238e-054 -1.92501726e-054]
 [-1.60941080e-054 -2.42698527e-054]
 [ 3.26609104e-003  2.94994848e-003]
 [-4.11682928e-054 -6.20815529e-054]
 [-2.14417214e-053 -3.23339549e-053]
 [-5.72150196e-054 -8.62796118e-054]
 [-3.03973174e-054 -4.58389268e-054]
 [-5.29517259e-055 -7.98509382e-055]
 [ 2.42783268e-054  3.66115474e-054]
 [ 1.81781356e-054  2.74125118e-054]
 [ 1.32364763e-003 -2.16651580e-002]
 [-2.80133013e-054 -4.22438902e-054]
 [ 1.83466444e-054  2.76665822e-054]
 [ 7.16347299e-005  6.56969427e-005]
 [ 1.19157005e-003  1.17607247e-003]
 [ 6.19358356e-055  9.33998138e-055]
 [-1.74975623e-054 -2.63861763e-054]
 [-2.03300040e-054 -3.06574859e-054]
 [-6.37404134e-054 -9.61200680e-054]
 [ 3.38122038e-054  5.09885465e-054]
 [ 1.61206550e-054  2.43097437e-054]
 [ 3.67136025e-054  5.53638772e-054]
 [-3.07222500e-054 -4.63289312e-054]
 [ 8.12565773e-054  1.22534385e-053]
 [-2.87445174e-054 -4.33465022e-054]
 [ 1.25835726e-054  1.89759270e-054]
 [ 7.54665221e-055  1.13802601e-054]
 [ 7.09219935e-007 -1.30397499e-005]
 [ 1.19456965e-054  1.80140149e-054]
 [ 1.25980831e-054  1.89978256e-054]
 [ 5.68336839e-054  8.57048835e-054]
 [ 2.05265003e-054  3.09538032e-054]
 [ 5.66388133e-055  8.54114254e-055]
 [ 6.78710971e-055  1.02349187e-054]
 [-5.53088953e-054 -8.34054623e-054]
 [-5.56822968e-054 -8.39685064e-054]
 [-7.53237058e-055 -1.13587333e-054]
 [-1.23595298e-053 -1.86380789e-053]
 [ 5.11163185e-006 -2.60367427e-004]
 [ 4.10204967e-055  6.18585327e-055]
 [ 9.39213567e-054  1.41632632e-053]
 [-8.24536419e-066 -1.24339536e-065]
 [ 2.17135826e-066  3.27439463e-066]
 [ 6.25127130e-067  9.42686608e-067]
 [-3.73914265e-066 -5.63860137e-066]
 [ 2.97162974e-066  4.48120477e-066]
 [ 1.17319704e-001  1.11196436e-001]
 [-3.07524539e-066 -4.63744197e-066]
 [ 6.46542764e-004  6.78687747e-004]
 [-2.93459463e-066 -4.42534619e-066]
 [ 8.37910582e-005  6.99193712e-005]
 [ 4.37365957e-066  6.59545276e-066]
 [ 2.71389823e-005 -3.82352081e-004]
 [-7.99320730e-066 -1.20536967e-065]
 [-5.79012800e-066 -8.73147188e-066]
 [ 4.31189020e-066  6.50230351e-066]
 [-3.60193604e-066 -5.43169284e-066]
 [ 1.09500552e-066  1.65127469e-066]
 [ 1.61728031e-066  2.43884724e-066]
 [ 1.83692213e-065  2.77006563e-065]
 [ 1.64645890e-004  1.77738589e-004]
 [ 1.68091259e-066  2.53480738e-066]
 [-3.54402667e-066 -5.34435907e-066]
 [-1.27101392e-065 -1.91667883e-065]
 [ 3.26867301e-006 -9.18011304e-006]
 [ 2.53509880e-003 -5.70302810e-002]
 [ 2.71677031e-006 -1.22886366e-004]
 [ 9.79236398e-066  1.47668231e-065]
 [ 5.04909692e-066  7.61400528e-066]
 [ 7.05280019e-067  1.06355817e-066]
 [ 2.57367125e-066  3.88107366e-066]
 [ 8.42870434e-004 -1.64642827e-002]
 [-6.50537186e-067 -9.81014832e-067]
 [ 4.14612200e-005 -1.32018317e-003]
 [ 1.85670313e-066  2.79989623e-066]
 [ 3.63158810e-067  5.47640278e-067]
 [-4.40060751e-066 -6.63608784e-066]
 [ 3.44749421e-003 -8.29275700e-002]
 [ 7.38229026e-066  1.11324375e-065]
 [-1.00612952e-066 -1.51724557e-066]
 [ 1.40928023e-003 -2.85483222e-002]
 [-1.16840098e-066 -1.76194301e-066]
 [ 2.23625850e-003 -5.87084321e-002]
 [-3.29416021e-066 -4.96757407e-066]
 [ 1.46876454e-006 -8.24406099e-005]
 [-5.01039198e-066 -7.55563673e-066]
 [-1.02500558e-065 -1.54570179e-065]
 [ 4.98499857e-070  7.48997890e-070]
 [ 1.10541445e-066  1.66695113e-066]
 [-6.32013100e-066 -9.53070905e-066]
 [-1.87600327e-066 -2.82899200e-066]
 [-1.60490926e-066 -2.42019377e-066]
 [-1.12571265e-065 -1.69756792e-065]
 [-2.52000479e-066 -3.80014849e-066]
 [ 3.91429372e-067  5.90274831e-067]
 [ 1.13432477e-065  1.71055436e-065]
 [-1.02893158e-066 -1.55161795e-066]
 [ 6.71778824e-066  1.01303603e-065]
 [ 1.31128655e+000  1.12150221e+000]
 [ 8.07099512e-067  1.21710416e-066]
 [-5.24631850e-066 -7.91141155e-066]
 [ 1.62529635e-066  2.45092928e-066]
 [-3.61203814e-066 -5.44692361e-066]
 [-3.47223503e-066 -5.23610403e-066]
 [ 4.73590224e-066  7.14171716e-066]
 [ 9.63148201e-081  1.45242124e-080]
 [-6.76361807e-081 -1.01994916e-080]
 [ 1.90461825e-080  2.87215148e-080]
 [ 1.16285754e-080  1.75358134e-080]
 [ 7.94949798e-081  1.19877881e-080]
 [-1.00133162e-080 -1.51000082e-080]
 [ 9.26262333e-081  1.39679709e-080]
 [ 1.00687619e-080  1.51836231e-080]
 [ 1.31996169e-002  1.27515425e-002]
 [ 1.38604840e-001  1.31555376e-001]
 [-4.14745184e-081 -6.25433128e-081]
 [ 4.74808726e-081  7.16008598e-081]
 [-2.21860039e-081 -3.34563047e-081]
 [-5.30675654e-081 -8.00255613e-081]
 [ 9.55044268e-004  1.98586312e-004]
 [ 2.61009267e-082  3.93596358e-082]
 [ 3.17738855e-006 -9.31776937e-006]
 [-6.69456759e-081 -1.00953595e-080]
 [-7.59053813e-081 -1.14464740e-080]
 [-2.20414301e-080 -3.32383236e-080]
 [-1.07329055e-080 -1.61851486e-080]
 [ 1.43785460e-081  2.16827540e-081]
 [-2.47829024e-081 -3.73724492e-081]
 [-1.89747420e-006 -7.34076253e-005]
 [ 2.42025290e-081  3.64972345e-081]
 [ 3.29404313e-081  4.96739481e-081]
 [-1.32630472e-080 -2.00005858e-080]
 [ 4.00636142e-081  6.04156791e-081]
 [-1.00258356e-080 -1.51188945e-080]
 [ 3.73257416e-081  5.62869764e-081]
 [-2.09933864e-080 -3.16578827e-080]
 [ 2.21476091e-080  3.33984515e-080]
 [ 3.02992364e-081  4.56910244e-081]
 [-8.52854221e-082 -1.28609723e-081]
 [ 3.88017902e-003 -1.03876347e-001]
 [ 2.34581059e-003  2.14065095e-003]
 [-5.17841943e-081 -7.80901802e-081]
 [-4.58204045e-081 -6.90968623e-081]
 [-1.13042186e-080 -1.70466850e-080]
 [ 2.33496083e-002  2.16238450e-002]
 [-9.58620808e-082 -1.44559433e-081]
 [ 2.33840684e-080  3.52630038e-080]
 [ 2.94265080e-081  4.43749563e-081]
 [ 4.80242467e-004 -1.11134570e-002]
 [-1.85640371e-080 -2.79944407e-080]
 [-4.22736301e-080 -6.37483528e-080]
 [-4.88798523e-082 -7.37104637e-082]
 [ 2.07327090e-081  3.12647566e-081]
 [ 1.17952663e-081  1.77871538e-081]
 [-4.21134082e-082 -6.35067632e-082]
 [ 1.55521436e-080  2.34525302e-080]
 [ 5.60788086e-005  5.63479582e-005]
 [-4.39608680e-081 -6.62926837e-081]
 [ 2.69436130e-080  4.06307804e-080]
 [ 3.16865302e-004 -5.36339844e-003]
 [-7.36705836e-081 -1.11094733e-080]
 [ 1.17114714e-081  1.76608440e-081]
 [-9.54253836e-081 -1.43900833e-080]
 [ 2.02202249e-080  3.04919573e-080]
 [ 2.64164818e-080  3.98358687e-080]
 [-1.03359436e-080 -1.55865340e-080]
 [ 1.32852310e-080  2.00340363e-080]
 [ 3.97720479e-004 -6.90636047e-003]
 [ 5.86002821e-001  5.60727559e-001]
 [ 7.13416093e-097  1.07582667e-096]
 [-6.79853643e-097 -1.02521457e-096]
 [ 7.13735586e-098  1.07630841e-097]
 [ 2.56332630e-099  3.86547846e-099]
 [-3.20321409e-002 -5.79968939e-001]
 [ 1.90061195e-004 -4.03081921e-003]
 [ 2.83712220e-101  4.27835872e-101]
 [ 1.16866607e-005  3.51417528e-006]
 [-5.14584948e-102 -7.75990911e-102]
 [-6.89027298e-103 -1.03904839e-102]
 [ 7.16775839e-104  1.08089297e-103]
 [ 3.04522297e-105  4.59217611e-105]
 [-3.23087597e-107 -4.87213619e-107]
 [-5.47522135e-124 -8.25654006e-124]
 [ 2.86232015e-123  4.31635684e-123]
 [-8.77523270e-123 -1.32329777e-122]
 [ 1.02104217e-002  9.85231132e-003]
 [ 6.02891266e-124  9.09156378e-124]
 [-1.71104474e-124 -2.58024051e-124]
 [-1.42479315e-124 -2.14861617e-124]
 [-1.03768026e-123 -1.56481406e-123]
 [ 2.22628495e-125  3.35723785e-125]
 [ 5.85255599e-125  8.82561902e-125]
 [ 1.70127054e-003 -3.47252390e-002]
 [-6.26984313e-140 -9.45488904e-140]
 [ 8.65573829e-004  6.70363594e-004]
 [ 2.23219126e-140  3.36612529e-140]
 [ 1.38714796e-004  1.24525502e-004]
 [-1.20944452e-007 -2.84008802e-005]
 [-1.22766761e-140 -1.85131697e-140]
 [ 3.57850801e-004 -9.54847682e-003]
 [ 5.65443067e-141  8.52683980e-141]
 [ 1.89001654e-003  1.77782914e-003]
 [-2.93019611e-156 -4.41871387e-156]
 [ 1.65877916e-156  2.50142601e-156]
 [ 7.34483184e-005 -2.42890513e-003]
 [-3.56739187e-173 -5.37961050e-173]
 [-3.31385406e-174 -4.99726099e-174]
 [ 2.86018827e-005 -5.35347762e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [-6.54254302e-006 -2.35614084e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]]new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.08411574363708496
PCA fit: 0.8003408908843994
PCA Transform: 0.3108077049255371
[[0.87863    0.9241276 ]
 [0.51823586 0.7082001 ]]
total iterations: 100
TLDA fit: 39.150726556777954
PCA Reverse Transform: 0.0008318424224853516
final fac2: 
[[ 0.00000000e+000  0.00000000e+000]
 [ 1.00272106e-017  1.94598422e-017]
 [-1.75875507e-017 -3.41322538e-017]
 [-8.35437688e-018 -1.62133832e-017]
 [ 4.26141377e-018  8.27014808e-018]
 [-4.66573475e-019 -9.05481593e-019]
 [ 2.75358796e-019  5.34390262e-019]
 [ 7.08340947e-020  1.37468100e-019]
 [-3.38586595e-020 -6.57096783e-020]
 [-5.63011550e-021 -1.09263947e-020]
 [ 2.11122111e-021  4.09725790e-021]
 [ 3.89376006e-003 -9.14165850e-003]
 [-8.41318350e-024 -1.63275094e-023]
 [ 7.82846426e-028  1.51927371e-027]
 [ 1.34484628e-025  2.60995028e-025]
 [-2.21890986e-026 -4.30625003e-026]
 [ 4.02993767e-027  7.82092315e-027]
 [ 2.07211326e-027  4.02136287e-027]
 [ 7.95405157e-028  1.54364785e-027]
 [-1.78727568e-028 -3.46857751e-028]
 [ 1.14865623e-005 -3.03749840e-007]
 [ 2.65904712e-029  5.16043640e-029]
 [ 4.14405142e-001  4.10100001e-001]
 [ 4.02493875e-028  7.81122263e-028]
 [ 5.44740023e-002  4.99515889e-002]
 [ 1.25808065e-028  2.44156526e-028]
 [ 1.81515069e-029  3.52267144e-029]
 [ 3.03161162e-028  5.88346898e-028]
 [ 2.96540624e-004 -9.15825147e-004]
 [ 8.10057252e-028  1.57208349e-027]
 [-3.10919507e-028 -6.03403370e-028]
 [ 1.60962378e-028  3.12380691e-028]
 [-2.14125862e-028 -4.15555427e-028]
 [-2.39556619e-028 -4.64908975e-028]
 [ 7.06959695e-004 -1.85919960e-003]
 [ 2.25931587e-028  4.38466718e-028]
 [ 7.50107679e-029  1.45573932e-028]
 [ 3.51056409e-028  6.81297337e-028]
 [ 4.85800516e-002 -1.36920718e-001]
 [ 9.59207125e-006 -2.65762251e-005]
 [ 4.81606579e-029  9.34655429e-029]
 [-2.79567813e-028 -5.42558955e-028]
 [-8.57578164e-029 -1.66431089e-028]
 [-5.76585849e-028 -1.11898373e-027]
 [ 6.37581786e-028  1.23735889e-027]
 [ 4.72562843e-028  9.17105655e-028]
 [ 4.53669595e-029  8.80438889e-029]
 [ 2.42690231e-029  4.70990495e-029]
 [-4.08256705e-028 -7.92306294e-028]
 [-1.06248941e-027 -2.06197971e-027]
 [ 9.77212958e-028  1.89648322e-027]
 [ 1.97615200e-028  3.83512822e-028]
 [-2.29079151e-028 -4.44575524e-028]
 [ 2.41047287e-002 -6.01205779e-002]
 [-8.86916380e-029 -1.72124445e-028]
 [ 2.44436563e-028  4.74379560e-028]
 [-6.24515469e-029 -1.21200118e-028]
 [ 1.19331670e-005 -2.20328469e-005]
 [ 6.56994452e-029  1.27503289e-028]
 [ 6.83323439e-029  1.32613002e-028]
 [ 6.79239294e-002  6.21575305e-002]
 [-7.23626658e-029 -1.40434845e-028]
 [ 1.01316138e-029  1.96625085e-029]
 [-7.42546340e-029 -1.44106407e-028]
 [-1.80323291e-028 -3.49954485e-028]
 [ 3.59470885e-030  6.97625622e-030]
 [ 7.17935573e-029  1.39330225e-028]
 [-1.26672124e-029 -2.45833197e-029]
 [ 5.26770273e-005  3.44432343e-005]
 [ 2.82736312e-030  5.48707818e-030]
 [ 2.53406558e-029  4.91787631e-029]
 [ 1.25264244e-003  9.84879696e-004]
 [ 1.25923893e-002 -3.13735190e-002]
 [ 2.48622052e-029  4.82502389e-029]
 [ 1.16549500e-005 -4.51685953e-005]
 [-4.60163540e-031 -8.93041556e-031]
 [ 3.23828266e-001  3.09470554e-001]
 [ 7.00549710e-004  6.13899685e-004]
 [ 3.11814805e-005  1.53225058e-005]
 [ 1.29789740e-029  2.51883776e-029]
 [ 1.89851436e-004  1.87679413e-004]
 [ 7.27783611e-030  1.41241388e-029]
 [ 2.45808518e-029  4.77042167e-029]
 [ 7.09121731e-005  6.98322692e-005]
 [-3.32537526e-030 -6.45357572e-030]
 [-6.45721045e-030 -1.25315510e-029]
 [-1.27663482e-030 -2.47757725e-030]
 [ 1.02618181e-029  1.99151778e-029]
 [-4.63374471e-030 -8.99273314e-030]
 [ 7.29166477e-005 -1.52235733e-004]
 [-7.98266468e-030 -1.54920074e-029]
 [-1.68784018e-030 -3.27560013e-030]
 [-1.05066770e-029 -2.03903752e-029]
 [ 1.52973295e-030  2.96876226e-030]
 [ 1.48887491e-029  2.88946900e-029]
 [ 6.72048515e-030  1.30424887e-029]
 [-7.47056306e-030 -1.44981697e-029]
 [ 1.33850315e-029  2.59764099e-029]
 [-5.92483424e-030 -1.14983637e-029]
 [ 3.51617193e-030  6.82385429e-030]
 [-6.73595457e-030 -1.30725020e-029]
 [-9.29261831e-031 -1.80342415e-030]
 [ 8.05420250e-030  1.56308431e-029]
 [ 3.26939369e-030  6.34493459e-030]
 [-6.98557039e-030 -1.35569554e-029]
 [ 2.60662606e-030  5.05869899e-030]
 [ 1.27027197e-030  2.46522523e-030]
 [-3.12659600e-030 -6.06780535e-030]
 [ 1.64862359e-002 -4.00856922e-002]
 [ 2.22426008e-030  4.31663814e-030]
 [ 2.55360951e-030  4.95580601e-030]
 [ 1.62302940e-030  3.14981990e-030]
 [ 1.39209702e-029  2.70165124e-029]
 [ 3.35249943e-030  6.50621851e-030]
 [ 1.24167342e-003 -2.74212128e-003]
 [ 1.73188983e-002  1.58957828e-002]
 [-4.53200725e-031 -8.79529489e-031]
 [-1.10345442e-030 -2.14148185e-030]
 [ 8.43546057e-030  1.63707487e-029]
 [ 7.82119897e-031  1.51786749e-030]
 [-7.21565768e-030 -1.40034735e-029]
 [ 3.76964400e-030  7.31577430e-030]
 [-4.85057690e-031 -9.41354495e-031]
 [ 3.29988460e-031  6.40411098e-031]
 [ 1.64260532e-031  3.18781615e-031]
 [ 1.63819887e-031  3.17926391e-031]
 [ 4.14259948e-003 -9.38180061e-003]
 [ 2.10454093e-005 -4.95402788e-005]
 [ 5.88918504e-002  5.50681738e-002]
 [-7.86383437e-038 -1.52613800e-037]
 [ 2.52769954e-037  4.90552506e-037]
 [ 2.81113075e-036  5.45558132e-036]
 [ 1.33317180e-036  2.58729578e-036]
 [ 1.77059377e-036  3.43620262e-036]
 [ 4.91524613e-036  9.53904915e-036]
 [-2.38235430e-036 -4.62345098e-036]
 [ 1.36483237e-004 -2.54829765e-004]
 [ 9.25178550e-037  1.79550013e-036]
 [ 2.96318168e-036  5.75066725e-036]
 [ 7.03414528e-003  6.12680675e-003]
 [-1.47665998e-036 -2.86576350e-036]
 [-5.33494845e-037 -1.03535694e-036]
 [ 2.00037422e-036  3.88214236e-036]
 [-5.74486560e-037 -1.11490990e-036]
 [-4.56239658e-036 -8.85427291e-036]
 [ 1.05066526e-004  9.09242368e-005]
 [ 8.54019406e-037  1.65740132e-036]
 [-6.03947782e-037 -1.17208551e-036]
 [ 1.53703947e-038  2.98292970e-038]
 [ 2.31153157e-004  2.00046540e-004]
 [ 1.49386365e-036  2.89915091e-036]
 [-9.12646266e-037 -1.77117930e-036]
 [-3.89018325e-036 -7.54970278e-036]
 [-1.12675199e-036 -2.18669537e-036]
 [-8.52552730e-037 -1.65455459e-036]
 [ 4.28670213e-036  8.31923170e-036]
 [ 2.62155350e-005 -6.21024002e-005]
 [ 3.51090248e-004 -9.45535191e-004]
 [ 1.98889032e-002  1.78103784e-002]
 [-9.16393160e-038 -1.77845089e-037]
 [-2.64206871e-036 -5.12748073e-036]
 [ 2.33491460e-036  4.53138373e-036]
 [ 4.18983722e-004 -1.10751955e-003]
 [-3.41141009e-037 -6.62054488e-037]
 [-1.62910069e-036 -3.16160679e-036]
 [ 2.12104563e-037  4.11633174e-037]
 [ 1.62984625e-005 -3.61569039e-005]
 [ 1.01041561e-036  1.96092051e-036]
 [ 9.80754653e-037  1.90335684e-036]
 [-4.28754237e-038 -8.32086953e-038]
 [ 1.47472521e-036  2.86200909e-036]
 [ 3.08299539e-003  3.15871240e-003]
 [ 2.43798062e-036  4.73140513e-036]
 [ 8.36992366e-037  1.62435649e-036]
 [ 1.78001421e-036  3.45448560e-036]
 [ 5.95383479e-037  1.15546466e-036]
 [ 2.18205290e-036  4.23472398e-036]
 [-1.64304199e-036 -3.18866252e-036]
 [ 1.07837071e-002 -2.67943414e-002]
 [-2.56553988e-037 -4.97895914e-037]
 [-5.11655026e-038 -9.92973588e-038]
 [ 6.58517934e-006 -9.81591093e-006]
 [-3.27744651e-036 -6.36056116e-036]
 [ 8.87479775e-005  1.04446307e-004]
 [-1.12283929e-037 -2.17910158e-037]
 [ 1.82262704e-036  3.53718387e-036]
 [ 3.39489448e-037  6.58850870e-037]
 [-3.11514839e-038 -6.04563543e-038]
 [-1.94274121e-037 -3.77029065e-037]
 [-2.56458939e-038 -4.97710589e-038]
 [ 8.03470025e-037  1.55929938e-036]
 [ 5.24966687e-037  1.01880648e-036]
 [ 6.39303944e-003  5.98980280e-003]
 [ 3.03328415e-038  5.88671509e-038]
 [ 2.02490087e-038  3.92973872e-038]
 [-1.81510115e-038 -3.52257819e-038]
 [ 2.32173541e-039  4.50579988e-039]
 [ 4.60284821e-038  8.93277882e-038]
 [-4.06055391e-038 -7.88034288e-038]
 [-5.20095450e-038 -1.00935258e-037]
 [-3.82525261e-039 -7.42372123e-039]
 [-7.72279420e-038 -1.49876805e-037]
 [-2.50910858e-038 -4.86944295e-038]
 [ 4.50838047e-039  8.74944955e-039]
 [ 1.27861196e-001  1.18893010e-001]
 [ 7.67581075e-005 -1.44780590e-004]
 [-1.25905888e-039 -2.44346135e-039]
 [ 4.51552190e-039  8.76330198e-039]
 [ 7.85560797e-039  1.52454325e-038]
 [ 1.61917031e-038  3.14233429e-038]
 [ 1.87518050e-039  3.63917599e-039]
 [ 1.93597753e-005  1.49758372e-005]
 [ 3.45474218e-039  6.70464097e-039]
 [ 1.18013187e-005 -4.48845372e-005]
 [ 1.58863160e-038  3.08306851e-038]
 [ 2.16155584e-003 -4.91116634e-003]
 [ 2.25084747e-040  4.36823927e-040]
 [ 1.44857075e-002  1.28829308e-002]
 [ 1.22507969e-039  2.37752037e-039]
 [ 1.17807988e-038  2.28630719e-038]
 [-2.00980018e-038 -3.90043238e-038]
 [-9.40727232e-039 -1.82567607e-038]
 [ 3.89528861e-038  7.55961139e-038]
 [ 9.41085403e-006 -4.95237251e-005]
 [ 3.49188954e-038  6.77673294e-038]
 [-9.74518350e-041 -1.89125272e-040]
 [-8.13176382e-039 -1.57813696e-038]
 [ 3.37302437e-004  3.15667832e-004]
 [ 6.42177767e-039  1.24627852e-038]
 [-4.90879023e-039 -9.52652080e-039]
 [-1.80941648e-039 -3.51154639e-039]
 [ 6.43277103e-002 -1.86393496e-001]
 [-1.65700498e-039 -3.21576030e-039]
 [-5.49054365e-040 -1.06555342e-039]
 [-5.46144778e-040 -1.05990719e-039]
 [-1.25883908e-041 -2.44303553e-041]
 [ 4.70751639e-002 -1.27933263e-001]
 [ 9.65150113e-041  1.87307316e-040]
 [ 2.05377178e-004 -6.86022112e-004]
 [ 3.47771365e-041  6.74922198e-041]
 [-1.75143784e-041 -3.39902629e-041]
 [-3.63123569e-041 -7.04716283e-041]
 [ 5.34188789e-041  1.03670346e-040]
 [ 4.79891993e-005  4.79413275e-005]
 [ 7.62039351e-005 -2.13643486e-004]
 [ 1.18448512e-042  2.29873806e-042]
 [ 3.21417096e-043  6.23776312e-043]
 [ 4.63620314e-044  8.99750912e-044]
 [ 1.66422852e-004 -3.77492237e-004]
 [ 8.98571751e-046  1.74386404e-045]
 [ 7.51494687e-047  1.45843064e-046]
 [ 5.17478251e-003 -1.20107768e-002]
 [ 4.23599681e-002 -1.14284902e-001]
 [-9.94580070e-048 -1.93018663e-047]
 [ 1.73358309e-047  3.36437437e-047]
 [ 3.24254724e-003  3.06163153e-003]
 [-1.68887588e-049 -3.27768531e-049]
 [ 4.91844115e-053  9.54528005e-053]
 [-1.46533261e-052 -2.84377442e-052]
 [ 2.79713587e-051  5.42841754e-051]
 [ 2.50226186e-051  4.85615413e-051]
 [-2.75217152e-051 -5.34115535e-051]
 [ 6.07474264e-006 -1.08065183e-005]
 [-2.32624968e-051 -4.51456805e-051]
 [ 8.59124997e-052  1.66730727e-051]
 [-1.08134184e-051 -2.09856541e-051]
 [ 7.39291547e-052  1.43474733e-051]
 [-2.23888549e-051 -4.34501884e-051]
 [-1.17043482e-051 -2.27147060e-051]
 [ 7.56696563e-052  1.46852676e-051]
 [ 6.63251608e-051  1.28717668e-050]
 [-1.58466173e-051 -3.07536215e-051]
 [ 1.67211490e-052  3.24509591e-052]
 [ 2.21234243e-004 -3.84098451e-004]
 [-4.13222070e-051 -8.01942499e-051]
 [ 5.55204035e-002 -1.54136644e-001]
 [ 4.01438124e-051  7.79073813e-051]
 [-1.43384492e-051 -2.78267114e-051]
 [ 3.14833331e-052  6.11000711e-052]
 [-3.63919141e-052 -7.06260742e-052]
 [ 3.27069416e-051  6.34745579e-051]
 [ 3.68771325e-004  3.54143924e-004]
 [ 5.38864202e-051  1.04577631e-050]
 [-1.55916221e-051 -3.02587560e-051]
 [ 2.59579307e-051  5.03766928e-051]
 [ 3.82266189e-051  7.41866755e-051]
 [ 5.82076931e-052  1.12963995e-051]
 [ 3.30395528e-004  2.11880449e-004]
 [-1.76596220e-051 -3.42721472e-051]
 [ 4.43664953e-002 -1.18819057e-001]
 [-3.27485868e-051 -6.35553837e-051]
 [ 2.69311895e-004  2.51505606e-004]
 [ 3.19290049e-051  6.19647841e-051]
 [-4.91069206e-051 -9.53021378e-051]
 [ 3.12488961e-051  6.06449131e-051]
 [-2.80173380e-052 -5.43736484e-052]
 [ 1.45229630e-003 -3.44055859e-003]
 [ 3.26630722e-051  6.33894740e-051]
 [ 2.01807663e-051  3.91649162e-051]
 [ 6.00645367e-052  1.16567657e-051]
 [ 2.92680487e-051  5.68006822e-051]
 [ 1.75697850e-052  3.40978767e-052]
 [ 8.86104593e-002 -3.40619071e-001]
 [-2.43545804e-051 -4.72650723e-051]
 [ 6.14492321e-003 -1.48053512e-002]
 [ 1.58191121e-051  3.07002127e-051]
 [-2.36245290e-051 -4.58482911e-051]
 [ 3.93616786e-054  7.63910258e-054]
 [-1.16499672e-053 -2.26094949e-053]
 [ 3.40318636e-005 -9.21248089e-005]
 [-2.74049224e-051 -5.31849236e-051]
 [-4.74896668e-052 -9.21635769e-052]
 [-2.96479766e-051 -5.75380106e-051]
 [ 4.04537957e-051  7.85089119e-051]
 [ 6.97121856e-051  1.35290836e-050]
 [-1.38016782e-050 -2.67849982e-050]
 [ 2.31857797e-051  4.49967722e-051]
 [ 1.35669565e-052  2.63294495e-052]
 [ 3.88434691e-051  7.53837898e-051]
 [ 6.94952842e-052  1.34870098e-051]
 [ 1.48983992e-002 -3.40837027e-002]
 [ 1.04288260e-066  2.02392787e-066]
 [-3.10817709e-066 -6.03205635e-066]
 [ 3.14531420e-066  6.10412961e-066]
 [-3.80081392e-066 -7.37626267e-066]
 [-6.05682742e-066 -1.17545310e-065]
 [ 6.24878962e-066  1.21270691e-065]
 [-7.28508257e-066 -1.41382029e-065]
 [ 2.64182078e-002 -6.68835030e-002]
 [ 5.70085639e-066  1.10636964e-065]
 [-5.57154169e-066 -1.08127260e-065]
 [ 6.25094273e-066  1.21312439e-065]
 [ 1.82034614e-001  1.71176123e-001]
 [ 1.35935412e-066  2.63810451e-066]
 [-1.79981470e-066 -3.49291490e-066]
 [ 6.83420114e-067  1.32631717e-066]
 [ 1.31478919e-005 -1.96754233e-005]
 [ 5.44589551e-002  4.97190246e-002]
 [-1.45464026e-066 -2.82302884e-066]
 [-3.80491572e-067 -7.38422716e-067]
 [ 2.35593222e-066  4.57217206e-066]
 [-1.33721962e-065 -2.59515134e-065]
 [-4.45004561e-066 -8.63623015e-066]
 [ 1.45616836e-066  2.82599354e-066]
 [ 2.04363575e-066  3.96609647e-066]
 [-1.95081437e-066 -3.78595769e-066]
 [ 3.10071001e-066  6.01757049e-066]
 [ 2.48810710e-067  4.82869410e-067]
 [-3.06766401e-066 -5.95343346e-066]
 [-2.98241749e-066 -5.78799727e-066]
 [ 1.32756008e-066  2.57640708e-066]
 [ 2.04028497e-067  3.95965339e-067]
 [ 2.21112614e-002  2.06319991e-002]
 [-2.74042587e-067 -5.31836442e-067]
 [ 2.09067232e-003 -4.59681425e-003]
 [-4.39726599e-066 -8.53380427e-066]
 [-3.30580362e-067 -6.41556725e-067]
 [-4.52836659e-066 -8.78823146e-066]
 [ 3.82797936e-067  7.42897729e-067]
 [ 1.36096189e-066  2.64122520e-066]
 [ 5.93250247e-066  1.15132487e-065]
 [ 5.41923083e-004 -1.14008007e-003]
 [ 4.90776338e-066  9.52452846e-066]
 [ 7.73764077e-067  1.50164981e-066]
 [ 1.99806661e-004 -3.57895682e-004]
 [ 4.05708596e-066  7.87361653e-066]
 [-3.82288391e-066 -7.41909458e-066]
 [-1.01755095e-066 -1.97476883e-066]
 [ 4.80255021e-066  9.32033817e-066]
 [ 6.53782131e-066  1.26879909e-065]
 [ 1.87036882e-065  3.62983686e-065]
 [ 2.03013964e-067  3.93984524e-067]
 [ 2.67143440e-066  5.18446908e-066]
 [-2.14320924e-066 -4.15933639e-066]
 [-1.41399726e-066 -2.74414904e-066]
 [ 5.56445167e-005 -1.40564240e-004]
 [ 1.88517156e-004  1.62494198e-004]
 [ 1.83778350e-067  3.56657939e-067]
 [ 1.79360300e-066  3.48085598e-066]
 [ 5.68775788e-067  1.10382602e-066]
 [ 2.52593695e-066  4.90210171e-066]
 [ 3.20577983e-003 -7.60715846e-003]
 [-1.24550606e-066 -2.41716115e-066]
 [-3.94365351e-066 -7.65347959e-066]
 [ 9.44406445e-067  1.83281535e-066]
 [ 1.33244622e-081  2.58588619e-081]
 [ 3.57314058e-082  6.93441399e-082]
 [-5.28544835e-082 -1.02574687e-081]
 [-2.22185758e-081 -4.31197294e-081]
 [-6.89553674e-083 -1.33819834e-082]
 [-2.78657788e-081 -5.40792829e-081]
 [ 2.88811631e-081  5.60498576e-081]
 [ 9.82520319e-003  8.76414614e-003]
 [ 1.04652565e+000  8.24247549e-001]
 [ 1.01235029e-081  1.96467465e-081]
 [ 1.29085564e-081  2.50517027e-081]
 [ 5.74277627e-082  1.11450384e-081]
 [ 9.82334599e-082  1.90642378e-081]
 [ 4.45291741e-081  8.64180373e-081]
 [ 9.43983207e-081  1.83199391e-080]
 [ 1.35166840e-003 -3.34210232e-003]
 [ 1.98452407e-004  1.59179792e-004]
 [ 7.53879638e-081  1.46305890e-080]
 [ 1.63228221e-002 -3.94764033e-002]
 [-1.25867879e-081 -2.44272830e-081]
 [-4.53774667e-081 -8.80643439e-081]
 [ 1.52547269e-081  2.96049719e-081]
 [ 8.95505871e-002 -4.09745366e-001]
 [ 3.72759832e-003  3.34769486e-003]
 [-1.85223255e-081 -3.59464005e-081]
 [ 7.21947418e-082  1.40108777e-081]
 [-5.87235065e-081 -1.13965116e-080]
 [ 1.68502629e-003 -4.25426369e-003]
 [-2.06191566e-083 -4.00139998e-083]
 [-4.19269942e-081 -8.13679807e-081]
 [ 1.63532544e-081  3.17368827e-081]
 [ 2.42814671e-081  4.71232357e-081]
 [-1.42834183e-081 -2.77199286e-081]
 [ 6.96505711e-082  1.35171706e-081]
 [-4.58308917e-081 -8.89442883e-081]
 [ 5.90566990e-081  1.14611690e-080]
 [ 1.03854090e-081  2.01550435e-081]
 [ 4.48535069e-082  8.70474589e-082]
 [ 1.05418457e-082  2.04587009e-082]
 [-1.18530194e-081 -2.30032507e-081]
 [ 6.26672849e-004 -1.58569251e-003]
 [ 1.99025813e-081  3.86250657e-081]
 [ 1.67682456e-081  3.25422481e-081]
 [ 9.13671091e-082  1.77316657e-081]
 [ 1.96807976e-004 -4.08906888e-004]
 [ 5.31898810e-083  1.03228131e-082]
 [ 1.89056336e-002 -4.40219493e-002]
 [ 7.69860059e-001  8.45256151e-001]
 [ 4.75845013e-082  9.23475973e-082]
 [-1.43311696e-081 -2.78125996e-081]
 [-4.64472286e-081 -9.01404295e-081]
 [ 8.43791537e-082  1.63755413e-081]
 [ 4.83874579e-081  9.39058461e-081]
 [-4.87815617e-081 -9.46706981e-081]
 [-3.14434705e-081 -6.10225695e-081]
 [-3.05069280e-081 -5.92050095e-081]
 [-2.09962920e-081 -4.07476358e-081]
 [ 9.90274257e-082  1.92183234e-081]
 [ 1.45345725e-081  2.82073390e-081]
 [-3.66560297e-081 -7.11386018e-081]
 [-4.40213784e-081 -8.54325799e-081]
 [ 3.50026227e-081  6.79298280e-081]
 [-3.30081298e-081 -6.40591003e-081]
 [ 1.23694826e-001  1.16320699e-001]
 [ 5.63263891e-098  1.09313009e-097]
 [ 2.00930415e-097  3.89947383e-097]
 [ 2.17572141e-002  1.98997053e-002]
 [ 7.35079783e-003  6.78652744e-003]
 [-1.15300725e-098 -2.23764835e-098]
 [-5.54255918e-099 -1.07564807e-098]
 [ 5.22655397e-100  1.01432074e-099]
 [-2.51903561e-102 -4.88870852e-102]
 [ 2.32939250e-102  4.52066640e-102]
 [-1.23248702e-125 -2.39189494e-125]
 [ 7.90913476e-125  1.53493068e-124]
 [-1.32984241e-125 -2.58083286e-125]
 [-3.63565199e-126 -7.05572915e-126]
 [-3.37749940e-126 -6.55473412e-126]
 [-7.62024571e-127 -1.47886386e-126]
 [ 4.06842733e-127  7.89561885e-127]
 [ 5.31636254e-142  1.03174952e-141]
 [ 7.78089345e-143  1.51004358e-142]
 [-1.23341447e-142 -2.39369509e-142]
 [-5.42109956e-142 -1.05207572e-141]
 [ 3.06371252e-142  5.94576453e-142]
 [ 4.99340620e-005 -1.06454937e-004]
 [ 1.82390534e-002 -4.45699212e-002]
 [-5.22349262e-159 -1.01373007e-158]
 [-3.14439798e-159 -6.10235754e-159]
 [ 1.50650616e-158  2.92368005e-158]
 [ 3.98524415e-160  7.73414184e-160]
 [-5.97955781e-159 -1.16045646e-158]
 [ 1.48144415e-174  2.87504664e-174]
 [ 7.71731308e-174  1.49770403e-173]
 [ 4.78497358e-174  9.28623166e-174]
 [ 9.69696862e-004  9.10277074e-004]
 [-1.61033063e-175 -3.12518010e-175]
 [-6.03529258e-192 -1.17124706e-191]
 [ 2.06702582e-005  1.75190988e-005]
 [ 1.51531324e-190  2.94077992e-190]
 [-1.57591406e-190 -3.05838604e-190]
 [ 2.25806479e-003  1.96448933e-003]
 [-5.42886217e-206 -1.05358258e-205]
 [ 3.50935460e-004 -7.65069118e-004]
 [-3.55029159e-224 -6.89007254e-224]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.00039786e-003 -4.52345649e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]]new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.08003830909729004
PCA fit: 0.7578141689300537
PCA Transform: 0.312558650970459
[[0.35050002 0.12360992]
 [0.98083764 0.62684757]]
total iterations: 1209
TLDA fit: 483.46319794654846
PCA Reverse Transform: 0.0008823871612548828
final fac2: 
[[ 1.35661961e-004  1.06358004e-004]
 [-6.88817726e-017 -7.61614206e-017]
 [ 1.70240944e-001  1.22669040e-001]
 [ 2.44449109e-004  1.46632705e-004]
 [ 1.90150132e-002  1.69004477e-002]
 [ 9.24730664e-005  9.49725735e-005]
 [ 1.65870735e-003  1.01209413e-003]
 [-1.83570420e-019 -2.02971910e-019]
 [ 1.18229679e-005  5.79890851e-006]
 [ 1.43132424e-001  1.27331733e-001]
 [ 1.04508421e-004  5.73649268e-005]
 [ 1.36264453e-002  8.41852112e-003]
 [ 2.34169452e-002  2.08147525e-002]
 [ 1.95404200e-023  2.16056398e-023]
 [ 1.17361254e-003  7.37582137e-004]
 [ 1.01237134e-002  9.10609866e-003]
 [ 1.94390878e-002  1.72529631e-002]
 [ 6.44214769e-004  6.03193989e-004]
 [ 1.47735187e-004  1.19707110e-004]
 [ 3.23516383e-002  2.55585773e-002]
 [ 2.28462378e-005  1.07136149e-005]
 [-1.74238732e-027 -1.92653960e-027]
 [ 1.52298892e-002  1.34135634e-002]
 [ 2.46042566e-029  2.72046714e-029]
 [ 5.51680255e-030  6.09987139e-030]
 [ 5.64901643e-004  4.79130530e-004]
 [ 8.14989865e-003  4.77073106e-003]
 [ 1.30996371e-005  7.21053519e-006]
 [ 1.25152361e-005  6.56437398e-006]
 [ 3.08248988e-002  2.55506666e-002]
 [ 3.67619736e-002  3.26243119e-002]
 [ 6.85339965e-003  6.12296938e-003]
 [ 4.02035281e-037  4.44526242e-037]
 [ 5.08013997e-005  3.43495397e-005]
 [ 2.90794526e-003  1.76055717e-003]
 [-6.95323125e-039 -7.68811571e-039]
 [-6.77283365e-041 -7.48865196e-041]
 [ 3.91269303e-002  2.51217796e-002]
 [ 2.01577034e-003  1.78511378e-003]
 [ 2.66334824e-002  2.36365861e-002]
 [ 1.13325335e-003  1.01299031e-003]
 [ 3.30613258e-004  3.07364958e-004]
 [ 4.84957673e-004  4.12557776e-004]
 [ 3.16412580e-003  1.94925628e-003]
 [ 6.09636199e-004  5.06771304e-004]
 [-2.76490129e-049 -3.05712270e-049]
 [ 3.97095085e-003  3.56869783e-003]
 [ 1.01429746e-002  6.58894543e-003]
 [ 1.00690229e-003  6.04167227e-004]
 [ 9.74468565e-005  9.31982665e-005]
 [ 1.39444768e-002  1.25014629e-002]
 [ 9.00975803e-003  5.55417304e-003]
 [ 1.21361687e-003  6.94530664e-004]
 [ 5.07780605e-060  5.61447751e-060]
 [ 3.28164188e-004  2.39194435e-004]
 [ 6.34855144e-003  3.81185503e-003]
 [ 2.09868548e-002  1.82806022e-002]
 [-3.34532900e-062 -3.69889560e-062]
 [ 2.75415536e-002  1.64361294e-002]
 [ 7.62329421e-003  6.32688868e-003]
 [-4.46744653e-066 -4.93960932e-066]
 [ 1.13631180e-066  1.25640818e-066]
 [ 4.02657462e-004  2.41552674e-004]
 [ 3.93283450e-003  3.45381729e-003]
 [ 7.43161609e-002  6.57026776e-002]
 [ 3.00646271e-003  1.76765684e-003]
 [ 2.15262560e-003  1.25999648e-003]
 [ 7.70767368e-004  4.95821137e-004]
 [-3.60332864e-076 -3.98416312e-076]
 [ 1.96929081e-002  1.22821556e-002]
 [ 1.77890088e-002  1.08534837e-002]
 [ 4.30234534e-004  4.10241484e-004]
 [ 3.82866632e-080  4.23331666e-080]
 [ 1.24959522e-002  1.11107845e-002]
 [ 7.08637460e-005  7.10794189e-005]
 [ 7.64242178e-003  6.48622031e-003]
 [ 1.25579182e-003  7.33889586e-004]
 [ 1.79201029e-086  1.98140720e-086]
 [ 8.00014059e-088  8.84567251e-088]
 [ 1.17138230e-089  1.29518527e-089]
 [ 1.13733995e-005  5.30181276e-006]
 [-1.29325070e-091 -1.42993390e-091]
 [-1.66320343e-093 -1.83898679e-093]
 [ 5.43263067e-002  3.26391900e-002]
 [ 5.01485891e-003  4.55563738e-003]
 [ 3.27806430e-002  2.92623501e-002]
 [ 1.52350032e-003  1.39356601e-003]
 [ 4.73236566e-100  5.23252765e-100]
 [ 3.47828172e-002  3.09160480e-002]
 [ 7.36580059e-004  5.01662315e-004]
 [ 3.93955982e-003  2.47205778e-003]
 [ 3.40796433e-003  3.06986373e-003]
 [-3.73716685e-108 -4.13214665e-108]
 [ 1.70194564e-004  8.63516118e-005]
 [ 1.23015626e-004  1.14195583e-004]
 [ 1.38787431e-113  1.53455824e-113]
 [ 4.34416215e-002  3.87151877e-002]
 [ 4.88766923e-002  4.32917480e-002]
 [ 6.42767191e-120  7.10701020e-120]
 [ 2.71557727e-004  2.34794370e-004]
 [ 1.52105105e-001  1.33761347e-001]
 [ 5.86230552e-004  4.95439336e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.40429195e-005  8.25351570e-006]
 [ 1.31593459e-002  8.13481020e-003]
 [ 1.21837744e-004  1.05619726e-004]
 [ 8.75591401e-003  7.95015390e-003]
 [ 1.31162882e-004  7.95627870e-005]
 [ 4.63478541e-002  4.10993662e-002]
 [ 8.69326241e-005  4.52051289e-005]
 [ 3.68896581e-005  2.62410613e-005]
 [ 1.40418732e-005  8.25233180e-006]
 [ 4.21018675e-005  3.20041674e-005]
 [ 6.12908437e-004  5.83126404e-004]
 [ 6.75952757e-003  4.09170254e-003]
 [ 1.14883593e-002  1.00694384e-002]
 [ 3.38376474e-002  2.98709923e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 3.41973058e-002  3.05305108e-002]
 [ 1.64018158e-004  1.08616746e-004]
 [ 6.25026504e-004  3.78318873e-004]
 [ 1.21861181e-002  1.10518815e-002]
 [ 6.08419917e-003  5.45431916e-003]
 [ 4.01935163e-005  2.26207148e-005]
 [ 3.00976140e-004  2.09134021e-004]
 [ 4.21994914e-003  3.75672711e-003]
 [ 7.86697223e-003  4.85795174e-003]
 [ 7.66492232e-005  4.11085145e-005]
 [ 1.32870925e-002  1.18691577e-002]
 [ 1.02243632e-002  7.00624379e-003]
 [ 2.91542806e-002  1.82702070e-002]
 [ 3.41700328e-002  2.05429508e-002]
 [ 2.35724480e-002  2.10376024e-002]
 [ 1.28925845e-005  6.98158968e-006]
 [ 3.71673399e-004  2.14567483e-004]
 [ 8.36790208e-003  5.12815197e-003]
 [ 2.78049533e-002  2.47354782e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 5.23649031e-003  4.56793310e-003]
 [ 1.08422831e-004  6.16928938e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 3.49191161e-005  1.67888287e-005]
 [ 5.30987793e-003  4.41633187e-003]
 [ 2.88445478e-004  2.02552473e-004]
 [ 1.41611430e-002  8.57331321e-003]
 [ 1.15590944e-004  9.87125686e-005]
 [ 1.43030102e-002  8.97747785e-003]
 [ 1.63336685e-003  1.03499086e-003]
 [ 6.51911415e-003  5.84064067e-003]
 [ 5.97311936e-002  3.59822733e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.06994817e-002  6.38235520e-003]
 [ 7.83597408e-003  7.12210547e-003]
 [ 1.06691933e-002  6.72708905e-003]
 [ 4.74315835e-002  2.83398104e-002]
 [ 6.58266713e-004  4.07798650e-004]
 [ 2.68294944e-002  1.72780769e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.85045667e-004  1.91520011e-004]
 [ 2.95475652e-004  2.83061244e-004]
 [ 8.12205596e-003  4.97997336e-003]
 [ 1.91843370e-002  1.70658455e-002]
 [ 1.56685074e-002  9.17076690e-003]
 [ 7.38746323e-002  6.57018588e-002]
 [ 1.17882252e-003  9.76090952e-004]
 [ 2.31622539e-003  2.11004914e-003]
 [ 6.11436908e-002  5.41712188e-002]
 [ 1.45956064e-002  8.86457920e-003]
 [ 4.02963938e-004  2.70985672e-004]
 [ 2.95720143e-002  2.63328782e-002]
 [ 1.84446531e-004  1.09383421e-004]
 [ 1.15781213e-003  6.91016556e-004]
 [ 7.32943685e-002  6.50529707e-002]
 [ 5.94542613e-003  5.24268829e-003]
 [ 8.45642091e-004  7.02257314e-004]
 [ 1.96578095e-002  1.73638719e-002]
 [ 7.20055008e-005  7.23418397e-005]
 [ 2.65748360e-003  1.79638677e-003]
 [ 9.99140683e-005  6.68318162e-005]
 [ 1.32081518e-001  1.18029657e-001]
 [ 7.16095241e-003  6.39756273e-003]
 [ 2.55435981e-002  1.66636550e-002]
 [ 1.57856026e-003  1.11986310e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.86190342e-004  2.58247223e-004]
 [ 3.22527850e-002  1.93468152e-002]
 [ 2.01699407e-001  1.77744647e-001]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.47109987e-005  8.99221235e-006]
 [ 9.68379270e-004  8.67060446e-004]
 [ 6.72116980e-003  4.00564644e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 3.93553786e-004  3.69684086e-004]
 [ 9.18331991e-004  6.08066283e-004]
 [ 7.67676081e-003  6.89514772e-003]
 [ 4.88903666e-003  2.81634407e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 6.31497642e-003  5.73130798e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.31223895e-004  1.53830883e-004]
 [ 1.55629925e-002  9.38141508e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.08561165e-002  1.29282192e-002]
 [ 1.79386641e-002  1.56012480e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.25242530e-004  1.25396604e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 5.04796666e-003  2.96297521e-003]
 [ 1.59773325e-002  1.40654351e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 4.91121597e-003  3.11726479e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 5.90030618e-005  2.88709179e-005]
 [ 2.42118164e-002  2.14463204e-002]
 [ 6.76316440e-003  5.80499361e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 4.52046165e-005  4.27084233e-005]
 [ 3.04057201e-003  2.71455994e-003]
 [ 4.15742267e-004  2.41472869e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.93364968e-003  2.65452716e-003]
 [ 5.25852456e-004  4.65048393e-004]
 [ 1.72270418e-004  1.10467568e-004]
 [ 1.91848880e-002  1.70810038e-002]
 [ 3.28568591e-002  2.95430122e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 6.15842349e-005  5.35456168e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.60187372e-002  1.43730683e-002]
 [ 7.32782672e-002  6.01182840e-002]
 [ 5.60077524e-003  3.44328585e-003]
 [ 3.57028600e-002  2.18959185e-002]
 [ 7.05413883e-003  5.74849359e-003]
 [ 8.65679206e-003  7.65144134e-003]
 [ 2.94466327e-005  2.52850398e-005]
 [ 2.32456319e-002  2.08508154e-002]
 [ 2.78200532e-002  1.88533735e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.64995787e-002  2.37722049e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 9.47464723e-005  7.56655623e-005]
 [ 1.11123528e-003  8.43175787e-004]
 [ 5.30553243e-003  3.16048050e-003]
 [ 3.98153869e-003  3.44220690e-003]
 [ 4.26454666e-003  2.51862810e-003]
 [ 4.41375048e-004  2.84361624e-004]
 [ 1.13059196e-005  5.22720299e-006]
 [ 1.07083197e-005 -2.70745440e-006]
 [ 8.13962028e-004  4.85392486e-004]
 [ 2.56502781e-003  2.26876810e-003]
 [ 5.22829286e-002  4.62578747e-002]
 [ 1.64649289e-004  8.74937185e-005]
 [ 9.46279516e-003  8.35351185e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 3.42739324e-002  2.08760405e-002]
 [ 2.78206033e-004  2.27598046e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 9.08587223e-004  5.53650172e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 3.53187039e-002  3.14431084e-002]
 [ 5.42851989e-002  4.82753605e-002]
 [ 1.60112836e-002  1.43793750e-002]
 [ 9.54976620e-003  6.70403574e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 5.74644704e-003  5.02995230e-003]
 [ 3.87797822e-002  3.45426072e-002]
 [ 7.40577933e-002  6.15256809e-002]
 [ 9.23955955e-003  5.78644329e-003]
 [ 1.29798992e-005  7.07814629e-006]
 [ 2.34602796e-002  2.10008658e-002]
 [ 2.94428138e-002  1.77600501e-002]
 [ 4.61798839e-004  3.94225710e-004]
 [ 9.16444180e-002  8.21564831e-002]
 [ 1.10186351e-004  5.63693510e-005]
 [ 8.77825815e-005  5.34183690e-005]
 [ 1.50997444e-004  1.01493449e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.38405647e-004  1.23938457e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.53376132e-005  1.34683150e-005]
 [ 1.33910038e-002  8.69643248e-003]
 [ 2.26042061e-002  2.02870783e-002]
 [ 5.65018143e-005  4.79258707e-005]
 [ 3.31141750e-002  2.49979029e-002]
 [ 2.74347356e-002  2.43988744e-002]
 [ 1.42949262e-003  1.10778479e-003]
 [ 8.47563970e-004  7.77118314e-004]
 [ 6.18212770e-002  5.50076868e-002]
 [ 8.54124068e-001  8.33200593e-001]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.28125830e-004  1.28584451e-004]
 [ 2.45063730e-002  2.19465798e-002]
 [ 4.11748751e-005  3.82527431e-005]
 [ 1.11565178e-002  6.83678083e-003]
 [ 2.60554653e-004  1.93535058e-004]
 [ 5.30590591e-005  3.68458488e-005]
 [ 2.25893748e-002  1.36736180e-002]
 [ 5.02591290e-004  3.22953796e-004]
 [ 2.73419838e-003  1.64845906e-003]
 [ 1.50217606e-003  1.12995884e-003]
 [ 3.67209413e-003  2.29997651e-003]
 [ 1.93508215e-004  1.70317406e-004]
 [ 1.44667597e-001  9.89535587e-002]
 [ 1.88457717e-001  1.66551236e-001]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 9.61113736e-004  5.97180756e-004]
 [ 1.03673954e-002  9.22278606e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.24719344e-005  6.51643499e-006]
 [ 3.21576945e-004  2.02817931e-004]
 [ 7.59991386e-002  6.25448507e-002]
 [ 9.37427993e-002  8.30291719e-002]
 [ 7.67797028e-004  6.52553929e-004]
 [ 1.35519684e-002  9.33991149e-003]
 [ 1.09384013e-004  9.91231832e-005]
 [ 2.93837005e-004  1.93967173e-004]
 [ 4.07420396e-002  2.47255377e-002]
 [ 8.78516917e-002  7.81593027e-002]
 [ 1.24270279e-004  1.22856640e-004]
 [ 2.23947323e-002  1.99245336e-002]
 [ 9.40605515e-005  7.49067227e-005]
 [ 1.25166344e-003  7.65693043e-004]
 [ 4.72072859e-002  2.90155534e-002]
 [ 6.67752571e-005  4.47381310e-005]
 [ 4.19420002e-004  2.67360444e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 5.23015600e-005  5.05554449e-005]
 [ 2.36404439e-003  1.47921152e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 7.78691287e-005  7.15514526e-005]
 [ 3.15311968e-004  2.03164585e-004]
 [ 5.80445044e-005  2.78110781e-005]
 [ 1.22433545e-002  1.07078363e-002]
 [ 3.24956694e-003  2.88017868e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.29770054e-002  7.94774500e-003]
 [ 1.22665710e-002  1.09735421e-002]
 [ 4.92527317e-003  2.87823087e-003]
 [ 1.72237643e-002  1.51890176e-002]
 [ 7.29334560e-004  3.99094662e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 8.70880172e-005  5.26506304e-005]
 [ 3.16823913e-002  2.81207734e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.21120856e-004  1.86300576e-004]
 [ 6.82261693e-004  5.57976756e-004]
 [ 1.34918595e-002  1.20519254e-002]
 [ 2.43506168e-002  2.13524975e-002]
 [ 1.47855135e-001  1.31455228e-001]
 [ 7.72218926e-003  4.98153192e-003]
 [ 3.20757587e-003  2.82647474e-003]
 [ 4.41363661e-005  3.42537399e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 9.30598995e-003  7.11092573e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 5.13629069e-004  4.73354477e-004]
 [ 2.76011125e-003  2.46992001e-003]
 [ 9.24152374e-003  6.55960025e-003]
 [ 3.43318553e-003  3.00319217e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.08314781e-005  8.48585186e-006]
 [ 2.62711499e-005  1.45005490e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.21426983e-003  1.04437981e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 4.63142004e-004  4.17531784e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 4.75581805e-002  2.87780086e-002]
 [ 2.52146542e-003  1.65327068e-003]
 [ 7.50238935e-005  4.65843558e-005]
 [ 1.35996825e-002  9.75634332e-003]
 [ 6.20807989e-003  4.51481069e-003]
 [ 4.69135569e-002  2.79707013e-002]
 [ 5.72303393e-002  5.07753023e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 5.20685549e-004  5.02977962e-004]
 [ 7.71135929e-003  6.93340648e-003]
 [ 1.05393100e-003  9.68927527e-004]
 [ 8.39308900e-002  5.18510292e-002]
 [ 3.46310026e-002  3.01154007e-002]
 [ 8.04301664e-004  7.43831672e-004]
 [ 1.44894290e-002  1.18311315e-002]
 [ 2.38393155e-004  1.69030863e-004]
 [ 4.21926373e-003  2.54857142e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.35177629e-002  1.19932800e-002]
 [ 1.68916946e-001  1.47229315e-001]
 [ 6.96615360e-004  4.42927035e-004]
 [ 3.22514646e-003  2.48950237e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 6.57587186e-003  5.70700783e-003]
 [ 7.52951869e-005  6.87054154e-005]
 [ 1.03816115e-001  6.53496252e-002]
 [ 1.23935768e-003  1.09393998e-003]
 [ 3.43602237e-002  2.11532897e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.67449978e-004  1.57517590e-004]
 [ 1.71889930e-004  1.24592902e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.35493634e-002  2.08884254e-002]
 [ 2.27462147e-001  1.96068638e-001]
 [ 1.91470037e-002  1.17731104e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 5.70928382e-003  3.83965274e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.44819509e-002  1.29502652e-002]
 [ 1.17373668e-004  6.43163432e-005]
 [ 4.36493468e-005  1.91680705e-005]
 [ 1.26903013e-003  1.14856973e-003]
 [ 5.93116515e-003  3.60493231e-003]
 [ 3.01071133e-002  2.67717823e-002]
 [ 2.29490378e-002  2.06392542e-002]
 [ 7.35205696e-004  4.05586250e-004]
 [ 1.11106613e-003  6.61151150e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.01451130e-003  9.39888946e-004]
 [ 4.51067785e-004  4.11455444e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.96998949e-004  1.15988982e-004]
 [ 1.59922656e-002  9.83423345e-003]
 [ 8.90719785e-004  5.48441416e-004]
 [ 3.30371782e-004  2.05265165e-004]
 [ 6.27885414e-004  5.41496220e-004]
 [ 4.98396800e-003  3.05223344e-003]
 [ 8.00658016e-005  4.48862604e-005]
 [ 2.45317927e-003  1.41774796e-003]
 [ 7.82010633e-002  4.79231814e-002]
 [ 1.57889773e-002  9.54399743e-003]
 [ 4.57866201e-004  2.44407473e-004]
 [ 3.62190481e-003  3.17548809e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 3.61416757e-004  3.19602659e-004]
 [ 3.83715665e-005  2.78795205e-005]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.87242867e-002  1.72274319e-002]
 [ 3.82889030e-004  2.48788859e-004]
 [ 1.91005824e-002  1.71187158e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 5.62483469e-003  3.48443524e-003]
 [ 2.14876131e-003  1.89579159e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 2.48006907e-002  2.20101586e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.85757199e-001  1.63012480e-001]
 [ 6.77363571e-004  3.63451540e-004]
 [ 1.60253325e-002  1.42130644e-002]
 [ 1.59120152e-005  1.03201248e-005]
 [ 1.48131651e-001  1.10318772e-001]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.94903321e-003  1.70404894e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 5.23696380e-003  4.67756179e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.32196369e-004  1.17072840e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 3.98662998e-004  2.66230199e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 4.33440606e-003  2.58859775e-003]
 [ 1.65208358e-002  1.12114727e-002]
 [ 3.27356410e-002  2.83324935e-002]
 [ 2.80534259e-003  1.71257375e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.09845205e-002  6.73388636e-003]
 [ 2.97430887e-004  2.56128221e-004]
 [ 2.24613227e-003  2.06891724e-003]
 [ 4.86907050e-002  4.21842052e-002]
 [ 5.69506605e-003  5.03133229e-003]
 [ 1.17929619e-002  7.22045177e-003]
 [ 0.00000000e+000  0.00000000e+000]
 [ 1.18622211e-001  1.02086006e-001]
 [ 9.43973398e-002  6.36926544e-002]
 [ 3.85659235e-004  3.46407701e-004]
 [ 0.00000000e+000  0.00000000e+000]
 [ 4.55294621e-004  3.65214670e-004]
 [ 6.85576215e-003  4.80907644e-003]
 [ 4.18004522e-002  2.62813025e-002]
 [ 0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000]
 [ 3.64213455e-002  2.19266118e-002]
 [ 2.91007021e-005  2.49025339e-005]
 [ 0.00000000e+000  0.00000000e+000]]new version
Vocab: 500
num_tweets: 20000
density: 15
Traceback (most recent call last):
  File "generate_tables.py", line 452, in <module>
    main()
  File "generate_tables.py", line 398, in main
    create_data(vocab=vocab, seed=seed_arr[j])
  File "generate_tables.py", line 65, in create_data
    x, mu, _, alpha_0 = test_util.get_mu(num_tops, vocab, num_tweets, density, seed)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/test_util.py", line 51, in get_mu
    z = numpy.random.multinomial(1,theta[0],size = 1)
  File "mtrand.pyx", line 4235, in numpy.random.mtrand.RandomState.multinomial
  File "_common.pyx", line 359, in numpy.random._common.check_array_constraint
ValueError: pvals < 0, pvals > 1 or pvals contains NaNs
  File "generate_tables.py", line 190
    for i in range(num_tops)
                           ^
SyntaxError: invalid syntax
new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0791161060333252
PCA fit: 0.7589309215545654
PCA Transform: 0.307497501373291
[[0.67732286 0.62398094]
 [0.9994059  0.7636398 ]]
total iterations: 1552
TLDA fit: 615.7882535457611
PCA Reverse Transform: 0.0008637905120849609
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05616060352956342
Fit RMSE: 0.05825216170749198
 Test Against Ground Truth
[0.05791525]
[0.95382973]
Traceback (most recent call last):
  File "generate_tables.py", line 461, in <module>
    main()
  File "generate_tables.py", line 411, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 366, in gen_fit_0_20
    res2, accuracy = postprocess(factors_unwhitened, mu, x, vocab, num_tops, smoothing, True, alpha_0 = alpha_0)
  File "generate_tables.py", line 185, in postprocess
    if tl.sum(accuracy_fwd) >= tl.sum(accuracy_rev):
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/__init__.py", line 161, in inner
    return _get_backend_method(name)(*args, **kwargs)
  File "<__array_function__ internals>", line 5, in sum
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 2259, in sum
    return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 86, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
  File "cupy/_core/core.pyx", line 1273, in cupy._core.core.ndarray.__array__
TypeError: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.
new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.08175516128540039
PCA fit: 0.7596447467803955
PCA Transform: 0.31899261474609375
[[0.40163246 0.27358294]
 [0.08807798 0.08840285]]
Traceback (most recent call last):
  File "generate_tables.py", line 462, in <module>
    main()
  File "generate_tables.py", line 412, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 351, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 86, in fit
    step =  lr*cumulant_gradient(self.factors_, y, self.alpha_0,self.theta)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/cumulant_gradient.py", line 23, in cumulant_gradient
    gradient -= 3*(1 + alpha)*(2 + alpha)/(2*y.shape[0])*tl.dot(y.T, tl.dot(y, phi)**2)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/__init__.py", line 161, in inner
    return _get_backend_method(name)(*args, **kwargs)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cupy/linalg/_product.py", line 36, in dot
    return a.dot(b, out)
KeyboardInterrupt
new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.08250045776367188
PCA fit: 0.7344865798950195
PCA Transform: 0.3223278522491455
[[0.7467944  0.755178  ]
 [0.45220008 0.06315601]]
total iterations: 307
TLDA fit: 120.91644382476807
PCA Reverse Transform: 0.0008862018585205078
decenter with new strategy:
[-0.15218547  0.24264321]
decenter with old strategy:
[-0.1625465  -0.03220029]
Fit RMSE new decenter: 0.0564257484854616
Fit RMSE: 0.05864500560319067
 Test Against Ground Truth
[0.96270602]
[-0.00827205]
[0.64587957]
[0.78142628]
Traceback (most recent call last):
  File "generate_tables.py", line 462, in <module>
    main()
  File "generate_tables.py", line 412, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 367, in gen_fit_0_20
    res2, accuracy = postprocess(factors_unwhitened, mu, x, vocab, num_tops, smoothing, True, alpha_0 = alpha_0)
  File "generate_tables.py", line 186, in postprocess
    if tl.sum(cp.asnumpy(accuracy_fwd)) >= tl.sum(cp.asnumpy(accuracy_rev)):
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cupy/__init__.py", line 778, in asnumpy
    return _numpy.asarray(a, order=order)
  File "cupy/_core/core.pyx", line 1273, in cupy._core.core.ndarray.__array__
TypeError: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.
new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0814046859741211
PCA fit: 0.8026924133300781
PCA Transform: 0.32625722885131836
[[0.69590735 0.03523707]
 [0.11083741 0.4673118 ]]
total iterations: 773
TLDA fit: 309.0340597629547
Whitened factor: 
[[ 0.48131824 -0.47619152]
 [-0.15513276  0.30209675]]
PCA Reverse Transform: 0.0009989738464355469
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05668327934938105
Fit RMSE: 0.05619917437956902
 Test Against Ground Truth
[0.99010568]
[-0.01428759]
[0.91515522]
[0.31086703]
Traceback (most recent call last):
  File "generate_tables.py", line 464, in <module>
    main()
  File "generate_tables.py", line 414, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 369, in gen_fit_0_20
    res2, accuracy = postprocess(factors_unwhitened, mu, x, vocab, num_tops, smoothing, True, alpha_0 = alpha_0)
  File "generate_tables.py", line 186, in postprocess
    if cp.sum(accuracy_fwd) >= cp.sum(accuracy_rev):
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cupy/_math/sumprod.py", line 38, in sum
    return a.sum(axis, dtype, out, keepdims)
AttributeError: 'list' object has no attribute 'sum'
new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.08722352981567383
PCA fit: 0.7597348690032959
PCA Transform: 0.31697893142700195
Traceback (most recent call last):
  File "generate_tables.py", line 467, in <module>
    main()
  File "generate_tables.py", line 417, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 353, in gen_fit_0_20
    tlda = TLDA(num_tops, alpha_0, n_iter_train,n_iter_test ,batch_size_grad ,learning_rate,gamma_shape = 1.0, smoothing = smoothing,theta=theta_param, seed=seed)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 34, in __init__
    std_factors = (std/tl.sqrt(n_topic))**(1/order)
UnboundLocalError: local variable 'std' referenced before assignment
new version
Vocab: 500
num_tweets: 20000
density: 15
Traceback (most recent call last):
  File "generate_tables.py", line 467, in <module>
    main()
  File "generate_tables.py", line 413, in main
    create_data(vocab=vocab, seed=seed_arr[j])
  File "generate_tables.py", line 65, in create_data
    x, mu, _, alpha_0 = test_util.get_mu(num_tops, vocab, num_tweets, density, seed)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/test_util.py", line 64, in get_mu
    if w[0][k] == 1:
KeyboardInterrupt
new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.08484888076782227
PCA fit: 0.7800612449645996
PCA Transform: 0.30980849266052246
[[0.98 0.02]
 [0.02 0.98]]
total iterations: 5256
TLDA fit: 216.1491572856903
Whitened factor: 
[[ 0.11368032  0.1482005 ]
 [-0.59322138  0.59331309]]
PCA Reverse Transform: 0.0009312629699707031
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05615222848620479
Fit RMSE: 0.05541043270085821
 Test Against Ground Truth
[0.92063043]
[0.35352096]
[0.99434964]
[-0.01545075]
[(' decentering', 0.004733085632324219), (' smoothing and normalization', 0.004796266555786133)]
Smoothing and Normalization: 0.0005104541778564453
Fit RMSE: 0.0553016862483632
sklearn Test Against Ground Truth
[0.99998573]
[0.99998867]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.0057010650634765625
PCA fit: 0.7410876750946045
PCA Transform: 0.009650230407714844
[[0.98 0.02]
 [0.02 0.98]]
total iterations: 15001
TLDA fit: 643.7381861209869
Whitened factor: 
[[0.16195629 0.27421608]
 [0.20715965 0.35079547]]
PCA Reverse Transform: 0.00015211105346679688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04081338933535562
Fit RMSE: 0.04196099441258766
 Test Against Ground Truth
[0.23019662]
[0.92878313]
[0.92176198]
[0.04886602]
[(' decentering', 0.0011844635009765625), (' smoothing and normalization', 0.0004184246063232422)]
Smoothing and Normalization: 0.0005099773406982422
Fit RMSE: 0.03969982947786217
sklearn Test Against Ground Truth
[0.99996814]
[0.99996617]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008383989334106445
PCA fit: 1.4794254302978516
PCA Transform: 0.01560068130493164
[[0.98000001 0.01999999]
 [0.01999999 0.98000001]]
total iterations: 4051
TLDA fit: 171.45504808425903
Whitened factor: 
[[ 0.13099278  0.15333848]
 [-0.56642318  0.56819706]]
PCA Reverse Transform: 0.00016450881958007812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03315523204520604
Fit RMSE: 0.03280282395568591
 Test Against Ground Truth
[0.94286116]
[0.2765593]
[0.99793275]
[-0.00861099]
[(' decentering', 0.0012097358703613281), (' smoothing and normalization', 0.00042366981506347656)]
Smoothing and Normalization: 0.0004894733428955078
Fit RMSE: 0.03275992805031502
sklearn Test Against Ground Truth
[0.99996033]
[0.99994147]
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011359453201293945
PCA fit: 2.6746749877929688
PCA Transform: 0.01873946189880371
[[0.98 0.02]
 [0.02 0.98]]
total iterations: 15001
TLDA fit: 623.7830183506012
Whitened factor: 
[[0.20830464 0.3028624 ]
 [0.23614833 0.34334868]]
PCA Reverse Transform: 0.00015163421630859375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028988072969379816
Fit RMSE: 0.029825543025747775
 Test Against Ground Truth
[0.1102753]
[0.9348096]
[0.91963188]
[0.01264882]
[(' decentering', 0.0012383460998535156), (' smoothing and normalization', 0.00040984153747558594)]
Smoothing and Normalization: 0.0004642009735107422
Fit RMSE: 0.028265260548696484
sklearn Test Against Ground Truth
[0.99995475]
[0.99990372]
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014210224151611328
PCA fit: 4.709536790847778
PCA Transform: 0.02454090118408203
[[0.98 0.02]
 [0.02 0.98]]
total iterations: 520
TLDA fit: 21.493600845336914
Whitened factor: 
[[ 0.2539625  -0.24558688]
 [ 0.14034958  0.36771671]]
PCA Reverse Transform: 0.00016188621520996094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02620451444013647
Fit RMSE: 0.02659770986024158
 Test Against Ground Truth
[0.98600381]
[-0.00861163]
[0.34891711]
[0.84687951]
[(' decentering', 0.00135040283203125), (' smoothing and normalization', 0.0004215240478515625)]
Smoothing and Normalization: 0.0005028247833251953
Fit RMSE: 0.025653417022466894
sklearn Test Against Ground Truth
[0.99995588]
[0.99986904]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0028672218322753906
PCA fit: 0.28072404861450195
PCA Transform: 0.00511932373046875
[[0.98 0.02]
 [0.02 0.98]]
total iterations: 283
TLDA fit: 12.141286373138428
Whitened factor: 
[[0.24172257 0.24081973]
 [0.23584275 0.23496246]]
PCA Reverse Transform: 0.00015282630920410156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05894228682018112
Fit RMSE: 0.05933632712571941
 Test Against Ground Truth
[0.97957072]
[0.06134648]
[0.07450424]
[0.98012087]
[(' decentering', 0.0011892318725585938), (' smoothing and normalization', 0.0004229545593261719)]
Smoothing and Normalization: 0.0004913806915283203
Fit RMSE: 0.05610984742957494
sklearn Test Against Ground Truth
[0.99997951]
[0.9999971]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.0057218074798583984
PCA fit: 0.7319481372833252
PCA Transform: 0.009603023529052734
[[0.97999999 0.02000001]
 [0.02000001 0.97999999]]
total iterations: 3358
TLDA fit: 142.10119080543518
Whitened factor: 
[[ 0.1178349   0.18005029]
 [-0.55506752  0.57095234]]
PCA Reverse Transform: 0.0001556873321533203
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04003346656867203
Fit RMSE: 0.039496722218499085
 Test Against Ground Truth
[0.91335933]
[0.32656182]
[0.99861439]
[-0.01264065]
[(' decentering', 0.0011925697326660156), (' smoothing and normalization', 0.0004162788391113281)]
Smoothing and Normalization: 0.0004973411560058594
Fit RMSE: 0.03940315741529488
sklearn Test Against Ground Truth
[0.99997179]
[0.99995618]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008329153060913086
PCA fit: 1.5171303749084473
PCA Transform: 0.014483451843261719
[[0.97999999 0.02000001]
 [0.02000001 0.97999999]]
total iterations: 15001
TLDA fit: 648.4398319721222
Whitened factor: 
[[0.24796081 0.2669475 ]
 [0.2454868  0.26427286]]
PCA Reverse Transform: 0.00016355514526367188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03472807804509934
Fit RMSE: 0.034943377591254085
 Test Against Ground Truth
[0.97314102]
[0.04270699]
[0.04831131]
[0.97327886]
[(' decentering', 0.001234292984008789), (' smoothing and normalization', 0.00043463706970214844)]
Smoothing and Normalization: 0.0004913806915283203
Fit RMSE: 0.033508585909147244
sklearn Test Against Ground Truth
[0.99996184]
[0.99997335]
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011565208435058594
PCA fit: 2.6538748741149902
PCA Transform: 0.01882791519165039
[[0.98 0.02]
 [0.02 0.98]]
total iterations: 11522
TLDA fit: 481.89365005493164
Whitened factor: 
[[ 0.55806503 -0.53863755]
 [-0.39649056  0.51207785]]
PCA Reverse Transform: 0.00016379356384277344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02857514497607579
Fit RMSE: 0.02859288576368719
 Test Against Ground Truth
[0.99364109]
[-0.00376947]
[0.92264773]
[0.05866635]
[(' decentering', 0.0012593269348144531), (' smoothing and normalization', 0.0004570484161376953)]
Smoothing and Normalization: 0.00047278404235839844
Fit RMSE: 0.028569314524612664
sklearn Test Against Ground Truth
[0.99993648]
[0.99995521]
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014230728149414062
PCA fit: 2.6416664123535156
PCA Transform: 0.024497509002685547
[[0.98 0.02]
 [0.02 0.98]]
total iterations: 5995
TLDA fit: 255.26828932762146
Whitened factor: 
[[ 0.31337057 -0.10253956]
 [-0.31353865  0.45730632]]
PCA Reverse Transform: 0.0001678466796875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025892114819971522
Fit RMSE: 0.025894679829109014
 Test Against Ground Truth
[0.98256132]
[0.10965802]
[0.87622597]
[0.32792419]
[(' decentering', 0.0013518333435058594), (' smoothing and normalization', 0.0004220008850097656)]
Smoothing and Normalization: 0.0005064010620117188
Fit RMSE: 0.025862988075918462
sklearn Test Against Ground Truth
[0.99990232]
[0.99996827]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0027692317962646484new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0850076675415039
PCA fit: 0.7446181774139404
PCA Transform: 0.30929994583129883
[[9.96977350e-01 5.50751611e-06]
 [1.98846203e-04 5.03149750e+02]]
total iterations: 101
TLDA fit: 4.003007650375366
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0008902549743652344
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.044721359549995794
Fit RMSE: 0.044721359549995794
 Test Against Ground Truth
[nan]
[nan]
[nan]
[nan]
Traceback (most recent call last):
  File "generate_tables.py", line 467, in <module>
    main()
  File "generate_tables.py", line 417, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 372, in gen_fit_0_20
    res2, accuracy = postprocess(factors_unwhitened, mu, x, vocab, num_tops, smoothing, True, alpha_0 = alpha_0)
  File "generate_tables.py", line 190, in postprocess
    permutation_fac2[1] = [(x + 1)%2 for x in permutation_fac2[1]]
TypeError: 'tuple' object does not support item assignment
new version
Vocab: 500
num_tweets: 20000
density: 15
Traceback (most recent call last):
  File "generate_tables.py", line 467, in <module>
    main()
  File "generate_tables.py", line 413, in main
    create_data(vocab=vocab, seed=seed_arr[j])
  File "generate_tables.py", line 65, in create_data
    x, mu, _, alpha_0 = test_util.get_mu(num_tops, vocab, num_tweets, density, seed)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/test_util.py", line 51, in get_mu
    z = numpy.random.multinomial(1,theta[0],size = 1)
  File "mtrand.pyx", line 4235, in numpy.random.mtrand.RandomState.multinomial
  File "_common.pyx", line 359, in numpy.random._common.check_array_constraint
ValueError: pvals < 0, pvals > 1 or pvals contains NaNs
new version
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.08125495910644531
PCA fit: 0.765740156173706
PCA Transform: 0.3179619312286377
[[0.98016103 0.01891925]
 [0.02046981 0.97990723]]
total iterations: 101
TLDA fit: 4.098877906799316
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0008795261383056641
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.044721359549995794
Fit RMSE: 0.044721359549995794
 Test Against Ground Truth
[nan]
[nan]
[nan]
[nan]
Traceback (most recent call last):
  File "generate_tables.py", line 467, in <module>
    main()
  File "generate_tables.py", line 417, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 372, in gen_fit_0_20
    res2, accuracy = postprocess(factors_unwhitened, mu, x, vocab, num_tops, smoothing, True, alpha_0 = alpha_0)
  File "generate_tables.py", line 190, in postprocess
    permutation_fac2[1] = [(x + 1)%2 for x in permutation_fac2[1]]
TypeError: 'tuple' object does not support item assignment
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07751274108886719
PCA fit: 0.5403470993041992
PCA Transform: 0.30923986434936523
[[0.98483698 0.01464534]
 [0.01016301 0.98386711]]
total iterations: 14749
TLDA fit: 574.3330948352814
Whitened factor: 
[[ 0.67747374 -0.66195359]
 [-0.42621889  0.51393879]]
PCA Reverse Transform: 0.0009450912475585938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11798713009408156
Fit RMSE: 0.11742198438355553
 Test Against Ground Truth
[0.99998715]
[-0.02759232]
[0.96125122]
[-0.02319676]
[(' decentering', 0.0036776065826416016), (' smoothing and normalization', 0.0037255287170410156)]
Smoothing and Normalization: 0.00047087669372558594
Fit RMSE: 0.116942141375899
sklearn Test Against Ground Truth
[0.99999942]
[0.9999783]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0032854080200195312
PCA fit: 0.2590517997741699
PCA Transform: 0.005199432373046875
[[0.98433715 0.02417247]
 [0.02478818 0.98884041]]
total iterations: 30000
TLDA fit: 1253.1003131866455
Whitened factor: 
[[0.08885225 0.29164204]
 [0.11527859 0.37825168]]
PCA Reverse Transform: 0.00015592575073242188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05824380092131249
Fit RMSE: 0.05887941001752609
 Test Against Ground Truth
[0.35341698]
[0.87220428]
[0.84668607]
[0.14730423]
[(' decentering', 0.0011687278747558594), (' smoothing and normalization', 0.0017375946044921875)]
Smoothing and Normalization: 0.0005474090576171875
Fit RMSE: 0.05527191209427384
sklearn Test Against Ground Truth
[0.99996721]
[0.99996156]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006487369537353516
PCA fit: 0.7353675365447998
PCA Transform: 0.010580778121948242
[[0.98182586 0.01441416]
 [0.01857664 0.98391026]]
total iterations: 8054
TLDA fit: 348.59717535972595
Whitened factor: 
[[ 0.06310418  0.16389601]
 [-0.58430868  0.61682884]]
PCA Reverse Transform: 0.00015497207641601562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03806200568985853
Fit RMSE: 0.039134435551048084
 Test Against Ground Truth
[0.82405366]
[0.52180884]
[0.99098924]
[-0.00236292]
[(' decentering', 0.0012156963348388672), (' smoothing and normalization', 0.00041413307189941406)]
Smoothing and Normalization: 0.0005121231079101562
Fit RMSE: 0.0389026905091685
sklearn Test Against Ground Truth
[0.99994227]
[0.99996033]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.009068965911865234
PCA fit: 1.4639103412628174
PCA Transform: 0.014534473419189453
[[0.98635803 0.01692495]
 [0.01062339 0.98257183]]
total iterations: 30000
TLDA fit: 1283.363874912262
Whitened factor: 
[[ 0.43404829 -0.43131308]
 [-0.26987963  0.39151417]]
PCA Reverse Transform: 0.00015926361083984375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03269871288135409
Fit RMSE: 0.03289037901541538
 Test Against Ground Truth
[0.980694]
[-0.01061815]
[0.94973806]
[0.08393244]
[(' decentering', 0.0012383460998535156), (' smoothing and normalization', 0.0004124641418457031)]
Smoothing and Normalization: 0.00048089027404785156
Fit RMSE: 0.032817623085943264
sklearn Test Against Ground Truth
[0.99994917]
[0.99994637]
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011754751205444336
PCA fit: 2.681514263153076
PCA Transform: 0.018867969512939453
[[0.9819381  0.01632734]
 [0.0195564  0.98389965]]
total iterations: 30000
TLDA fit: 1294.8161725997925
Whitened factor: 
[[0.21481094 0.26586436]
 [0.26075528 0.32258813]]
PCA Reverse Transform: 0.00015664100646972656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02935203957356675
Fit RMSE: 0.029965606575674354
 Test Against Ground Truth
[0.13968133]
[0.9487137]
[0.93986307]
[0.0230932]
[(' decentering', 0.0012710094451904297), (' smoothing and normalization', 0.00041031837463378906)]
Smoothing and Normalization: 0.0005056858062744141
Fit RMSE: 0.028589013791599716
sklearn Test Against Ground Truth
[0.99994466]
[0.99994742]
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014603853225708008
PCA fit: 2.6434202194213867
PCA Transform: 0.024698972702026367
[[0.98271603 0.01783562]
 [0.02008105 0.96893122]]
total iterations: 1001
TLDA fit: 43.184627056121826
Whitened factor: 
[[ 0.2408884  -0.23634161]
 [ 0.35546767  0.27534013]]
PCA Reverse Transform: 0.00016379356384277344
decenter with new strategy:
[ 0.00233383 -0.01767865]
decenter with old strategy:
[-0.00220006 -0.00362926]
Fit RMSE new decenter: 0.025786060030291665
Fit RMSE: 0.026434534303924676
 Test Against Ground Truth
[0.06451989]
[0.98087303]
[0.95986043]
[-0.00503525]
[(' decentering', 0.0013430118560791016), (' smoothing and normalization', 0.0004279613494873047)]
Smoothing and Normalization: 0.0005071163177490234
Fit RMSE: 0.0250089751717875
sklearn Test Against Ground Truth
[0.99990047]
[0.99987219]
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013802051544189453
PCA fit: 0.04657888412475586
PCA Transform: 0.0014526844024658203
[[0.97796027 0.02255931]
 [0.01990397 0.98575199]]
total iterations: 30000
TLDA fit: 1303.4074504375458
Whitened factor: 
[[ 0.61035098 -0.60433099]
 [-0.27851984  0.36285849]]
PCA Reverse Transform: 0.00015234947204589844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11391806804810643
Fit RMSE: 0.11603067836716428
 Test Against Ground Truth
[0.99538604]
[-0.03690654]
[0.97753825]
[0.0354353]
[(' decentering', 0.0011560916900634766), (' smoothing and normalization', 0.0003635883331298828)]
Smoothing and Normalization: 0.0004723072052001953
Fit RMSE: 0.11586684614040793
sklearn Test Against Ground Truth
[0.99999441]
[0.99999731]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0019462108612060547
PCA fit: 0.2671494483947754
PCA Transform: 0.005209207534790039
[[0.98362001 0.01598376]
 [0.01834569 0.98292824]]
total iterations: 17037
TLDA fit: 743.0440855026245
Whitened factor: 
[[ 0.70719576 -0.69190004]
 [-0.43456177  0.51098352]]
PCA Reverse Transform: 0.00014925003051757812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05498815467710009
Fit RMSE: 0.05567781330348419
 Test Against Ground Truth
[0.99972656]
[-0.01485262]
[0.94536821]
[-0.01406626]
[(' decentering', 0.0011806488037109375), (' smoothing and normalization', 0.00041604042053222656)]
Smoothing and Normalization: 0.0005238056182861328
Fit RMSE: 0.05555124874290224
sklearn Test Against Ground Truth
[0.99997731]
[0.99996287]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.004530668258666992
PCA fit: 0.7345860004425049
PCA Transform: 0.009559869766235352
[[0.97993094 0.01817608]
 [0.01932079 0.98374872]]
total iterations: 3503
TLDA fit: 153.5115156173706
Whitened factor: 
[[ 0.12002467  0.17560997]
 [-0.5651681   0.55677051]]
PCA Reverse Transform: 0.00014925003051757812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03847995901532598
Fit RMSE: 0.040006617038930006
 Test Against Ground Truth
[0.94194484]
[0.25563965]
[0.99899791]
[-0.01060529]
[(' decentering', 0.001201629638671875), (' smoothing and normalization', 0.0004096031188964844)]
Smoothing and Normalization: 0.0005178451538085938
Fit RMSE: 0.03992707387063298
sklearn Test Against Ground Truth
[0.99994835]
[0.99999238]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.006596565246582031
PCA fit: 1.4888849258422852
PCA Transform: 0.015623092651367188
[[0.97762536 0.01516684]
 [0.02356371 0.97519715]]
total iterations: 19157
TLDA fit: 828.4138789176941
Whitened factor: 
[[ 0.52091653 -0.48473923]
 [-0.4402129   0.51804832]]
PCA Reverse Transform: 0.00015473365783691406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03316189176802348
Fit RMSE: 0.033188749143533064
 Test Against Ground Truth
[0.96430404]
[-0.0082393]
[0.93228744]
[0.20276634]
[(' decentering', 0.001275777816772461), (' smoothing and normalization', 0.0004277229309082031)]
Smoothing and Normalization: 0.0004897117614746094
Fit RMSE: 0.03317254395220838
sklearn Test Against Ground Truth
[0.99997203]
[0.99995336]
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.00994110107421875
PCA fit: 2.623652219772339
PCA Transform: 0.01990056037902832
[[0.97409664 0.01346853]
 [0.02038278 0.97746066]]
total iterations: 30000
TLDA fit: 1317.7202315330505
Whitened factor: 
[[0.17787106 0.2194466 ]
 [0.23398991 0.28865215]]
PCA Reverse Transform: 0.00016832351684570312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0294664216881176
Fit RMSE: 0.030025605579878167
 Test Against Ground Truth
[0.20910527]
[0.94661686]
[0.95134136]
[0.03539002]
[(' decentering', 0.0012624263763427734), (' smoothing and normalization', 0.0004169940948486328)]
Smoothing and Normalization: 0.0005218982696533203
Fit RMSE: 0.028747156646449952
sklearn Test Against Ground Truth
[0.99996215]
[0.99993812]
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.012230634689331055
PCA fit: 4.629165410995483
PCA Transform: 0.024657726287841797
[[0.98261787 0.01481672]
 [0.02828272 0.98002745]]
total iterations: 30000
TLDA fit: 1327.1445779800415
Whitened factor: 
[[0.25436227 0.27792217]
 [0.30049467 0.328332  ]]
PCA Reverse Transform: 0.00016808509826660156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025829242340404318
Fit RMSE: 0.026571811921070772
 Test Against Ground Truth
[0.06133358]
[0.94504935]
[0.92538258]
[-0.00870129]
[(' decentering', 0.0013527870178222656), (' smoothing and normalization', 0.00042939186096191406)]
Smoothing and Normalization: 0.00054168701171875
Fit RMSE: 0.02508666718359831
sklearn Test Against Ground Truth
[0.99990741]
[0.99990088]
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014460086822509766
PCA fit: 0.041919708251953125
PCA Transform: 0.0014519691467285156
[[0.97718006 0.01703651]
 [0.01514718 0.985004  ]]
total iterations: 16648
TLDA fit: 730.5884997844696
Whitened factor: 
[[ 0.64226584 -0.61974354]
 [-0.50136926  0.58475552]]
PCA Reverse Transform: 0.0001556873321533203
decenter with new strategy:
[ 2.33256420e-04 -7.83215264e-05]
decenter with old strategy:
[ 2.89582584e-05 -3.21893893e-05]
Fit RMSE new decenter: 0.1242087900149856
Fit RMSE: 0.12403605898921799
 Test Against Ground Truth
[0.99993363]
[-0.01618796]
[0.96930305]
[-0.01769108]
[(' decentering', 0.0011720657348632812), (' smoothing and normalization', 0.0003802776336669922)]
Smoothing and Normalization: 0.0004851818084716797
Fit RMSE: 0.12384757320989488
sklearn Test Against Ground Truth
[0.99999895]
[0.99999431]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.002708911895751953
PCA fit: 0.263080358505249
PCA Transform: 0.005201578140258789
[[0.97491125 0.01591879]
 [0.01867821 0.98486923]]
total iterations: 17569
TLDA fit: 770.0356771945953
Whitened factor: 
[[ 0.68687845 -0.67564294]
 [-0.42027354  0.50598227]]
PCA Reverse Transform: 0.0001571178436279297
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055469403948407305
Fit RMSE: 0.056115163630126225
 Test Against Ground Truth
[0.99892277]
[-0.01361816]
[0.94578048]
[-0.01208587]
[(' decentering', 0.0011851787567138672), (' smoothing and normalization', 0.0004248619079589844)]
Smoothing and Normalization: 0.0005316734313964844
Fit RMSE: 0.0559673340369881
sklearn Test Against Ground Truth
[0.99997114]
[0.99998486]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.00444340705871582
PCA fit: 0.7526767253875732
PCA Transform: 0.00955820083618164
[[0.98685566 0.01573595]
 [0.01573822 0.98687809]]
total iterations: 30000
TLDA fit: 1337.9399955272675
Whitened factor: 
[[0.23635518 0.28049469]
 [0.25067307 0.29744115]]
PCA Reverse Transform: 0.00015687942504882812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04157705085534286
Fit RMSE: 0.04241812191848336
 Test Against Ground Truth
[0.06628166]
[0.96765043]
[0.96567341]
[0.02101556]
[(' decentering', 0.0012099742889404297), (' smoothing and normalization', 0.0004150867462158203)]
Smoothing and Normalization: 0.0005495548248291016
Fit RMSE: 0.04036368149017641
sklearn Test Against Ground Truth
[0.9999715]
[0.99997413]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.006449699401855469
PCA fit: 1.4881691932678223
PCA Transform: 0.015591144561767578
[[0.9877568  0.02501187]
 [0.02313044 0.9826508 ]]
total iterations: 5418
TLDA fit: 237.9641306400299
Whitened factor: 
[[ 0.11329211  0.15012569]
 [-0.59887332  0.60353525]]
PCA Reverse Transform: 0.00015735626220703125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03152557777438214
Fit RMSE: 0.032562932264914954
 Test Against Ground Truth
[0.91129096]
[0.36508753]
[0.99506586]
[-0.01168628]
[(' decentering', 0.001234292984008789), (' smoothing and normalization', 0.0004425048828125)]
Smoothing and Normalization: 0.0005130767822265625
Fit RMSE: 0.032523660374401936
sklearn Test Against Ground Truth
[0.99992973]
[0.99992634]
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.00971364974975586
PCA fit: 2.678274154663086
PCA Transform: 0.019950389862060547
[[0.97943177 0.02107739]
 [0.01811994 0.98742932]]
total iterations: 3982
TLDA fit: 173.60855293273926
Whitened factor: 
[[ 0.1154767   0.16469087]
 [-0.56779386  0.59878717]]
PCA Reverse Transform: 0.00016450881958007812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.027401092919657696
Fit RMSE: 0.028346287200103595
 Test Against Ground Truth
[0.92098982]
[0.31327366]
[0.99742342]
[-0.01066564]
[(' decentering', 0.0012695789337158203), (' smoothing and normalization', 0.0004184246063232422)]
Smoothing and Normalization: 0.0006530284881591797
Fit RMSE: 0.028296588325405853
sklearn Test Against Ground Truth
[0.99996894]
[0.99988615]
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.011972188949584961
PCA fit: 4.664241552352905
PCA Transform: 0.024684429168701172
[[0.98344327 0.01447493]
 [0.01949002 0.98233891]]
total iterations: 18452
TLDA fit: 815.1229665279388
Whitened factor: 
[[ 0.44466369 -0.43817014]
 [-0.24744227  0.41246605]]
PCA Reverse Transform: 0.0001652240753173828
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025330861627035815
Fit RMSE: 0.025539682802970556
 Test Against Ground Truth
[0.98543672]
[-0.0062193]
[0.92792519]
[0.10821395]
[(' decentering', 0.0013554096221923828), (' smoothing and normalization', 0.00043010711669921875)]
Smoothing and Normalization: 0.0005273818969726562
Fit RMSE: 0.025504183204440978
sklearn Test Against Ground Truth
[0.99994447]
[0.99991606]
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.001352071762084961
PCA fit: 0.04268622398376465
PCA Transform: 0.001455545425415039
[[0.98467823 0.03026523]
 [0.01807962 0.98006259]]
total iterations: 18269
TLDA fit: 784.1039667129517
Whitened factor: 
[[ 0.68212688 -0.66779746]
 [-0.4472383   0.52623321]]
PCA Reverse Transform: 0.0001494884490966797
decenter with new strategy:
[-2.14489571  1.18729004]
decenter with old strategy:
[-0.29709089  0.19956755]
Fit RMSE new decenter: 0.12330933252313736
Fit RMSE: 0.12324929152130558
 Test Against Ground Truth
[0.99982245]
[-0.0163875]
[0.95858381]
[-0.00627824]
[(' decentering', 0.0011723041534423828), (' smoothing and normalization', 0.0003769397735595703)]
Smoothing and Normalization: 0.0004723072052001953
Fit RMSE: 0.12326075370797265
sklearn Test Against Ground Truth
[0.99999136]
[0.99999678]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.002679109573364258
PCA fit: 0.27199316024780273
PCA Transform: 0.005227088928222656
[[0.9807184  0.01572618]
 [0.01593459 0.97956636]]
total iterations: 25205
TLDA fit: 1103.6679599285126
Whitened factor: 
[[ 0.66522665 -0.65628396]
 [-0.32406789  0.40945459]]
PCA Reverse Transform: 0.0001552104949951172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05773108251053324
Fit RMSE: 0.05807895779051049
 Test Against Ground Truth
[0.99853172]
[-0.00678077]
[0.97768147]
[0.00644699]
[(' decentering', 0.0011887550354003906), (' smoothing and normalization', 0.00043964385986328125)]
Smoothing and Normalization: 0.0005195140838623047
Fit RMSE: 0.058004259517218465
sklearn Test Against Ground Truth
[0.99999692]
[0.99999097]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.00426793098449707
PCA fit: 0.7458758354187012
PCA Transform: 0.009575605392456055
[[0.98798161 0.01369167]
 [0.01401605 0.97896195]]
total iterations: 30000
TLDA fit: 1309.0446395874023
Whitened factor: 
[[0.0989983  0.30228148]
 [0.12350684 0.37754483]]
PCA Reverse Transform: 0.00017118453979492188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04205770275574182
Fit RMSE: 0.04233633821695494
 Test Against Ground Truth
[0.35742686]
[0.86318138]
[0.84486915]
[0.16708294]
[(' decentering', 0.0012333393096923828), (' smoothing and normalization', 0.00042176246643066406)]
Smoothing and Normalization: 0.0005097389221191406
Fit RMSE: 0.0403125302114843
sklearn Test Against Ground Truth
[0.99997652]
[0.99996914]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.0066986083984375
PCA fit: 1.4880568981170654
PCA Transform: 0.015604734420776367
[[0.97696288 0.01607744]
 [0.01183007 0.98418448]]
total iterations: 30000
TLDA fit: 1317.592577457428
Whitened factor: 
[[0.17300686 0.2715399 ]
 [0.22489603 0.35304923]]
PCA Reverse Transform: 0.00016164779663085938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03377726189722754
Fit RMSE: 0.03450746774547279
 Test Against Ground Truth
[0.22057157]
[0.92981825]
[0.91819324]
[0.03213182]
[(' decentering', 0.0012476444244384766), (' smoothing and normalization', 0.0004291534423828125)]
Smoothing and Normalization: 0.0005333423614501953
Fit RMSE: 0.032887878037023796
sklearn Test Against Ground Truth
[0.99996675]
[0.99996524]
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.009716272354125977
PCA fit: 2.6459572315216064
PCA Transform: 0.02001476287841797
[[0.9807872  0.01411318]
 [0.01481411 0.98203828]]
total iterations: 30000
TLDA fit: 1336.5473477840424
Whitened factor: 
[[0.22883678 0.27766308]
 [0.28130427 0.34130101]]
PCA Reverse Transform: 0.00015497207641601562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028955383905722085
Fit RMSE: 0.029706038445935213
 Test Against Ground Truth
[0.12138387]
[0.94599059]
[0.93063267]
[-0.00088822]
[(' decentering', 0.0012705326080322266), (' smoothing and normalization', 0.00041937828063964844)]
Smoothing and Normalization: 0.0004949569702148438
Fit RMSE: 0.028088852485110276
sklearn Test Against Ground Truth
[0.99991218]
[0.99992019]
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.012323617935180664
PCA fit: 4.665399551391602
PCA Transform: 0.02566695213317871
[[0.98388858 0.01390468]
 [0.02388645 0.98245624]]
total iterations: 2654
TLDA fit: 117.06493973731995
Whitened factor: 
[[ 0.12873173  0.18348284]
 [-0.53859862  0.57174968]]
PCA Reverse Transform: 0.0001621246337890625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.024521007676608364
Fit RMSE: 0.025377584514630035
 Test Against Ground Truth
[0.92601366]
[0.30035279]
[0.99259891]
[-0.00992932]
[(' decentering', 0.0013418197631835938), (' smoothing and normalization', 0.0004239082336425781)]
Smoothing and Normalization: 0.0005199909210205078
Fit RMSE: 0.025341986862317027
sklearn Test Against Ground Truth
[0.99994913]
[0.99993487]
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0015287399291992188
PCA fit: 0.04721379280090332
PCA Transform: 0.0014851093292236328
[[0.98187088 0.0220061 ]
 [0.02027499 0.97547833]]
total iterations: 18877
TLDA fit: 831.0205762386322
Whitened factor: 
[[ 0.6954164  -0.67868223]
 [-0.464549    0.54470465]]
PCA Reverse Transform: 0.00016379356384277344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11961058283106389
Fit RMSE: 0.12009744322939274
 Test Against Ground Truth
[0.99969059]
[-0.02351371]
[0.95616576]
[-0.02338629]
[(' decentering', 0.001203298568725586), (' smoothing and normalization', 0.0003905296325683594)]
Smoothing and Normalization: 0.0005040168762207031
Fit RMSE: 0.11964769471250837
sklearn Test Against Ground Truth
[0.99999852]
[0.99999558]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0017080307006835938
PCA fit: 0.2701730728149414
PCA Transform: 0.005681514739990234
[[0.97926703 0.01705819]
 [0.01561861 0.98613998]]
total iterations: 30000
TLDA fit: 1325.188892364502
Whitened factor: 
[[0.1345662  0.30046491]
 [0.16089107 0.35927269]]
PCA Reverse Transform: 0.00015687942504882812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05803548604556289
Fit RMSE: 0.058849652581995864
 Test Against Ground Truth
[0.23443453]
[0.91526204]
[0.90160586]
[0.083011]
[(' decentering', 0.0011887550354003906), (' smoothing and normalization', 0.00041294097900390625)]
Smoothing and Normalization: 0.0004978179931640625
Fit RMSE: 0.0551322184038044
sklearn Test Against Ground Truth
[0.99996769]
[0.99998974]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.004507780075073242
PCA fit: 0.753659725189209
PCA Transform: 0.009578704833984375
[[0.98446047 0.01657006]
 [0.01916893 0.98397161]]
total iterations: 30000
TLDA fit: 1329.5127527713776
Whitened factor: 
[[0.15260609 0.28487582]
 [0.1902316  0.35533746]]
PCA Reverse Transform: 0.0001571178436279297
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04164789717733621
Fit RMSE: 0.04248552566196412
 Test Against Ground Truth
[0.2462267]
[0.92336747]
[0.91193745]
[0.03915448]
[(' decentering', 0.0012247562408447266), (' smoothing and normalization', 0.00041747093200683594)]
Smoothing and Normalization: 0.0005431175231933594
Fit RMSE: 0.04068602219656936
sklearn Test Against Ground Truth
[0.99997104]
[0.99996643]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.004914283752441406
PCA fit: 1.490455150604248
PCA Transform: 0.01564645767211914
[[0.98002809 0.02052528]
 [0.02460375 0.97910923]]
total iterations: 30000
TLDA fit: 1312.2357552051544
Whitened factor: 
[[0.12978992 0.26932344]
 [0.19329959 0.40084505]]
PCA Reverse Transform: 0.00016236305236816406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03347886466527736
Fit RMSE: 0.034219483126990566
 Test Against Ground Truth
[0.36399852]
[0.87762125]
[0.85203959]
[0.03941477]
[(' decentering', 0.001233816146850586), (' smoothing and normalization', 0.0004165172576904297)]
Smoothing and Normalization: 0.000553131103515625
Fit RMSE: 0.03252955025776837
sklearn Test Against Ground Truth
[0.99994376]
[0.99992934]
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.0077669620513916016
PCA fit: 1.6947674751281738
PCA Transform: 0.019977569580078125
[[0.98107224 0.013433  ]
 [0.01425656 0.98207365]]
total iterations: 30000
TLDA fit: 1323.8954071998596
Whitened factor: 
[[0.23287171 0.2826346 ]
 [0.26824601 0.3254751 ]]
PCA Reverse Transform: 0.00015735626220703125
decenter with new strategy:
[ 0.02543903 -0.02827064]
decenter with old strategy:
[-0.02447206 -0.02819531]
Fit RMSE new decenter: 0.02922866742735283
Fit RMSE: 0.029918703891650824
 Test Against Ground Truth
[0.13056539]
[0.94430198]
[0.92492673]
[0.02133372]
[(' decentering', 0.0012707710266113281), (' smoothing and normalization', 0.0004355907440185547)]
Smoothing and Normalization: 0.0005130767822265625
Fit RMSE: 0.028525142985881016
sklearn Test Against Ground Truth
[0.99993722]
[0.99991417]
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.012749433517456055
PCA fit: 4.640334844589233
PCA Transform: 0.024689435958862305
[[0.98701372 0.0182004 ]
 [0.01281972 0.97185822]]
total iterations: 3918
TLDA fit: 171.5213987827301
Whitened factor: 
[[ 0.13476651  0.14148504]
 [-0.58975086  0.59453143]]
PCA Reverse Transform: 0.00016736984252929688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02422906139087704
Fit RMSE: 0.025222340378071244
 Test Against Ground Truth
[0.93265175]
[0.30264172]
[0.99860889]
[-0.00997582]
[(' decentering', 0.0013859272003173828), (' smoothing and normalization', 0.0004696846008300781)]
Smoothing and Normalization: 0.0005481243133544922
Fit RMSE: 0.02518656351013336
sklearn Test Against Ground Truth
[0.99990539]
[0.99991618]
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014374256134033203
PCA fit: 0.046212196350097656
PCA Transform: 0.0015769004821777344
[[0.97655823 0.01115094]
 [0.01821574 0.98778501]]
total iterations: 17633
TLDA fit: 772.1768801212311
Whitened factor: 
[[ 0.68970905 -0.6746523 ]
 [-0.45086327  0.5290979 ]]
PCA Reverse Transform: 0.0001633167266845703
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1143566184411221
Fit RMSE: 0.11489433794118165
 Test Against Ground Truth
[0.99962266]
[-0.03638407]
[0.95569234]
[-0.03276851]
[(' decentering', 0.0011928081512451172), (' smoothing and normalization', 0.00037097930908203125)]
Smoothing and Normalization: 0.000484466552734375
Fit RMSE: 0.11415928911122712
sklearn Test Against Ground Truth
[0.9999999]
[0.99997596]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0026116371154785156
PCA fit: 0.2863349914550781
PCA Transform: 0.005199432373046875
[[0.98580357 0.02043241]
 [0.02138126 0.98564351]]
total iterations: 30000
TLDA fit: 1347.719958305359
Whitened factor: 
[[ 0.45093559 -0.44818487]
 [-0.14669459  0.27104638]]
PCA Reverse Transform: 0.00015783309936523438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055808057601240614
Fit RMSE: 0.05678850936289163
 Test Against Ground Truth
[0.97235286]
[0.01923811]
[0.92335042]
[0.3370428]
[(' decentering', 0.0011942386627197266), (' smoothing and normalization', 0.00041675567626953125)]
Smoothing and Normalization: 0.0004940032958984375
Fit RMSE: 0.05673543055625161
sklearn Test Against Ground Truth
[0.99998264]
[0.99998405]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.004540205001831055
PCA fit: 0.7312016487121582
PCA Transform: 0.009738922119140625
[[0.98803503 0.02494806]
 [0.01454247 0.98784569]]
total iterations: 21066
TLDA fit: 933.0249819755554
Whitened factor: 
[[ 0.4478442  -0.44298586]
 [-0.01351345  0.21464694]]
PCA Reverse Transform: 0.00015592575073242188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03967708021284551
Fit RMSE: 0.04057851303361233
 Test Against Ground Truth
[0.98243117]
[0.03217847]
[0.71632086]
[0.69833013]
[(' decentering', 0.0012156963348388672), (' smoothing and normalization', 0.000415802001953125)]
Smoothing and Normalization: 0.0005273818969726562
Fit RMSE: 0.04033736489357332
sklearn Test Against Ground Truth
[0.99998128]
[0.99995783]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.00674748420715332
PCA fit: 1.5068471431732178
PCA Transform: 0.014543294906616211
[[0.98378055 0.01625952]
 [0.01612916 0.98845141]]
total iterations: 4439
TLDA fit: 193.59335780143738
Whitened factor: 
[[ 0.10218346  0.17006603]
 [-0.5635296   0.60464115]]
PCA Reverse Transform: 0.0001537799835205078
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03175477430023847
Fit RMSE: 0.03277001542846185
 Test Against Ground Truth
[0.88889294]
[0.42279192]
[0.99194907]
[0.00344691]
[(' decentering', 0.0012469291687011719), (' smoothing and normalization', 0.0004208087921142578)]
Smoothing and Normalization: 0.0005202293395996094
Fit RMSE: 0.03273568648168599
sklearn Test Against Ground Truth
[0.9999762]
[0.99993842]
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.009233236312866211
PCA fit: 2.6399478912353516
PCA Transform: 0.018857717514038086
[[0.98150891 0.02304409]
 [0.01198222 0.97959246]]
total iterations: 3661
TLDA fit: 162.6788957118988
Whitened factor: 
[[ 0.08864131  0.19745558]
 [-0.55869811  0.57409785]]
PCA Reverse Transform: 0.00016355514526367188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02784346815008217
Fit RMSE: 0.028913271572394608
 Test Against Ground Truth
[0.87837532]
[0.40392369]
[0.99789595]
[-0.00308271]
[(' decentering', 0.0012831687927246094), (' smoothing and normalization', 0.0004208087921142578)]
Smoothing and Normalization: 0.0005331039428710938
Fit RMSE: 0.02879952933404582
sklearn Test Against Ground Truth
[0.99993832]
[0.99996139]
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.012359142303466797
PCA fit: 4.644718170166016
PCA Transform: 0.024833202362060547
[[0.9761249  0.02904419]
 [0.01771631 0.99071902]]
total iterations: 30000
TLDA fit: 1329.2362082004547
Whitened factor: 
[[0.08160729 0.28621195]
 [0.12458268 0.43656549]]
PCA Reverse Transform: 0.00016045570373535156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026237595679721434
Fit RMSE: 0.02667653304362399
 Test Against Ground Truth
[0.46192974]
[0.80992021]
[0.74866098]
[0.09380026]
[(' decentering', 0.001352071762084961), (' smoothing and normalization', 0.00042057037353515625)]
Smoothing and Normalization: 0.0005328655242919922
Fit RMSE: 0.025391946362269793
sklearn Test Against Ground Truth
[0.99991809]
[0.99991875]
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.001369476318359375
PCA fit: 0.045018672943115234
PCA Transform: 0.0015134811401367188
[[0.98346577 0.01639552]
 [0.02206186 0.97955091]]
total iterations: 30000
TLDA fit: 1318.3366484642029
Whitened factor: 
[[ 0.67054146 -0.66505996]
 [-0.21046574  0.27635305]]
PCA Reverse Transform: 0.000156402587890625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12099950958140977
Fit RMSE: 0.12170806770556103
 Test Against Ground Truth
[0.99982314]
[0.02563037]
[0.96508754]
[0.20767289]
[(' decentering', 0.0011742115020751953), (' smoothing and normalization', 0.0003924369812011719)]
Smoothing and Normalization: 0.0004951953887939453
Fit RMSE: 0.12093715762177354
sklearn Test Against Ground Truth
[0.99999769]
[0.99999622]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0026187896728515625
PCA fit: 0.27135157585144043
PCA Transform: 0.00519871711730957
[[0.98039835 0.01994777]
 [0.02011709 0.97565896]]
total iterations: 7894
TLDA fit: 346.8059573173523
Whitened factor: 
[[ 0.22949857 -0.00456137]
 [-0.55288667  0.585173  ]]
PCA Reverse Transform: 0.00016164779663085938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05489294670976397
Fit RMSE: 0.056280716346125165
 Test Against Ground Truth
[0.99837165]
[0.04566777]
[0.9894201]
[-0.01283937]
[(' decentering', 0.0011837482452392578), (' smoothing and normalization', 0.0004248619079589844)]
Smoothing and Normalization: 0.0005145072937011719
Fit RMSE: 0.05615221751418825
sklearn Test Against Ground Truth
[0.9999967]
[0.99996411]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.004559755325317383
PCA fit: 0.7582671642303467
PCA Transform: 0.009729862213134766
[[0.97747019 0.02816591]
 [0.02130487 0.98792137]]
total iterations: 30000
TLDA fit: 1337.2471301555634
Whitened factor: 
[[0.17995223 0.2968384 ]
 [0.20698339 0.34131377]]
PCA Reverse Transform: 0.0001544952392578125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.041103304738100935
Fit RMSE: 0.042059748151144084
 Test Against Ground Truth
[0.13705766]
[0.94007969]
[0.92504564]
[0.02426128]
[(' decentering', 0.0012187957763671875), (' smoothing and normalization', 0.0004317760467529297)]
Smoothing and Normalization: 0.0005383491516113281
Fit RMSE: 0.03971386377971746
sklearn Test Against Ground Truth
[0.99993941]
[0.99998653]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.006812572479248047
PCA fit: 1.0374467372894287
PCA Transform: 0.01452493667602539
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07812237739562988
PCA fit: 0.5380203723907471
PCA Transform: 0.31881284713745117
[[0.23807912 0.15040258]
 [0.64009494 0.8739072 ]]
total iterations: 1664
TLDA fit: 219.492666721344
Whitened factor: 
[[-0.2536897  -0.253729  ]
 [ 0.52802587  0.52810705]]
PCA Reverse Transform: 0.001207113265991211
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12363815056134819
Fit RMSE: 0.123140724425989
 Test Against Ground Truth
[0.98543456]
[0.08044513]
[0.99899976]
[-0.00357076]
[(' decentering', 0.005930662155151367), (' smoothing and normalization', 0.004757881164550781)]
Smoothing and Normalization: 0.0005865097045898438
Fit RMSE: 0.12322774869509887
sklearn Test Against Ground Truth
[0.99999892]
[0.99999998]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003509998321533203
PCA fit: 0.21596860885620117
PCA Transform: 0.006464481353759766
[[0.73487926 0.66896516]
 [0.5194025  0.87884915]]
total iterations: 1043
TLDA fit: 154.57739353179932
Whitened factor: 
[[0.3283502  0.32837367]
 [0.58187634 0.5819167 ]]
PCA Reverse Transform: 0.00029158592224121094
decenter with new strategy:
[0.00390463 0.00436213]
decenter with old strategy:
[0.00215197 0.00380849]
Fit RMSE new decenter: 0.05887562710974203
Fit RMSE: 0.05848346302287656
 Test Against Ground Truth
[-0.01701792]
[0.97734909]
[0.94561796]
[-0.01687485]
Traceback (most recent call last):
  File "generate_tables.py", line 467, in <module>
    main()
  File "generate_tables.py", line 417, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 372, in gen_fit_0_20
    res2, accuracy = postprocess(factors_unwhitened, mu, x, vocab, num_tops, smoothing, True, alpha_0 = alpha_0)
  File "generate_tables.py", line 190, in postprocess
    permutation_fac2[1] = [(x + 1)%2 for x in permutation_fac2[1]]
TypeError: 'tuple' object does not support item assignment
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08171343803405762
PCA fit: 0.5336611270904541
PCA Transform: 0.31322717666625977
[[0.18496862 0.84745365]
 [0.344827   0.17209895]]
total iterations: 2002
TLDA fit: 93.99137663841248
Whitened factor: 
[[-0.19529323  0.22682971]
 [ 0.31704244 -0.2131067 ]]
PCA Reverse Transform: 0.0008862018585205078
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1269449396034873
Fit RMSE: 0.12664179820801788
 Test Against Ground Truth
[0.99013352]
[0.12173642]
[0.99989241]
[-0.0113977]
[(' decentering', 0.004151582717895508), (' smoothing and normalization', 0.0036787986755371094)]
Smoothing and Normalization: 0.0004589557647705078
Fit RMSE: 0.12638571443021202
sklearn Test Against Ground Truth
[0.99999991]
[0.9999991]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0033288002014160156
PCA fit: 0.25359678268432617
PCA Transform: 0.005262613296508789
[[0.85719216 0.23258094]
 [0.4829807  0.1786503 ]]
total iterations: 7811
TLDA fit: 379.891729593277
Whitened factor: 
[[0.10263717 0.10253576]
 [0.19620119 0.19600633]]
PCA Reverse Transform: 0.00019621849060058594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05650326822390356
Fit RMSE: 0.058918763364097265
 Test Against Ground Truth
[0.2477086]
[0.9608125]
[0.96968204]
[-0.01415347]
[(' decentering', 0.0011763572692871094), (' smoothing and normalization', 0.0017840862274169922)]
Smoothing and Normalization: 0.0004966259002685547
Fit RMSE: 0.05586511255662633
sklearn Test Against Ground Truth
[0.99997796]
[0.99998884]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006189823150634766
PCA fit: 0.7488198280334473
PCA Transform: 0.010587692260742188
[[0.8759754  0.7722037 ]
 [0.8335543  0.02036839]]
total iterations: 2002
TLDA fit: 98.88260579109192
Whitened factor: 
[[ 0.13250147  0.03007581]
 [ 0.3254586  -0.3167648 ]]
PCA Reverse Transform: 0.00020742416381835938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.040202525001121266
Fit RMSE: 0.04137556437378676
 Test Against Ground Truth
[0.33018795]
[0.95252076]
[0.91781301]
[0.00074479]
[(' decentering', 0.0012085437774658203), (' smoothing and normalization', 0.0004143714904785156)]
Smoothing and Normalization: 0.0004761219024658203
Fit RMSE: 0.039385481429647595
sklearn Test Against Ground Truth
[0.99994058]
[0.99996322]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008883476257324219
PCA fit: 1.4620234966278076
PCA Transform: 0.01452016830444336
[[0.989746   0.14673917]
 [0.47960958 0.6197121 ]]
total iterations: 8213
TLDA fit: 405.6526963710785
Whitened factor: 
[[0.13882454 0.13868825]
 [0.17449667 0.17432459]]
PCA Reverse Transform: 0.0002033710479736328
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.033504613881672925
Fit RMSE: 0.03446886169910312
 Test Against Ground Truth
[0.28184884]
[0.94027591]
[0.96645375]
[0.04366802]
[(' decentering', 0.0012199878692626953), (' smoothing and normalization', 0.00041174888610839844)]
Smoothing and Normalization: 0.0005249977111816406
Fit RMSE: 0.03276533666980838
sklearn Test Against Ground Truth
[0.99995533]
[0.99996058]
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.01231241226196289
PCA fit: 1.7128574848175049
PCA Transform: 0.019963502883911133
[[0.880123   0.3628358 ]
 [0.5807154  0.42908025]]
total iterations: 11091
TLDA fit: 549.2799246311188
Whitened factor: 
[[0.13769715 0.13739492]
 [0.14574963 0.14543001]]
PCA Reverse Transform: 0.00020074844360351562
decenter with new strategy:
[-1.87028148e-06 -5.74528410e-06]
decenter with old strategy:
[-2.45398463e-05 -2.59764185e-05]
Fit RMSE new decenter: 0.029983295868601063
Fit RMSE: 0.030113220180853145
 Test Against Ground Truth
[0.25886255]
[0.95020144]
[0.96122329]
[0.19944006]
[(' decentering', 0.00128173828125), (' smoothing and normalization', 0.0004372596740722656)]
Smoothing and Normalization: 0.0005176067352294922
Fit RMSE: 0.02876548732165378
sklearn Test Against Ground Truth
[0.99991785]
[0.99996122]
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014504671096801758
PCA fit: 2.592094898223877
PCA Transform: 0.025727510452270508
[[0.7909248  0.59365094]
 [0.65629596 0.0440994 ]]
total iterations: 2002
TLDA fit: 98.75536727905273
Whitened factor: 
[[ 0.12304104  0.05568263]
 [ 0.3329809  -0.29685453]]
PCA Reverse Transform: 0.00020933151245117188
decenter with new strategy:
[ 0.00529846 -0.009113  ]
decenter with old strategy:
[-0.00370093 -0.01023032]
Fit RMSE new decenter: 0.025872384843393798
Fit RMSE: 0.026468769599057744
 Test Against Ground Truth
[0.33201368]
[0.94060293]
[0.92149737]
[-0.00043998]
[(' decentering', 0.0013527870178222656), (' smoothing and normalization', 0.0004246234893798828)]
Smoothing and Normalization: 0.0005035400390625
Fit RMSE: 0.025395542085276217
sklearn Test Against Ground Truth
[0.99993385]
[0.99993153]
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013668537139892578
PCA fit: 0.04450082778930664
PCA Transform: 0.0014729499816894531
[[0.7258337  0.797298  ]
 [0.82934505 0.8725599 ]]
total iterations: 2002
TLDA fit: 99.42873287200928
Whitened factor: 
[[0.07458322 0.08061526]
 [0.02120952 0.02292384]]
PCA Reverse Transform: 0.0001957416534423828
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12682967553514438
Fit RMSE: 0.1314620133949149
 Test Against Ground Truth
[0.99841731]
[0.01278727]
[0.61656357]
[0.79465446]
[(' decentering', 0.0011785030364990234), (' smoothing and normalization', 0.0003674030303955078)]
Smoothing and Normalization: 0.0004706382751464844
Fit RMSE: 0.12611619061012574
sklearn Test Against Ground Truth
[0.99998886]
[0.9999999]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0033714771270751953
PCA fit: 0.26610422134399414
PCA Transform: 0.005170106887817383
[[0.9661025  0.15207258]
 [0.19746281 0.17197035]]
total iterations: 8996
TLDA fit: 447.1339018344879
Whitened factor: 
[[0.12974998 0.12986346]
 [0.18323822 0.18339746]]
PCA Reverse Transform: 0.0002033710479736328
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05650774682883202
Fit RMSE: 0.05870358277027331
 Test Against Ground Truth
[0.27835903]
[0.94541417]
[0.9697169]
[-0.01507552]
[(' decentering', 0.0011837482452392578), (' smoothing and normalization', 0.00041484832763671875)]
Smoothing and Normalization: 0.0005185604095458984
Fit RMSE: 0.0552035077543163
sklearn Test Against Ground Truth
[0.9999857]
[0.99997849]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006170988082885742
PCA fit: 0.7421801090240479
PCA Transform: 0.009556770324707031
[[0.7147334  0.19725335]
 [0.34671435 0.7830463 ]]
total iterations: 4801
TLDA fit: 240.360289812088
Whitened factor: 
[[0.13846177 0.13850686]
 [0.1930039  0.19306554]]
PCA Reverse Transform: 0.00019407272338867188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04028329114280975
Fit RMSE: 0.041781512173611764
 Test Against Ground Truth
[0.2544583]
[0.94731964]
[0.95622017]
[-0.01253551]
[(' decentering', 0.001203298568725586), (' smoothing and normalization', 0.00041675567626953125)]
Smoothing and Normalization: 0.0005366802215576172
Fit RMSE: 0.03945942371754363
sklearn Test Against Ground Truth
[0.999963]
[0.99994033]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.009294271469116211
PCA fit: 1.5180327892303467
PCA Transform: 0.01560211181640625
[[0.0931745  0.89492023]
 [0.97337085 0.67005765]]
total iterations: 4386
TLDA fit: 215.95667052268982
Whitened factor: 
[[0.13162524 0.13167274]
 [0.19501168 0.19508152]]
PCA Reverse Transform: 0.00019669532775878906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03277168324742255
Fit RMSE: 0.034025185130095684
 Test Against Ground Truth
[0.26975928]
[0.94418464]
[0.94914958]
[-0.01247497]
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07668590545654297
PCA fit: 0.534451961517334
PCA Transform: 0.31999659538269043
[[0.36095425 0.41935143]
 [0.89760417 0.9198467 ]]
total iterations: 20274
TLDA fit: 963.2213871479034
Whitened factor: 
[[-0.10352299 -0.10333748]
 [ 0.14515215  0.14489053]]
PCA Reverse Transform: 0.0010008811950683594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12187328730837421
Fit RMSE: 0.12006754941496386
 Test Against Ground Truth
[0.87932287]
[0.44504476]
[0.99542211]
[0.04273479]
[(' decentering', 0.004238128662109375), (' smoothing and normalization', 0.003649473190307617)]
Smoothing and Normalization: 0.0004572868347167969
Fit RMSE: 0.11622913856071815
sklearn Test Against Ground Truth
[0.61972526]
[-0.01330296]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003326416015625
PCA fit: 0.26569485664367676
PCA Transform: 0.005572319030761719
[[0.5761268  0.29902077]
 [0.532392   0.25448382]]
total iterations: 9776
TLDA fit: 476.5398783683777
Whitened factor: 
[[0.10641482 0.10629607]
 [0.15311848 0.1529484 ]]
PCA Reverse Transform: 0.00019288063049316406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055755177900707074
Fit RMSE: 0.05855485018557831
 Test Against Ground Truth
[0.32104944]
[0.93298161]
[0.97040806]
[-0.016365]
[(' decentering', 0.0011794567108154297), (' smoothing and normalization', 0.0017821788787841797)]
Smoothing and Normalization: 0.0005030632019042969
Fit RMSE: 0.054844426679676206
sklearn Test Against Ground Truth
[0.99997153]
[0.99995415]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005530595779418945
PCA fit: 0.761690616607666
PCA Transform: 0.010600805282592773
[[0.19457975 0.5476629 ]
 [0.79997617 0.51028806]]
total iterations: 11222
TLDA fit: 556.0931408405304
Whitened factor: 
[[0.11821486 0.11847571]
 [0.13447624 0.13477449]]
PCA Reverse Transform: 0.00019550323486328125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.042073382176704234
Fit RMSE: 0.042425710884094386
 Test Against Ground Truth
[0.26136559]
[0.95242982]
[0.97039106]
[0.14481594]
[(' decentering', 0.0012128353118896484), (' smoothing and normalization', 0.0004086494445800781)]
Smoothing and Normalization: 0.0005052089691162109
Fit RMSE: 0.04045847146113285
sklearn Test Against Ground Truth
[0.99997109]
[0.99997967]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008294105529785156
PCA fit: 1.4933655261993408
PCA Transform: 0.01450967788696289
[[0.17790172 0.79454774]
 [0.43676904 0.09213004]]
total iterations: 2002
TLDA fit: 98.17457842826843
Whitened factor: 
[[ 0.07069988  0.07422519]
 [ 0.27001986 -0.2642431 ]]
PCA Reverse Transform: 0.00020170211791992188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03284993420628561
Fit RMSE: 0.033874759380607626
 Test Against Ground Truth
[0.46365961]
[0.89009446]
[0.89303594]
[0.00644164]
[(' decentering', 0.0012326240539550781), (' smoothing and normalization', 0.00041174888610839844)]
Smoothing and Normalization: 0.0004820823669433594
Fit RMSE: 0.03242093517582316
sklearn Test Against Ground Truth
[0.99993427]
[0.99992163]
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011294126510620117
PCA fit: 2.6645431518554688
PCA Transform: 0.02008509635925293
[[0.3429699  0.02787148]
 [0.0348768  0.36240205]]
total iterations: 7462
TLDA fit: 366.9807345867157
Whitened factor: 
[[0.12908873 0.1292301 ]
 [0.14116493 0.141319  ]]
PCA Reverse Transform: 0.00019478797912597656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02975279060890981
Fit RMSE: 0.02990136372797251
 Test Against Ground Truth
[0.26535778]
[0.94736082]
[0.95771254]
[0.18922259]
[(' decentering', 0.001285552978515625), (' smoothing and normalization', 0.00040841102600097656)]
Smoothing and Normalization: 0.0005242824554443359
Fit RMSE: 0.028392426806229634
sklearn Test Against Ground Truth
[0.99991865]
[0.99995766]
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014240264892578125
PCA fit: 4.648269176483154
PCA Transform: 0.025728225708007812
[[0.9460888  0.38330087]
 [0.0680097  0.23154244]]
total iterations: 2002
TLDA fit: 97.69772505760193
Whitened factor: 
[[ 0.03132625  0.10968253]
 [-0.24917176  0.25960085]]
PCA Reverse Transform: 0.000209808349609375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02576407926865095
Fit RMSE: 0.025681236975006145
 Test Against Ground Truth
[0.77573578]
[0.60208776]
[0.99375819]
[-0.00895527]
[(' decentering', 0.0013515949249267578), (' smoothing and normalization', 0.0004143714904785156)]
Smoothing and Normalization: 0.00048041343688964844
Fit RMSE: 0.02547744680501398
sklearn Test Against Ground Truth
[0.99992023]
[0.99992134]
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.001413106918334961
PCA fit: 0.04207587242126465
PCA Transform: 0.001451253890991211
[[0.7267118  0.64436316]
 [0.82770365 0.38579747]]
total iterations: 30000
TLDA fit: 1491.9516286849976
Whitened factor: 
[[0.02359436 0.08861989]
 [0.00408198 0.01531406]]
PCA Reverse Transform: 0.000186920166015625
decenter with new strategy:
[5.49413884e-06 5.49575979e-05]
decenter with old strategy:
[4.87022803e-06 1.66973058e-06]
Fit RMSE new decenter: 0.12213557002416899
Fit RMSE: 0.1275097898082837
 Test Against Ground Truth
[0.98980637]
[-0.01306855]
[0.65824203]
[0.74284973]
[(' decentering', 0.0011744499206542969), (' smoothing and normalization', 0.0003731250762939453)]
Smoothing and Normalization: 0.00046515464782714844
Fit RMSE: 0.12420851340133285
sklearn Test Against Ground Truth
[0.99999959]
[0.99999855]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0030698776245117188
PCA fit: 0.2658877372741699
PCA Transform: 0.005186796188354492
[[0.74269384 0.38947472]
 [0.5250989  0.33493114]]
total iterations: 7947
TLDA fit: 394.3418765068054
Whitened factor: 
[[0.10714753 0.10694832]
 [0.14459747 0.14433056]]
PCA Reverse Transform: 0.00018978118896484375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05719139555141607
Fit RMSE: 0.05927423377254808
 Test Against Ground Truth
[0.35674842]
[0.93705002]
[0.97455049]
[0.03104431]
[(' decentering', 0.0011870861053466797), (' smoothing and normalization', 0.0004105567932128906)]
Smoothing and Normalization: 0.0005176067352294922
Fit RMSE: 0.05636377172762086
sklearn Test Against Ground Truth
[0.99999589]
[0.99998322]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005879878997802734
PCA fit: 0.7380554676055908
PCA Transform: 0.009522199630737305
[[0.81285244 0.32279402]
 [0.43599147 0.0579707 ]]
total iterations: 2002
TLDA fit: 99.78848171234131
Whitened factor: 
[[ 0.09562933  0.02261987]
 [ 0.26133397 -0.24720891]]
PCA Reverse Transform: 0.00018906593322753906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.040952573427351244
Fit RMSE: 0.041908832241331156
 Test Against Ground Truth
[0.42455321]
[0.90045309]
[0.93999571]
[-0.01068397]
[(' decentering', 0.0018281936645507812), (' smoothing and normalization', 0.00041747093200683594)]
Smoothing and Normalization: 0.000476837158203125
Fit RMSE: 0.04027320393373719
sklearn Test Against Ground Truth
[0.99997343]
[0.99993911]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008610010147094727
PCA fit: 1.5082647800445557
PCA Transform: 0.015585660934448242
[[0.13350122 0.39656204]
 [0.71571535 0.89943326]]
total iterations: 9401
TLDA fit: 462.9952585697174
Whitened factor: 
[[0.10465517 0.10478339]
 [0.15829502 0.15849034]]
PCA Reverse Transform: 0.00019121170043945312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03322831160654669
Fit RMSE: 0.03428921155380301
 Test Against Ground Truth
[0.44237074]
[0.88634333]
[0.95864008]
[0.00744468]
[(' decentering', 0.0012333393096923828), (' smoothing and normalization', 0.00043582916259765625)]
Smoothing and Normalization: 0.0005128383636474609
Fit RMSE: 0.032675350007829035
sklearn Test Against Ground Truth
[0.99991195]
[0.99996248]
Vocab: 2000
num_tweets: 20000
density: 15
Traceback (most recent call last):
  File "generate_tables.py", line 467, in <module>
    main()
  File "generate_tables.py", line 413, in main
    create_data(vocab=vocab, seed=seed_arr[j])
  File "generate_tables.py", line 65, in create_data
    x, mu, _, alpha_0 = test_util.get_mu(num_tops, vocab, num_tweets, density, seed)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/test_util.py", line 51, in get_mu
    z = numpy.random.multinomial(1,theta[0],size = 1)
  File "mtrand.pyx", line 4235, in numpy.random.mtrand.RandomState.multinomial
  File "_common.pyx", line 359, in numpy.random._common.check_array_constraint
ValueError: pvals < 0, pvals > 1 or pvals contains NaNs
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07753849029541016
PCA fit: 0.52890944480896
PCA Transform: 0.3116035461425781
[[0.2952403  0.09343544]
 [0.38433215 0.26985314]]
Traceback (most recent call last):
  File "generate_tables.py", line 467, in <module>
    main()
  File "generate_tables.py", line 417, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 354, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 104, in fit
    if max(abs(tl.dot(self.factors_.T, self.factors_) - tl.eye(self.n_topic))) >= 0.15:
  File "cupy/_core/core.pyx", line 1050, in cupy._core.core.ndarray.__nonzero__
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07708358764648438
PCA fit: 0.5323948860168457
PCA Transform: 0.3164517879486084
[[0.14526747 0.5641567 ]
 [0.8025933  0.07577164]]
Traceback (most recent call last):
  File "generate_tables.py", line 467, in <module>
    main()
  File "generate_tables.py", line 417, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 354, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 106, in fit
    self.factors_ = tl.tensor(cp.random.uniform(0, 1, size=(n_topic, n_topic)))
NameError: name 'n_topic' is not defined
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08043098449707031
PCA fit: 0.5516490936279297
PCA Transform: 0.316037654876709
[[0.5668568  0.24039893]
 [0.86934847 0.72682756]]
total iterations: 30000
TLDA fit: 1436.0578775405884
Whitened factor: 
[[ 0.6475252  -0.6324716 ]
 [-0.27290642  0.42767397]]
PCA Reverse Transform: 0.0009951591491699219
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12048593485459337
Fit RMSE: 0.12005726340248861
 Test Against Ground Truth
[0.99776115]
[-0.02498676]
[0.97259305]
[0.06056534]
[(' decentering', 0.004324197769165039), (' smoothing and normalization', 0.003734111785888672)]
Smoothing and Normalization: 0.0004723072052001953
Fit RMSE: 0.11982575154064139
sklearn Test Against Ground Truth
[0.9999989]
[0.99999257]
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0034208297729492188
PCA fit: 0.2862436771392822
PCA Transform: 0.005181789398193359
[[0.9207398  0.31064993]
 [0.59887385 0.98730946]]
total iterations: 18010
TLDA fit: 880.6708633899689
Whitened factor: 
[[ 0.18451698  0.25325653]
 [ 0.6590147  -0.59584284]]
PCA Reverse Transform: 0.00020051002502441406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05776362730718879
Fit RMSE: 0.05906792700672921
 Test Against Ground Truth
[0.07940112]
[0.97334441]
[0.94087691]
[-0.00903859]
[(' decentering', 0.0011892318725585938), (' smoothing and normalization', 0.0017862319946289062)]
Smoothing and Normalization: 0.0005156993865966797
Fit RMSE: 0.05632789195932413
sklearn Test Against Ground Truth
[0.99997745]
[0.99999531]
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006087541580200195
PCA fit: 0.7719247341156006
PCA Transform: 0.00956583023071289
[[0.3639783  0.26060963]
 [0.51904297 0.80322206]]
total iterations: 10006
TLDA fit: 499.0870797634125
Whitened factor: 
[[ 0.19946745  0.21685907]
 [-0.61463714  0.63936496]]
PCA Reverse Transform: 0.00020647048950195312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.040317094723598616
Fit RMSE: 0.04012717855092044
 Test Against Ground Truth
[0.9689046]
[0.1225489]
[0.9985625]
[-0.0101133]
[(' decentering', 0.0012240409851074219), (' smoothing and normalization', 0.00040531158447265625)]
Smoothing and Normalization: 0.0005147457122802734
Fit RMSE: 0.04003376348199062
sklearn Test Against Ground Truth
[0.99996417]
[0.99995174]
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008620738983154297
PCA fit: 1.4551775455474854
PCA Transform: 0.015590429306030273
[[0.92843074 0.87722385]
 [0.57682127 0.42738143]]
total iterations: 30000
TLDA fit: 1487.283350944519
Whitened factor: 
[[ 0.3880349  -0.3078502 ]
 [-0.24763325  0.54647195]]
PCA Reverse Transform: 0.00020837783813476562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.032182462202356196
Fit RMSE: 0.032164291541634
 Test Against Ground Truth
[0.98718486]
[-0.01428544]
[0.86123626]
[0.16977271]
[(' decentering', 0.0012531280517578125), (' smoothing and normalization', 0.000415802001953125)]
Smoothing and Normalization: 0.0004825592041015625
Fit RMSE: 0.032136600538525635
sklearn Test Against Ground Truth
[0.99994027]
[0.99997035]
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011643648147583008
PCA fit: 2.671630859375
PCA Transform: 0.018889427185058594
[[0.3482417  0.51807475]
 [0.37770075 0.52216023]]
total iterations: 30000
TLDA fit: 1487.061522245407
Whitened factor: 
[[0.2227506  0.2199228 ]
 [0.34887677 0.34448066]]
PCA Reverse Transform: 0.00020074844360351562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02918332920814173
Fit RMSE: 0.029745238490768617
 Test Against Ground Truth
[0.10572207]
[0.96635394]
[0.94395382]
[-0.00755069]
[(' decentering', 0.0013163089752197266), (' smoothing and normalization', 0.0004165172576904297)]
Smoothing and Normalization: 0.0004956722259521484
Fit RMSE: 0.028366329724392278
sklearn Test Against Ground Truth
[0.99993525]
[0.99993962]
Vocab: 2500
num_tweets: 20000
density: 15
Traceback (most recent call last):
  File "generate_tables.py", line 467, in <module>
    main()
  File "generate_tables.py", line 413, in main
    create_data(vocab=vocab, seed=seed_arr[j])
  File "generate_tables.py", line 65, in create_data
    x, mu, _, alpha_0 = test_util.get_mu(num_tops, vocab, num_tweets, density, seed)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/test_util.py", line 51, in get_mu
    z = numpy.random.multinomial(1,theta[0],size = 1)
  File "mtrand.pyx", line 4235, in numpy.random.mtrand.RandomState.multinomial
  File "_common.pyx", line 359, in numpy.random._common.check_array_constraint
ValueError: pvals < 0, pvals > 1 or pvals contains NaNs
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08095192909240723
PCA fit: 0.5445542335510254
PCA Transform: 0.32143235206604004
[[0.6071349  0.3956217 ]
 [0.40605366 0.33902884]]
total iterations: 2471
TLDA fit: 117.23716640472412
Whitened factor: 
[[0.15459083 0.11809372]
 [0.00179475 0.00137257]]
PCA Reverse Transform: 0.0009708404541015625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12196307431805671
Fit RMSE: 0.12918071495899408
 Test Against Ground Truth
[0.99989134]
[-0.02431271]
[0.23346681]
[0.96626161]
Traceback (most recent call last):
  File "generate_tables.py", line 475, in <module>
    main()
  File "generate_tables.py", line 425, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 387, in gen_fit_0_20
    print(str(seed_arr), file=outFile)
NameError: name 'seed_arr' is not defined
  File "generate_tables.py", line 375
    print(str(tlda.factors_)), file=outFile)
                                           ^
SyntaxError: unmatched ')'
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07848811149597168
PCA fit: 0.5394549369812012
PCA Transform: 0.30704474449157715
[[0.27882415 0.17914046]
 [0.00805843 0.73205817]]
total iterations: 9758
TLDA fit: 459.6454927921295
Whitened factor: 
[[ 0.6442756  -0.6242882 ]
 [-0.4656731   0.54723245]]
PCA Reverse Transform: 0.0009791851043701172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12097107276915876
Fit RMSE: 0.12097821637347173
 Test Against Ground Truth
[0.99903047]
[-0.0197026]
[0.9755258]
[-0.02039316]
Traceback (most recent call last):
  File "generate_tables.py", line 478, in <module>
    main()
  File "generate_tables.py", line 428, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 391, in gen_fit_0_20
    print(str(pca.transform(factors_sklearn)), file=outFile)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/pca.py", line 48, in transform
    return tl.dot(X, (self.projection_weights_ / tl.sqrt(self.whitening_weights_)[None, :]))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/__init__.py", line 161, in inner
    return _get_backend_method(name)(*args, **kwargs)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/numpy_backend.py", line 38, in dot
    return a.dot(b)
  File "cupy/_core/core.pyx", line 1273, in cupy._core.core.ndarray.__array__
TypeError: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08014607429504395
PCA fit: 0.5441696643829346
PCA Transform: 0.30608510971069336
[[0.00885284 0.8988643 ]
 [0.11285925 0.6861329 ]]
total iterations: 25403
TLDA fit: 1204.62620139122
Whitened factor: 
[[-0.7096011   0.71230835]
 [ 0.33880734 -0.27979183]]
PCA Reverse Transform: 0.0009875297546386719
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12408371756770034
Fit RMSE: 0.12412566539675754
 Test Against Ground Truth
[0.99757728]
[-0.0145104]
[0.99995056]
[-0.01557571]
Traceback (most recent call last):
  File "generate_tables.py", line 478, in <module>
    main()
  File "generate_tables.py", line 428, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 391, in gen_fit_0_20
    print(str(pca.transform(tl.tensor(cp.asarray(factors_sklearn)))), file=outFile)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/pca.py", line 48, in transform
    return tl.dot(X, (self.projection_weights_ / tl.sqrt(self.whitening_weights_)[None, :]))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/__init__.py", line 161, in inner
    return _get_backend_method(name)(*args, **kwargs)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/numpy_backend.py", line 38, in dot
    return a.dot(b)
  File "cupy/_core/core.pyx", line 1273, in cupy._core.core.ndarray.__array__
TypeError: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07700181007385254
PCA fit: 0.5309484004974365
PCA Transform: 0.3077969551086426
[[0.68991196 0.17292508]
 [0.6338224  0.5210299 ]]
total iterations: 19087
TLDA fit: 909.5366480350494
Whitened factor: 
[[ 0.60449475 -0.5684032 ]
 [-0.3587241   0.4050847 ]]
PCA Reverse Transform: 0.0010013580322265625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1205106482271504
Fit RMSE: 0.12092839003796878
 Test Against Ground Truth
[0.75787572]
[-0.02745546]
[0.99036294]
[0.57921568]
Traceback (most recent call last):
  File "generate_tables.py", line 478, in <module>
    main()
  File "generate_tables.py", line 428, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 391, in gen_fit_0_20
    print(str(pca.transform(cp.asnumpy(factors_sklearn))), file=outFile)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/pca.py", line 48, in transform
    return tl.dot(X, (self.projection_weights_ / tl.sqrt(self.whitening_weights_)[None, :]))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/__init__.py", line 161, in inner
    return _get_backend_method(name)(*args, **kwargs)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/numpy_backend.py", line 38, in dot
    return a.dot(b)
  File "cupy/_core/core.pyx", line 1273, in cupy._core.core.ndarray.__array__
TypeError: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08130455017089844
PCA fit: 0.5495872497558594
Traceback (most recent call last):
  File "generate_tables.py", line 491, in <module>
    main()
  File "generate_tables.py", line 441, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 346, in gen_fit_0_20
    M2 = torch.zeros(1, (n**2))
NameError: name 'torch' is not defined
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08239197731018066
PCA fit: 0.5303044319152832
Traceback (most recent call last):
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/__init__.py", line 93, in _get_backend_method
    return getattr(_LOCAL_STATE.backend, key)
AttributeError: 'CupyBackend' object has no attribute 'linalg'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "generate_tables.py", line 491, in <module>
    main()
  File "generate_tables.py", line 441, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 351, in gen_fit_0_20
    M2 += tl.linalg.kronecker([x_cent[i,:],x_cent[i,:]]) # expand(1, -1)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/__init__.py", line 95, in _get_backend_method
    return getattr(_LOADED_BACKENDS[_DEFAULT_BACKEND], key)
AttributeError: 'CupyBackend' object has no attribute 'linalg'
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07952094078063965
PCA fit: 0.5764622688293457
Traceback (most recent call last):
  File "generate_tables.py", line 491, in <module>
    main()
  File "generate_tables.py", line 441, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 355, in gen_fit_0_20
    whitening_test = tl.dot(tl.dot(W.T, M2_img), W)
NameError: name 'M2_img' is not defined
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08214163780212402
PCA fit: 0.5397465229034424
[[9.80392156e-01 1.23303106e-08]
 [1.23303105e-08 9.80392228e-01]]
PCA Transform: 0.0023157596588134766
Traceback (most recent call last):
  File "generate_tables.py", line 491, in <module>
    main()
  File "generate_tables.py", line 441, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 372, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 81, in fit
    init1 = tl.tensor(cp.random.uniform(0, 1, size=(n_topic, n_topic)))
NameError: name 'n_topic' is not defined
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08066391944885254
PCA fit: 0.5448775291442871
[[9.80392149e-01 3.56241114e-09]
 [3.56241065e-09 9.80392168e-01]]
PCA Transform: 0.0022706985473632812
Traceback (most recent call last):
  File "generate_tables.py", line 491, in <module>
    main()
  File "generate_tables.py", line 441, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 372, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 146, in fit
    print("whitened factors:\n" + str(tlda.factors_), file=outFile)
NameError: name 'tlda' is not defined
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07774972915649414
PCA fit: 0.536562442779541
[[ 9.80392159e-01 -1.03957256e-09]
 [-1.03957292e-09  9.80392164e-01]]
PCA Transform: 0.002214670181274414
total iterations: 6
TLDA fit: 2.3240299224853516
Whitened factor: 
[[-0.32551444  0.9482152 ]
 [ 0.9658148   0.3011395 ]]
PCA Reverse Transform: 0.0010161399841308594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12068799409199231
Fit RMSE: 0.123528894416236
 Test Against Ground Truth
[(' decentering', 0.005524873733520508), (' smoothing and normalization', 0.0003020763397216797)]
Smoothing and Normalization: 0.0004737377166748047
Fit RMSE: 0.12080214373466121
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0035042762756347656
PCA fit: 0.27152562141418457
[[ 9.80392157e-01 -6.67548915e-09]
 [-6.67548909e-09  9.80392148e-01]]
PCA Transform: 0.0058248043060302734
total iterations: 7
TLDA fit: 2.8358376026153564
Whitened factor: 
[[ 0.44733265 -0.8980043 ]
 [ 0.90056825  0.4259974 ]]
PCA Reverse Transform: 0.00022268295288085938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055209186398919995
Fit RMSE: 0.05654724732276009
 Test Against Ground Truth
[(' decentering', 0.001363992691040039), (' smoothing and normalization', 0.0002963542938232422)]
Smoothing and Normalization: 0.0005059242248535156
Fit RMSE: 0.05540154598549723
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005888462066650391
PCA fit: 0.560173511505127
[[9.80392160e-01 3.05198798e-09]
 [3.05198813e-09 9.80392201e-01]]
PCA Transform: 0.011265993118286133
total iterations: 5
TLDA fit: 1.8661508560180664
Whitened factor: 
[[ 0.7570038  0.6787814]
 [-0.6960351  0.7590909]]
PCA Reverse Transform: 0.00023412704467773438
decenter with new strategy:
[-2.44952164e-05 -3.36241070e-05]
decenter with old strategy:
[1.56501321e-05 4.82046028e-06]
Fit RMSE new decenter: 0.042048673300986945
Fit RMSE: 0.04193485070248904
 Test Against Ground Truth
[(' decentering', 0.001360177993774414), (' smoothing and normalization', 0.00035953521728515625)]
Smoothing and Normalization: 0.0004906654357910156
Fit RMSE: 0.03967801917080152
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008924484252929688
PCA fit: 1.0154457092285156
[[ 9.80392158e-01 -6.41389186e-09]
 [-6.41389183e-09  9.80392166e-01]]
PCA Transform: 0.01692676544189453
total iterations: 6
TLDA fit: 2.393202066421509
Whitened factor: 
[[ 0.75968856  0.6649961 ]
 [ 0.6680989  -0.77261114]]
PCA Reverse Transform: 0.00020551681518554688
decenter with new strategy:
[-6.97720866e-05 -5.16174298e-05]
decenter with old strategy:
[5.10520677e-05 6.34399145e-05]
Fit RMSE new decenter: 0.03439135616918115
Fit RMSE: 0.034342545854166934
 Test Against Ground Truth
[(' decentering', 0.0013475418090820312), (' smoothing and normalization', 0.00035834312438964844)]
Smoothing and Normalization: 0.0005218982696533203
Fit RMSE: 0.032497521392443135
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011515140533447266
PCA fit: 2.6528525352478027
[[ 9.80392158e-01 -2.04041474e-09]
 [-2.04041452e-09  9.80392128e-01]]
PCA Transform: 0.02239513397216797
total iterations: 6
TLDA fit: 2.3655107021331787
Whitened factor: 
[[ 0.7037511   0.7276474 ]
 [-0.72861314  0.7166825 ]]
PCA Reverse Transform: 0.00024056434631347656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02973279885725714
Fit RMSE: 0.029539219456468774
 Test Against Ground Truth
[(' decentering', 0.001435995101928711), (' smoothing and normalization', 0.0004131793975830078)]
Smoothing and Normalization: 0.000507354736328125
Fit RMSE: 0.02813406502586766
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014252424240112305
PCA fit: 2.639669895172119
[[9.80392157e-01 9.97548240e-10]
 [9.97547987e-10 9.80392184e-01]]
PCA Transform: 0.02794671058654785
total iterations: 8
TLDA fit: 3.3778586387634277
Whitened factor: 
[[ 0.44984704 -0.9004255 ]
 [ 0.9182734   0.4325521 ]]
PCA Reverse Transform: 0.0002415180206298828
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025428947966983664
Fit RMSE: 0.025826693197441256
 Test Against Ground Truth
[(' decentering', 0.0015778541564941406), (' smoothing and normalization', 0.0003840923309326172)]
Smoothing and Normalization: 0.000514984130859375
Fit RMSE: 0.02541293145109218
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0015003681182861328
PCA fit: 0.043657779693603516
[[9.80392169e-01 3.69016317e-08]
 [3.69016315e-08 9.80392054e-01]]
PCA Transform: 0.0015935897827148438
total iterations: 6
TLDA fit: 2.3827719688415527
Whitened factor: 
[[ 0.87882894 -0.48163965]
 [ 0.4593223   0.88163304]]
PCA Reverse Transform: 0.00021791458129882812
decenter with new strategy:
[-0.03121689  0.00935475]
decenter with old strategy:
[-0.00178436  0.03282561]
Fit RMSE new decenter: 0.12634234415525378
Fit RMSE: 0.12864920034400731
 Test Against Ground Truth
[(' decentering', 0.0013051033020019531), (' smoothing and normalization', 0.0003402233123779297)]
Smoothing and Normalization: 0.0004000663757324219
Fit RMSE: 0.12628603527077514
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003426074981689453
PCA fit: 0.2722201347351074
[[9.80392158e-01 5.01835216e-10]
 [5.01835231e-10 9.80392134e-01]]
PCA Transform: 0.005848884582519531
total iterations: 7
TLDA fit: 2.908013343811035
Whitened factor: 
[[-0.87273836  0.49785268]
 [ 0.4838714   0.87319994]]
PCA Reverse Transform: 0.00023746490478515625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05617957361194409
Fit RMSE: 0.05726066930198564
 Test Against Ground Truth
[(' decentering', 0.0013804435729980469), (' smoothing and normalization', 0.00037550926208496094)]
Smoothing and Normalization: 0.0005443096160888672
Fit RMSE: 0.05626950053770482
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005955219268798828
PCA fit: 0.5583298206329346
[[ 9.80392154e-01 -1.00816135e-08]
 [-1.00816128e-08  9.80392154e-01]]
PCA Transform: 0.011250734329223633
total iterations: 8
TLDA fit: 3.3188676834106445
Whitened factor: 
[[-0.8560428   0.5301231 ]
 [ 0.5172552   0.86443585]]
PCA Reverse Transform: 0.00022339820861816406
decenter with new strategy:
[-8.57547111e-05 -2.59089488e-05]
decenter with old strategy:
[1.09849434e-06 4.80164431e-05]
Fit RMSE new decenter: 0.039575315051728635
Fit RMSE: 0.04029855456004343
 Test Against Ground Truth
[(' decentering', 0.0013785362243652344), (' smoothing and normalization', 0.00038123130798339844)]
Smoothing and Normalization: 0.0004780292510986328
Fit RMSE: 0.03958294225193109
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.00881648063659668
PCA fit: 1.0401687622070312
[[ 9.80392158e-01 -1.16004828e-08]
 [-1.16004829e-08  9.80392193e-01]]
PCA Transform: 0.01691436767578125
total iterations: 6
TLDA fit: 2.357923746109009
Whitened factor: 
[[ 0.7528203   0.6708362 ]
 [-0.67314416  0.7600279 ]]
PCA Reverse Transform: 0.00022745132446289062
decenter with new strategy:
[0.00010959 0.0001063 ]
decenter with old strategy:
[0.00023016 0.00022153]
Fit RMSE new decenter: 0.034773529836910975
Fit RMSE: 0.03475132273602327
 Test Against Ground Truth
[(' decentering', 0.0013575553894042969), (' smoothing and normalization', 0.0004177093505859375)]
Smoothing and Normalization: 0.0005071163177490234
Fit RMSE: 0.03315944371895565
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011166810989379883
PCA fit: 2.685634136199951
[[9.80392157e-01 2.69972774e-09]
 [2.69972802e-09 9.80392159e-01]]
PCA Transform: 0.02218770980834961
total iterations: 6
TLDA fit: 2.4056427478790283
Whitened factor: 
[[ 0.7379208  0.691063 ]
 [-0.6868737  0.7575234]]
PCA Reverse Transform: 0.00026297569274902344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.029698018076563283
Fit RMSE: 0.029450126262509424
 Test Against Ground Truth
[(' decentering', 0.0014047622680664062), (' smoothing and normalization', 0.00036144256591796875)]
Smoothing and Normalization: 0.0004982948303222656
Fit RMSE: 0.02805105499977843
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014440536499023438
PCA fit: 4.633973598480225
[[9.80392157e-01 8.66085467e-10]
 [8.66085554e-10 9.80392154e-01]]
PCA Transform: 0.027709484100341797
total iterations: 5
TLDA fit: 1.9559905529022217
Whitened factor: 
[[ 0.6393543   0.78007513]
 [ 0.78959954 -0.64110935]]
PCA Reverse Transform: 0.000225067138671875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02653571435666198
Fit RMSE: 0.02622335059287434
 Test Against Ground Truth
[(' decentering', 0.0014865398406982422), (' smoothing and normalization', 0.00036835670471191406)]
Smoothing and Normalization: 0.0005004405975341797
Fit RMSE: 0.025094068380418767
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014929771423339844
PCA fit: 0.04970145225524902
[[9.80392159e-01 3.05658907e-08]
 [3.05658906e-08 9.80392266e-01]]
PCA Transform: 0.0015246868133544922
total iterations: 6
TLDA fit: 2.370201587677002
Whitened factor: 
[[ 0.90604293 -0.43097344]
 [ 0.41128677  0.9179716 ]]
PCA Reverse Transform: 0.0002067089080810547
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12162640127278475
Fit RMSE: 0.12393589990713606
 Test Against Ground Truth
[(' decentering', 0.0012841224670410156), (' smoothing and normalization', 0.00034499168395996094)]
Smoothing and Normalization: 0.00046372413635253906
Fit RMSE: 0.1214508853228908
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0034546852111816406
PCA fit: 0.2758362293243408
[[9.80392153e-01 1.71710420e-10]
 [1.71710294e-10 9.80392204e-01]]
PCA Transform: 0.005906105041503906
total iterations: 7
TLDA fit: 2.929919481277466
Whitened factor: 
[[ 0.44335347 -0.90192753]
 [ 0.90603423  0.4305045 ]]
PCA Reverse Transform: 0.0002391338348388672
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055220506427397134
Fit RMSE: 0.05649229995021014
 Test Against Ground Truth
[(' decentering', 0.00136566162109375), (' smoothing and normalization', 0.00037670135498046875)]
Smoothing and Normalization: 0.0005228519439697266
Fit RMSE: 0.05534074616364658
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.002576112747192383
PCA fit: 0.7416470050811768
[[ 9.80392162e-01 -2.66131456e-09]
 [-2.66131454e-09  9.80392150e-01]]
PCA Transform: 0.011277914047241211
total iterations: 9
TLDA fit: 3.8780064582824707
Whitened factor: 
[[ 0.29784426  0.9552525 ]
 [ 0.96165764 -0.3005818 ]]
PCA Reverse Transform: 0.0002474784851074219
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.041824672418749
Fit RMSE: 0.041207816024605
 Test Against Ground Truth
[(' decentering', 0.001348733901977539), (' smoothing and normalization', 0.0003693103790283203)]
Smoothing and Normalization: 0.0005118846893310547
Fit RMSE: 0.040261039626764875
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008969545364379883
PCA fit: 1.495182991027832
[[9.80392153e-01 5.02755450e-09]
 [5.02755469e-09 9.80392182e-01]]
PCA Transform: 0.01695704460144043
total iterations: 6
TLDA fit: 2.455112934112549
Whitened factor: 
[[ 0.8074458   0.6095004 ]
 [-0.6138612   0.81735957]]
PCA Reverse Transform: 0.0002434253692626953
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03469601395678029
Fit RMSE: 0.034721296123871155
 Test Against Ground Truth
[(' decentering', 0.0014493465423583984), (' smoothing and normalization', 0.0003650188446044922)]
Smoothing and Normalization: 0.0005068778991699219
Fit RMSE: 0.03312580558281466
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.01226186752319336
PCA fit: 2.675638437271118
[[9.80392159e-01 1.20026892e-09]
 [1.20026884e-09 9.80392160e-01]]
PCA Transform: 0.022426366806030273
total iterations: 6
TLDA fit: 2.443289279937744
Whitened factor: 
[[ 0.6220801  0.8025321]
 [-0.7985681  0.6376637]]
PCA Reverse Transform: 0.00022459030151367188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.029971041366885352
Fit RMSE: 0.02997406466559844
 Test Against Ground Truth
[(' decentering', 0.0013911724090576172), (' smoothing and normalization', 0.0003714561462402344)]
Smoothing and Normalization: 0.0005092620849609375
Fit RMSE: 0.02849493391230807
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014786958694458008
PCA fit: 2.618147373199463
[[ 9.80392156e-01 -7.83262524e-09]
 [-7.83262517e-09  9.80392138e-01]]
PCA Transform: 0.027697086334228516
total iterations: 6
TLDA fit: 2.3519232273101807
Whitened factor: 
[[ 0.6993694   0.7342203 ]
 [-0.7345292   0.71376026]]
PCA Reverse Transform: 0.00023221969604492188
decenter with new strategy:
[2.23506506e-04 6.59368761e-05]
decenter with old strategy:
[0.00034008 0.00018481]
Fit RMSE new decenter: 0.026787538701678833
Fit RMSE: 0.02675561144177764
 Test Against Ground Truth
[(' decentering', 0.0014793872833251953), (' smoothing and normalization', 0.0003490447998046875)]
Smoothing and Normalization: 0.0005035400390625
Fit RMSE: 0.02540786969404401
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014340877532958984
PCA fit: 0.047147274017333984
[[ 9.80392157e-01 -9.89072557e-09]
 [-9.89072501e-09  9.80392084e-01]]
PCA Transform: 0.0015511512756347656
total iterations: 7
TLDA fit: 2.868590831756592
Whitened factor: 
[[ 0.89787704 -0.45499685]
 [ 0.44207639  0.9023586 ]]
PCA Reverse Transform: 0.00024318695068359375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11937533075794884
Fit RMSE: 0.12224591931841805
 Test Against Ground Truth
[(' decentering', 0.0013279914855957031), (' smoothing and normalization', 0.0003464221954345703)]
Smoothing and Normalization: 0.0004801750183105469
Fit RMSE: 0.11938571902666474
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003055572509765625
PCA fit: 0.25721287727355957
[[ 9.80392160e-01 -1.24904924e-08]
 [-1.24904926e-08  9.80392141e-01]]
PCA Transform: 0.005900382995605469
total iterations: 6
TLDA fit: 2.4722158908843994
Whitened factor: 
[[ 0.7157824  0.715341 ]
 [-0.7288541  0.7147663]]
PCA Reverse Transform: 0.00022983551025390625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05960092696592413
Fit RMSE: 0.05950246830298406
 Test Against Ground Truth
[(' decentering', 0.0014026165008544922), (' smoothing and normalization', 0.00035953521728515625)]
Smoothing and Normalization: 0.0005290508270263672
Fit RMSE: 0.05629588858395288
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005570411682128906
PCA fit: 0.7479801177978516
[[9.80392155e-01 3.23874414e-09]
 [3.23874409e-09 9.80392152e-01]]
PCA Transform: 0.011260032653808594
total iterations: 5
TLDA fit: 1.9381446838378906
Whitened factor: 
[[ 0.71601725  0.71084106]
 [-0.7136797   0.7265651 ]]
PCA Reverse Transform: 0.00023245811462402344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04187511950494802
Fit RMSE: 0.041822738942246036
 Test Against Ground Truth
[(' decentering', 0.0013904571533203125), (' smoothing and normalization', 0.0003750324249267578)]
Smoothing and Normalization: 0.0005176067352294922
Fit RMSE: 0.03934478101701721
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08098459243774414
PCA fit: 0.5438449382781982
[[9.80392154e-01 1.59540526e-09]
 [1.59540535e-09 9.80392153e-01]]
PCA Transform: 0.0023956298828125
total iterations: 7
TLDA fit: 2.733630418777466
Whitened factor: 
[[-0.48757416  0.8971252 ]
 [ 1.0133327   0.31205115]]
PCA Reverse Transform: 0.0011169910430908203
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1225723365298697
Fit RMSE: 0.12431500487460596
 Test Against Ground Truth
[(' decentering', 0.005987405776977539), (' smoothing and normalization', 0.00029754638671875)]
Smoothing and Normalization: 0.00046634674072265625
Fit RMSE: 0.12229352792372587
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003644227981567383
PCA fit: 0.26254916191101074
[[ 9.80392154e-01 -5.33789357e-09]
 [-5.33789356e-09  9.80392183e-01]]
PCA Transform: 0.0058498382568359375
total iterations: 7
TLDA fit: 2.7866406440734863
Whitened factor: 
[[ 0.57379043 -0.8745598 ]
 [ 0.970871    0.44552025]]
PCA Reverse Transform: 0.00021219253540039062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05678260650411933
Fit RMSE: 0.057561519721165855
 Test Against Ground Truth
[(' decentering', 0.0013170242309570312), (' smoothing and normalization', 0.00029349327087402344)]
Smoothing and Normalization: 0.0005066394805908203
Fit RMSE: 0.05677931541096844
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006118297576904297
PCA fit: 0.7537655830383301
[[9.80392158e-01 5.23255810e-09]
 [5.23255807e-09 9.80392115e-01]]
PCA Transform: 0.011255025863647461
total iterations: 12
TLDA fit: 4.924327850341797
Whitened factor: 
[[ 0.86732244 -0.59693277]
 [ 0.5444779   0.9111229 ]]
PCA Reverse Transform: 0.0002009868621826172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.039567157685930936
Fit RMSE: 0.04013238018492057
 Test Against Ground Truth
[(' decentering', 0.0012967586517333984), (' smoothing and normalization', 0.000362396240234375)]
Smoothing and Normalization: 0.000518798828125
Fit RMSE: 0.03950175617493386
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.009151697158813477
PCA fit: 1.518397569656372
[[ 9.80392159e-01 -7.63569812e-10]
 [-7.63569911e-10  9.80392181e-01]]
PCA Transform: 0.017100811004638672
total iterations: 6
TLDA fit: 2.3541994094848633
Whitened factor: 
[[ 0.79172266  0.8030522 ]
 [ 0.91807926 -0.7916384 ]]
PCA Reverse Transform: 0.00021696090698242188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03436417935389488
Fit RMSE: 0.034093415194306705
 Test Against Ground Truth
[(' decentering', 0.0013797283172607422), (' smoothing and normalization', 0.0003612041473388672)]
Smoothing and Normalization: 0.0004999637603759766
Fit RMSE: 0.03249232345077927
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.012137174606323242
PCA fit: 2.653958559036255
[[ 9.80392156e-01 -1.23875410e-09]
 [-1.23875421e-09  9.80392183e-01]]
PCA Transform: 0.022372722625732422
total iterations: 5
TLDA fit: 1.9000730514526367
Whitened factor: 
[[ 0.818724   0.7608487]
 [ 0.8553461 -0.8262916]]
PCA Reverse Transform: 0.000209808349609375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.029783429842505933
Fit RMSE: 0.029585991758931507
 Test Against Ground Truth
[(' decentering', 0.0013785362243652344), (' smoothing and normalization', 0.0003485679626464844)]
Smoothing and Normalization: 0.0004932880401611328
Fit RMSE: 0.028193638003853317
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014637947082519531
PCA fit: 4.652823209762573
[[ 9.80392153e-01 -5.23426569e-09]
 [-5.23426583e-09  9.80392165e-01]]
PCA Transform: 0.02793574333190918
total iterations: 7
TLDA fit: 2.7719550132751465
Whitened factor: 
[[ 0.7384999  0.791314 ]
 [-0.6886516  0.9280738]]
PCA Reverse Transform: 0.000217437744140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026754431218580444
Fit RMSE: 0.02670029242150928
 Test Against Ground Truth
[(' decentering', 0.0014505386352539062), (' smoothing and normalization', 0.0003733634948730469)]
Smoothing and Normalization: 0.0004949569702148438
Fit RMSE: 0.02543843687357869
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0015194416046142578
PCA fit: 0.04382896423339844
[[ 9.80392159e-01 -8.45686543e-09]
 [-8.45686520e-09  9.80392191e-01]]
PCA Transform: 0.0015869140625
total iterations: 7
TLDA fit: 2.7915537357330322
Whitened factor: 
[[-0.53067523  0.8957896 ]
 [ 1.0824248   0.35174364]]
PCA Reverse Transform: 0.00020074844360351562
decenter with new strategy:
[ 0.05249052 -0.05120376]
decenter with old strategy:
[0.10231206 0.00909417]
Fit RMSE new decenter: 0.12068645341455475
Fit RMSE: 0.12196618791670809
 Test Against Ground Truth
[(' decentering', 0.0013055801391601562), (' smoothing and normalization', 0.0003230571746826172)]
Smoothing and Normalization: 0.0004456043243408203
Fit RMSE: 0.11992462085866991
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0033981800079345703
PCA fit: 0.2756967544555664
[[ 9.80392154e-01 -1.00064158e-08]
 [-1.00064161e-08  9.80392159e-01]]
PCA Transform: 0.005953788757324219
total iterations: 6
TLDA fit: 2.3615212440490723
Whitened factor: 
[[ 0.74028903  0.8241201 ]
 [-0.8552045   0.8141154 ]]
PCA Reverse Transform: 0.000232696533203125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0595444367288181
Fit RMSE: 0.0594463445499649
 Test Against Ground Truth
[(' decentering', 0.0014214515686035156), (' smoothing and normalization', 0.00034427642822265625)]
Smoothing and Normalization: 0.0005552768707275391
Fit RMSE: 0.05605950248018295
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006114482879638672
PCA fit: 0.755845308303833
[[9.80392159e-01 2.16050898e-09]
 [2.16050884e-09 9.80392179e-01]]
PCA Transform: 0.011374711990356445
total iterations: 6
TLDA fit: 2.3076000213623047
Whitened factor: 
[[ 0.6376539  -0.8483383 ]
 [ 0.9609009   0.46645257]]
PCA Reverse Transform: 0.00023126602172851562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03976773846028129
Fit RMSE: 0.04039627084197084
 Test Against Ground Truth
[(' decentering', 0.0014026165008544922), (' smoothing and normalization', 0.0003535747528076172)]
Smoothing and Normalization: 0.0005066394805908203
Fit RMSE: 0.03978691246901078
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008803606033325195
PCA fit: 1.5044376850128174
[[ 9.80392159e-01 -2.83875624e-09]
 [-2.83875630e-09  9.80392183e-01]]
PCA Transform: 0.01712822914123535
total iterations: 7
TLDA fit: 2.8935749530792236
Whitened factor: 
[[ 0.8996129  -0.5634415 ]
 [ 0.4809549   0.97871804]]
PCA Reverse Transform: 0.0002448558807373047
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.032351037984568266
Fit RMSE: 0.03285531726183794
 Test Against Ground Truth
[(' decentering', 0.0014150142669677734), (' smoothing and normalization', 0.0003535747528076172)]
Smoothing and Normalization: 0.0004754066467285156
Fit RMSE: 0.03235578340608972
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.0119781494140625
PCA fit: 1.6838059425354004
[[ 9.80392158e-01 -4.71308300e-09]
 [-4.71308295e-09  9.80392155e-01]]
PCA Transform: 0.022157669067382812
total iterations: 10
TLDA fit: 4.276872634887695
Whitened factor: 
[[ 1.0089905   0.08161778]
 [-0.12140096  1.0725673 ]]
PCA Reverse Transform: 0.0002155303955078125
decenter with new strategy:
[-0.01658426  0.00393086]
decenter with old strategy:
[0.00160859 0.01797007]
Fit RMSE new decenter: 0.028857441129346694
Fit RMSE: 0.028892924622058228
 Test Against Ground Truth
[(' decentering', 0.0013549327850341797), (' smoothing and normalization', 0.0003407001495361328)]
Smoothing and Normalization: 0.000507354736328125
Fit RMSE: 0.02831234928853156
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.015132904052734375
PCA fit: 4.679356098175049
[[9.80392158e-01 8.79821582e-09]
 [8.79821597e-09 9.80392197e-01]]
PCA Transform: 0.027732133865356445
total iterations: 6
TLDA fit: 2.3486671447753906
Whitened factor: 
[[-0.7574497   0.80706733]
 [ 0.5982149   0.8619117 ]]
PCA Reverse Transform: 0.00021958351135253906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02537900238649729
Fit RMSE: 0.025852765140984436
 Test Against Ground Truth
[(' decentering', 0.0014655590057373047), (' smoothing and normalization', 0.00036978721618652344)]
Smoothing and Normalization: 0.0005404949188232422
Fit RMSE: 0.02547275078402917
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013604164123535156
PCA fit: 0.041825294494628906
[[9.80392155e-01 1.46123644e-08]
 [1.46123640e-08 9.80392186e-01]]
PCA Transform: 0.0015447139739990234
total iterations: 6
TLDA fit: 2.382957935333252
Whitened factor: 
[[ 0.9052148 -0.4602892]
 [ 0.2999813  1.0297862]]
PCA Reverse Transform: 0.00022101402282714844
decenter with new strategy:
[ 3.60365912 -0.96742648]
decenter with old strategy:
[6.82117728 1.64207408]
Fit RMSE new decenter: 0.11618151196247326
Fit RMSE: 0.11925741100821376
 Test Against Ground Truth
[(' decentering', 0.0013129711151123047), (' smoothing and normalization', 0.0003254413604736328)]
Smoothing and Normalization: 0.0004794597625732422
Fit RMSE: 0.11648375168724188
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0033957958221435547
PCA fit: 0.25826501846313477
[[9.80392155e-01 1.18640210e-08]
 [1.18640207e-08 9.80392179e-01]]
PCA Transform: 0.00590205192565918
total iterations: 7
TLDA fit: 2.783626079559326
Whitened factor: 
[[ 0.5267562  -0.87925464]
 [ 1.0240929   0.34573233]]
PCA Reverse Transform: 0.0002193450927734375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05601101122671159
Fit RMSE: 0.056969202012456484
 Test Against Ground Truth
[(' decentering', 0.0012884140014648438), (' smoothing and normalization', 0.00034427642822265625)]
Smoothing and Normalization: 0.0005512237548828125
Fit RMSE: 0.05601955629554984
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005465507507324219
PCA fit: 0.7335131168365479
[[9.80392156e-01 1.63976752e-09]
 [1.63976783e-09 9.80392149e-01]]
PCA Transform: 0.01123499870300293
total iterations: 5
TLDA fit: 1.925365686416626
Whitened factor: 
[[ 0.7996554   0.7665783 ]
 [-0.79903054  0.86607796]]
PCA Reverse Transform: 0.00023865699768066406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04199373892137979
Fit RMSE: 0.041831503624473715
 Test Against Ground Truth
[(' decentering', 0.001416921615600586), (' smoothing and normalization', 0.00037169456481933594)]
Smoothing and Normalization: 0.0004892349243164062
Fit RMSE: 0.03951672941345986
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008899211883544922
PCA fit: 1.0194082260131836
[[9.80392153e-01 2.91564456e-09]
 [2.91564498e-09 9.80392166e-01]]
PCA Transform: 0.017269372940063477
total iterations: 5
TLDA fit: 1.9589555263519287
Whitened factor: 
[[ 0.77316374  0.81788516]
 [ 0.9082828  -0.7932137 ]]
PCA Reverse Transform: 0.00023436546325683594
decenter with new strategy:
[0.00048473 0.00026163]
decenter with old strategy:
[0.00079604 0.00057812]
Fit RMSE new decenter: 0.03508003140171109
Fit RMSE: 0.03505949917467891
 Test Against Ground Truth
[(' decentering', 0.0014276504516601562), (' smoothing and normalization', 0.0003635883331298828)]
Smoothing and Normalization: 0.0004951953887939453
Fit RMSE: 0.03376705935387416
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.01230001449584961
PCA fit: 2.6794216632843018
[[ 9.80392153e-01 -1.17854000e-08]
 [-1.17853999e-08  9.80392142e-01]]
PCA Transform: 0.022144794464111328
total iterations: 6
TLDA fit: 2.3484835624694824
Whitened factor: 
[[ 0.7357831  -0.80439997]
 [ 0.8730421   0.63007426]]
PCA Reverse Transform: 0.0002346038818359375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028305702966353617
Fit RMSE: 0.02876514121422851
 Test Against Ground Truth
[(' decentering', 0.0014560222625732422), (' smoothing and normalization', 0.00038313865661621094)]
Smoothing and Normalization: 0.0005018711090087891
Fit RMSE: 0.02835846648734601
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013925313949584961
PCA fit: 2.6018548011779785
[[9.80392159e-01 6.45375786e-09]
 [6.45375812e-09 9.80392174e-01]]
PCA Transform: 0.027693510055541992
total iterations: 6
TLDA fit: 2.314770460128784
Whitened factor: 
[[ 0.6231766   0.9145589 ]
 [-0.8195207   0.77847624]]
PCA Reverse Transform: 0.0002205371856689453
decenter with new strategy:
[0.0143096  0.01244803]
decenter with old strategy:
[0.02527768 0.02520169]
Fit RMSE new decenter: 0.026875868605550524
Fit RMSE: 0.026928018735779876
 Test Against Ground Truth
[(' decentering', 0.0014739036560058594), (' smoothing and normalization', 0.00034546852111816406)]
Smoothing and Normalization: 0.0005352497100830078
Fit RMSE: 0.025700609089688627
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014493465423583984
PCA fit: 0.043434858322143555
[[ 9.80392152e-01 -1.01933009e-08]
 [-1.01933013e-08  9.80392170e-01]]
PCA Transform: 0.0015869140625
total iterations: 7
TLDA fit: 2.8514440059661865
Whitened factor: 
[[ 0.88080335 -0.571533  ]
 [ 0.42418578  0.97701526]]
PCA Reverse Transform: 0.0002071857452392578
decenter with new strategy:
[ 0.09377058 -0.03983298]
decenter with old strategy:
[0.1871612  0.03896678]
Fit RMSE new decenter: 0.11987565366463254
Fit RMSE: 0.12188767913097094
 Test Against Ground Truth
[(' decentering', 0.0012996196746826172), (' smoothing and normalization', 0.0003285408020019531)]
Smoothing and Normalization: 0.0004544258117675781
Fit RMSE: 0.11953692509881769
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003374338150024414
PCA fit: 0.28522634506225586
[[ 9.80392160e-01 -2.42203060e-09]
 [-2.42203048e-09  9.80392138e-01]]
PCA Transform: 0.005812644958496094
total iterations: 6
TLDA fit: 2.391317129135132
Whitened factor: 
[[ 0.7744099  0.801676 ]
 [-0.8386616  0.8316437]]
PCA Reverse Transform: 0.0002033710479736328
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05893687117104981
Fit RMSE: 0.05892789115440735
 Test Against Ground Truth
[(' decentering', 0.0013103485107421875), (' smoothing and normalization', 0.0003516674041748047)]
Smoothing and Normalization: 0.0005514621734619141
Fit RMSE: 0.05493247638607356
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005587577819824219
PCA fit: 0.5945422649383545
[[9.80392156e-01 4.58160537e-09]
 [4.58160559e-09 9.80392189e-01]]
PCA Transform: 0.011264562606811523
total iterations: 6
TLDA fit: 2.405982494354248
Whitened factor: 
[[ 0.79276305  0.78399867]
 [ 0.8670043  -0.820327  ]]
PCA Reverse Transform: 0.00022029876708984375
decenter with new strategy:
[-0.12240891 -0.11887727]
decenter with old strategy:
[0.05820532 0.06002211]
Fit RMSE new decenter: 0.04213341589145942
Fit RMSE: 0.04210006041354833
 Test Against Ground Truth
[(' decentering', 0.0013654232025146484), (' smoothing and normalization', 0.0003714561462402344)]
Smoothing and Normalization: 0.0005028247833251953
Fit RMSE: 0.039718086583628284
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008601188659667969
PCA fit: 1.4821562767028809
[[9.80392158e-01 6.56376722e-09]
 [6.56376756e-09 9.80392183e-01]]
PCA Transform: 0.017105579376220703
total iterations: 11
TLDA fit: 4.7528228759765625
Whitened factor: 
[[-0.7929435   0.75125474]
 [ 0.68856645  0.8570591 ]]
PCA Reverse Transform: 0.00021314620971679688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03345758305654331
Fit RMSE: 0.03378512727180786
 Test Against Ground Truth
[(' decentering', 0.0013611316680908203), (' smoothing and normalization', 0.0003502368927001953)]
Smoothing and Normalization: 0.0005462169647216797
Fit RMSE: 0.03345131279907259
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.012169122695922852
PCA fit: 2.6550843715667725
[[ 9.80392155e-01 -5.50891625e-09]
 [-5.50891604e-09  9.80392140e-01]]
PCA Transform: 0.022459983825683594
total iterations: 8
TLDA fit: 3.3300206661224365
Whitened factor: 
[[ 0.7168917   0.76668197]
 [-0.66703314  0.85770875]]
PCA Reverse Transform: 0.00022172927856445312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.029689325095373864
Fit RMSE: 0.029672954812232637
 Test Against Ground Truth
[(' decentering', 0.0014526844024658203), (' smoothing and normalization', 0.0003619194030761719)]
Smoothing and Normalization: 0.0004899501800537109
Fit RMSE: 0.028045502010565573
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.015331029891967773
PCA fit: 4.666949272155762
[[ 9.80392157e-01 -4.13501670e-09]
 [-4.13501674e-09  9.80392162e-01]]
PCA Transform: 0.02772688865661621
total iterations: 6
TLDA fit: 2.3431408405303955
Whitened factor: 
[[ 0.568291   -0.8745008 ]
 [ 1.0295186   0.37424895]]
PCA Reverse Transform: 0.00021219253540039062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025407673087875123
Fit RMSE: 0.025801244908255933
 Test Against Ground Truth
[(' decentering', 0.0015113353729248047), (' smoothing and normalization', 0.0003612041473388672)]
Smoothing and Normalization: 0.00047326087951660156
Fit RMSE: 0.025412315581612013
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0015878677368164062
PCA fit: 0.047783851623535156
[[ 9.80392157e-01 -2.29788535e-08]
 [-2.29788536e-08  9.80392088e-01]]
PCA Transform: 0.0015645027160644531
total iterations: 7
TLDA fit: 2.8003907203674316
Whitened factor: 
[[ 0.95910084 -0.30308995]
 [ 0.19169158  1.0749859 ]]
PCA Reverse Transform: 0.00021982192993164062
decenter with new strategy:
[0.02855301 0.00638938]
decenter with old strategy:
[0.05681922 0.02777926]
Fit RMSE new decenter: 0.12345054711656972
Fit RMSE: 0.12556774317655472
 Test Against Ground Truth
[(' decentering', 0.0013015270233154297), (' smoothing and normalization', 0.0003390312194824219)]
Smoothing and Normalization: 0.0004780292510986328
Fit RMSE: 0.12366415107118349
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0033049583435058594
PCA fit: 0.2816028594970703
[[9.80392156e-01 2.17989427e-09]
 [2.17989390e-09 9.80392136e-01]]
PCA Transform: 0.005881071090698242
total iterations: 7
TLDA fit: 2.817091703414917
Whitened factor: 
[[-0.86385417  0.5976295 ]
 [ 0.4007747   1.033696  ]]
PCA Reverse Transform: 0.00023174285888671875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05508566653423101
Fit RMSE: 0.05607370424259676
 Test Against Ground Truth
[(' decentering', 0.0013127326965332031), (' smoothing and normalization', 0.00034809112548828125)]
Smoothing and Normalization: 0.0005183219909667969
Fit RMSE: 0.05507245184360798
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.002784252166748047
PCA fit: 0.5703577995300293
[[9.80392157e-01 1.19900083e-09]
 [1.19900116e-09 9.80392173e-01]]
PCA Transform: 0.012028217315673828
total iterations: 8
TLDA fit: 3.27647066116333
Whitened factor: 
[[ 0.659024    0.85195667]
 [-0.80488914  0.77399087]]
PCA Reverse Transform: 0.00021219253540039062
decenter with new strategy:
[0.00288235 0.00259162]
decenter with old strategy:
[0.00512918 0.00510849]
Fit RMSE new decenter: 0.042545938992281866
Fit RMSE: 0.04250722762770382
 Test Against Ground Truth
[(' decentering', 0.0013549327850341797), (' smoothing and normalization', 0.00041985511779785156)]
Smoothing and Normalization: 0.0005376338958740234
Fit RMSE: 0.04052942805308551
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008939027786254883
PCA fit: 1.5020973682403564
[[9.80392157e-01 7.08325317e-10]
 [7.08325378e-10 9.80392160e-01]]
PCA Transform: 0.016951322555541992
total iterations: 10
TLDA fit: 4.22861123085022
Whitened factor: 
[[ 0.22859664  0.9803549 ]
 [ 1.0638758  -0.24548887]]
PCA Reverse Transform: 0.00022530555725097656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03377062679575001
Fit RMSE: 0.03331566753235617
 Test Against Ground Truth
[(' decentering', 0.0013434886932373047), (' smoothing and normalization', 0.00037741661071777344)]
Smoothing and Normalization: 0.0005040168762207031
Fit RMSE: 0.0326278218880755
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.012174844741821289
PCA fit: 2.678922176361084
[[9.80392159e-01 6.81753652e-11]
 [6.81757216e-11 9.80392157e-01]]
PCA Transform: 0.02216339111328125
total iterations: 7
TLDA fit: 2.768256425857544
Whitened factor: 
[[ 0.8794016  -0.5308635 ]
 [ 0.35454008  0.9949302 ]]
PCA Reverse Transform: 0.00024199485778808594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028682862675611587
Fit RMSE: 0.029046868859608097
 Test Against Ground Truth
[(' decentering', 0.0014243125915527344), (' smoothing and normalization', 0.00035572052001953125)]
Smoothing and Normalization: 0.0005438327789306641
Fit RMSE: 0.028656781651900998
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014826059341430664
PCA fit: 4.6918816566467285
[[ 9.80392154e-01 -7.29557606e-09]
 [-7.29557632e-09  9.80392143e-01]]
PCA Transform: 0.028971195220947266
total iterations: 5
TLDA fit: 1.9062602519989014
Whitened factor: 
[[ 0.69302547  0.8911454 ]
 [-0.8558739   0.81304646]]
PCA Reverse Transform: 0.00022792816162109375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026822027504813162
Fit RMSE: 0.0267112294575096
 Test Against Ground Truth
[(' decentering', 0.0014801025390625), (' smoothing and normalization', 0.0003483295440673828)]
Smoothing and Normalization: 0.0005028247833251953
Fit RMSE: 0.025578071641959053
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013699531555175781
PCA fit: 0.042218685150146484
[[ 9.80392151e-01 -9.00640316e-09]
 [-9.00640308e-09  9.80392155e-01]]
PCA Transform: 0.0016129016876220703
total iterations: 7
TLDA fit: 2.9132683277130127
Whitened factor: 
[[ 0.8999329  -0.502339  ]
 [ 0.34413785  1.027719  ]]
PCA Reverse Transform: 0.0002090930938720703
decenter with new strategy:
[ 5.32071391 -1.30363208]
decenter with old strategy:
[9.98817651 2.52999323]
Fit RMSE new decenter: 0.12214191646718851
Fit RMSE: 0.1246089589553299
 Test Against Ground Truth
[(' decentering', 0.0013346672058105469), (' smoothing and normalization', 0.00032901763916015625)]
Smoothing and Normalization: 0.0004734992980957031
Fit RMSE: 0.11952251479014878
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0030410289764404297
PCA fit: 0.21566367149353027
[[9.80392160e-01 4.09014118e-10]
 [4.09014260e-10 9.80392136e-01]]
PCA Transform: 0.005845069885253906
total iterations: 7
TLDA fit: 2.8432400226593018
Whitened factor: 
[[-0.8496804   0.62428635]
 [ 0.45104358  0.96795875]]
PCA Reverse Transform: 0.00022912025451660156
decenter with new strategy:
[-0.17638235  0.16103949]
decenter with old strategy:
[0.03153839 0.34850336]
Fit RMSE new decenter: 0.05497309051809888
Fit RMSE: 0.056095795775353244
 Test Against Ground Truth
[(' decentering', 0.0013396739959716797), (' smoothing and normalization', 0.0003647804260253906)]
Smoothing and Normalization: 0.0005652904510498047
Fit RMSE: 0.05504885437915646
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005742549896240234
PCA fit: 0.7771494388580322
[[9.80392153e-01 1.19033317e-09]
 [1.19033305e-09 9.80392170e-01]]
PCA Transform: 0.011328935623168945
total iterations: 7
TLDA fit: 2.801191806793213
Whitened factor: 
[[ 0.7473806   0.7846142 ]
 [-0.72571856  0.8935667 ]]
PCA Reverse Transform: 0.0002257823944091797
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04203896386871937
Fit RMSE: 0.04205940554123904
 Test Against Ground Truth
[(' decentering', 0.0013375282287597656), (' smoothing and normalization', 0.0003609657287597656)]
Smoothing and Normalization: 0.0005393028259277344
Fit RMSE: 0.0396377647904821
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.007677316665649414
PCA fit: 1.4707951545715332
[[ 9.80392155e-01 -3.10886236e-09]
 [-3.10886216e-09  9.80392130e-01]]
PCA Transform: 0.01729726791381836
total iterations: 8
TLDA fit: 3.3180267810821533
Whitened factor: 
[[-0.8396153   0.67116505]
 [ 0.5484405   0.9327585 ]]
PCA Reverse Transform: 0.00020956993103027344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0332376409024561
Fit RMSE: 0.03363944875022104
 Test Against Ground Truth
[(' decentering', 0.0013730525970458984), (' smoothing and normalization', 0.0003437995910644531)]
Smoothing and Normalization: 0.0005025863647460938
Fit RMSE: 0.03323982519113114
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011291265487670898
PCA fit: 1.7176318168640137
[[ 9.80392156e-01 -3.00768856e-10]
 [-3.00768536e-10  9.80392152e-01]]
PCA Transform: 0.022381305694580078
total iterations: 6
TLDA fit: 2.3512253761291504
Whitened factor: 
[[ 0.6529697  -0.84165007]
 [ 1.0745226   0.42477745]]
PCA Reverse Transform: 0.00023365020751953125
decenter with new strategy:
[-0.00794377  0.02419614]
decenter with old strategy:
[0.01100043 0.04390767]
Fit RMSE new decenter: 0.02806641694409265
Fit RMSE: 0.028561699622032782
 Test Against Ground Truth
[(' decentering', 0.0014383792877197266), (' smoothing and normalization', 0.00038051605224609375)]
Smoothing and Normalization: 0.0005028247833251953
Fit RMSE: 0.02809092713125014
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013940095901489258
PCA fit: 4.637436389923096
[[9.80392154e-01 2.06969680e-09]
 [2.06969691e-09 9.80392174e-01]]
PCA Transform: 0.02796173095703125
total iterations: 5
TLDA fit: 1.9101588726043701
Whitened factor: 
[[ 0.75305206  0.8128229 ]
 [ 0.9441905  -0.7283558 ]]
PCA Reverse Transform: 0.00022459030151367188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026751831823101556
Fit RMSE: 0.026549546395527316
 Test Against Ground Truth
[(' decentering', 0.0014576911926269531), (' smoothing and normalization', 0.0003407001495361328)]
Smoothing and Normalization: 0.0005483627319335938
Fit RMSE: 0.025422682120354673
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.001667022705078125
PCA fit: 0.04761648178100586
[[ 9.80392158e-01 -1.16144487e-08]
 [-1.16144489e-08  9.80392054e-01]]
PCA Transform: 0.0015528202056884766
total iterations: 7
TLDA fit: 2.951195478439331
Whitened factor: 
[[ 0.8754589  -0.57189244]
 [ 0.43707496  0.9316423 ]]
PCA Reverse Transform: 0.00022029876708984375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1207574517136142
Fit RMSE: 0.12288070100395904
 Test Against Ground Truth
[(' decentering', 0.0013566017150878906), (' smoothing and normalization', 0.00033926963806152344)]
Smoothing and Normalization: 0.0005033016204833984
Fit RMSE: 0.12036788828584553
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003067493438720703
PCA fit: 0.3031430244445801
[[ 9.80392159e-01 -4.44252812e-09]
 [-4.44252810e-09  9.80392198e-01]]
PCA Transform: 0.005842685699462891
total iterations: 8
TLDA fit: 3.435023546218872
Whitened factor: 
[[ 0.86740613 -0.60788155]
 [ 0.49999332  0.9174986 ]]
PCA Reverse Transform: 0.0002238750457763672
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05792737062421687
Fit RMSE: 0.058432065527409906
 Test Against Ground Truth
[(' decentering', 0.0013031959533691406), (' smoothing and normalization', 0.0003809928894042969)]
Smoothing and Normalization: 0.0005536079406738281
Fit RMSE: 0.05783009779399021
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005272388458251953
PCA fit: 0.7666299343109131
[[ 9.80392156e-01 -3.27020389e-09]
 [-3.27020375e-09  9.80392187e-01]]
PCA Transform: 0.011959075927734375
total iterations: 6
TLDA fit: 2.30998158454895
Whitened factor: 
[[ 0.66722226  0.8894077 ]
 [ 0.9532627  -0.7086641 ]]
PCA Reverse Transform: 0.0002086162567138672
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04248197944927389
Fit RMSE: 0.04240259857730671
 Test Against Ground Truth
[(' decentering', 0.001352071762084961), (' smoothing and normalization', 0.00040078163146972656)]
Smoothing and Normalization: 0.0005166530609130859
Fit RMSE: 0.04042736241818579
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.007807254791259766
PCA fit: 1.4845552444458008
[[ 9.80392159e-01 -1.44435450e-09]
 [-1.44435507e-09  9.80392154e-01]]
PCA Transform: 0.01689743995666504
total iterations: 6
TLDA fit: 2.3395261764526367
Whitened factor: 
[[-0.8177148   0.7090101 ]
 [ 0.5046051   0.95996976]]
PCA Reverse Transform: 0.0002295970916748047
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0326114085252732
Fit RMSE: 0.03319485400332518
 Test Against Ground Truth
[(' decentering', 0.0013616085052490234), (' smoothing and normalization', 0.0003609657287597656)]
Smoothing and Normalization: 0.0005004405975341797
Fit RMSE: 0.03268886442978308
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.009929180145263672
PCA fit: 2.6446657180786133
[[9.80392156e-01 3.30550825e-09]
 [3.30550818e-09 9.80392169e-01]]
PCA Transform: 0.022184133529663086
total iterations: 6
TLDA fit: 2.3625924587249756
Whitened factor: 
[[ 0.76098263  0.80598277]
 [ 0.9352023  -0.73700804]]
PCA Reverse Transform: 0.00021958351135253906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02970935667262706
Fit RMSE: 0.029431853003928855
 Test Against Ground Truth
[(' decentering', 0.0014443397521972656), (' smoothing and normalization', 0.00035881996154785156)]
Smoothing and Normalization: 0.0005586147308349609
Fit RMSE: 0.028089494061640404
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.01156926155090332
PCA fit: 2.6097841262817383
[[9.80392158e-01 3.42738566e-09]
 [3.42738581e-09 9.80392171e-01]]
PCA Transform: 0.027721166610717773
total iterations: 5
TLDA fit: 1.8482391834259033
Whitened factor: 
[[ 0.84022653  0.74294055]
 [-0.763419    0.9369187 ]]
PCA Reverse Transform: 0.00022554397583007812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026666836778760052
Fit RMSE: 0.0264516808877574
 Test Against Ground Truth
[(' decentering', 0.0014836788177490234), (' smoothing and normalization', 0.00036406517028808594)]
Smoothing and Normalization: 0.0005519390106201172
Fit RMSE: 0.02523343599599932
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013875961303710938
PCA fit: 0.04540205001831055
[[9.80392153e-01 6.23729683e-09]
 [6.23729641e-09 9.80392219e-01]]
PCA Transform: 0.0015730857849121094
total iterations: 6
TLDA fit: 2.4376578330993652
Whitened factor: 
[[ 0.74013025  0.81990945]
 [ 0.8423515  -0.8149176 ]]
PCA Reverse Transform: 0.00024628639221191406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1279978627746262
Fit RMSE: 0.1274128089414698
 Test Against Ground Truth
[(' decentering', 0.0014023780822753906), (' smoothing and normalization', 0.00034689903259277344)]
Smoothing and Normalization: 0.0005214214324951172
Fit RMSE: 0.11551272570546582
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0028502941131591797
PCA fit: 0.2788200378417969
[[ 9.80392161e-01 -6.16961354e-09]
 [-6.16961292e-09  9.80392172e-01]]
PCA Transform: 0.005903005599975586
total iterations: 8
TLDA fit: 3.4450607299804688
Whitened factor: 
[[ 0.86462086 -0.5674033 ]
 [ 0.4538314   0.9255531 ]]
PCA Reverse Transform: 0.00023698806762695312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055986265191568914
Fit RMSE: 0.05683413396928273
 Test Against Ground Truth
[(' decentering', 0.0013661384582519531), (' smoothing and normalization', 0.00035858154296875)]
Smoothing and Normalization: 0.0005142688751220703
Fit RMSE: 0.055927563972117775
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005405426025390625
PCA fit: 0.5755741596221924
[[ 9.80392158e-01 -5.22534316e-09]
 [-5.22534341e-09  9.80392169e-01]]
PCA Transform: 0.011195898056030273
total iterations: 10
TLDA fit: 4.370891332626343
Whitened factor: 
[[-0.7712726   0.729101  ]
 [ 0.70366716  0.77494127]]
PCA Reverse Transform: 0.0002219676971435547
decenter with new strategy:
[-1.45909126e-04  7.78808432e-05]
decenter with old strategy:
[1.27239770e-05 2.33197943e-04]
Fit RMSE new decenter: 0.04034902259134422
Fit RMSE: 0.04086123926253351
 Test Against Ground Truth
[(' decentering', 0.0013797283172607422), (' smoothing and normalization', 0.00035452842712402344)]
Smoothing and Normalization: 0.0004725456237792969
Fit RMSE: 0.04035768017350635
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.007138729095458984
PCA fit: 1.4925086498260498
[[9.80392159e-01 5.76402907e-09]
 [5.76402953e-09 9.80392163e-01]]
PCA Transform: 0.016964435577392578
total iterations: 6
TLDA fit: 2.4498984813690186
Whitened factor: 
[[ 0.86257714 -0.59939015]
 [ 0.44449303  0.9595099 ]]
PCA Reverse Transform: 0.0002353191375732422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03329903389129547
Fit RMSE: 0.03371901429894639
 Test Against Ground Truth
[(' decentering', 0.0014002323150634766), (' smoothing and normalization', 0.00038123130798339844)]
Smoothing and Normalization: 0.0005705356597900391
Fit RMSE: 0.033296952478101285
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.01014399528503418
PCA fit: 1.681433916091919
[[ 9.80392156e-01 -4.28248969e-09]
 [-4.28248983e-09  9.80392136e-01]]
PCA Transform: 0.02330780029296875
total iterations: 6
TLDA fit: 2.4093337059020996
Whitened factor: 
[[ 0.57631326 -0.8659609 ]
 [ 1.0112755   0.3865086 ]]
PCA Reverse Transform: 0.0002231597900390625
decenter with new strategy:
[ 0.00078964 -0.00112423]
decenter with old strategy:
[0.00195117 0.00017457]
Fit RMSE new decenter: 0.028306258922590814
Fit RMSE: 0.028779590919254302
 Test Against Ground Truth
[(' decentering', 0.0014262199401855469), (' smoothing and normalization', 0.00033855438232421875)]
Smoothing and Normalization: 0.0005142688751220703
Fit RMSE: 0.02832949635027675
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.012959718704223633
PCA fit: 4.6635901927948
[[9.80392155e-01 1.80061938e-09]
 [1.80061957e-09 9.80392163e-01]]
PCA Transform: 0.02767205238342285
total iterations: 6
TLDA fit: 2.4083030223846436
Whitened factor: 
[[-0.8392227   0.64076144]
 [ 0.43116292  1.04122   ]]
PCA Reverse Transform: 0.00021648406982421875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025121506744639387
Fit RMSE: 0.025601816083914916
 Test Against Ground Truth
[(' decentering', 0.0014722347259521484), (' smoothing and normalization', 0.00034737586975097656)]
Smoothing and Normalization: 0.000492095947265625
Fit RMSE: 0.025176244872472716
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0016124248504638672
PCA fit: 0.05146026611328125
[[9.80392155e-01 2.23640710e-08]
 [2.23640707e-08 9.80392087e-01]]
PCA Transform: 0.0015268325805664062
total iterations: 7
TLDA fit: 2.97756028175354
Whitened factor: 
[[ 0.89074993 -0.5011492 ]
 [ 0.33450252  0.987948  ]]
PCA Reverse Transform: 0.00022721290588378906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12313683428648421
Fit RMSE: 0.12474315413910343
 Test Against Ground Truth
[(' decentering', 0.0012929439544677734), (' smoothing and normalization', 0.00032901763916015625)]
Smoothing and Normalization: 0.00047898292541503906
Fit RMSE: 0.12276691978176757
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0028533935546875
PCA fit: 0.2672710418701172
[[9.80392159e-01 4.50276572e-09]
 [4.50276590e-09 9.80392194e-01]]
PCA Transform: 0.0058116912841796875
total iterations: 7
TLDA fit: 2.877399206161499
Whitened factor: 
[[ 0.89262515 -0.5117534 ]
 [ 0.3311868   1.0239894 ]]
PCA Reverse Transform: 0.00022292137145996094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05664086262141875
Fit RMSE: 0.0573783719173749
 Test Against Ground Truth
[(' decentering', 0.0013942718505859375), (' smoothing and normalization', 0.0003559589385986328)]
Smoothing and Normalization: 0.0005254745483398438
Fit RMSE: 0.05654183402255707
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.004958629608154297
PCA fit: 0.7682838439941406
[[ 9.80392156e-01 -3.71921090e-09]
 [-3.71921096e-09  9.80392154e-01]]
PCA Transform: 0.011212825775146484
total iterations: 8
TLDA fit: 3.2582757472991943
Whitened factor: 
[[ 0.69964767  0.7915329 ]
 [-0.76676285  0.7858256 ]]
PCA Reverse Transform: 0.00022864341735839844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04227680610037146
Fit RMSE: 0.04230742178542842
 Test Against Ground Truth
[(' decentering', 0.0013155937194824219), (' smoothing and normalization', 0.0003647804260253906)]
Smoothing and Normalization: 0.0005238056182861328
Fit RMSE: 0.04004061440230977
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.00729060173034668
PCA fit: 1.0614824295043945
[[9.80392154e-01 1.02940760e-09]
 [1.02940755e-09 9.80392142e-01]]
PCA Transform: 0.01692652702331543
total iterations: 6
TLDA fit: 2.3355045318603516
Whitened factor: 
[[-0.51834923  0.88234425]
 [ 1.0322936   0.34634998]]
PCA Reverse Transform: 0.00022554397583007812
decenter with new strategy:
[ 0.00302075 -0.00191249]
decenter with old strategy:
[0.00543054 0.00088377]
Fit RMSE new decenter: 0.032382108978904256
Fit RMSE: 0.03291781186735828
 Test Against Ground Truth
[(' decentering', 0.001383066177368164), (' smoothing and normalization', 0.00036406517028808594)]
Smoothing and Normalization: 0.0005030632019042969
Fit RMSE: 0.03236903235295547
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.010091781616210938
PCA fit: 2.6604175567626953
[[ 9.80392158e-01 -3.79130213e-09]
 [-3.79130190e-09  9.80392141e-01]]
PCA Transform: 0.022202253341674805
total iterations: 6
TLDA fit: 2.350857973098755
Whitened factor: 
[[ 0.80099076  0.7490111 ]
 [-0.7363385   0.9272568 ]]
PCA Reverse Transform: 0.00022745132446289062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02987335961858649
Fit RMSE: 0.029667975133622307
 Test Against Ground Truth
[(' decentering', 0.001415252685546875), (' smoothing and normalization', 0.00034999847412109375)]
Smoothing and Normalization: 0.0005233287811279297
Fit RMSE: 0.028402141167305828
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.01342153549194336
PCA fit: 4.710779428482056
[[ 9.80392154e-01 -2.48755089e-09]
 [-2.48755084e-09  9.80392156e-01]]
PCA Transform: 0.027706384658813477
total iterations: 7
TLDA fit: 2.77518892288208
Whitened factor: 
[[ 0.56729186 -0.86025727]
 [ 0.9808536   0.36634517]]
PCA Reverse Transform: 0.00023674964904785156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025484456392720095
Fit RMSE: 0.025878458084386723
 Test Against Ground Truth
[(' decentering', 0.0014729499816894531), (' smoothing and normalization', 0.0003559589385986328)]
Smoothing and Normalization: 0.0005364418029785156
Fit RMSE: 0.025490663832910744
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013556480407714844
PCA fit: 0.04461026191711426
[[9.80392164e-01 2.33461682e-08]
 [2.33461675e-08 9.80392171e-01]]
PCA Transform: 0.0015568733215332031
total iterations: 7
TLDA fit: 2.848667860031128
Whitened factor: 
[[-0.5477666   0.88862413]
 [ 1.0445706   0.36721644]]
PCA Reverse Transform: 0.00024056434631347656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12242484903424124
Fit RMSE: 0.12396760728608691
 Test Against Ground Truth
[(' decentering', 0.0012810230255126953), (' smoothing and normalization', 0.0003352165222167969)]
Smoothing and Normalization: 0.0004825592041015625
Fit RMSE: 0.12204010615928342
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0027348995208740234
PCA fit: 0.2782125473022461
[[ 9.80392151e-01 -4.88480574e-09]
 [-4.88480526e-09  9.80392159e-01]]
PCA Transform: 0.005845308303833008
total iterations: 6
TLDA fit: 2.3690593242645264
Whitened factor: 
[[ 0.90271   -0.5183208]
 [ 0.3503237  1.0529503]]
PCA Reverse Transform: 0.00022172927856445312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05660619766844258
Fit RMSE: 0.057336712050478
 Test Against Ground Truth
[(' decentering', 0.0013260841369628906), (' smoothing and normalization', 0.0003752708435058594)]
Smoothing and Normalization: 0.0005228519439697266
Fit RMSE: 0.056544960265984705
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.0024971961975097656
PCA fit: 0.7355141639709473
[[9.80392155e-01 2.24922496e-09]
 [2.24922526e-09 9.80392192e-01]]
PCA Transform: 0.011228561401367188
total iterations: 7
TLDA fit: 2.87624192237854
Whitened factor: 
[[ 0.6548256  -0.8349926 ]
 [ 0.96946365  0.4745305 ]]
PCA Reverse Transform: 0.0002231597900390625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03944824304500136
Fit RMSE: 0.040209188045142695
 Test Against Ground Truth
[(' decentering', 0.0013904571533203125), (' smoothing and normalization', 0.00036978721618652344)]
Smoothing and Normalization: 0.00048804283142089844
Fit RMSE: 0.03954621097304102
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.0074787139892578125
PCA fit: 1.5102922916412354
[[9.80392159e-01 3.09468499e-09]
 [3.09468498e-09 9.80392169e-01]]
PCA Transform: 0.016920089721679688
total iterations: 5
TLDA fit: 1.8893544673919678
Whitened factor: 
[[ 0.8104062   0.7617869 ]
 [-0.78472334  0.8872972 ]]
PCA Reverse Transform: 0.0002186298370361328
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03438278037625758
Fit RMSE: 0.034199580083166675
 Test Against Ground Truth
[(' decentering', 0.0013506412506103516), (' smoothing and normalization', 0.0003616809844970703)]
Smoothing and Normalization: 0.0005066394805908203
Fit RMSE: 0.03252724294164798
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.008327007293701172
PCA fit: 2.6335127353668213
[[ 9.80392158e-01 -2.14923946e-09]
 [-2.14923986e-09  9.80392149e-01]]
PCA Transform: 0.023303508758544922
total iterations: 9
TLDA fit: 3.857142448425293
Whitened factor: 
[[ 0.91189384  0.54950714]
 [ 0.65457773 -0.82817   ]]
PCA Reverse Transform: 0.000209808349609375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03003251313317177
Fit RMSE: 0.030000170336302187
 Test Against Ground Truth
[(' decentering', 0.001425027847290039), (' smoothing and normalization', 0.0004131793975830078)]
Smoothing and Normalization: 0.0005095005035400391
Fit RMSE: 0.02863653210986786
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013872623443603516
PCA fit: 4.629374742507935
[[ 9.80392160e-01 -8.51368042e-10]
 [-8.51368098e-10  9.80392159e-01]]
PCA Transform: 0.02772808074951172
total iterations: 14
TLDA fit: 6.195409297943115
Whitened factor: 
[[-0.6865432   0.81841207]
 [ 0.9034782   0.6650602 ]]
PCA Reverse Transform: 0.00022101402282714844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02552094773428245
Fit RMSE: 0.025811995191922715
 Test Against Ground Truth
[(' decentering', 0.001539468765258789), (' smoothing and normalization', 0.00035572052001953125)]
Smoothing and Normalization: 0.0005156993865966797
Fit RMSE: 0.025495595261186416
sklearn Test Against Ground Truth
Done!
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07738876342773438
PCA fit: 0.5362296104431152
[[ 9.80392164e-01 -2.50501953e-09]
 [-2.50501939e-09  9.80391979e-01]]
PCA Transform: 0.0022704601287841797
Traceback (most recent call last):
  File "generate_tables.py", line 496, in <module>
    main()
  File "generate_tables.py", line 446, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 377, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 104, in fit
    step =  lr*cumulant_gradient(self.factors_, y, self.alpha_0,self.theta)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/cumulant_gradient.py", line 25, in cumulant_gradient
    gradient -= 3*(1 + alpha)*(2 + alpha)/(2*y.shape[0])*tl.dot(y.T, tl.dot(y, phi)**2)
UnboundLocalError: local variable 'gradient' referenced before assignment
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07766866683959961
PCA fit: 0.5272805690765381
[[ 9.80392148e-01 -7.89681875e-09]
 [-7.89681942e-09  9.80392196e-01]]
PCA Transform: 0.002309083938598633
total iterations: 246
TLDA fit: 15.361703395843506
Whitened factor: 
[[-0.31703213  0.44375506]
 [-0.94841474  0.8961481 ]]
PCA Reverse Transform: 0.0010499954223632812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12207958546418247
Fit RMSE: 0.126539943919887
 Test Against Ground Truth
[(' decentering', 0.004926919937133789), (' smoothing and normalization', 0.0003161430358886719)]
Smoothing and Normalization: 0.00048232078552246094
Fit RMSE: 0.1203447118350984
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0035016536712646484
PCA fit: 0.2796602249145508
[[9.80392169e-01 3.13919084e-08]
 [3.13919080e-08 9.80392194e-01]]
PCA Transform: 0.005855560302734375
total iterations: 232
TLDA fit: 14.815523624420166
Whitened factor: 
[[ 0.9988941  -0.9758581 ]
 [-0.04701692  0.21840616]]
PCA Reverse Transform: 0.0002028942108154297
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056978325222471114
Fit RMSE: 0.05697837505101208
 Test Against Ground Truth
[(' decentering', 0.0012946128845214844), (' smoothing and normalization', 0.00033402442932128906)]
Smoothing and Normalization: 0.0004913806915283203
Fit RMSE: 0.05685590123582667
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005532264709472656
PCA fit: 0.7608697414398193
[[ 9.80392156e-01 -7.50282503e-10]
 [-7.50282336e-10  9.80392130e-01]]
PCA Transform: 0.011406421661376953
total iterations: 125
TLDA fit: 7.98955512046814
Whitened factor: 
[[ 0.24421398  0.22847351]
 [-0.9697213   0.97355014]]
PCA Reverse Transform: 0.00020170211791992188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04205444550391183
Fit RMSE: 0.041936937854166544
 Test Against Ground Truth
[(' decentering', 0.0013053417205810547), (' smoothing and normalization', 0.0003466606140136719)]
Smoothing and Normalization: 0.0004906654357910156
Fit RMSE: 0.03948115087236869
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.009112358093261719
PCA fit: 1.5126018524169922
[[9.80392158e-01 4.38763398e-09]
 [4.38763388e-09 9.80392169e-01]]
PCA Transform: 0.016947269439697266
total iterations: 74
TLDA fit: 4.737171411514282
Whitened factor: 
[[0.89314014 0.25609687]
 [0.44977853 0.96665114]]
PCA Reverse Transform: 0.00022745132446289062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03322078737087964
Fit RMSE: 0.033482181929144185
 Test Against Ground Truth
[(' decentering', 0.0014221668243408203), (' smoothing and normalization', 0.000377655029296875)]
Smoothing and Normalization: 0.0004646778106689453
Fit RMSE: 0.03244486274056978
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011630773544311523
PCA fit: 1.6721718311309814
[[9.80392158e-01 2.92208188e-09]
 [2.92208171e-09 9.80392164e-01]]
PCA Transform: 0.02241659164428711
total iterations: 79
TLDA fit: 5.073261499404907
Whitened factor: 
[[0.888951   0.33125573]
 [0.45800236 0.943541  ]]
PCA Reverse Transform: 0.000225067138671875
decenter with new strategy:
[-0.0025446  0.0003104]
decenter with old strategy:
[0.00219822 0.00411507]
Fit RMSE new decenter: 0.029298471126061345
Fit RMSE: 0.029060864063120157
 Test Against Ground Truth
[(' decentering', 0.0014340877532958984), (' smoothing and normalization', 0.0003540515899658203)]
Smoothing and Normalization: 0.0005137920379638672
Fit RMSE: 0.028148533480853267
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.012686014175415039
PCA fit: 2.570499897003174
[[9.80392156e-01 1.19620643e-09]
 [1.19620714e-09 9.80392153e-01]]
PCA Transform: 0.027678966522216797
total iterations: 78
TLDA fit: 4.910238742828369
Whitened factor: 
[[0.41933316 0.9506384 ]
 [0.90783244 0.3103008 ]]
PCA Reverse Transform: 0.00020313262939453125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026397678713265118
Fit RMSE: 0.026267402417504963
 Test Against Ground Truth
[(' decentering', 0.0014798641204833984), (' smoothing and normalization', 0.00035262107849121094)]
Smoothing and Normalization: 0.00057220458984375
Fit RMSE: 0.025605110895954773
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014495849609375
PCA fit: 0.04605388641357422
[[ 9.80392145e-01 -9.12203801e-09]
 [-9.12203776e-09  9.80392133e-01]]
PCA Transform: 0.0015745162963867188
total iterations: 42
TLDA fit: 2.6237151622772217
Whitened factor: 
[[ 0.930936   -0.772061  ]
 [-0.36518264  0.6355484 ]]
PCA Reverse Transform: 0.00020360946655273438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12319440837550162
Fit RMSE: 0.12416637443539821
 Test Against Ground Truth
[(' decentering', 0.0012924671173095703), (' smoothing and normalization', 0.00032520294189453125)]
Smoothing and Normalization: 0.0004832744598388672
Fit RMSE: 0.11551950148346489
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003074169158935547
PCA fit: 0.2986443042755127
[[9.80392163e-01 7.55226766e-09]
 [7.55226776e-09 9.80392193e-01]]
PCA Transform: 0.005845308303833008
total iterations: 204
TLDA fit: 13.097279787063599
Whitened factor: 
[[ 0.9975332  -0.9927163 ]
 [ 0.07019667  0.12047575]]
PCA Reverse Transform: 0.00022745132446289062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05596257719234379
Fit RMSE: 0.05601309573465334
 Test Against Ground Truth
[(' decentering', 0.0012984275817871094), (' smoothing and normalization', 0.0003826618194580078)]
Smoothing and Normalization: 0.0005533695220947266
Fit RMSE: 0.05582397772850698
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005699634552001953
PCA fit: 0.7534213066101074
[[9.80392156e-01 8.23819364e-10]
 [8.23819451e-10 9.80392142e-01]]
PCA Transform: 0.011264801025390625
total iterations: 54
TLDA fit: 3.385972023010254
Whitened factor: 
[[0.8783082  0.30708706]
 [0.4780949  0.95168144]]
PCA Reverse Transform: 0.0002346038818359375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04037497158933837
Fit RMSE: 0.04075786527316814
 Test Against Ground Truth
[(' decentering', 0.001363992691040039), (' smoothing and normalization', 0.0003600120544433594)]
Smoothing and Normalization: 0.0005290508270263672
Fit RMSE: 0.03928939712084881
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008627891540527344
PCA fit: 1.0408618450164795
[[ 9.80392154e-01 -6.65910351e-09]
 [-6.65910350e-09  9.80392131e-01]]
PCA Transform: 0.016974449157714844
total iterations: 10000
TLDA fit: 634.9632816314697
Whitened factor: 
[[ 0.37581208  0.09114313]
 [ 0.92669594 -0.99583775]]
PCA Reverse Transform: 0.00021505355834960938
decenter with new strategy:
[-0.05504527  0.02776164]
decenter with old strategy:
[0.33394676 0.36675701]
Fit RMSE new decenter: 0.03418597292645182
Fit RMSE: 0.03462477946079545
 Test Against Ground Truth
[(' decentering', 0.0013239383697509766), (' smoothing and normalization', 0.00038242340087890625)]
Smoothing and Normalization: 0.0004761219024658203
Fit RMSE: 0.03285178408386461
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.010677337646484375
PCA fit: 2.6355247497558594
[[9.80392157e-01 1.83124259e-09]
 [1.83124234e-09 9.80392171e-01]]
PCA Transform: 0.02233719825744629
total iterations: 53
TLDA fit: 3.433669090270996
Whitened factor: 
[[0.24726549 0.8245167 ]
 [0.9689477  0.56583756]]
PCA Reverse Transform: 0.0002186298370361328
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028742673392391265
Fit RMSE: 0.02903557928650313
 Test Against Ground Truth
[(' decentering', 0.0014073848724365234), (' smoothing and normalization', 0.0003712177276611328)]
Smoothing and Normalization: 0.0005099773406982422
Fit RMSE: 0.02805064917688526
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.015085458755493164
PCA fit: 4.696343898773193
[[ 9.80392152e-01 -8.15538208e-09]
 [-8.15538213e-09  9.80392162e-01]]
PCA Transform: 0.029962539672851562
total iterations: 96
TLDA fit: 6.014615774154663
Whitened factor: 
[[ 0.447612    0.10312413]
 [ 0.89422786 -0.9946686 ]]
PCA Reverse Transform: 0.00020742416381835938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026697448112341798
Fit RMSE: 0.026861997602200666
 Test Against Ground Truth
[(' decentering', 0.001833200454711914), (' smoothing and normalization', 0.0003466606140136719)]
Smoothing and Normalization: 0.0005030632019042969
Fit RMSE: 0.02565120552783802
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014390945434570312
PCA fit: 0.045931339263916016
[[ 9.80392157e-01 -4.11729787e-09]
 [-4.11729821e-09  9.80392147e-01]]
PCA Transform: 0.001676321029663086
total iterations: 46
TLDA fit: 2.8801887035369873
Whitened factor: 
[[-0.78610194  0.9374054 ]
 [ 0.6180967  -0.34824017]]
PCA Reverse Transform: 0.00020503997802734375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12001278630907922
Fit RMSE: 0.1211526337458052
 Test Against Ground Truth
[(' decentering', 0.0016872882843017578), (' smoothing and normalization', 0.0003173351287841797)]
Smoothing and Normalization: 0.00046443939208984375
Fit RMSE: 0.11981647591766711
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.002978801727294922
PCA fit: 0.22283101081848145
[[ 9.80392155e-01 -1.09666027e-08]
 [-1.09666033e-08  9.80392089e-01]]
PCA Transform: 0.006052494049072266
total iterations: 69
TLDA fit: 4.40435266494751
Whitened factor: 
[[ 0.23476177  0.30168957]
 [-0.972053    0.9534062 ]]
PCA Reverse Transform: 0.00020742416381835938
decenter with new strategy:
[0.00090291 0.00049033]
decenter with old strategy:
[0.00205308 0.00169914]
Fit RMSE new decenter: 0.05955414622483083
Fit RMSE: 0.05948465496158702
 Test Against Ground Truth
[(' decentering', 0.0016551017761230469), (' smoothing and normalization', 0.0003437995910644531)]
Smoothing and Normalization: 0.0005176067352294922
Fit RMSE: 0.05601374596227046
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.002704143524169922
PCA fit: 0.7680680751800537
[[9.80392156e-01 1.47344216e-09]
 [1.47344192e-09 9.80392118e-01]]
PCA Transform: 0.011306524276733398
total iterations: 47
TLDA fit: 2.9768381118774414
Whitened factor: 
[[0.85209304 0.27132913]
 [0.5233903  0.9624867 ]]
PCA Reverse Transform: 0.00021409988403320312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.040348997198018766
Fit RMSE: 0.040731704547755494
 Test Against Ground Truth
[(' decentering', 0.001338958740234375), (' smoothing and normalization', 0.0003426074981689453)]
Smoothing and Normalization: 0.0005581378936767578
Fit RMSE: 0.03928274072676135
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008702754974365234
PCA fit: 1.5139131546020508
[[9.80392159e-01 2.68485643e-09]
 [2.68485642e-09 9.80392156e-01]]
PCA Transform: 0.016961097717285156
total iterations: 50
TLDA fit: 3.240074872970581
Whitened factor: 
[[0.2776379 0.8448311]
 [0.9606858 0.5350331]]
PCA Reverse Transform: 0.00023055076599121094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.032986317165009646
Fit RMSE: 0.03332720960237372
 Test Against Ground Truth
[(' decentering', 0.0013680458068847656), (' smoothing and normalization', 0.0003523826599121094)]
Smoothing and Normalization: 0.0005502700805664062
Fit RMSE: 0.032109240948051565
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011746406555175781
PCA fit: 2.617720127105713
[[ 9.80392157e-01 -1.42320469e-09]
 [-1.42320488e-09  9.80392167e-01]]
PCA Transform: 0.022353649139404297
total iterations: 63
TLDA fit: 4.0371012687683105
Whitened factor: 
[[0.31209767 0.8815313 ]
 [0.95005006 0.47212553]]
PCA Reverse Transform: 0.00022602081298828125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02903211987887386
Fit RMSE: 0.029244453701361205
 Test Against Ground Truth
[(' decentering', 0.0013768672943115234), (' smoothing and normalization', 0.0003533363342285156)]
Smoothing and Normalization: 0.0005602836608886719
Fit RMSE: 0.028371885357509525
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.01349782943725586
PCA fit: 2.6580686569213867
[[9.80392158e-01 4.82510349e-10]
 [4.82510842e-10 9.80392183e-01]]
PCA Transform: 0.027724266052246094
total iterations: 35
TLDA fit: 2.185917377471924
Whitened factor: 
[[ 0.2992404   0.28510147]
 [-0.9541778   0.95849735]]
PCA Reverse Transform: 0.00020599365234375
decenter with new strategy:
[-0.00033415 -0.00021501]
decenter with old strategy:
[0.00356433 0.00365402]
Fit RMSE new decenter: 0.026755441863027415
Fit RMSE: 0.026750500369028953
 Test Against Ground Truth
[(' decentering', 0.0014798641204833984), (' smoothing and normalization', 0.00034618377685546875)]
Smoothing and Normalization: 0.0005080699920654297
Fit RMSE: 0.025358522416012674
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014085769653320312
PCA fit: 0.04662680625915527
[[ 9.80392167e-01 -1.50590609e-08]
 [-1.50590609e-08  9.80392262e-01]]
PCA Transform: 0.0015459060668945312
total iterations: 75
TLDA fit: 4.82732367515564
Whitened factor: 
[[-0.21056864 -0.31194395]
 [ 0.97757906 -0.9501005 ]]
PCA Reverse Transform: 0.0002067089080810547
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.13140339628453399
Fit RMSE: 0.13118101985540162
 Test Against Ground Truth
[(' decentering', 0.0012755393981933594), (' smoothing and normalization', 0.00034236907958984375)]
Smoothing and Normalization: 0.0004715919494628906
Fit RMSE: 0.12259039173174428
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0030968189239501953
PCA fit: 0.2726922035217285
[[ 9.80392158e-01 -7.31347934e-09]
 [-7.31347913e-09  9.80392125e-01]]
PCA Transform: 0.005888700485229492
total iterations: 139
TLDA fit: 8.919856071472168
Whitened factor: 
[[0.3179442 0.917005 ]
 [0.9481094 0.3988757]]
PCA Reverse Transform: 0.00020647048950195312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058088634028052394
Fit RMSE: 0.05738286679848836
 Test Against Ground Truth
[(' decentering', 0.0013072490692138672), (' smoothing and normalization', 0.0003635883331298828)]
Smoothing and Normalization: 0.0005314350128173828
Fit RMSE: 0.05553046040341569
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005621433258056641
PCA fit: 0.7776565551757812
[[ 9.80392161e-01 -6.48793774e-09]
 [-6.48793735e-09  9.80392177e-01]]
PCA Transform: 0.011566877365112305
total iterations: 230
TLDA fit: 14.709981918334961
Whitened factor: 
[[ 0.9765383  -0.9999614 ]
 [ 0.21534376 -0.00879042]]
PCA Reverse Transform: 0.00021219253540039062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.039683702586289615
Fit RMSE: 0.03976367367396866
 Test Against Ground Truth
[(' decentering', 0.0013549327850341797), (' smoothing and normalization', 0.0003674030303955078)]
Smoothing and Normalization: 0.0005307197570800781
Fit RMSE: 0.039618018015549754
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008903980255126953
PCA fit: 1.0618562698364258
[[ 9.80392157e-01 -7.74052999e-09]
 [-7.74053015e-09  9.80392118e-01]]
PCA Transform: 0.0169527530670166
total iterations: 77
TLDA fit: 5.00998592376709
Whitened factor: 
[[0.39470086 0.93207324]
 [0.9188097  0.3622698 ]]
PCA Reverse Transform: 0.0002238750457763672
decenter with new strategy:
[0.02469915 0.04692614]
decenter with old strategy:
[0.06540455 0.09779738]
Fit RMSE new decenter: 0.0338851451844885
Fit RMSE: 0.033784866411836444
 Test Against Ground Truth
[(' decentering', 0.0013782978057861328), (' smoothing and normalization', 0.00036525726318359375)]
Smoothing and Normalization: 0.0005018711090087891
Fit RMSE: 0.03287824576744318
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011670589447021484
PCA fit: 2.6612837314605713
[[ 9.80392157e-01 -4.59612686e-09]
 [-4.59612699e-09  9.80392133e-01]]
PCA Transform: 0.022133827209472656
total iterations: 55
TLDA fit: 3.605072021484375
Whitened factor: 
[[0.36060193 0.8977188 ]
 [0.9327198  0.4405688 ]]
PCA Reverse Transform: 0.00021767616271972656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0288499361067081
Fit RMSE: 0.02910381158994257
 Test Against Ground Truth
[(' decentering', 0.00139617919921875), (' smoothing and normalization', 0.00037670135498046875)]
Smoothing and Normalization: 0.00048065185546875
Fit RMSE: 0.028190750205538143
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.01401519775390625
PCA fit: 4.656869411468506
[[ 9.80392156e-01 -4.80844689e-09]
 [-4.80844714e-09  9.80392163e-01]]
PCA Transform: 0.0277249813079834
total iterations: 144
TLDA fit: 9.320432901382446
Whitened factor: 
[[0.9150118  0.31905255]
 [0.40342686 0.94773704]]
PCA Reverse Transform: 0.000213623046875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026500099780312873
Fit RMSE: 0.02622501652134269
 Test Against Ground Truth
[(' decentering', 0.0015044212341308594), (' smoothing and normalization', 0.0003883838653564453)]
Smoothing and Normalization: 0.0005345344543457031
Fit RMSE: 0.025551488924020982
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0016946792602539062
PCA fit: 0.046041250228881836
[[ 9.80392158e-01 -6.56946533e-09]
 [-6.56946520e-09  9.80392105e-01]]
PCA Transform: 0.001596212387084961
total iterations: 45
TLDA fit: 2.938682794570923
Whitened factor: 
[[ 0.92931217 -0.7708252 ]
 [-0.3692951   0.63704675]]
PCA Reverse Transform: 0.0002090930938720703
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12305586678781485
Fit RMSE: 0.12411164319073749
 Test Against Ground Truth
[(' decentering', 0.0012934207916259766), (' smoothing and normalization', 0.00033354759216308594)]
Smoothing and Normalization: 0.0004863739013671875
Fit RMSE: 0.11547795169015732
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003266572952270508
PCA fit: 0.2548389434814453
[[9.80392159e-01 5.36350276e-09]
 [5.36350320e-09 9.80392168e-01]]
PCA Transform: 0.005858421325683594
total iterations: 127
TLDA fit: 8.230098247528076
Whitened factor: 
[[ 0.08676813  0.22076459]
 [-0.9962286   0.9753271 ]]
PCA Reverse Transform: 0.00021409988403320312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058407371472749915
Fit RMSE: 0.05897647789427597
 Test Against Ground Truth
[(' decentering', 0.002744913101196289), (' smoothing and normalization', 0.0009400844573974609)]
Smoothing and Normalization: 0.0005211830139160156
Fit RMSE: 0.05542402038037962
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005749940872192383
PCA fit: 0.7647132873535156
[[9.80392156e-01 3.36909810e-09]
 [3.36909794e-09 9.80392152e-01]]
PCA Transform: 0.011352300643920898
total iterations: 43
TLDA fit: 2.7350082397460938
Whitened factor: 
[[ 0.24360657  0.3354137 ]
 [-0.96987414  0.942071  ]]
PCA Reverse Transform: 0.00022125244140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04190109119925292
Fit RMSE: 0.04191731267166827
 Test Against Ground Truth
[(' decentering', 0.0013554096221923828), (' smoothing and normalization', 0.0003533363342285156)]
Smoothing and Normalization: 0.0005412101745605469
Fit RMSE: 0.03938078566008988
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008900880813598633
PCA fit: 1.0575413703918457
[[ 9.80392156e-01 -7.62108109e-10]
 [-7.62108225e-10  9.80392144e-01]]
PCA Transform: 0.016949892044067383
total iterations: 41
TLDA fit: 2.607787847518921
Whitened factor: 
[[ 0.15583691  0.35808146]
 [-0.98778284  0.93369037]]
PCA Reverse Transform: 0.0002257823944091797
decenter with new strategy:
[ 7.13636646e-08 -1.45112751e-05]
decenter with old strategy:
[0.00011255 0.00011101]
Fit RMSE new decenter: 0.03430132374830608
Fit RMSE: 0.034618639712700745
 Test Against Ground Truth
[(' decentering', 0.0013856887817382812), (' smoothing and normalization', 0.0003578662872314453)]
Smoothing and Normalization: 0.00048732757568359375
Fit RMSE: 0.03293301088329473
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.012385845184326172
PCA fit: 2.66298246383667
[[ 9.80392155e-01 -2.76900937e-09]
 [-2.76900957e-09  9.80392154e-01]]
PCA Transform: 0.02220940589904785
total iterations: 46
TLDA fit: 2.9342596530914307
Whitened factor: 
[[0.848232   0.2759538 ]
 [0.52962494 0.9611709 ]]
PCA Reverse Transform: 0.00022029876708984375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02906677314281059
Fit RMSE: 0.029308283832129998
 Test Against Ground Truth
[(' decentering', 0.0014505386352539062), (' smoothing and normalization', 0.00037407875061035156)]
Smoothing and Normalization: 0.0005273818969726562
Fit RMSE: 0.02846023001561558
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.015004158020019531
PCA fit: 4.678531169891357
[[ 9.80392158e-01 -5.11885418e-09]
 [-5.11885412e-09  9.80392151e-01]]
PCA Transform: 0.027761459350585938
total iterations: 38
TLDA fit: 2.346850872039795
Whitened factor: 
[[-0.7551732   0.9487487 ]
 [ 0.6555253  -0.31603146]]
PCA Reverse Transform: 0.00021719932556152344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02528582653398067
Fit RMSE: 0.02552192908544337
 Test Against Ground Truth
[(' decentering', 0.0014891624450683594), (' smoothing and normalization', 0.0003409385681152344)]
Smoothing and Normalization: 0.0005123615264892578
Fit RMSE: 0.025263133792357263
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014147758483886719
PCA fit: 0.046193599700927734
[[9.80392157e-01 2.33242574e-08]
 [2.33242577e-08 9.80392171e-01]]
PCA Transform: 0.0016012191772460938
total iterations: 42
TLDA fit: 2.723592519760132
Whitened factor: 
[[-0.7349067  0.9085073]
 [ 0.6781683 -0.4178691]]
PCA Reverse Transform: 0.00022649765014648438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12451585101513421
Fit RMSE: 0.12578042047765553
 Test Against Ground Truth
[(' decentering', 0.0013628005981445312), (' smoothing and normalization', 0.00035309791564941406)]
Smoothing and Normalization: 0.0005099773406982422
Fit RMSE: 0.12431922875111989
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.002453327178955078
PCA fit: 0.2932159900665283
[[ 9.80392160e-01 -4.76609363e-09]
 [-4.76609356e-09  9.80392125e-01]]
PCA Transform: 0.005857944488525391
total iterations: 66
TLDA fit: 4.402745008468628
Whitened factor: 
[[ 0.28669184  0.24879532]
 [-0.9580228   0.9685561 ]]
PCA Reverse Transform: 0.00025725364685058594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05888461019634648
Fit RMSE: 0.058842497486818454
 Test Against Ground Truth
[(' decentering', 0.0013580322265625), (' smoothing and normalization', 0.0003864765167236328)]
Smoothing and Normalization: 0.0005333423614501953
Fit RMSE: 0.054834459493442414
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.0055141448974609375
PCA fit: 0.5796022415161133
[[ 9.80392156e-01 -2.43068867e-09]
 [-2.43068867e-09  9.80392155e-01]]
PCA Transform: 0.011240482330322266
total iterations: 61
TLDA fit: 4.0322535037994385
Whitened factor: 
[[0.27586123 0.8595999 ]
 [0.9611975  0.5109678 ]]
PCA Reverse Transform: 0.00021076202392578125
decenter with new strategy:
[0.25273357 0.52206553]
decenter with old strategy:
[0.69982225 1.08929327]
Fit RMSE new decenter: 0.04011602270214123
Fit RMSE: 0.04052586857542039
 Test Against Ground Truth
[(' decentering', 0.0013377666473388672), (' smoothing and normalization', 0.00035262107849121094)]
Smoothing and Normalization: 0.0005509853363037109
Fit RMSE: 0.03905701580930803
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.00761103630065918
PCA fit: 1.4967634677886963
[[9.80392156e-01 1.32045402e-10]
 [1.32045659e-10 9.80392158e-01]]
PCA Transform: 0.017122268676757812
total iterations: 82
TLDA fit: 5.446461915969849
Whitened factor: 
[[0.46692127 0.9675934 ]
 [0.8842989  0.25251347]]
PCA Reverse Transform: 0.00022482872009277344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03378220408444742
Fit RMSE: 0.03354873364268758
 Test Against Ground Truth
[(' decentering', 0.0013589859008789062), (' smoothing and normalization', 0.0003726482391357422)]
Smoothing and Normalization: 0.0005414485931396484
Fit RMSE: 0.03265283253713721
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.01105809211730957
PCA fit: 2.6200716495513916
[[ 9.80392158e-01 -3.84294623e-09]
 [-3.84294611e-09  9.80392197e-01]]
PCA Transform: 0.022430896759033203
total iterations: 165
TLDA fit: 10.83384084701538
Whitened factor: 
[[0.9018001  0.12636574]
 [0.43215346 0.9919837 ]]
PCA Reverse Transform: 0.00020551681518554688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028711465105901317
Fit RMSE: 0.028952818419984896
 Test Against Ground Truth
[(' decentering', 0.0013842582702636719), (' smoothing and normalization', 0.00036597251892089844)]
Smoothing and Normalization: 0.0005109310150146484
Fit RMSE: 0.028204325073316834
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014749288558959961
PCA fit: 4.691832780838013
[[9.80392157e-01 2.89426896e-11]
 [2.89426299e-11 9.80392176e-01]]
PCA Transform: 0.027985572814941406
total iterations: 38
TLDA fit: 2.429110527038574
Whitened factor: 
[[0.803163   0.22677775]
 [0.59575945 0.9739465 ]]
PCA Reverse Transform: 0.00023484230041503906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025961886155457383
Fit RMSE: 0.02623244763630598
 Test Against Ground Truth
[(' decentering', 0.0015244483947753906), (' smoothing and normalization', 0.0003528594970703125)]
Smoothing and Normalization: 0.0005745887756347656
Fit RMSE: 0.025400798379050113
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014350414276123047
PCA fit: 0.04526209831237793
[[9.80392155e-01 2.96952988e-09]
 [2.96952991e-09 9.80392201e-01]]
PCA Transform: 0.0015344619750976562
total iterations: 66
TLDA fit: 4.337478160858154
Whitened factor: 
[[ 0.2808678   0.24629624]
 [-0.95974654  0.9691946 ]]
PCA Reverse Transform: 0.00022554397583007812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12907604984170412
Fit RMSE: 0.12875119599183274
 Test Against Ground Truth
[(' decentering', 0.0013229846954345703), (' smoothing and normalization', 0.0003414154052734375)]
Smoothing and Normalization: 0.0005068778991699219
Fit RMSE: 0.11729368847527698
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0031006336212158203
PCA fit: 0.275158166885376
[[ 9.80392160e-01 -1.01151509e-08]
 [-1.01151507e-08  9.80392176e-01]]
PCA Transform: 0.005835056304931641
total iterations: 58
TLDA fit: 3.7950620651245117
Whitened factor: 
[[ 0.2238037   0.31901145]
 [ 0.97463423 -0.9477509 ]]
PCA Reverse Transform: 0.0002357959747314453
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05887719630291896
Fit RMSE: 0.05969712174757939
 Test Against Ground Truth
[(' decentering', 0.0013685226440429688), (' smoothing and normalization', 0.0003871917724609375)]
Smoothing and Normalization: 0.0005285739898681641
Fit RMSE: 0.05657929710999254
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.0056302547454833984
PCA fit: 0.7395355701446533
[[9.80392163e-01 3.15536121e-09]
 [3.15536120e-09 9.80392182e-01]]
PCA Transform: 0.011962890625
total iterations: 39
TLDA fit: 2.534193754196167
Whitened factor: 
[[-0.80977     0.9605562 ]
 [ 0.5867474  -0.27808583]]
PCA Reverse Transform: 0.00022792816162109375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.040674438351871116
Fit RMSE: 0.0409072313076646
 Test Against Ground Truth
[(' decentering', 0.0013692378997802734), (' smoothing and normalization', 0.00039887428283691406)]
Smoothing and Normalization: 0.0004999637603759766
Fit RMSE: 0.040618873700350694
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008427858352661133
PCA fit: 1.4974539279937744
[[9.80392157e-01 2.13404843e-09]
 [2.13404863e-09 9.80392177e-01]]
PCA Transform: 0.016993999481201172
total iterations: 34
TLDA fit: 2.1722044944763184
Whitened factor: 
[[ 0.2692632   0.28313136]
 [ 0.96306664 -0.9590811 ]]
PCA Reverse Transform: 0.0002143383026123047
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03410179490411286
Fit RMSE: 0.034235547138633214
 Test Against Ground Truth
[(' decentering', 0.0013303756713867188), (' smoothing and normalization', 0.0003695487976074219)]
Smoothing and Normalization: 0.0004980564117431641
Fit RMSE: 0.03221207411813447
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011430740356445312
PCA fit: 2.6902313232421875
[[9.80392155e-01 1.30294234e-10]
 [1.30294147e-10 9.80392181e-01]]
PCA Transform: 0.022185564041137695
total iterations: 82
TLDA fit: 5.491132974624634
Whitened factor: 
[[0.9447677  0.3243792 ]
 [0.3277408  0.94592714]]
PCA Reverse Transform: 0.0002529621124267578
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02932470389986305
Fit RMSE: 0.029438963109306333
 Test Against Ground Truth
[(' decentering', 0.0013988018035888672), (' smoothing and normalization', 0.00035262107849121094)]
Smoothing and Normalization: 0.0005326271057128906
Fit RMSE: 0.028757397374290887
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013573884963989258
PCA fit: 4.645989656448364
[[ 9.80392155e-01 -3.87794965e-09]
 [-3.87794930e-09  9.80392148e-01]]
PCA Transform: 0.027704954147338867
total iterations: 52
TLDA fit: 3.332948923110962
Whitened factor: 
[[0.86891675 0.31208596]
 [0.49495834 0.9500538 ]]
PCA Reverse Transform: 0.0002090930938720703
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02596345488833152
Fit RMSE: 0.02617946493159955
 Test Against Ground Truth
[(' decentering', 0.0014958381652832031), (' smoothing and normalization', 0.00038933753967285156)]
Smoothing and Normalization: 0.0005214214324951172
Fit RMSE: 0.025388785695633888
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013761520385742188
PCA fit: 0.04543948173522949
[[9.80392158e-01 1.77137029e-08]
 [1.77137030e-08 9.80392094e-01]]
PCA Transform: 0.0015263557434082031
total iterations: 82
TLDA fit: 5.378691911697388
Whitened factor: 
[[-0.02149936  0.3180451 ]
 [ 0.9997688  -0.9480756 ]]
PCA Reverse Transform: 0.00024366378784179688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12382007275821456
Fit RMSE: 0.1272426625447052
 Test Against Ground Truth
[(' decentering', 0.0013210773468017578), (' smoothing and normalization', 0.0003428459167480469)]
Smoothing and Normalization: 0.00047898292541503906
Fit RMSE: 0.11729655250854289
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0029571056365966797
PCA fit: 0.27263331413269043
[[9.80392157e-01 1.54330808e-09]
 [1.54330849e-09 9.80392144e-01]]
PCA Transform: 0.005832672119140625
total iterations: 120
TLDA fit: 7.728131532669067
Whitened factor: 
[[ 0.99768674 -0.96094745]
 [-0.06798021  0.27673092]]
PCA Reverse Transform: 0.0002353191375732422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05630011275611954
Fit RMSE: 0.05637286808730701
 Test Against Ground Truth
[(' decentering', 0.0027205944061279297), (' smoothing and normalization', 0.0009191036224365234)]
Smoothing and Normalization: 0.00052642822265625
Fit RMSE: 0.056204769277242134
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005281209945678711
PCA fit: 0.7413208484649658
[[9.80392158e-01 4.74443012e-09]
 [4.74443025e-09 9.80392162e-01]]
PCA Transform: 0.011231660842895508
total iterations: 122
TLDA fit: 7.843263626098633
Whitened factor: 
[[-0.11381525  0.42697415]
 [-0.99350196  0.90426385]]
PCA Reverse Transform: 0.00022673606872558594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04128738169747009
Fit RMSE: 0.04220713418288164
 Test Against Ground Truth
[(' decentering', 0.001394510269165039), (' smoothing and normalization', 0.00041222572326660156)]
Smoothing and Normalization: 0.0004849433898925781
Fit RMSE: 0.040226813183700943
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.007890701293945312
PCA fit: 1.486145257949829
[[9.80392155e-01 1.50909271e-09]
 [1.50909268e-09 9.80392195e-01]]
PCA Transform: 0.016894817352294922
total iterations: 65
TLDA fit: 4.185075998306274
Whitened factor: 
[[0.31754756 0.898952  ]
 [0.94824237 0.4380472 ]]
PCA Reverse Transform: 0.0002148151397705078
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.033323523207489485
Fit RMSE: 0.03353608229827184
 Test Against Ground Truth
[(' decentering', 0.0013697147369384766), (' smoothing and normalization', 0.0003676414489746094)]
Smoothing and Normalization: 0.0005209445953369141
Fit RMSE: 0.03245684881780709
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.009725570678710938
PCA fit: 2.6441843509674072
[[9.80392155e-01 2.64383081e-09]
 [2.64383113e-09 9.80392153e-01]]
PCA Transform: 0.02241373062133789
total iterations: 10000
TLDA fit: 644.3319540023804
Whitened factor: 
[[0.3923876  0.9154611 ]
 [0.9197999  0.40240648]]
PCA Reverse Transform: 0.00021409988403320312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028940974613394416
Fit RMSE: 0.02915381899106076
 Test Against Ground Truth
[(' decentering', 0.001407623291015625), (' smoothing and normalization', 0.00034356117248535156)]
Smoothing and Normalization: 0.0005214214324951172
Fit RMSE: 0.028280224314456447
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.01281428337097168
PCA fit: 4.628596067428589
[[ 9.80392156e-01 -7.83402763e-09]
 [-7.83402759e-09  9.80392196e-01]]
PCA Transform: 0.027967214584350586
total iterations: 49
TLDA fit: 3.128622531890869
Whitened factor: 
[[ 0.14182185  0.3525371 ]
 [-0.98989224  0.9357978 ]]
PCA Reverse Transform: 0.000225067138671875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026717526884142535
Fit RMSE: 0.027053368164973647
 Test Against Ground Truth
[(' decentering', 0.0014770030975341797), (' smoothing and normalization', 0.00037097930908203125)]
Smoothing and Normalization: 0.0005505084991455078
Fit RMSE: 0.025923269070322708
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0015325546264648438
PCA fit: 0.0504758358001709
[[ 9.80392148e-01 -6.60201838e-09]
 [-6.60201790e-09  9.80392150e-01]]
PCA Transform: 0.0016222000122070312
total iterations: 52
TLDA fit: 3.405135154724121
Whitened factor: 
[[-0.74670786  0.9002699 ]
 [ 0.66515213 -0.4353321 ]]
PCA Reverse Transform: 0.00023126602172851562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12045427489299138
Fit RMSE: 0.12188697048284379
 Test Against Ground Truth
[(' decentering', 0.0013396739959716797), (' smoothing and normalization', 0.0003516674041748047)]
Smoothing and Normalization: 0.0004909038543701172
Fit RMSE: 0.12006269639871905
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003197193145751953
PCA fit: 0.2853996753692627
[[9.80392158e-01 1.21274193e-08]
 [1.21274196e-08 9.80392116e-01]]
PCA Transform: 0.005904436111450195
total iterations: 105
TLDA fit: 6.8381712436676025
Whitened factor: 
[[-0.06514817  0.33540413]
 [ 0.99787563 -0.9420744 ]]
PCA Reverse Transform: 0.0002257823944091797
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05937951123200999
Fit RMSE: 0.05984834965068574
 Test Against Ground Truth
[(' decentering', 0.0013990402221679688), (' smoothing and normalization', 0.0003848075866699219)]
Smoothing and Normalization: 0.0005469322204589844
Fit RMSE: 0.05749370982618452
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005768775939941406
PCA fit: 0.7555520534515381
[[ 9.80392156e-01 -8.72591254e-09]
 [-8.72591251e-09  9.80392142e-01]]
PCA Transform: 0.01122140884399414
total iterations: 100
TLDA fit: 6.628762245178223
Whitened factor: 
[[0.37247515 0.9488996 ]
 [0.9280422  0.3155779 ]]
PCA Reverse Transform: 0.00023555755615234375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.041100705124074384
Fit RMSE: 0.041051649320477133
 Test Against Ground Truth
[(' decentering', 0.0013892650604248047), (' smoothing and normalization', 0.0003840923309326172)]
Smoothing and Normalization: 0.0005092620849609375
Fit RMSE: 0.03992448223855523
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.0074193477630615234
PCA fit: 1.495464563369751
[[9.80392157e-01 1.94903545e-10]
 [1.94903575e-10 9.80392149e-01]]
PCA Transform: 0.017229557037353516
total iterations: 49
TLDA fit: 3.2780089378356934
Whitened factor: 
[[0.24001816 0.8233383 ]
 [0.9707684  0.56755084]]
PCA Reverse Transform: 0.00022459030151367188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.033085551712063975
Fit RMSE: 0.03343558365857544
 Test Against Ground Truth
[(' decentering', 0.0014128684997558594), (' smoothing and normalization', 0.0003857612609863281)]
Smoothing and Normalization: 0.0005207061767578125
Fit RMSE: 0.03226644628278184
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011668920516967773
PCA fit: 2.666358232498169
[[ 9.80392154e-01 -2.86107039e-09]
 [-2.86107052e-09  9.80392151e-01]]
PCA Transform: 0.02222418785095215
total iterations: 73
TLDA fit: 4.7891130447387695
Whitened factor: 
[[0.892944   0.32227588]
 [0.45016778 0.94664586]]
PCA Reverse Transform: 0.00022482872009277344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.029461683000317818
Fit RMSE: 0.029200332723320947
 Test Against Ground Truth
[(' decentering', 0.0014448165893554688), (' smoothing and normalization', 0.00035381317138671875)]
Smoothing and Normalization: 0.0005691051483154297
Fit RMSE: 0.028353805621450155
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.012070655822753906
PCA fit: 4.715673923492432
[[ 9.80392158e-01 -4.53149827e-09]
 [-4.53149844e-09  9.80392130e-01]]
PCA Transform: 0.02771759033203125
total iterations: 55
TLDA fit: 3.6388397216796875
Whitened factor: 
[[0.32600752 0.9121423 ]
 [0.94536716 0.40987393]]
PCA Reverse Transform: 0.0002117156982421875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026218031084329768
Fit RMSE: 0.026374497828117446
 Test Against Ground Truth
[(' decentering', 0.0014564990997314453), (' smoothing and normalization', 0.0003476142883300781)]
Smoothing and Normalization: 0.0005476474761962891
Fit RMSE: 0.02570616948410771
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.001596689224243164
PCA fit: 0.05058002471923828
[[9.80392174e-01 2.33217521e-08]
 [2.33217519e-08 9.80392191e-01]]
PCA Transform: 0.0015342235565185547
total iterations: 61
TLDA fit: 3.9747567176818848
Whitened factor: 
[[ 0.97002494 -0.876544  ]
 [-0.24300545  0.48132175]]
PCA Reverse Transform: 0.00021839141845703125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12201410393763287
Fit RMSE: 0.12276072828163366
 Test Against Ground Truth
[(' decentering', 0.001276254653930664), (' smoothing and normalization', 0.0003352165222167969)]
Smoothing and Normalization: 0.0005078315734863281
Fit RMSE: 0.1216808594507228
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0030274391174316406
PCA fit: 0.26932215690612793
[[9.80392156e-01 1.93263168e-09]
 [1.93263148e-09 9.80392141e-01]]
PCA Transform: 0.005844593048095703
total iterations: 52
TLDA fit: 3.420921802520752
Whitened factor: 
[[ 0.95262295 -0.80853504]
 [-0.30415362  0.58844805]]
PCA Reverse Transform: 0.00020813941955566406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05554241006975816
Fit RMSE: 0.05597446981047777
 Test Against Ground Truth
[(' decentering', 0.0013310909271240234), (' smoothing and normalization', 0.0003743171691894531)]
Smoothing and Normalization: 0.0005688667297363281
Fit RMSE: 0.05542829118222615
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.002706289291381836
PCA fit: 0.7460918426513672
[[9.80392153e-01 6.13982285e-09]
 [6.13982306e-09 9.80392160e-01]]
PCA Transform: 0.01127934455871582
total iterations: 133
TLDA fit: 8.578441858291626
Whitened factor: 
[[ 0.99185365 -0.9492536 ]
 [-0.12738255  0.31451157]]
PCA Reverse Transform: 0.00021266937255859375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04063042027686093
Fit RMSE: 0.04068702813235862
 Test Against Ground Truth
[(' decentering', 0.0013234615325927734), (' smoothing and normalization', 0.0003790855407714844)]
Smoothing and Normalization: 0.0005552768707275391
Fit RMSE: 0.04055626116758352
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.0074634552001953125
PCA fit: 1.4885294437408447
[[ 9.80392156e-01 -4.52373451e-10]
 [-4.52373894e-10  9.80392160e-01]]
PCA Transform: 0.017099857330322266
total iterations: 93
TLDA fit: 5.9386186599731445
Whitened factor: 
[[ 0.30974543  0.25223008]
 [ 0.95081955 -0.9676674 ]]
PCA Reverse Transform: 0.00021409988403320312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03452644238396112
Fit RMSE: 0.03453420376235114
 Test Against Ground Truth
[(' decentering', 0.0013699531555175781), (' smoothing and normalization', 0.0003452301025390625)]
Smoothing and Normalization: 0.0005555152893066406
Fit RMSE: 0.032804604127925976
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.009929180145263672
PCA fit: 2.629981756210327
[[ 9.80392160e-01 -1.70923714e-09]
 [-1.70923713e-09  9.80392182e-01]]
PCA Transform: 0.02259993553161621
total iterations: 46
TLDA fit: 2.994852304458618
Whitened factor: 
[[ 0.9447159 -0.7874895]
 [-0.3278901  0.6163281]]
PCA Reverse Transform: 0.0002224445343017578
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02838418057896714
Fit RMSE: 0.028648060055184624
 Test Against Ground Truth
[(' decentering', 0.0013995170593261719), (' smoothing and normalization', 0.0003521442413330078)]
Smoothing and Normalization: 0.0005538463592529297
Fit RMSE: 0.028379726990092943
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014070987701416016
PCA fit: 4.672270774841309
[[ 9.80392158e-01 -3.68652502e-09]
 [-3.68652494e-09  9.80392140e-01]]
PCA Transform: 0.02826070785522461
total iterations: 142
TLDA fit: 9.047690391540527
Whitened factor: 
[[ 0.00829723  0.23152165]
 [ 0.99996555 -0.97282976]]
PCA Reverse Transform: 0.0002167224884033203
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025820547194549737
Fit RMSE: 0.026526143252106026
 Test Against Ground Truth
[(' decentering', 0.0015261173248291016), (' smoothing and normalization', 0.0003571510314941406)]
Smoothing and Normalization: 0.0005440711975097656
Fit RMSE: 0.025241572473765447
sklearn Test Against Ground Truth
Done!
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07764434814453125
PCA fit: 0.5312542915344238
[[ 9.80392157e-01 -5.29901305e-09]
 [-5.29901255e-09  9.80392221e-01]]
PCA Transform: 0.002227306365966797
Traceback (most recent call last):
  File "generate_tables.py", line 496, in <module>
    main()
  File "generate_tables.py", line 446, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 377, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 114, in fit
    if curr_max_step == None or curr_max >= curr_max_step:
  File "cupy/_core/core.pyx", line 1027, in cupy._core.core.ndarray.__richcmp__
TypeError: operand type(s) all returned NotImplemented from __array_ufunc__(<ufunc 'equal'>, '__call__', array(0.00624339, dtype=float32), None): 'ndarray', 'NoneType'
Traceback (most recent call last):
  File "generate_tables.py", line 25, in <module>
    from version0_20.tlda_final import TLDA
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 118
    self.factors_ = -= lr*cumulant_gradient(self.factors_, y, self.alpha_0,self.theta)
                    ^
SyntaxError: invalid syntax
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07879161834716797
PCA fit: 0.5236518383026123
[[9.80392161e-01 1.93122003e-08]
 [1.93122006e-08 9.80391325e-01]]
PCA Transform: 0.002235889434814453
total iterations: 750
TLDA fit: 45.93847632408142
Whitened factor: 
[[-0.88437456  0.97184837]
 [ 0.46677816 -0.23560725]]
PCA Reverse Transform: 0.0011780261993408203
decenter with new strategy:
[ 0.00036112 -0.00027816]
decenter with old strategy:
[0.00078278 0.00016518]
Fit RMSE new decenter: 0.12539382222112666
Fit RMSE: 0.12588285844653566
 Test Against Ground Truth
[(' decentering', 0.004525661468505859), (' smoothing and normalization', 0.0002930164337158203)]
Smoothing and Normalization: 0.0004558563232421875
Fit RMSE: 0.12530484806666953
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003263711929321289
PCA fit: 0.2653536796569824
[[ 9.80392156e-01 -5.72527303e-09]
 [-5.72527284e-09  9.80392149e-01]]
PCA Transform: 0.005762815475463867
total iterations: 1303
TLDA fit: 79.9473135471344
Whitened factor: 
[[ 0.2756606   0.31070387]
 [ 0.96125513 -0.9505068 ]]
PCA Reverse Transform: 0.00032711029052734375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05910764563420803
Fit RMSE: 0.059052788854289856
 Test Against Ground Truth
[(' decentering', 0.0013034343719482422), (' smoothing and normalization', 0.0002827644348144531)]
Smoothing and Normalization: 0.0005023479461669922
Fit RMSE: 0.055202618213320544
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005996227264404297
PCA fit: 0.779181718826294
[[9.80392159e-01 5.05297665e-09]
 [5.05297703e-09 9.80392167e-01]]
PCA Transform: 0.011499166488647461
total iterations: 503
TLDA fit: 30.858353853225708
Whitened factor: 
[[0.91102135 0.3122171 ]
 [0.41235906 0.9500108 ]]
PCA Reverse Transform: 0.0002777576446533203
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0417025429792996
Fit RMSE: 0.04134211589731311
 Test Against Ground Truth
[(' decentering', 0.0014033317565917969), (' smoothing and normalization', 0.00035858154296875)]
Smoothing and Normalization: 0.0004825592041015625
Fit RMSE: 0.04023363588981157
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.009190559387207031
PCA fit: 1.4610517024993896
[[ 9.80392154e-01 -5.17868418e-09]
 [-5.17868360e-09  9.80392125e-01]]
PCA Transform: 0.016918659210205078
total iterations: 4744
TLDA fit: 295.77071356773376
Whitened factor: 
[[-0.9912645   0.9931816 ]
 [ 0.131889    0.11657751]]
PCA Reverse Transform: 0.0006346702575683594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03332146947308929
Fit RMSE: 0.03337966617617867
 Test Against Ground Truth
[(' decentering', 0.0013887882232666016), (' smoothing and normalization', 0.00034308433532714844)]
Smoothing and Normalization: 0.000492095947265625
Fit RMSE: 0.033285670811250036
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011097908020019531
PCA fit: 2.6482748985290527
[[ 9.80392155e-01 -2.87643500e-09]
 [-2.87643507e-09  9.80392148e-01]]
PCA Transform: 0.02215743064880371
total iterations: 595
TLDA fit: 36.820231676101685
Whitened factor: 
[[0.19652745 0.81580985]
 [0.9804983  0.5783202 ]]
PCA Reverse Transform: 0.0002562999725341797
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028377451951696653
Fit RMSE: 0.02873768124574589
 Test Against Ground Truth
[(' decentering', 0.001377105712890625), (' smoothing and normalization', 0.00034165382385253906)]
Smoothing and Normalization: 0.000499725341796875
Fit RMSE: 0.027644357774875485
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.015180826187133789
PCA fit: 4.678357124328613
[[ 9.80392156e-01 -2.23862495e-09]
 [-2.23862508e-09  9.80392150e-01]]
PCA Transform: 0.027822256088256836
total iterations: 1199
TLDA fit: 74.75655937194824
Whitened factor: 
[[0.8310039 0.2643356]
 [0.5562666 0.9644308]]
PCA Reverse Transform: 0.00030493736267089844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025733853747445867
Fit RMSE: 0.02601467359920281
 Test Against Ground Truth
[(' decentering', 0.0014529228210449219), (' smoothing and normalization', 0.0003466606140136719)]
Smoothing and Normalization: 0.0004999637603759766
Fit RMSE: 0.025105311644879228
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.001373291015625
PCA fit: 0.04584622383117676
[[ 9.80392150e-01 -4.54105504e-09]
 [-4.54105486e-09  9.80392078e-01]]
PCA Transform: 0.0015270709991455078
total iterations: 1258
TLDA fit: 79.22001791000366
Whitened factor: 
[[-0.84283394  0.9643238 ]
 [ 0.5381738  -0.26472574]]
PCA Reverse Transform: 0.00028204917907714844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12116020662155448
Fit RMSE: 0.12186489344909641
 Test Against Ground Truth
[(' decentering', 0.0013306140899658203), (' smoothing and normalization', 0.0003256797790527344)]
Smoothing and Normalization: 0.0004475116729736328
Fit RMSE: 0.12094890776692922
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0034399032592773438
PCA fit: 0.28397297859191895
[[9.80392159e-01 4.81915549e-09]
 [4.81915574e-09 9.80392168e-01]]
PCA Transform: 0.005765199661254883
total iterations: 2104
TLDA fit: 131.1926624774933
Whitened factor: 
[[ 0.09863989  0.2614981 ]
 [-0.99512327  0.965204  ]]
PCA Reverse Transform: 0.00043463706970214844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05884296038180699
Fit RMSE: 0.05935607741527459
 Test Against Ground Truth
[(' decentering', 0.0013811588287353516), (' smoothing and normalization', 0.0003643035888671875)]
Smoothing and Normalization: 0.0004909038543701172
Fit RMSE: 0.05618525221637197
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006089210510253906
PCA fit: 0.7705831527709961
[[ 9.80392169e-01 -3.16806743e-08]
 [-3.16806740e-08  9.80392234e-01]]
PCA Transform: 0.011348724365234375
total iterations: 1739
TLDA fit: 108.19870853424072
Whitened factor: 
[[ 0.43987593  0.07433856]
 [-0.89805853  0.99723315]]
PCA Reverse Transform: 0.0003178119659423828
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.041944529163522905
Fit RMSE: 0.042600671868262836
 Test Against Ground Truth
[(' decentering', 0.00131988525390625), (' smoothing and normalization', 0.0003464221954345703)]
Smoothing and Normalization: 0.0005979537963867188
Fit RMSE: 0.0408068084680225
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.00896000862121582
PCA fit: 1.4911274909973145
[[9.80392156e-01 3.92447367e-09]
 [3.92447380e-09 9.80392164e-01]]
PCA Transform: 0.01694035530090332
total iterations: 816
TLDA fit: 50.927329540252686
Whitened factor: 
[[0.2511707  0.805023  ]
 [0.96794283 0.5932436 ]]
PCA Reverse Transform: 0.00027871131896972656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03293826756578878
Fit RMSE: 0.03334260995800384
 Test Against Ground Truth
[(' decentering', 0.0013446807861328125), (' smoothing and normalization', 0.0003387928009033203)]
Smoothing and Normalization: 0.0005242824554443359
Fit RMSE: 0.03209071677535213
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011008262634277344
PCA fit: 2.643597364425659
[[ 9.80392155e-01 -3.18260597e-09]
 [-3.18260613e-09  9.80392157e-01]]
PCA Transform: 0.02219986915588379
total iterations: 2345
TLDA fit: 146.97559905052185
Whitened factor: 
[[ 0.3352664   0.25121596]
 [ 0.9421235  -0.9679311 ]]
PCA Reverse Transform: 0.00040602684020996094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02985713570439285
Fit RMSE: 0.02990222007978848
 Test Against Ground Truth
[(' decentering', 0.0014078617095947266), (' smoothing and normalization', 0.0003466606140136719)]
Smoothing and Normalization: 0.0004978179931640625
Fit RMSE: 0.02839933452596648
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.01493525505065918
PCA fit: 2.6588282585144043
[[9.80392155e-01 3.87958095e-10]
 [3.87958827e-10 9.80392126e-01]]
PCA Transform: 0.0279541015625
total iterations: 700
TLDA fit: 43.87793946266174
Whitened factor: 
[[0.9407672  0.38221082]
 [0.33905336 0.9240752 ]]
PCA Reverse Transform: 0.000286102294921875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026294398511631963
Fit RMSE: 0.026256400842389845
 Test Against Ground Truth
[(' decentering', 0.0015158653259277344), (' smoothing and normalization', 0.00037097930908203125)]
Smoothing and Normalization: 0.0005259513854980469
Fit RMSE: 0.025545179257393406
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013604164123535156
PCA fit: 0.04642200469970703
[[9.80392158e-01 5.84844284e-09]
 [5.84844266e-09 9.80392156e-01]]
PCA Transform: 0.001577138900756836
total iterations: 918
TLDA fit: 57.70982909202576
Whitened factor: 
[[ 0.9621558 -0.8305153]
 [-0.2724999  0.5569958]]
PCA Reverse Transform: 0.0002646446228027344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.120010098441136
Fit RMSE: 0.12089922464717603
 Test Against Ground Truth
[(' decentering', 0.0012784004211425781), (' smoothing and normalization', 0.0003218650817871094)]
Smoothing and Normalization: 0.0004985332489013672
Fit RMSE: 0.11655692217078673
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0034935474395751953
PCA fit: 0.28534507751464844
[[ 9.80392163e-01 -2.77076013e-09]
 [-2.77076035e-09  9.80392103e-01]]
PCA Transform: 0.005769968032836914
total iterations: 1946
TLDA fit: 121.24260354042053
Whitened factor: 
[[ 0.41257215  0.15240133]
 [-0.9109249   0.98831874]]
PCA Reverse Transform: 0.0004119873046875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057765593774444936
Fit RMSE: 0.05939559644263544
 Test Against Ground Truth
[(' decentering', 0.0013849735260009766), (' smoothing and normalization', 0.0003895759582519531)]
Smoothing and Normalization: 0.0005545616149902344
Fit RMSE: 0.05614735910610355
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005883216857910156
PCA fit: 0.7406215667724609
[[9.80392155e-01 5.74147351e-10]
 [5.74147272e-10 9.80392178e-01]]
PCA Transform: 0.011240005493164062
total iterations: 462
TLDA fit: 28.69931650161743
Whitened factor: 
[[0.24584393 0.8315339 ]
 [0.96930945 0.5554739 ]]
PCA Reverse Transform: 0.00026226043701171875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04030136418843461
Fit RMSE: 0.04075979666822635
 Test Against Ground Truth
[(' decentering', 0.0013241767883300781), (' smoothing and normalization', 0.0003495216369628906)]
Smoothing and Normalization: 0.0005202293395996094
Fit RMSE: 0.03926033185909121
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008468866348266602
PCA fit: 1.483888864517212
[[9.80392157e-01 1.10821784e-09]
 [1.10821773e-09 9.80392117e-01]]
PCA Transform: 0.016965150833129883
total iterations: 887
TLDA fit: 55.384559869766235
Whitened factor: 
[[0.8533805  0.24838655]
 [0.5212886  0.968661  ]]
PCA Reverse Transform: 0.0002837181091308594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03361007567700102
Fit RMSE: 0.033846796858285184
 Test Against Ground Truth
[(' decentering', 0.0013935565948486328), (' smoothing and normalization', 0.0003657341003417969)]
Smoothing and Normalization: 0.0004968643188476562
Fit RMSE: 0.03291165152106721
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011859893798828125
PCA fit: 1.719916820526123
[[ 9.80392154e-01 -3.40171911e-09]
 [-3.40171908e-09  9.80392156e-01]]
PCA Transform: 0.02326822280883789
total iterations: 700
TLDA fit: 43.337427377700806
Whitened factor: 
[[0.8804629  0.23996393]
 [0.47411513 0.9707818 ]]
PCA Reverse Transform: 0.000270843505859375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028759129128574177
Fit RMSE: 0.029043053686413422
 Test Against Ground Truth
[(' decentering', 0.0014660358428955078), (' smoothing and normalization', 0.00035381317138671875)]
Smoothing and Normalization: 0.0004951953887939453
Fit RMSE: 0.028159732615443567
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014615058898925781
PCA fit: 4.699761629104614
[[9.80392159e-01 6.11349613e-09]
 [6.11349702e-09 9.80392179e-01]]
PCA Transform: 0.02777385711669922
total iterations: 44
TLDA fit: 2.7417120933532715
Whitened factor: 
[[0.50412905 0.9772434 ]
 [0.8636283  0.21212126]]
PCA Reverse Transform: 0.00024080276489257812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02665244538455716
Fit RMSE: 0.026424092602639098
 Test Against Ground Truth
[(' decentering', 0.001516580581665039), (' smoothing and normalization', 0.00034928321838378906)]
Smoothing and Normalization: 0.0004901885986328125
Fit RMSE: 0.025837976315145075
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0015447139739990234
PCA fit: 0.04555845260620117
[[ 9.80392148e-01 -3.44771163e-08]
 [-3.44771166e-08  9.80392034e-01]]
PCA Transform: 0.0016031265258789062
total iterations: 954
TLDA fit: 59.718026876449585
Whitened factor: 
[[ 0.9717105  -0.8944483 ]
 [-0.23617518  0.44717133]]
PCA Reverse Transform: 0.0002727508544921875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12460003951463207
Fit RMSE: 0.12513303961829106
 Test Against Ground Truth
[(' decentering', 0.0013217926025390625), (' smoothing and normalization', 0.0003306865692138672)]
Smoothing and Normalization: 0.000392913818359375
Fit RMSE: 0.12419794967654174
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003419160842895508
PCA fit: 0.21633625030517578
[[9.80392156e-01 3.48344467e-09]
 [3.48344466e-09 9.80392119e-01]]
PCA Transform: 0.005776166915893555
total iterations: 738
TLDA fit: 46.32930040359497
Whitened factor: 
[[0.934245   0.36302853]
 [0.35663196 0.9317781 ]]
PCA Reverse Transform: 0.00028252601623535156
decenter with new strategy:
[0.01807668 0.00914075]
decenter with old strategy:
[0.03749804 0.0244013 ]
Fit RMSE new decenter: 0.05771076803005366
Fit RMSE: 0.0568126937192687
 Test Against Ground Truth
[(' decentering', 0.001363992691040039), (' smoothing and normalization', 0.00034618377685546875)]
Smoothing and Normalization: 0.0005161762237548828
Fit RMSE: 0.05479489933778758
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006033420562744141
PCA fit: 0.726372241973877
[[9.80392155e-01 1.53509226e-09]
 [1.53509233e-09 9.80392163e-01]]
PCA Transform: 0.01248311996459961
total iterations: 1879
TLDA fit: 115.59140467643738
Whitened factor: 
[[-0.81097215  0.9576464 ]
 [ 0.58508474 -0.28794673]]
PCA Reverse Transform: 0.0003421306610107422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03996004776181082
Fit RMSE: 0.040251985908526106
 Test Against Ground Truth
[(' decentering', 0.0013620853424072266), (' smoothing and normalization', 0.00035190582275390625)]
Smoothing and Normalization: 0.0005316734313964844
Fit RMSE: 0.0398879180491197
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.009011507034301758
PCA fit: 1.4789175987243652
[[ 9.80392156e-01 -4.25049158e-09]
 [-4.25049164e-09  9.80392187e-01]]
PCA Transform: 0.01696634292602539
total iterations: 3723
TLDA fit: 233.12362051010132
Whitened factor: 
[[ 0.30017734  0.29148728]
 [ 0.95388347 -0.95657474]]
PCA Reverse Transform: 0.0004138946533203125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03476293915706462
Fit RMSE: 0.03473869980026235
 Test Against Ground Truth
[(' decentering', 0.0013992786407470703), (' smoothing and normalization', 0.0003674030303955078)]
Smoothing and Normalization: 0.0005161762237548828
Fit RMSE: 0.0331443199188113
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011107921600341797
PCA fit: 2.6464641094207764
[[ 9.80392156e-01 -3.87860744e-09]
 [-3.87860765e-09  9.80392128e-01]]
PCA Transform: 0.022174596786499023
total iterations: 503
TLDA fit: 31.815366506576538
Whitened factor: 
[[0.79443765 0.26541087]
 [0.60734576 0.96413547]]
PCA Reverse Transform: 0.0003139972686767578
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028409096570765683
Fit RMSE: 0.028797516068829403
 Test Against Ground Truth
[(' decentering', 0.001499176025390625), (' smoothing and normalization', 0.00038361549377441406)]
Smoothing and Normalization: 0.0005080699920654297
Fit RMSE: 0.02763830567450611
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014710187911987305
PCA fit: 4.656131029129028
[[9.80392156e-01 7.87979009e-10]
 [7.87979093e-10 9.80392145e-01]]
PCA Transform: 0.027709245681762695
total iterations: 1866
TLDA fit: 116.58346581459045
Whitened factor: 
[[0.33523792 0.8987376 ]
 [0.94213355 0.4384868 ]]
PCA Reverse Transform: 0.0003325939178466797
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02598953323661116
Fit RMSE: 0.02613307395742208
 Test Against Ground Truth
[(' decentering', 0.0015342235565185547), (' smoothing and normalization', 0.0003712177276611328)]
Smoothing and Normalization: 0.0005044937133789062
Fit RMSE: 0.025365273584347155
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.001501321792602539
PCA fit: 0.04591846466064453
[[ 9.80392150e-01 -1.56382133e-08]
 [-1.56382130e-08  9.80392314e-01]]
PCA Transform: 0.0015060901641845703
total iterations: 998
TLDA fit: 63.91454863548279
Whitened factor: 
[[-0.91338843  0.9791703 ]
 [ 0.4070892  -0.20304058]]
PCA Reverse Transform: 0.00025773048400878906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12099654245692902
Fit RMSE: 0.1215344441031128
 Test Against Ground Truth
[(' decentering', 0.0013575553894042969), (' smoothing and normalization', 0.0003337860107421875)]
Smoothing and Normalization: 0.00045990943908691406
Fit RMSE: 0.12084504214831059
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0031659603118896484
PCA fit: 0.22179698944091797
[[9.80392162e-01 1.27304533e-08]
 [1.27304535e-08 9.80392135e-01]]
PCA Transform: 0.0057582855224609375
total iterations: 1371
TLDA fit: 85.06201648712158
Whitened factor: 
[[ 0.29282495  0.278095  ]
 [ 0.9561661  -0.9605537 ]]
PCA Reverse Transform: 0.0002906322479248047
decenter with new strategy:
[0.0002024  0.00029578]
decenter with old strategy:
[0.00059675 0.00068607]
Fit RMSE new decenter: 0.05941602415847904
Fit RMSE: 0.05940300202000732
 Test Against Ground Truth
[(' decentering', 0.0012974739074707031), (' smoothing and normalization', 0.00036072731018066406)]
Smoothing and Normalization: 0.0005724430084228516
Fit RMSE: 0.05593072403424869
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006580829620361328
PCA fit: 0.74265456199646
[[9.80392156e-01 1.39079202e-09]
 [1.39079193e-09 9.80392181e-01]]
PCA Transform: 0.011362791061401367
total iterations: 1095
TLDA fit: 68.5176100730896
Whitened factor: 
[[0.9142825  0.3310364 ]
 [0.40507725 0.943618  ]]
PCA Reverse Transform: 0.00034332275390625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04208719938307468
Fit RMSE: 0.041545984296160715
 Test Against Ground Truth
[(' decentering', 0.0014047622680664062), (' smoothing and normalization', 0.0003540515899658203)]
Smoothing and Normalization: 0.0005657672882080078
Fit RMSE: 0.04056640492304633
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008118152618408203
PCA fit: 1.0289928913116455
[[ 9.80392158e-01 -3.32489547e-09]
 [-3.32489549e-09  9.80392141e-01]]
PCA Transform: 0.01714920997619629
total iterations: 266
TLDA fit: 16.721567153930664
Whitened factor: 
[[0.32130858 0.8560054 ]
 [0.9469746  0.51696706]]
PCA Reverse Transform: 0.000255584716796875
decenter with new strategy:
[5.55397680e-05 4.85446025e-05]
decenter with old strategy:
[9.21190794e-05 9.41871844e-05]
Fit RMSE new decenter: 0.03323446898321426
Fit RMSE: 0.03358265352989916
 Test Against Ground Truth
[(' decentering', 0.001390218734741211), (' smoothing and normalization', 0.00034809112548828125)]
Smoothing and Normalization: 0.0005202293395996094
Fit RMSE: 0.03243092666555216
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011867284774780273
PCA fit: 2.658153533935547
[[9.80392152e-01 9.91104250e-09]
 [9.91104287e-09 9.80392140e-01]]
PCA Transform: 0.022195100784301758
total iterations: 3464
TLDA fit: 216.65999937057495
Whitened factor: 
[[ 0.53864354  0.01309336]
 [-0.84253377  0.9999143 ]]
PCA Reverse Transform: 0.0003933906555175781
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02959885999914055
Fit RMSE: 0.030048058184122973
 Test Against Ground Truth
[(' decentering', 0.0014002323150634766), (' smoothing and normalization', 0.0003509521484375)]
Smoothing and Normalization: 0.0004889965057373047
Fit RMSE: 0.028646424162644412
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014522790908813477
PCA fit: 2.6313588619232178
[[9.80392159e-01 5.81019438e-09]
 [5.81019461e-09 9.80392173e-01]]
PCA Transform: 0.027944087982177734
total iterations: 556
TLDA fit: 35.02007007598877
Whitened factor: 
[[0.8334046  0.23302639]
 [0.5526634  0.97247046]]
PCA Reverse Transform: 0.00027060508728027344
decenter with new strategy:
[-1.63243661e-03  2.04349942e-06]
decenter with old strategy:
[0.00048072 0.00168336]
Fit RMSE new decenter: 0.025864585654229767
Fit RMSE: 0.026114014286598444
 Test Against Ground Truth
[(' decentering', 0.0015330314636230469), (' smoothing and normalization', 0.0003445148468017578)]
Smoothing and Normalization: 0.0005435943603515625
Fit RMSE: 0.025293457092566345
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.001544952392578125
PCA fit: 0.047627925872802734
[[ 9.80392160e-01 -2.43728244e-08]
 [-2.43728242e-08  9.80392134e-01]]
PCA Transform: 0.0015289783477783203
total iterations: 1059
TLDA fit: 66.25040364265442
Whitened factor: 
[[-0.73149365  0.9084664 ]
 [ 0.6818483  -0.41795796]]
PCA Reverse Transform: 0.00024700164794921875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12232827310216668
Fit RMSE: 0.12370900837040599
 Test Against Ground Truth
[(' decentering', 0.0013265609741210938), (' smoothing and normalization', 0.0003314018249511719)]
Smoothing and Normalization: 0.0004990100860595703
Fit RMSE: 0.12187567667955289
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0031871795654296875
PCA fit: 0.27407264709472656
[[ 9.80392159e-01 -4.24688255e-10]
 [-4.24688091e-10  9.80392135e-01]]
PCA Transform: 0.005769252777099609
total iterations: 1368
TLDA fit: 84.68156409263611
Whitened factor: 
[[0.42525023 0.952438  ]
 [0.9050759  0.3047327 ]]
PCA Reverse Transform: 0.0003781318664550781
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05734794342822342
Fit RMSE: 0.056486239913984916
 Test Against Ground Truth
[(' decentering', 0.0013480186462402344), (' smoothing and normalization', 0.00034499168395996094)]
Smoothing and Normalization: 0.0005657672882080078
Fit RMSE: 0.05447011759511816
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006002664566040039
PCA fit: 0.713918924331665
[[9.80392157e-01 4.84669359e-09]
 [4.84669328e-09 9.80392159e-01]]
PCA Transform: 0.011191129684448242
total iterations: 1900
TLDA fit: 117.22646594047546
Whitened factor: 
[[ 0.98885834 -0.91833586]
 [-0.14885975  0.39580202]]
PCA Reverse Transform: 0.0003364086151123047
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04110666617897688
Fit RMSE: 0.04118343965514549
 Test Against Ground Truth
[(' decentering', 0.0014057159423828125), (' smoothing and normalization', 0.00035500526428222656)]
Smoothing and Normalization: 0.0005247592926025391
Fit RMSE: 0.04105360359218613
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.00884866714477539
PCA fit: 1.4753830432891846
[[ 9.80392153e-01 -3.19598804e-09]
 [-3.19598706e-09  9.80392139e-01]]
PCA Transform: 0.016962766647338867
total iterations: 556
TLDA fit: 34.49658918380737
Whitened factor: 
[[0.38451132 0.9276907 ]
 [0.92312026 0.37334982]]
PCA Reverse Transform: 0.00024008750915527344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03395982941186381
Fit RMSE: 0.03373772579279892
 Test Against Ground Truth
[(' decentering', 0.0013401508331298828), (' smoothing and normalization', 0.00034546852111816406)]
Smoothing and Normalization: 0.0004930496215820312
Fit RMSE: 0.03279382530393953
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.012075424194335938
PCA fit: 2.6762335300445557
[[ 9.80392155e-01 -3.90643976e-09]
 [-3.90644012e-09  9.80392114e-01]]
PCA Transform: 0.022339820861816406
total iterations: 1146
TLDA fit: 71.30597352981567
Whitened factor: 
[[0.32286304 0.89195985]
 [0.9464457  0.45211473]]
PCA Reverse Transform: 0.0002880096435546875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.029624061670028217
Fit RMSE: 0.02926859698193501
 Test Against Ground Truth
[(' decentering', 0.001382589340209961), (' smoothing and normalization', 0.0003418922424316406)]
Smoothing and Normalization: 0.0004901885986328125
Fit RMSE: 0.028444643961616136
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014288902282714844
PCA fit: 4.682062387466431
[[ 9.80392157e-01 -3.71069242e-09]
 [-3.71069256e-09  9.80392159e-01]]
PCA Transform: 0.027757883071899414
total iterations: 2020
TLDA fit: 125.82798719406128
Whitened factor: 
[[ 0.24257681  0.36640817]
 [ 0.97013223 -0.93045425]]
PCA Reverse Transform: 0.0003218650817871094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025692353702930913
Fit RMSE: 0.026363818451049393
 Test Against Ground Truth
[(' decentering', 0.0015146732330322266), (' smoothing and normalization', 0.0003445148468017578)]
Smoothing and Normalization: 0.0005095005035400391
Fit RMSE: 0.024990795308234455
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013959407806396484
PCA fit: 0.046863555908203125
[[ 9.80392142e-01 -4.51694381e-09]
 [-4.51694409e-09  9.80391120e-01]]
PCA Transform: 0.0015621185302734375
total iterations: 1427
TLDA fit: 89.67971754074097
Whitened factor: 
[[ 0.97110957 -0.9412744 ]
 [-0.23863405  0.33764258]]
PCA Reverse Transform: 0.000278472900390625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1255922921609643
Fit RMSE: 0.125801135070801
 Test Against Ground Truth
[(' decentering', 0.0013058185577392578), (' smoothing and normalization', 0.00032973289489746094)]
Smoothing and Normalization: 0.0004639625549316406
Fit RMSE: 0.12542787909532935
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0030972957611083984
PCA fit: 0.2961277961730957
[[9.80392157e-01 1.70704451e-10]
 [1.70704898e-10 9.80392132e-01]]
PCA Transform: 0.00578618049621582
total iterations: 4386
TLDA fit: 273.3720519542694
Whitened factor: 
[[ 0.40001836 -0.12260732]
 [ 0.9165072  -0.99245524]]
PCA Reverse Transform: 0.0005280971527099609
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05858206349473607
Fit RMSE: 0.05945954062683691
 Test Against Ground Truth
[(' decentering', 0.00136566162109375), (' smoothing and normalization', 0.0003654956817626953)]
Smoothing and Normalization: 0.000492095947265625
Fit RMSE: 0.05688701323977049
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.00491023063659668
PCA fit: 0.732064962387085
[[9.80392156e-01 1.15189881e-08]
 [1.15189881e-08 9.80392157e-01]]
PCA Transform: 0.011179924011230469
total iterations: 1377
TLDA fit: 85.72718548774719
Whitened factor: 
[[ 0.3065186   0.29022607]
 [ 0.9518647  -0.9569582 ]]
PCA Reverse Transform: 0.0002808570861816406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04233567244446533
Fit RMSE: 0.04233625993251833
 Test Against Ground Truth
[(' decentering', 0.0013873577117919922), (' smoothing and normalization', 0.00034689903259277344)]
Smoothing and Normalization: 0.0005087852478027344
Fit RMSE: 0.04013590459215162
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008588552474975586
PCA fit: 1.5034193992614746
[[9.80392153e-01 4.22078606e-10]
 [4.22078752e-10 9.80392136e-01]]
PCA Transform: 0.0169374942779541
total iterations: 1760
TLDA fit: 110.14634537696838
Whitened factor: 
[[-0.5863532   0.87406266]
 [ 0.81005555  0.48581323]]
PCA Reverse Transform: 0.00031685829162597656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03307910728942208
Fit RMSE: 0.033482127201528385
 Test Against Ground Truth
[(' decentering', 0.0015187263488769531), (' smoothing and normalization', 0.00036072731018066406)]
Smoothing and Normalization: 0.0005629062652587891
Fit RMSE: 0.03306234847953027
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011509895324707031
PCA fit: 2.6631786823272705
[[ 9.80392156e-01 -2.42093476e-09]
 [-2.42093480e-09  9.80392152e-01]]
PCA Transform: 0.022419214248657227
total iterations: 504
TLDA fit: 31.942866802215576
Whitened factor: 
[[0.23772787 0.79153705]
 [0.97133183 0.6111211 ]]
PCA Reverse Transform: 0.00028896331787109375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028948837157676474
Fit RMSE: 0.029263260154375435
 Test Against Ground Truth
[(' decentering', 0.001430511474609375), (' smoothing and normalization', 0.00035071372985839844)]
Smoothing and Normalization: 0.0005018711090087891
Fit RMSE: 0.028292083676325176
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013273477554321289
PCA fit: 4.663982152938843
[[9.80392158e-01 7.45549185e-09]
 [7.45549167e-09 9.80392155e-01]]
PCA Transform: 0.027741432189941406
total iterations: 969
TLDA fit: 59.95994710922241
Whitened factor: 
[[-0.5691668   0.76318127]
 [ 0.8222221   0.6461845 ]]
PCA Reverse Transform: 0.0002684593200683594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02579469932127122
Fit RMSE: 0.02615244847096626
 Test Against Ground Truth
[(' decentering', 0.001474142074584961), (' smoothing and normalization', 0.0003407001495361328)]
Smoothing and Normalization: 0.0005323886871337891
Fit RMSE: 0.025764632292051042
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.001405954360961914
PCA fit: 0.0458986759185791
[[ 9.80392166e-01 -7.77065424e-09]
 [-7.77065399e-09  9.80392135e-01]]
PCA Transform: 0.0016031265258789062
total iterations: 2074
TLDA fit: 128.44492840766907
Whitened factor: 
[[ 0.34315658  0.05363132]
 [-0.93927824  0.99856085]]
PCA Reverse Transform: 0.0002951622009277344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12343342030391109
Fit RMSE: 0.12921627994472987
 Test Against Ground Truth
[(' decentering', 0.0013210773468017578), (' smoothing and normalization', 0.0003445148468017578)]
Smoothing and Normalization: 0.0004885196685791016
Fit RMSE: 0.12001734512963563
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003074169158935547
PCA fit: 0.26900529861450195
[[ 9.80392157e-01 -6.56882881e-12]
 [-6.56875769e-12  9.80392180e-01]]
PCA Transform: 0.005841493606567383
total iterations: 1748
TLDA fit: 107.30895256996155
Whitened factor: 
[[ 0.9939072  -0.9367191 ]
 [-0.11022004  0.35008207]]
PCA Reverse Transform: 0.00030040740966796875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05750315092911283
Fit RMSE: 0.05764935714028476
 Test Against Ground Truth
[(' decentering', 0.0013206005096435547), (' smoothing and normalization', 0.0003654956817626953)]
Smoothing and Normalization: 0.0005238056182861328
Fit RMSE: 0.05742363403080902
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005705118179321289
PCA fit: 0.7501986026763916
[[ 9.80392156e-01 -6.97475502e-09]
 [-6.97475473e-09  9.80392187e-01]]
PCA Transform: 0.011228084564208984
total iterations: 775
TLDA fit: 47.65185046195984
Whitened factor: 
[[0.25766772 0.9360154 ]
 [0.9662336  0.35195908]]
PCA Reverse Transform: 0.0002498626708984375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04159202507263807
Fit RMSE: 0.04116549973700874
 Test Against Ground Truth
[(' decentering', 0.0013501644134521484), (' smoothing and normalization', 0.0003426074981689453)]
Smoothing and Normalization: 0.0005357265472412109
Fit RMSE: 0.04013915568543837
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008758068084716797
PCA fit: 1.0492544174194336
[[ 9.80392156e-01 -1.06254149e-08]
 [-1.06254148e-08  9.80392143e-01]]
PCA Transform: 0.01697683334350586
total iterations: 571
TLDA fit: 35.262364625930786
Whitened factor: 
[[0.26313037 0.8563917 ]
 [0.9647603  0.5163266 ]]
PCA Reverse Transform: 0.0002627372741699219
decenter with new strategy:
[-4.32230488e-06 -3.44111665e-05]
decenter with old strategy:
[3.21986462e-05 1.18788854e-05]
Fit RMSE new decenter: 0.033068638395678
Fit RMSE: 0.03337692116652035
 Test Against Ground Truth
[(' decentering', 0.0013616085052490234), (' smoothing and normalization', 0.0003414154052734375)]
Smoothing and Normalization: 0.00047326087951660156
Fit RMSE: 0.03221609697548132
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011530399322509766
PCA fit: 2.6564137935638428
[[9.80392156e-01 3.04426850e-09]
 [3.04426815e-09 9.80392193e-01]]
PCA Transform: 0.022206544876098633
total iterations: 683
TLDA fit: 42.150327920913696
Whitened factor: 
[[0.8603052  0.25811884]
 [0.5097794  0.9661132 ]]
PCA Reverse Transform: 0.0002582073211669922
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028895635096146264
Fit RMSE: 0.02918319784069226
 Test Against Ground Truth
[(' decentering', 0.001392364501953125), (' smoothing and normalization', 0.0003495216369628906)]
Smoothing and Normalization: 0.0004730224609375
Fit RMSE: 0.028263502116883416
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014148473739624023
PCA fit: 4.697779417037964
[[ 9.80392156e-01 -1.96152714e-09]
 [-1.96152740e-09  9.80392153e-01]]
PCA Transform: 0.0277557373046875
total iterations: 3750
TLDA fit: 229.0012650489807
Whitened factor: 
[[ 0.32460937  0.30637977]
 [ 0.9458482  -0.9519094 ]]
PCA Reverse Transform: 0.00040268898010253906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026938419853605924
Fit RMSE: 0.026941184389838824
 Test Against Ground Truth
[(' decentering', 0.0015337467193603516), (' smoothing and normalization', 0.0003581047058105469)]
Smoothing and Normalization: 0.0004937648773193359
Fit RMSE: 0.025687242642236204
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014383792877197266
PCA fit: 0.045453548431396484
[[9.80392159e-01 2.98627678e-09]
 [2.98627685e-09 9.80392152e-01]]
PCA Transform: 0.0015611648559570312
total iterations: 527
TLDA fit: 32.8788161277771
Whitened factor: 
[[ 0.9030854   0.8198985 ]
 [-0.42946094  0.57250893]]
PCA Reverse Transform: 0.0002524852752685547
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12987454788936398
Fit RMSE: 0.12817086384685245
 Test Against Ground Truth
[(' decentering', 0.0012936592102050781), (' smoothing and normalization', 0.0003285408020019531)]
Smoothing and Normalization: 0.000476837158203125
Fit RMSE: 0.12152828393108872
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.002852916717529297
PCA fit: 0.25510644912719727
[[ 9.80392157e-01 -1.04541204e-08]
 [-1.04541208e-08  9.80392161e-01]]
PCA Transform: 0.005789756774902344
total iterations: 1868
TLDA fit: 114.51135182380676
Whitened factor: 
[[ 0.28746203  0.29189253]
 [ 0.95779204 -0.9564512 ]]
PCA Reverse Transform: 0.0002548694610595703
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05927064410229689
Fit RMSE: 0.05923066409821735
 Test Against Ground Truth
[(' decentering', 0.001310586929321289), (' smoothing and normalization', 0.0003552436828613281)]
Smoothing and Normalization: 0.000522613525390625
Fit RMSE: 0.05559672596851261
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005576610565185547
PCA fit: 0.7734637260437012
[[ 9.80392162e-01 -1.89533162e-08]
 [-1.89533163e-08  9.80392207e-01]]
PCA Transform: 0.011204719543457031
total iterations: 1808
TLDA fit: 111.03509426116943
Whitened factor: 
[[ 0.3935136  -0.05083639]
 [-0.9193188   0.998707  ]]
PCA Reverse Transform: 0.00024008750915527344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.042010297051427135
Fit RMSE: 0.04252919645603075
 Test Against Ground Truth
[(' decentering', 0.0013592243194580078), (' smoothing and normalization', 0.0003573894500732422)]
Smoothing and Normalization: 0.00046896934509277344
Fit RMSE: 0.04092397389820401
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008527278900146484
PCA fit: 1.5027296543121338
[[9.80392156e-01 5.74561169e-09]
 [5.74561169e-09 9.80392166e-01]]
PCA Transform: 0.016913652420043945
total iterations: 1543
TLDA fit: 94.71103858947754
Whitened factor: 
[[ 0.38185784  0.18766244]
 [ 0.92422116 -0.9822336 ]]
PCA Reverse Transform: 0.00027251243591308594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03439740058303011
Fit RMSE: 0.03460451278120069
 Test Against Ground Truth
[(' decentering', 0.0013625621795654297), (' smoothing and normalization', 0.00034809112548828125)]
Smoothing and Normalization: 0.00048351287841796875
Fit RMSE: 0.03287826776513907
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011600017547607422
PCA fit: 2.656816005706787
[[ 9.80392152e-01 -6.89295137e-09]
 [-6.89295141e-09  9.80392159e-01]]
PCA Transform: 0.022248506546020508
total iterations: 810
TLDA fit: 49.926957845687866
Whitened factor: 
[[0.4066122  0.92023426]
 [0.9136009  0.39136803]]
PCA Reverse Transform: 0.0002751350402832031
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02870539491560825
Fit RMSE: 0.029007645031322787
 Test Against Ground Truth
[(' decentering', 0.0014195442199707031), (' smoothing and normalization', 0.0003445148468017578)]
Smoothing and Normalization: 0.0004923343658447266
Fit RMSE: 0.02802934440498172
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014525413513183594
PCA fit: 4.642451286315918
[[ 9.80392155e-01 -6.85160856e-09]
 [-6.85160831e-09  9.80392117e-01]]
PCA Transform: 0.02776050567626953
total iterations: 988
TLDA fit: 60.84892773628235
Whitened factor: 
[[0.8450167  0.30318424]
 [0.53474003 0.95293194]]
PCA Reverse Transform: 0.00026154518127441406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02585128874331218
Fit RMSE: 0.026116944564707034
 Test Against Ground Truth
[(' decentering', 0.0014657974243164062), (' smoothing and normalization', 0.00035452842712402344)]
Smoothing and Normalization: 0.0004763603210449219
Fit RMSE: 0.025234570638992256
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014345645904541016
PCA fit: 0.04448533058166504
[[ 9.80392161e-01 -9.51666873e-09]
 [-9.51666941e-09  9.80392167e-01]]
PCA Transform: 0.0015726089477539062
total iterations: 846
TLDA fit: 52.4017858505249
Whitened factor: 
[[0.77797157 0.08290768]
 [0.6282996  0.99655724]]
PCA Reverse Transform: 0.00026297569274902344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12049543409697831
Fit RMSE: 0.12073714666685906
 Test Against Ground Truth
[(' decentering', 0.0013353824615478516), (' smoothing and normalization', 0.0003292560577392578)]
Smoothing and Normalization: 0.0004608631134033203
Fit RMSE: 0.11486075440072013
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003124713897705078
PCA fit: 0.2634572982788086
[[9.80392162e-01 1.03881851e-08]
 [1.03881852e-08 9.80392200e-01]]
PCA Transform: 0.005776166915893555
total iterations: 3243
TLDA fit: 198.5767195224762
Whitened factor: 
[[ 0.11342448  0.3399821 ]
 [-0.9935467   0.94043195]]
PCA Reverse Transform: 0.0003867149353027344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058328264347129594
Fit RMSE: 0.05918929457071968
 Test Against Ground Truth
[(' decentering', 0.0013129711151123047), (' smoothing and normalization', 0.0003554821014404297)]
Smoothing and Normalization: 0.0005257129669189453
Fit RMSE: 0.055798636631818124
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.0049593448638916016
PCA fit: 0.7362322807312012
[[9.80392155e-01 3.78378803e-09]
 [3.78378776e-09 9.80392111e-01]]
PCA Transform: 0.01120138168334961
total iterations: 1952
TLDA fit: 119.07198143005371
Whitened factor: 
[[-0.8609489   0.94968385]
 [ 0.5086914  -0.3132102 ]]
PCA Reverse Transform: 0.0003135204315185547
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04060127563707077
Fit RMSE: 0.04078148684632029
 Test Against Ground Truth
[(' decentering', 0.001336812973022461), (' smoothing and normalization', 0.00035452842712402344)]
Smoothing and Normalization: 0.00047707557678222656
Fit RMSE: 0.04053281607310871
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.0074160099029541016
PCA fit: 1.474015235900879
[[ 9.80392157e-01 -2.50514809e-09]
 [-2.50514856e-09  9.80392119e-01]]
PCA Transform: 0.016976356506347656
total iterations: 455
TLDA fit: 28.197856664657593
Whitened factor: 
[[0.4225567  0.9763775 ]
 [0.90633655 0.21607213]]
PCA Reverse Transform: 0.0002543926239013672
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03423639335527605
Fit RMSE: 0.033759456872335156
 Test Against Ground Truth
[(' decentering', 0.0013744831085205078), (' smoothing and normalization', 0.0003476142883300781)]
Smoothing and Normalization: 0.0004937648773193359
Fit RMSE: 0.03299845245977519
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.010043859481811523
PCA fit: 2.7135863304138184
[[ 9.80392157e-01 -4.98243251e-09]
 [-4.98243230e-09  9.80392148e-01]]
PCA Transform: 0.02227783203125
total iterations: 408
TLDA fit: 25.309680223464966
Whitened factor: 
[[0.45852256 0.9637692 ]
 [0.8886828  0.2667376 ]]
PCA Reverse Transform: 0.00027370452880859375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02943105553601116
Fit RMSE: 0.029078221557257634
 Test Against Ground Truth
[(' decentering', 0.0014302730560302734), (' smoothing and normalization', 0.0003533363342285156)]
Smoothing and Normalization: 0.0005285739898681641
Fit RMSE: 0.02827252850680397
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.012891292572021484
PCA fit: 2.6003634929656982
[[ 9.80392156e-01 -7.92081772e-10]
 [-7.92081594e-10  9.80392165e-01]]
PCA Transform: 0.027721166610717773
total iterations: 1593
TLDA fit: 97.46992087364197
Whitened factor: 
[[0.84645885 0.27761292]
 [0.5324542  0.96069306]]
PCA Reverse Transform: 0.0003247261047363281
decenter with new strategy:
[-0.02823818  0.00276864]
decenter with old strategy:
[0.00974913 0.0332013 ]
Fit RMSE new decenter: 0.02572062538473363
Fit RMSE: 0.02595680967268774
 Test Against Ground Truth
[(' decentering', 0.0014786720275878906), (' smoothing and normalization', 0.0003466606140136719)]
Smoothing and Normalization: 0.0005404949188232422
Fit RMSE: 0.025084493113338914
sklearn Test Against Ground Truth
Done!
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08374905586242676
PCA fit: 0.5456023216247559
[[ 9.80392154e-01 -1.21290255e-08]
 [-1.21290257e-08  9.80392190e-01]]
PCA Transform: 0.0022432804107666016
Traceback (most recent call last):
  File "generate_tables.py", line 497, in <module>
    main()
  File "generate_tables.py", line 447, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 200, vocab=vocab, seed=seed_arr[j]) 
  File "generate_tables.py", line 378, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 130, in fit
    max_diff_arr.append(max_diff[0])
  File "cupy/_core/core.pyx", line 1313, in cupy._core.core.ndarray.__getitem__
  File "cupy/_core/_routines_indexing.pyx", line 36, in cupy._core._routines_indexing._ndarray_getitem
  File "cupy/_core/_routines_indexing.pyx", line 281, in cupy._core._routines_indexing._prepare_slice_list
IndexError: too many indices for array
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07792115211486816
PCA fit: 0.5345613956451416
[[9.80392158e-01 1.25286590e-08]
 [1.25286586e-08 9.80392116e-01]]
PCA Transform: 0.0022416114807128906
total iterations: 2040
TLDA fit: 1055.392084121704
Whitened factor: 
[[ 0.95466095 -0.9452822 ]
 [-0.29769546  0.3262537 ]]
PCA Reverse Transform: 0.0010554790496826172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12517737887314256
Fit RMSE: 0.12621652366038177
 Test Against Ground Truth
[(' decentering', 0.004586458206176758), (' smoothing and normalization', 0.0002818107604980469)]
Smoothing and Normalization: 0.0004413127899169922
Fit RMSE: 0.12591781297227214
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0033369064331054688
PCA fit: 0.2791171073913574
[[ 9.80392165e-01 -2.81900860e-08]
 [-2.81900853e-08  9.80392222e-01]]
PCA Transform: 0.005854368209838867
total iterations: 1728
TLDA fit: 927.670428276062
Whitened factor: 
[[ 0.97964203 -0.9714566 ]
 [-0.20075236  0.23721766]]
PCA Reverse Transform: 0.0002467632293701172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05882447273039867
Fit RMSE: 0.058930047304759474
 Test Against Ground Truth
[(' decentering', 0.0013508796691894531), (' smoothing and normalization', 0.0002925395965576172)]
Smoothing and Normalization: 0.0005128383636474609
Fit RMSE: 0.0588051818174452
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006104946136474609
PCA fit: 0.7485127449035645
[[9.80392159e-01 7.67006680e-10]
 [7.67006392e-10 9.80392195e-01]]
PCA Transform: 0.011216163635253906
total iterations: 578
TLDA fit: 312.41417503356934
Whitened factor: 
[[ 0.08895655  0.00147689]
 [ 0.9960356  -0.9999989 ]]
PCA Reverse Transform: 0.00023221969604492188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.041670111269391075
Fit RMSE: 0.04203764784080301
 Test Against Ground Truth
[(' decentering', 0.0013151168823242188), (' smoothing and normalization', 0.00033664703369140625)]
Smoothing and Normalization: 0.0004715919494628906
Fit RMSE: 0.03968866550822322
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008917570114135742
PCA fit: 1.4928364753723145
[[9.80392157e-01 2.56298045e-11]
 [2.56297311e-11 9.80392169e-01]]
PCA Transform: 0.016943693161010742
total iterations: 1015
TLDA fit: 546.8131682872772
Whitened factor: 
[[ 0.88699585 -0.8589272 ]
 [-0.46177745  0.51209784]]
PCA Reverse Transform: 0.00025773048400878906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03233810760421818
Fit RMSE: 0.032625012598348525
 Test Against Ground Truth
[(' decentering', 0.0013394355773925781), (' smoothing and normalization', 0.00034117698669433594)]
Smoothing and Normalization: 0.0004673004150390625
Fit RMSE: 0.0322873627727749
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011511802673339844
PCA fit: 2.6775262355804443
[[9.80392157e-01 4.29708802e-10]
 [4.29708858e-10 9.80392156e-01]]
PCA Transform: 0.022222042083740234
total iterations: 1250
TLDA fit: 674.5870931148529
Whitened factor: 
[[-7.3995307e-04  5.8211010e-02]
 [ 9.9999976e-01 -9.9830437e-01]]
PCA Reverse Transform: 0.0002453327178955078
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028945702953230026
Fit RMSE: 0.029498379408800922
 Test Against Ground Truth
[(' decentering', 0.0013763904571533203), (' smoothing and normalization', 0.0003445148468017578)]
Smoothing and Normalization: 0.0005009174346923828
Fit RMSE: 0.027837449581771548
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014384031295776367
PCA fit: 2.616952657699585
[[9.80392159e-01 5.62771682e-09]
 [5.62771649e-09 9.80392149e-01]]
PCA Transform: 0.02791118621826172
total iterations: 527
TLDA fit: 276.07035064697266
Whitened factor: 
[[ 0.09047803 -0.00178349]
 [ 0.99589854 -0.99999845]]
PCA Reverse Transform: 0.00023794174194335938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02612607010306668
Fit RMSE: 0.02665692906296107
 Test Against Ground Truth
[(' decentering', 0.0014638900756835938), (' smoothing and normalization', 0.0003654956817626953)]
Smoothing and Normalization: 0.0005009174346923828
Fit RMSE: 0.025359203240453224
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013628005981445312
PCA fit: 0.041925668716430664
[[ 9.80392151e-01 -3.59996039e-08]
 [-3.59996051e-08  9.80392097e-01]]
PCA Transform: 0.0015535354614257812
total iterations: 302
TLDA fit: 160.5593810081482
Whitened factor: 
[[ 0.8444628  -0.81022567]
 [-0.53561413  0.5861182 ]]
PCA Reverse Transform: 0.0002353191375732422
decenter with new strategy:
[ 7.15678882 -4.1491789 ]
decenter with old strategy:
[12.32659118  0.9107564 ]
Fit RMSE new decenter: 0.12457073702066818
Fit RMSE: 0.1256090006489501
 Test Against Ground Truth
[(' decentering', 0.0013003349304199219), (' smoothing and normalization', 0.0003666877746582031)]
Smoothing and Normalization: 0.000507354736328125
Fit RMSE: 0.12414215831348165
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0031485557556152344
PCA fit: 0.28646373748779297
[[ 9.80392147e-01 -7.97230887e-09]
 [-7.97230896e-09  9.80392106e-01]]
PCA Transform: 0.005850315093994141
total iterations: 1574
TLDA fit: 853.1962203979492
Whitened factor: 
[[-0.93684685  0.9514782 ]
 [ 0.34974003 -0.30771625]]
PCA Reverse Transform: 0.0002613067626953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056909150174665565
Fit RMSE: 0.05707573674394847
 Test Against Ground Truth
[(' decentering', 0.0013437271118164062), (' smoothing and normalization', 0.00035691261291503906)]
Smoothing and Normalization: 0.0005497932434082031
Fit RMSE: 0.05680770761370155
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.00609588623046875
PCA fit: 0.7669360637664795
[[9.80392158e-01 1.52386126e-08]
 [1.52386123e-08 9.80392167e-01]]
PCA Transform: 0.01128530502319336
total iterations: 521
TLDA fit: 276.96001410484314
Whitened factor: 
[[ 0.08945691  0.00177588]
 [ 0.9959908  -0.99999845]]
PCA Reverse Transform: 0.0002694129943847656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.042285179266785655
Fit RMSE: 0.04240952003459194
 Test Against Ground Truth
[(' decentering', 0.0013544559478759766), (' smoothing and normalization', 0.0003490447998046875)]
Smoothing and Normalization: 0.0005161762237548828
Fit RMSE: 0.04029316730353384
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.009111166000366211
PCA fit: 1.4612979888916016
[[ 9.80392155e-01 -5.55728343e-09]
 [-5.55728337e-09  9.80392164e-01]]
PCA Transform: 0.016979217529296875
total iterations: 705
TLDA fit: 383.7120826244354
Whitened factor: 
[[ 0.14388405 -0.06102502]
 [ 0.9895946  -0.9981363 ]]
PCA Reverse Transform: 0.00026988983154296875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.034470269263762725
Fit RMSE: 0.03456647663832813
 Test Against Ground Truth
[(' decentering', 0.0014421939849853516), (' smoothing and normalization', 0.0004019737243652344)]
Smoothing and Normalization: 0.0004978179931640625
Fit RMSE: 0.03274339365205563
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.01099085807800293
PCA fit: 1.716506004333496
[[9.80392158e-01 3.86801999e-09]
 [3.86801972e-09 9.80392171e-01]]
PCA Transform: 0.022591590881347656
total iterations: 1747
TLDA fit: 938.2037155628204
Whitened factor: 
[[ 0.9753015  -0.98433626]
 [ 0.22087798 -0.17630109]]
PCA Reverse Transform: 0.00027251243591308594
decenter with new strategy:
[ 5.94527177e-05 -3.75935948e-05]
decenter with old strategy:
[9.85974294e-05 1.71864917e-06]
Fit RMSE new decenter: 0.02870160481924715
Fit RMSE: 0.02879926910261883
 Test Against Ground Truth
[(' decentering', 0.0014407634735107422), (' smoothing and normalization', 0.0003628730773925781)]
Smoothing and Normalization: 0.0005068778991699219
Fit RMSE: 0.028663420784703523
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013427257537841797
PCA fit: 4.627697706222534
[[9.80392158e-01 6.66897137e-09]
 [6.66897132e-09 9.80392161e-01]]
PCA Transform: 0.028453350067138672
total iterations: 962
TLDA fit: 508.645183801651
Whitened factor: 
[[ 0.13751529 -0.07034845]
 [-0.9904997   0.9975225 ]]
PCA Reverse Transform: 0.0002357959747314453
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025662493014138966
Fit RMSE: 0.026424593093206432
 Test Against Ground Truth
[(' decentering', 0.0014979839324951172), (' smoothing and normalization', 0.00035119056701660156)]
Smoothing and Normalization: 0.0004935264587402344
Fit RMSE: 0.02507673148591049
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013911724090576172
PCA fit: 0.04583287239074707
[[9.80392166e-01 1.89941156e-08]
 [1.89941153e-08 9.80392183e-01]]
PCA Transform: 0.0015943050384521484
total iterations: 969
TLDA fit: 525.8516457080841
Whitened factor: 
[[-0.8197912   0.8491645 ]
 [ 0.5726626  -0.52812845]]
PCA Reverse Transform: 0.00023603439331054688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12234124663465824
Fit RMSE: 0.12350240240107548
 Test Against Ground Truth
[(' decentering', 0.0013489723205566406), (' smoothing and normalization', 0.0003254413604736328)]
Smoothing and Normalization: 0.0004973411560058594
Fit RMSE: 0.12190932080062843
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0029807090759277344
PCA fit: 0.25090527534484863
[[9.80392155e-01 2.24403857e-09]
 [2.24403852e-09 9.80392131e-01]]
PCA Transform: 0.005818367004394531
total iterations: 514
TLDA fit: 281.6301579475403
Whitened factor: 
[[ 0.1257987  -0.03639418]
 [-0.99205583  0.9993375 ]]
PCA Reverse Transform: 0.0002582073211669922
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05963113643459019
Fit RMSE: 0.05990218987087501
 Test Against Ground Truth
[(' decentering', 0.0013511180877685547), (' smoothing and normalization', 0.0003528594970703125)]
Smoothing and Normalization: 0.0005590915679931641
Fit RMSE: 0.0569245964688899
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005712032318115234
PCA fit: 0.7624588012695312
[[9.80392157e-01 2.70648498e-09]
 [2.70648545e-09 9.80392134e-01]]
PCA Transform: 0.011282920837402344
total iterations: 533
TLDA fit: 290.0392098426819
Whitened factor: 
[[ 0.23866923 -0.15325008]
 [-0.9711009   0.9881874 ]]
PCA Reverse Transform: 0.0002574920654296875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04208162738658589
Fit RMSE: 0.0420789438618405
 Test Against Ground Truth
[(' decentering', 0.0013303756713867188), (' smoothing and normalization', 0.00034689903259277344)]
Smoothing and Normalization: 0.0004794597625732422
Fit RMSE: 0.039652572153114615
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.00843358039855957
PCA fit: 1.018847942352295
[[9.80392160e-01 8.74771905e-09]
 [8.74771922e-09 9.80392185e-01]]
PCA Transform: 0.01697683334350586
total iterations: 1160
TLDA fit: 639.0631499290466
Whitened factor: 
[[-0.9629192   0.94660866]
 [-0.26979008  0.3223851 ]]
PCA Reverse Transform: 0.0002961158752441406
decenter with new strategy:
[-0.0021082   0.00349596]
decenter with old strategy:
[0.00056997 0.00615308]
Fit RMSE new decenter: 0.03308603296222024
Fit RMSE: 0.03327179332724992
 Test Against Ground Truth
[(' decentering', 0.0014452934265136719), (' smoothing and normalization', 0.0003578662872314453)]
Smoothing and Normalization: 0.0004951953887939453
Fit RMSE: 0.03304541595794628
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011222362518310547
PCA fit: 2.6667416095733643
[[9.80392157e-01 2.03485782e-09]
 [2.03485766e-09 9.80392153e-01]]
PCA Transform: 0.022254467010498047
total iterations: 31
TLDA fit: 16.37115168571472
Whitened factor: 
[[-0.241533    0.8709209 ]
 [ 0.9703925   0.49142316]]
PCA Reverse Transform: 0.0002722740173339844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0284671279089791
Fit RMSE: 0.02895537367149711
 Test Against Ground Truth
[(' decentering', 0.001390695571899414), (' smoothing and normalization', 0.0003421306610107422)]
Smoothing and Normalization: 0.0004913806915283203
Fit RMSE: 0.028373413676468263
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013421297073364258
PCA fit: 4.676063060760498
[[ 9.80392156e-01 -2.28358755e-10]
 [-2.28358855e-10  9.80392168e-01]]
PCA Transform: 0.02768397331237793
total iterations: 592
TLDA fit: 323.863893032074
Whitened factor: 
[[ 0.04376194  0.04489381]
 [-0.999042    0.9989918 ]]
PCA Reverse Transform: 0.0002384185791015625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0264489238606831
Fit RMSE: 0.02672814392808348
 Test Against Ground Truth
[(' decentering', 0.0014796257019042969), (' smoothing and normalization', 0.0003528594970703125)]
Smoothing and Normalization: 0.0005509853363037109
Fit RMSE: 0.025392931929437222
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014858245849609375
PCA fit: 0.04699301719665527
[[9.80392159e-01 2.42749842e-09]
 [2.42749841e-09 9.80392141e-01]]
PCA Transform: 0.0015649795532226562
total iterations: 587
TLDA fit: 320.9487051963806
Whitened factor: 
[[ 0.16799182 -0.07725783]
 [-0.98578846  0.9970111 ]]
PCA Reverse Transform: 0.000232696533203125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12343780238094115
Fit RMSE: 0.1272477314143367
 Test Against Ground Truth
[(' decentering', 0.0013303756713867188), (' smoothing and normalization', 0.0003249645233154297)]
Smoothing and Normalization: 0.0004954338073730469
Fit RMSE: 0.1148775580355623
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.002766132354736328
PCA fit: 0.25048041343688965
[[ 9.80392158e-01 -1.84187527e-09]
 [-1.84187525e-09  9.80392163e-01]]
PCA Transform: 0.0058422088623046875
total iterations: 531
TLDA fit: 290.1958496570587
Whitened factor: 
[[ 0.01727295  0.07173463]
 [ 0.99985087 -0.99742377]]
PCA Reverse Transform: 0.0002651214599609375
decenter with new strategy:
[9.63501649e-06 7.43044767e-06]
decenter with old strategy:
[4.86875384e-05 4.68394777e-05]
Fit RMSE new decenter: 0.05600777039249933
Fit RMSE: 0.057417943394392164
 Test Against Ground Truth
[(' decentering', 0.001336812973022461), (' smoothing and normalization', 0.0003464221954345703)]
Smoothing and Normalization: 0.00048661231994628906
Fit RMSE: 0.05288452029915514
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005448579788208008
PCA fit: 0.7739064693450928
[[ 9.80392155e-01 -6.30832458e-09]
 [-6.30832395e-09  9.80392176e-01]]
PCA Transform: 0.011370420455932617
total iterations: 809
TLDA fit: 442.6206614971161
Whitened factor: 
[[-0.14706375  0.21527088]
 [-0.98912704  0.97655445]]
PCA Reverse Transform: 0.00024437904357910156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.042257456967163425
Fit RMSE: 0.042399496226152036
 Test Against Ground Truth
[(' decentering', 0.0013248920440673828), (' smoothing and normalization', 0.0003719329833984375)]
Smoothing and Normalization: 0.0005459785461425781
Fit RMSE: 0.040410379971782706
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008426904678344727
PCA fit: 1.5023839473724365
[[ 9.80392154e-01 -3.98386459e-09]
 [-3.98386441e-09  9.80392123e-01]]
PCA Transform: 0.01696944236755371
total iterations: 575
TLDA fit: 317.6568295955658
Whitened factor: 
[[-0.07404696  0.16147196]
 [-0.9972548   0.9868773 ]]
PCA Reverse Transform: 0.00025081634521484375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03428905566796371
Fit RMSE: 0.0345804689592327
 Test Against Ground Truth
[(' decentering', 0.0013878345489501953), (' smoothing and normalization', 0.00036644935607910156)]
Smoothing and Normalization: 0.0005140304565429688
Fit RMSE: 0.0328701278556289
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.01153707504272461
PCA fit: 2.636414051055908
[[ 9.80392157e-01 -3.83025463e-09]
 [-3.83025445e-09  9.80392145e-01]]
PCA Transform: 0.022365093231201172
total iterations: 454
TLDA fit: 247.68434023857117
Whitened factor: 
[[ 0.07046736  0.02766291]
 [ 0.9975141  -0.99961734]]
PCA Reverse Transform: 0.0002536773681640625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.029475176322251637
Fit RMSE: 0.02978913338861016
 Test Against Ground Truth
[(' decentering', 0.0014162063598632812), (' smoothing and normalization', 0.0003590583801269531)]
Smoothing and Normalization: 0.0005061626434326172
Fit RMSE: 0.028222424081836728
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.01402902603149414
PCA fit: 4.627979755401611
[[ 9.80392157e-01 -8.73545188e-10]
 [-8.73545429e-10  9.80392155e-01]]
PCA Transform: 0.028853178024291992
total iterations: 510
TLDA fit: 275.2659561634064
Whitened factor: 
[[ 0.12692489 -0.03275948]
 [ 0.99191236 -0.9994633 ]]
PCA Reverse Transform: 0.0002655982971191406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026385011923257595
Fit RMSE: 0.0267194111918031
 Test Against Ground Truth
[(' decentering', 0.0014841556549072266), (' smoothing and normalization', 0.00034880638122558594)]
Smoothing and Normalization: 0.0005013942718505859
Fit RMSE: 0.02541435019530673
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013585090637207031
PCA fit: 0.04409980773925781
[[ 9.80392163e-01 -9.96936692e-09]
 [-9.96936715e-09  9.80391968e-01]]
PCA Transform: 0.0015518665313720703
total iterations: 703
TLDA fit: 385.216858625412
Whitened factor: 
[[ 0.84466976 -0.81142735]
 [-0.5352878   0.58445334]]
PCA Reverse Transform: 0.0002694129943847656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12423761943292203
Fit RMSE: 0.12532786782071306
 Test Against Ground Truth
[(' decentering', 0.0013310909271240234), (' smoothing and normalization', 0.00032591819763183594)]
Smoothing and Normalization: 0.0004885196685791016
Fit RMSE: 0.12400238723575378
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0031948089599609375
PCA fit: 0.25051283836364746
[[ 9.80392158e-01 -6.96605083e-09]
 [-6.96605084e-09  9.80392132e-01]]
PCA Transform: 0.005936384201049805
total iterations: 775
TLDA fit: 421.4675953388214
Whitened factor: 
[[-0.87049353  0.8975279 ]
 [ 0.4921798  -0.44095775]]
PCA Reverse Transform: 0.00024366378784179688
decenter with new strategy:
[-0.29394251  0.502245  ]
decenter with old strategy:
[0.06363223 0.86503044]
Fit RMSE new decenter: 0.05459930622439791
Fit RMSE: 0.05503676370478021
 Test Against Ground Truth
[(' decentering', 0.0013091564178466797), (' smoothing and normalization', 0.0003509521484375)]
Smoothing and Normalization: 0.0005214214324951172
Fit RMSE: 0.05449457640915021
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005528450012207031
PCA fit: 0.7660117149353027
[[9.80392155e-01 1.26516698e-08]
 [1.26516701e-08 9.80392097e-01]]
PCA Transform: 0.011386394500732422
total iterations: 277
TLDA fit: 149.63351440429688
Whitened factor: 
[[-0.8708402   0.9006194 ]
 [ 0.49156627 -0.43460882]]
PCA Reverse Transform: 0.0002613067626953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.041556899060717904
Fit RMSE: 0.041754519299846825
 Test Against Ground Truth
[(' decentering', 0.001432180404663086), (' smoothing and normalization', 0.00035691261291503906)]
Smoothing and Normalization: 0.0005099773406982422
Fit RMSE: 0.04152590013314435
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008821725845336914
PCA fit: 1.492790699005127
[[ 9.80392157e-01 -4.21652190e-09]
 [-4.21652143e-09  9.80392157e-01]]
PCA Transform: 0.01692938804626465
total iterations: 707
TLDA fit: 383.08853816986084
Whitened factor: 
[[ 0.04424797  0.03452769]
 [ 0.99902064 -0.9994037 ]]
PCA Reverse Transform: 0.0002677440643310547
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03419412900644417
Fit RMSE: 0.034457733776263905
 Test Against Ground Truth
[(' decentering', 0.0014333724975585938), (' smoothing and normalization', 0.00035691261291503906)]
Smoothing and Normalization: 0.0005025863647460938
Fit RMSE: 0.03264172118453958
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011606454849243164
PCA fit: 2.654325246810913
[[ 9.80392155e-01 -2.87895447e-09]
 [-2.87895450e-09  9.80392163e-01]]
PCA Transform: 0.02217268943786621
total iterations: 1955
TLDA fit: 1057.0680575370789
Whitened factor: 
[[-0.14451185  0.1838722 ]
 [-0.98950315  0.98295015]]
PCA Reverse Transform: 0.00029349327087402344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.029571285033872918
Fit RMSE: 0.0298382620206022
 Test Against Ground Truth
[(' decentering', 0.0014755725860595703), (' smoothing and normalization', 0.0003573894500732422)]
Smoothing and Normalization: 0.0005507469177246094
Fit RMSE: 0.028369839358850773
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014633655548095703
PCA fit: 2.588263511657715
[[9.80392158e-01 6.89826521e-10]
 [6.89826482e-10 9.80392150e-01]]
PCA Transform: 0.02770209312438965
total iterations: 588
TLDA fit: 318.36463236808777
Whitened factor: 
[[ 0.02425669  0.05950347]
 [ 0.9997058  -0.9982281 ]]
PCA Reverse Transform: 0.00023508071899414062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026214776086038727
Fit RMSE: 0.02669462677055113
 Test Against Ground Truth
[(' decentering', 0.001538991928100586), (' smoothing and normalization', 0.00034165382385253906)]
Smoothing and Normalization: 0.0004987716674804688
Fit RMSE: 0.02542056904441262
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013751983642578125
PCA fit: 0.04460453987121582
[[9.80392153e-01 1.97378523e-08]
 [1.97378525e-08 9.80392124e-01]]
PCA Transform: 0.0015108585357666016
total iterations: 898
TLDA fit: 487.56015038490295
Whitened factor: 
[[ 0.16705842 -0.10161172]
 [-0.985947    0.9948242 ]]
PCA Reverse Transform: 0.0002384185791015625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12641876783104505
Fit RMSE: 0.12881823219298333
 Test Against Ground Truth
[(' decentering', 0.0013759136199951172), (' smoothing and normalization', 0.0003261566162109375)]
Smoothing and Normalization: 0.00044727325439453125
Fit RMSE: 0.11908019144467354
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0033919811248779297
PCA fit: 0.25525999069213867
[[9.80392153e-01 2.14679807e-09]
 [2.14679889e-09 9.80392145e-01]]
PCA Transform: 0.00585484504699707
total iterations: 844
TLDA fit: 468.47584342956543
Whitened factor: 
[[-0.8774051   0.90384316]
 [ 0.47975016 -0.42786407]]
PCA Reverse Transform: 0.00027561187744140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057950458580391294
Fit RMSE: 0.05821428912249948
 Test Against Ground Truth
[(' decentering', 0.001394510269165039), (' smoothing and normalization', 0.0003559589385986328)]
Smoothing and Normalization: 0.0005393028259277344
Fit RMSE: 0.0578835283134141
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005503654479980469
PCA fit: 0.7572274208068848
[[9.80392158e-01 1.52418155e-09]
 [1.52418136e-09 9.80392139e-01]]
PCA Transform: 0.011236429214477539
total iterations: 1236
TLDA fit: 674.7564754486084
Whitened factor: 
[[-0.9771462   0.9642727 ]
 [-0.21256831  0.2649118 ]]
PCA Reverse Transform: 0.0002593994140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.039547763993578795
Fit RMSE: 0.039771823560116104
 Test Against Ground Truth
[(' decentering', 0.001337289810180664), (' smoothing and normalization', 0.00034809112548828125)]
Smoothing and Normalization: 0.0005681514739990234
Fit RMSE: 0.03947246582175868
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008585214614868164
PCA fit: 1.4737133979797363
[[ 9.80392156e-01 -4.70408318e-09]
 [-4.70408284e-09  9.80392132e-01]]
PCA Transform: 0.01699090003967285
total iterations: 562
TLDA fit: 303.92308044433594
Whitened factor: 
[[ 0.06367131  0.02539002]
 [ 0.99797094 -0.99967766]]
PCA Reverse Transform: 0.00025725364685058594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.034082508582850696
Fit RMSE: 0.034457999267029006
 Test Against Ground Truth
[(' decentering', 0.001375436782836914), (' smoothing and normalization', 0.0003504753112792969)]
Smoothing and Normalization: 0.0005121231079101562
Fit RMSE: 0.032654125523339135
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011190176010131836
PCA fit: 1.7086310386657715
[[9.80392159e-01 1.43616879e-09]
 [1.43616892e-09 9.80392155e-01]]
PCA Transform: 0.02240467071533203
total iterations: 723
TLDA fit: 394.48438811302185
Whitened factor: 
[[-0.9215005   0.93794024]
 [ 0.38837728 -0.34679702]]
PCA Reverse Transform: 0.00023794174194335938
decenter with new strategy:
[-0.00620845  0.01051076]
decenter with old strategy:
[0.00137199 0.01815158]
Fit RMSE new decenter: 0.02846191490319511
Fit RMSE: 0.028579057869563707
 Test Against Ground Truth
[(' decentering', 0.001375436782836914), (' smoothing and normalization', 0.0003440380096435547)]
Smoothing and Normalization: 0.0005023479461669922
Fit RMSE: 0.028410802098022513
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013932228088378906
PCA fit: 4.581904649734497
[[9.80392154e-01 2.81768326e-09]
 [2.81768308e-09 9.80392153e-01]]
PCA Transform: 0.02771925926208496
total iterations: 705
TLDA fit: 384.9240803718567
Whitened factor: 
[[ 0.90236425 -0.8729133 ]
 [-0.43097425  0.48787546]]
PCA Reverse Transform: 0.0002377033233642578
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025600956982023172
Fit RMSE: 0.025776484181295207
 Test Against Ground Truth
[(' decentering', 0.0014748573303222656), (' smoothing and normalization', 0.0003428459167480469)]
Smoothing and Normalization: 0.0005018711090087891
Fit RMSE: 0.025576741366451404
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014758110046386719
PCA fit: 0.046820640563964844
[[9.80392156e-01 7.98477338e-09]
 [7.98477360e-09 9.80392124e-01]]
PCA Transform: 0.0015411376953125
total iterations: 570
TLDA fit: 309.1695382595062
Whitened factor: 
[[-0.88253754  0.90823776]
 [ 0.4702421  -0.41845453]]
PCA Reverse Transform: 0.00027489662170410156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1182869526039606
Fit RMSE: 0.11928945722179946
 Test Against Ground Truth
[(' decentering', 0.0013697147369384766), (' smoothing and normalization', 0.00034117698669433594)]
Smoothing and Normalization: 0.0004799365997314453
Fit RMSE: 0.11791388021340143
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0019462108612060547
PCA fit: 0.28876471519470215
[[ 9.80392154e-01 -3.23195908e-09]
 [-3.23195881e-09  9.80392161e-01]]
PCA Transform: 0.005822420120239258
total iterations: 833
TLDA fit: 454.05873465538025
Whitened factor: 
[[ 0.9045495  -0.8775172 ]
 [-0.42636868  0.47954506]]
PCA Reverse Transform: 0.00025153160095214844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056551190826942856
Fit RMSE: 0.05691612182064729
 Test Against Ground Truth
[(' decentering', 0.001401662826538086), (' smoothing and normalization', 0.0003573894500732422)]
Smoothing and Normalization: 0.0005388259887695312
Fit RMSE: 0.056490758448940026
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005604982376098633
PCA fit: 0.7380378246307373
[[9.80392156e-01 8.66509017e-11]
 [8.66510856e-11 9.80392149e-01]]
PCA Transform: 0.01122426986694336
total iterations: 934
TLDA fit: 515.5559453964233
Whitened factor: 
[[ 0.23176546 -0.1682804 ]
 [ 0.97277176 -0.9857392 ]]
PCA Reverse Transform: 0.00023436546325683594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0421219634856335
Fit RMSE: 0.042066825168163076
 Test Against Ground Truth
[(' decentering', 0.0013611316680908203), (' smoothing and normalization', 0.0003445148468017578)]
Smoothing and Normalization: 0.0005583763122558594
Fit RMSE: 0.03978934593074704
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008068323135375977
PCA fit: 1.0336754322052002
[[ 9.80392157e-01 -3.76294636e-09]
 [-3.76294621e-09  9.80392189e-01]]
PCA Transform: 0.016913652420043945
total iterations: 716
TLDA fit: 396.1864893436432
Whitened factor: 
[[-0.8639727  0.8923131]
 [ 0.5035387 -0.451417 ]]
PCA Reverse Transform: 0.000240325927734375
decenter with new strategy:
[-0.0271733   0.04863089]
decenter with old strategy:
[0.0094386  0.08578164]
Fit RMSE new decenter: 0.033370763678847636
Fit RMSE: 0.03357321935915585
 Test Against Ground Truth
[(' decentering', 0.0013480186462402344), (' smoothing and normalization', 0.0003445148468017578)]
Smoothing and Normalization: 0.0004889965057373047
Fit RMSE: 0.03332413736132833
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.010785579681396484
PCA fit: 1.70147705078125
[[ 9.80392154e-01 -9.23427352e-09]
 [-9.23427320e-09  9.80392131e-01]]
PCA Transform: 0.022166013717651367
total iterations: 754
TLDA fit: 414.59766483306885
Whitened factor: 
[[ 0.18695611 -0.11491661]
 [ 0.9823683  -0.9933751 ]]
PCA Reverse Transform: 0.00025534629821777344
decenter with new strategy:
[-3.10903946e-05  5.25983665e-04]
decenter with old strategy:
[0.00104375 0.00157098]
Fit RMSE new decenter: 0.030136617012245636
Fit RMSE: 0.030148594589592924
 Test Against Ground Truth
[(' decentering', 0.0013840198516845703), (' smoothing and normalization', 0.00037670135498046875)]
Smoothing and Normalization: 0.0005407333374023438
Fit RMSE: 0.028804431821296102
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013862133026123047
PCA fit: 4.668127059936523
[[9.80392160e-01 5.21398744e-09]
 [5.21398747e-09 9.80392167e-01]]
PCA Transform: 0.027958393096923828
total iterations: 496
TLDA fit: 270.19174909591675
Whitened factor: 
[[ 0.07190826  0.02324404]
 [ 0.9974113  -0.9997299 ]]
PCA Reverse Transform: 0.00024056434631347656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026707079549317232
Fit RMSE: 0.026912980875869918
 Test Against Ground Truth
[(' decentering', 0.0015058517456054688), (' smoothing and normalization', 0.00034737586975097656)]
Smoothing and Normalization: 0.0004992485046386719
Fit RMSE: 0.025700474500561468
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0015273094177246094
PCA fit: 0.0447690486907959
[[9.80392155e-01 1.27778810e-09]
 [1.27778758e-09 9.80392102e-01]]
PCA Transform: 0.0015642642974853516
total iterations: 732
TLDA fit: 396.9389839172363
Whitened factor: 
[[ 0.9098821  -0.884435  ]
 [-0.41486704  0.46666348]]
PCA Reverse Transform: 0.0002474784851074219
decenter with new strategy:
[ 0.00022122 -0.00012957]
decenter with old strategy:
[3.79322046e-04 2.62113859e-05]
Fit RMSE new decenter: 0.11951217743841495
Fit RMSE: 0.12047907469303604
 Test Against Ground Truth
[(' decentering', 0.0012967586517333984), (' smoothing and normalization', 0.0003445148468017578)]
Smoothing and Normalization: 0.0004811286926269531
Fit RMSE: 0.11934952161504923
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.002743959426879883
PCA fit: 0.2839925289154053
[[ 9.80392152e-01 -2.01069769e-09]
 [-2.01069723e-09  9.80392148e-01]]
PCA Transform: 0.005838632583618164
total iterations: 1930
TLDA fit: 1054.8251135349274
Whitened factor: 
[[ 0.9787071  -0.98599803]
 [ 0.20526202 -0.16675732]]
PCA Reverse Transform: 0.0002486705780029297
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056295282411397835
Fit RMSE: 0.05644323100578149
 Test Against Ground Truth
[(' decentering', 0.001308441162109375), (' smoothing and normalization', 0.00036978721618652344)]
Smoothing and Normalization: 0.0005652904510498047
Fit RMSE: 0.05619677391588429
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005583286285400391
PCA fit: 0.740107536315918
[[ 9.80392156e-01 -6.10603884e-09]
 [-6.10603904e-09  9.80392144e-01]]
PCA Transform: 0.011203527450561523
total iterations: 1285
TLDA fit: 706.6003997325897
Whitened factor: 
[[-0.9541861   0.9371987 ]
 [-0.29921392  0.348796  ]]
PCA Reverse Transform: 0.0002613067626953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.040386281513475224
Fit RMSE: 0.040606079430136555
 Test Against Ground Truth
[(' decentering', 0.0013556480407714844), (' smoothing and normalization', 0.0003917217254638672)]
Smoothing and Normalization: 0.0005037784576416016
Fit RMSE: 0.04033178200651837
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.0066318511962890625
PCA fit: 1.4699201583862305
[[ 9.80392157e-01 -4.34329187e-09]
 [-4.34329198e-09  9.80392154e-01]]
PCA Transform: 0.017139911651611328
total iterations: 510
TLDA fit: 280.20691657066345
Whitened factor: 
[[ 0.04867196  0.04835578]
 [ 0.9988148  -0.9988302 ]]
PCA Reverse Transform: 0.0002455711364746094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03410756670279628
Fit RMSE: 0.03431962326273809
 Test Against Ground Truth
[(' decentering', 0.0014314651489257812), (' smoothing and normalization', 0.00034928321838378906)]
Smoothing and Normalization: 0.0005347728729248047
Fit RMSE: 0.032418961341205164
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.010722637176513672
PCA fit: 2.6597609519958496
[[9.80392157e-01 1.46958832e-09]
 [1.46958805e-09 9.80392153e-01]]
PCA Transform: 0.022226333618164062
total iterations: 489
TLDA fit: 269.77037048339844
Whitened factor: 
[[ 0.06676559  0.02947913]
 [-0.9977687   0.99956536]]
PCA Reverse Transform: 0.00025773048400878906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.029044726678608267
Fit RMSE: 0.029735830377035158
 Test Against Ground Truth
[(' decentering', 0.0014584064483642578), (' smoothing and normalization', 0.00035691261291503906)]
Smoothing and Normalization: 0.0005688667297363281
Fit RMSE: 0.028257844876797573
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.01222848892211914
PCA fit: 4.657469987869263
[[ 9.80392155e-01 -2.17818486e-09]
 [-2.17818495e-09  9.80392155e-01]]
PCA Transform: 0.0279388427734375
total iterations: 474
TLDA fit: 258.0786180496216
Whitened factor: 
[[ 0.08066259  0.0175004 ]
 [ 0.9967415  -0.99984694]]
PCA Reverse Transform: 0.00024127960205078125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026639632676371483
Fit RMSE: 0.026760124494100836
 Test Against Ground Truth
[(' decentering', 0.0014579296112060547), (' smoothing and normalization', 0.00036334991455078125)]
Smoothing and Normalization: 0.0005066394805908203
Fit RMSE: 0.025380376948426906
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0015556812286376953
PCA fit: 0.04712939262390137
[[9.80392156e-01 4.41763501e-09]
 [4.41763518e-09 9.80391479e-01]]
PCA Transform: 0.0015497207641601562
total iterations: 645
TLDA fit: 349.8128671646118
Whitened factor: 
[[ 0.8289952  -0.78804594]
 [-0.55925566  0.61561656]]
PCA Reverse Transform: 0.00026679039001464844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12629180366997605
Fit RMSE: 0.12736742815931312
 Test Against Ground Truth
[(' decentering', 0.0013163089752197266), (' smoothing and normalization', 0.0003733634948730469)]
Smoothing and Normalization: 0.0004949569702148438
Fit RMSE: 0.12610179520330414
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.002819538116455078
PCA fit: 0.2772486209869385
[[9.80392156e-01 1.05321762e-09]
 [1.05321759e-09 9.80392145e-01]]
PCA Transform: 0.00583338737487793
total iterations: 494
TLDA fit: 261.0087192058563
Whitened factor: 
[[ 0.054988    0.03761877]
 [ 0.99848706 -0.9992922 ]]
PCA Reverse Transform: 0.00024771690368652344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057564370233308515
Fit RMSE: 0.05842172038403004
 Test Against Ground Truth
[(' decentering', 0.0013184547424316406), (' smoothing and normalization', 0.000362396240234375)]
Smoothing and Normalization: 0.0005388259887695312
Fit RMSE: 0.05427364780917029
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005177974700927734
PCA fit: 0.7479901313781738
[[ 9.80392156e-01 -8.01674177e-10]
 [-8.01674386e-10  9.80392156e-01]]
PCA Transform: 0.011350870132446289
total iterations: 670
TLDA fit: 363.53805708885193
Whitened factor: 
[[ 0.01411199  0.07234143]
 [-0.9999004   0.99737996]]
PCA Reverse Transform: 0.0002434253692626953
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04116276305522101
Fit RMSE: 0.04191717241855237
 Test Against Ground Truth
[(' decentering', 0.0013246536254882812), (' smoothing and normalization', 0.00035953521728515625)]
Smoothing and Normalization: 0.0005147457122802734
Fit RMSE: 0.03964089691271389
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.006521940231323242
PCA fit: 1.4949743747711182
[[9.80392157e-01 3.25109417e-10]
 [3.25109328e-10 9.80392152e-01]]
PCA Transform: 0.0170900821685791
total iterations: 2018
TLDA fit: 1100.1173405647278
Whitened factor: 
[[ 0.6334774 -0.6050111]
 [-0.7737612  0.796217 ]]
PCA Reverse Transform: 0.00025725364685058594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03273266709657993
Fit RMSE: 0.033403908504499334
 Test Against Ground Truth
[(' decentering', 0.0013971328735351562), (' smoothing and normalization', 0.0003578662872314453)]
Smoothing and Normalization: 0.0005018711090087891
Fit RMSE: 0.03272192291095349
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011181354522705078
PCA fit: 2.6221506595611572
[[9.80392150e-01 1.77928048e-08]
 [1.77928046e-08 9.80392110e-01]]
PCA Transform: 0.022411584854125977
total iterations: 443
TLDA fit: 240.97137427330017
Whitened factor: 
[[-0.07172791  0.16825424]
 [ 0.99742424 -0.98574364]]
PCA Reverse Transform: 0.0002512931823730469
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.029913935387560393
Fit RMSE: 0.029921422448841065
 Test Against Ground Truth
[(' decentering', 0.0014290809631347656), (' smoothing and normalization', 0.0003666877746582031)]
Smoothing and Normalization: 0.0005340576171875
Fit RMSE: 0.02835854312894308
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.01159214973449707
PCA fit: 4.689730405807495
[[9.80392159e-01 7.28035608e-09]
 [7.28035638e-09 9.80392175e-01]]
PCA Transform: 0.027730941772460938
total iterations: 647
TLDA fit: 351.62248134613037
Whitened factor: 
[[-0.08331084  0.16699885]
 [-0.9965236   0.98595715]]
PCA Reverse Transform: 0.00024318695068359375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026087399192340825
Fit RMSE: 0.026597959147856648
 Test Against Ground Truth
[(' decentering', 0.0014681816101074219), (' smoothing and normalization', 0.00034737586975097656)]
Smoothing and Normalization: 0.0004963874816894531
Fit RMSE: 0.02518327215551968
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014150142669677734
PCA fit: 0.042597055435180664
[[9.80392157e-01 4.96272649e-08]
 [4.96272660e-08 9.80392026e-01]]
PCA Transform: 0.0015516281127929688
total iterations: 4745
TLDA fit: 2562.8452999591827
Whitened factor: 
[[ 0.99372584 -0.9958381 ]
 [ 0.11184387 -0.09114026]]
PCA Reverse Transform: 0.00026297569274902344
decenter with new strategy:
[-0.11871433  0.18663353]
decenter with old strategy:
[0.00646906 0.31195933]
Fit RMSE new decenter: 0.12442789114610923
Fit RMSE: 0.12459330574782969
 Test Against Ground Truth
[(' decentering', 0.001377105712890625), (' smoothing and normalization', 0.0003275871276855469)]
Smoothing and Normalization: 0.0004596710205078125
Fit RMSE: 0.12443308731356786
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0028879642486572266
PCA fit: 0.2595992088317871
[[ 9.80392159e-01 -7.28579863e-10]
 [-7.28579350e-10  9.80392189e-01]]
PCA Transform: 0.005854368209838867
total iterations: 1799
TLDA fit: 983.279908657074
Whitened factor: 
[[ 0.92325264 -0.9069544 ]
 [-0.3841935   0.421229  ]]
PCA Reverse Transform: 0.0002894401550292969
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05536792324279895
Fit RMSE: 0.05570392653010588
 Test Against Ground Truth
[(' decentering', 0.0014004707336425781), (' smoothing and normalization', 0.0003695487976074219)]
Smoothing and Normalization: 0.0005035400390625
Fit RMSE: 0.05525275939677327
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.0040056705474853516
PCA fit: 0.7684826850891113
[[9.80392155e-01 1.28766539e-10]
 [1.28766212e-10 9.80392147e-01]]
PCA Transform: 0.011230707168579102
total iterations: 628
TLDA fit: 342.6919994354248
Whitened factor: 
[[ 0.00838105  0.0739317 ]
 [-0.9999649   0.9972634 ]]
PCA Reverse Transform: 0.00028252601623535156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04108640900769925
Fit RMSE: 0.04187200680070437
 Test Against Ground Truth
[(' decentering', 0.0014188289642333984), (' smoothing and normalization', 0.0003592967987060547)]
Smoothing and Normalization: 0.00048065185546875
Fit RMSE: 0.03947090889651647
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.00786900520324707
PCA fit: 1.484612226486206
[[9.80392159e-01 1.28987116e-11]
 [1.28988914e-11 9.80392146e-01]]
PCA Transform: 0.016934871673583984
total iterations: 476
TLDA fit: 260.6715478897095
Whitened factor: 
[[ 0.1403748  -0.04482805]
 [ 0.9900985  -0.99899477]]
PCA Reverse Transform: 0.0002391338348388672
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.033022345206938745
Fit RMSE: 0.03385450251485687
 Test Against Ground Truth
[(' decentering', 0.0013594627380371094), (' smoothing and normalization', 0.00034737586975097656)]
Smoothing and Normalization: 0.0004897117614746094
Fit RMSE: 0.031686026277155166
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.010926246643066406
PCA fit: 1.7470908164978027
[[ 9.80392155e-01 -4.37654879e-09]
 [-4.37654872e-09  9.80392168e-01]]
PCA Transform: 0.022190570831298828
total iterations: 538
TLDA fit: 291.7617738246918
Whitened factor: 
[[ 0.08713215  0.00952759]
 [ 0.9961968  -0.99995464]]
PCA Reverse Transform: 0.0002512931823730469
decenter with new strategy:
[-3.68854512e-05  6.25895996e-05]
decenter with old strategy:
[2.52636679e-06 1.01639514e-04]
Fit RMSE new decenter: 0.029835400615234587
Fit RMSE: 0.029895141288299554
 Test Against Ground Truth
[(' decentering', 0.0014290809631347656), (' smoothing and normalization', 0.00035643577575683594)]
Smoothing and Normalization: 0.0005581378936767578
Fit RMSE: 0.02832199768748282
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013502836227416992
PCA fit: 4.631671667098999
[[ 9.80392157e-01 -4.34085260e-10]
 [-4.34085169e-10  9.80392147e-01]]
PCA Transform: 0.027930021286010742
total iterations: 876
TLDA fit: 474.4521265029907
Whitened factor: 
[[-0.02793066  0.09970561]
 [-0.9996099   0.99501705]]
PCA Reverse Transform: 0.0002319812774658203
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026882267468897873
Fit RMSE: 0.026858312555395544
 Test Against Ground Truth
[(' decentering', 0.0015232563018798828), (' smoothing and normalization', 0.0003452301025390625)]
Smoothing and Normalization: 0.0004966259002685547
Fit RMSE: 0.02561169748460435
sklearn Test Against Ground Truth
Done!
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08154892921447754
PCA fit: 0.5362586975097656
[[ 9.80392148e-01 -1.62507718e-08]
 [-1.62507715e-08  9.80392156e-01]]
PCA Transform: 0.002353191375732422
total iterations: 315
TLDA fit: 163.6945161819458
Whitened factor: 
[[-0.88144046  0.90202135]
 [ 0.47229522 -0.43169147]]
PCA Reverse Transform: 0.0010762214660644531
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.121455667287762
Fit RMSE: 0.12241566206399118
 Test Against Ground Truth
[(' decentering', 0.00485539436340332), (' smoothing and normalization', 0.0002841949462890625)]
Smoothing and Normalization: 0.00045752525329589844
Fit RMSE: 0.1211635829599553
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003440380096435547
PCA fit: 0.28214240074157715
[[ 9.80392156e-01 -2.83903728e-09]
 [-2.83903757e-09  9.80392192e-01]]
PCA Transform: 0.006680488586425781
total iterations: 310
TLDA fit: 166.2560498714447
Whitened factor: 
[[-0.04205665  0.12017638]
 [-0.9991153   0.9927526 ]]
PCA Reverse Transform: 0.0002384185791015625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05904878175296412
Fit RMSE: 0.0593172259619802
 Test Against Ground Truth
[(' decentering', 0.0014276504516601562), (' smoothing and normalization', 0.0003120899200439453)]
Smoothing and Normalization: 0.0005230903625488281
Fit RMSE: 0.05609268410920454
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005966663360595703
PCA fit: 0.5839893817901611
[[9.80392157e-01 1.73348639e-09]
 [1.73348630e-09 9.80392141e-01]]
PCA Transform: 0.011578798294067383
total iterations: 321
TLDA fit: 173.96266746520996
Whitened factor: 
[[ 0.19924645 -0.13131645]
 [ 0.9799494  -0.9913406 ]]
PCA Reverse Transform: 0.00023317337036132812
decenter with new strategy:
[-1.54706448e-05  4.00128209e-05]
decenter with old strategy:
[2.44892386e-05 7.85964972e-05]
Fit RMSE new decenter: 0.04203171426744436
Fit RMSE: 0.042187433245270745
 Test Against Ground Truth
[(' decentering', 0.0014026165008544922), (' smoothing and normalization', 0.00034236907958984375)]
Smoothing and Normalization: 0.0005028247833251953
Fit RMSE: 0.04000131408627952
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.009219646453857422
PCA fit: 1.5153274536132812
[[9.80392161e-01 1.09446653e-08]
 [1.09446655e-08 9.80392192e-01]]
PCA Transform: 0.017099857330322266
total iterations: 314
TLDA fit: 168.4356815814972
Whitened factor: 
[[-0.94780326  0.9260827 ]
 [-0.31885585  0.37732077]]
PCA Reverse Transform: 0.0002415180206298828
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03283798117050941
Fit RMSE: 0.033091318243857304
 Test Against Ground Truth
[(' decentering', 0.001336812973022461), (' smoothing and normalization', 0.0003483295440673828)]
Smoothing and Normalization: 0.0004813671112060547
Fit RMSE: 0.03282576586099667
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011776924133300781
PCA fit: 1.6819732189178467
[[9.80392156e-01 5.75799981e-10]
 [5.75800313e-10 9.80392157e-01]]
PCA Transform: 0.022413969039916992
total iterations: 318
TLDA fit: 171.91295170783997
Whitened factor: 
[[-0.16267878  0.23065643]
 [-0.9866791   0.9730353 ]]
PCA Reverse Transform: 0.0002613067626953125
decenter with new strategy:
[0.00085229 0.00078747]
decenter with old strategy:
[0.0034467  0.00345309]
Fit RMSE new decenter: 0.02971327516086987
Fit RMSE: 0.029742801111911087
 Test Against Ground Truth
[(' decentering', 0.001374959945678711), (' smoothing and normalization', 0.00034236907958984375)]
Smoothing and Normalization: 0.0005369186401367188
Fit RMSE: 0.02812331445716324
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.01480412483215332
PCA fit: 4.590530157089233
[[ 9.80392154e-01 -4.55861214e-09]
 [-4.55861288e-09  9.80392130e-01]]
PCA Transform: 0.027695894241333008
total iterations: 323
TLDA fit: 171.60196185112
Whitened factor: 
[[-0.99964285  0.99711174]
 [-0.02672446  0.07594892]]
PCA Reverse Transform: 0.0002372264862060547
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025463116018304886
Fit RMSE: 0.02556516332583837
 Test Against Ground Truth
[(' decentering', 0.0014765262603759766), (' smoothing and normalization', 0.00035262107849121094)]
Smoothing and Normalization: 0.0005347728729248047
Fit RMSE: 0.025434605893330447
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013637542724609375
PCA fit: 0.04172968864440918
[[9.80392159e-01 3.58074077e-09]
 [3.58073904e-09 9.80392613e-01]]
PCA Transform: 0.0015456676483154297
total iterations: 329
TLDA fit: 176.1759431362152
Whitened factor: 
[[ 0.9503104  -0.93892735]
 [-0.31130394  0.3441156 ]]
PCA Reverse Transform: 0.00023508071899414062
decenter with new strategy:
[ 0.0001808  -0.00011614]
decenter with old strategy:
[2.98872728e-04 1.16284282e-06]
Fit RMSE new decenter: 0.12627540269250045
Fit RMSE: 0.12646552291522375
 Test Against Ground Truth
[(' decentering', 0.0013060569763183594), (' smoothing and normalization', 0.0003452301025390625)]
Smoothing and Normalization: 0.0004506111145019531
Fit RMSE: 0.1259568227626249
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0033614635467529297
PCA fit: 0.26378464698791504
[[ 9.80392154e-01 -1.15388420e-08]
 [-1.15388420e-08  9.80392130e-01]]
PCA Transform: 0.005824565887451172
total iterations: 418
TLDA fit: 228.41007041931152
Whitened factor: 
[[-0.97794867  0.982262  ]
 [ 0.20884536 -0.18751389]]
PCA Reverse Transform: 0.00025010108947753906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05728282017666155
Fit RMSE: 0.05734644959998054
 Test Against Ground Truth
[(' decentering', 0.0013451576232910156), (' smoothing and normalization', 0.0003561973571777344)]
Smoothing and Normalization: 0.0005297660827636719
Fit RMSE: 0.05721034788172347
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.0059626102447509766
PCA fit: 0.7295708656311035
[[9.80392158e-01 3.57041888e-09]
 [3.57041806e-09 9.80392180e-01]]
PCA Transform: 0.011248111724853516
total iterations: 322
TLDA fit: 174.7683608531952
Whitened factor: 
[[-0.981796    0.9694693 ]
 [-0.18993844  0.24521264]]
PCA Reverse Transform: 0.0002415180206298828
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.039858627651451456
Fit RMSE: 0.040086220459140225
 Test Against Ground Truth
[(' decentering', 0.0013151168823242188), (' smoothing and normalization', 0.0003685951232910156)]
Smoothing and Normalization: 0.0005285739898681641
Fit RMSE: 0.0397989139665848
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008227825164794922
PCA fit: 1.4773344993591309
[[ 9.80392158e-01 -2.26325412e-09]
 [-2.26325376e-09  9.80392165e-01]]
PCA Transform: 0.01694631576538086
total iterations: 308
TLDA fit: 167.4996783733368
Whitened factor: 
[[ 0.90943956 -0.8853721 ]
 [-0.41583622  0.46488312]]
PCA Reverse Transform: 0.0002624988555908203
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03347201871891636
Fit RMSE: 0.03365324118333286
 Test Against Ground Truth
[(' decentering', 0.0014452934265136719), (' smoothing and normalization', 0.00035190582275390625)]
Smoothing and Normalization: 0.0005314350128173828
Fit RMSE: 0.033424941161060685
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011672258377075195
PCA fit: 2.6376311779022217
[[ 9.80392156e-01 -6.10215693e-10]
 [-6.10215808e-10  9.80392135e-01]]
PCA Transform: 0.02225184440612793
total iterations: 309
TLDA fit: 165.29266214370728
Whitened factor: 
[[ 0.07830453  0.02334278]
 [ 0.99692947 -0.9997276 ]]
PCA Reverse Transform: 0.0002694129943847656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028844325989925233
Fit RMSE: 0.029587989827822823
 Test Against Ground Truth
[(' decentering', 0.001447439193725586), (' smoothing and normalization', 0.0003597736358642578)]
Smoothing and Normalization: 0.0005035400390625
Fit RMSE: 0.028009839237071534
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.015001535415649414
PCA fit: 4.679527997970581
[[ 9.80392154e-01 -4.41452381e-09]
 [-4.41452375e-09  9.80392141e-01]]
PCA Transform: 0.027810096740722656
total iterations: 307
TLDA fit: 163.60539603233337
Whitened factor: 
[[-0.03148836  0.09362099]
 [ 0.9995041  -0.995608  ]]
PCA Reverse Transform: 0.0005846023559570312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02611731303137115
Fit RMSE: 0.026660758269817984
 Test Against Ground Truth
[(' decentering', 0.002763986587524414), (' smoothing and normalization', 0.0009191036224365234)]
Smoothing and Normalization: 0.0005123615264892578
Fit RMSE: 0.02545136214671199
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014426708221435547
PCA fit: 0.04529213905334473
[[9.80392171e-01 5.33190136e-09]
 [5.33190106e-09 9.80392279e-01]]
PCA Transform: 0.001604318618774414
total iterations: 331
TLDA fit: 182.35857558250427
Whitened factor: 
[[-0.9327978   0.94801146]
 [ 0.3604002  -0.3182364 ]]
PCA Reverse Transform: 0.0002460479736328125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12279619613030648
Fit RMSE: 0.12320067326620407
 Test Against Ground Truth
[(' decentering', 0.0013401508331298828), (' smoothing and normalization', 0.000324249267578125)]
Smoothing and Normalization: 0.0004413127899169922
Fit RMSE: 0.12224613532166148
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003239870071411133
PCA fit: 0.2850461006164551
[[ 9.80392152e-01 -1.23363177e-08]
 [-1.23363177e-08  9.80392083e-01]]
PCA Transform: 0.00583648681640625
total iterations: 310
TLDA fit: 169.58221697807312
Whitened factor: 
[[ 0.12624615 -0.0420945 ]
 [ 0.991999   -0.9991136 ]]
PCA Reverse Transform: 0.0006067752838134766
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0596467313991386
Fit RMSE: 0.0596668404034988
 Test Against Ground Truth
[(' decentering', 0.0026063919067382812), (' smoothing and normalization', 0.0009009838104248047)]
Smoothing and Normalization: 0.0005218982696533203
Fit RMSE: 0.056322664512399506
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006158113479614258
PCA fit: 0.7621536254882812
[[9.80392158e-01 5.92016494e-09]
 [5.92016513e-09 9.80392177e-01]]
PCA Transform: 0.011531352996826172
total iterations: 317
TLDA fit: 172.31503677368164
Whitened factor: 
[[-0.1145068   0.17878765]
 [-0.9934225   0.9838877 ]]
PCA Reverse Transform: 0.00025844573974609375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04230433595308906
Fit RMSE: 0.04231281368756835
 Test Against Ground Truth
[(' decentering', 0.0014574527740478516), (' smoothing and normalization', 0.00034928321838378906)]
Smoothing and Normalization: 0.0005097389221191406
Fit RMSE: 0.040144684629788444
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008555173873901367
PCA fit: 1.507075309753418
[[9.80392161e-01 4.92064214e-09]
 [4.92064270e-09 9.80392190e-01]]
PCA Transform: 0.01698780059814453
total iterations: 330
TLDA fit: 182.32528066635132
Whitened factor: 
[[-0.9918412   0.9847524 ]
 [-0.12748002  0.17396197]]
PCA Reverse Transform: 0.000682830810546875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03286489720042853
Fit RMSE: 0.032984354248049685
 Test Against Ground Truth
[(' decentering', 0.0026803016662597656), (' smoothing and normalization', 0.0009121894836425781)]
Smoothing and Normalization: 0.0004904270172119141
Fit RMSE: 0.03280860639897733
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011759757995605469
PCA fit: 2.6764891147613525
[[ 9.80392152e-01 -5.41356782e-10]
 [-5.41356496e-10  9.80392168e-01]]
PCA Transform: 0.022223472595214844
total iterations: 308
TLDA fit: 169.5308928489685
Whitened factor: 
[[ 0.08860669  0.01878161]
 [ 0.9960667  -0.9998237 ]]
PCA Reverse Transform: 0.0002582073211669922
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.029649502353335123
Fit RMSE: 0.02995932290048804
 Test Against Ground Truth
[(' decentering', 0.0014126300811767578), (' smoothing and normalization', 0.0003523826599121094)]
Smoothing and Normalization: 0.0005199909210205078
Fit RMSE: 0.02853552120168935
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014303922653198242
PCA fit: 4.690092325210571
[[ 9.80392159e-01 -4.82939996e-09]
 [-4.82940013e-09  9.80392129e-01]]
PCA Transform: 0.027718782424926758
total iterations: 313
TLDA fit: 171.6671462059021
Whitened factor: 
[[-0.9479995   0.9194359 ]
 [-0.31827193  0.39323997]]
PCA Reverse Transform: 0.00024318695068359375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025438167394671096
Fit RMSE: 0.025632776611751135
 Test Against Ground Truth
[(' decentering', 0.0015325546264648438), (' smoothing and normalization', 0.0003554821014404297)]
Smoothing and Normalization: 0.0005106925964355469
Fit RMSE: 0.025407709278258694
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013649463653564453
PCA fit: 0.04495716094970703
[[9.80392155e-01 2.27002006e-08]
 [2.27002010e-08 9.80392085e-01]]
PCA Transform: 0.0015578269958496094
total iterations: 312
TLDA fit: 171.68278741836548
Whitened factor: 
[[ 0.15567946 -0.08362924]
 [-0.98780763  0.9964969 ]]
PCA Reverse Transform: 0.000255584716796875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1268511631503048
Fit RMSE: 0.12982652695539582
 Test Against Ground Truth
[(' decentering', 0.0013043880462646484), (' smoothing and normalization', 0.0003399848937988281)]
Smoothing and Normalization: 0.0004763603210449219
Fit RMSE: 0.12051441776739637
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0032396316528320312
PCA fit: 0.2631063461303711
[[ 9.80392159e-01 -3.75049092e-09]
 [-3.75049084e-09  9.80392148e-01]]
PCA Transform: 0.0058727264404296875
total iterations: 309
TLDA fit: 168.70818614959717
Whitened factor: 
[[ 0.04355499  0.0372107 ]
 [ 0.99905103 -0.9993074 ]]
PCA Reverse Transform: 0.0002346038818359375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05862304688715469
Fit RMSE: 0.05898719595064417
 Test Against Ground Truth
[(' decentering', 0.0013737678527832031), (' smoothing and normalization', 0.0003542900085449219)]
Smoothing and Normalization: 0.0005137920379638672
Fit RMSE: 0.05501520813219769
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.006082296371459961
PCA fit: 0.5854015350341797
[[ 9.80392152e-01 -1.06099997e-08]
 [-1.06100003e-08  9.80392164e-01]]
PCA Transform: 0.011245965957641602
total iterations: 338
TLDA fit: 184.62434196472168
Whitened factor: 
[[-0.9739673   0.98146844]
 [ 0.22668855 -0.191624  ]]
PCA Reverse Transform: 0.00024890899658203125
decenter with new strategy:
[ 0.01251037 -0.00778115]
decenter with old strategy:
[0.02128091 0.00102261]
Fit RMSE new decenter: 0.041026434499557195
Fit RMSE: 0.04109767031423765
 Test Against Ground Truth
[(' decentering', 0.0013434886932373047), (' smoothing and normalization', 0.0003688335418701172)]
Smoothing and Normalization: 0.0005025863647460938
Fit RMSE: 0.040974098955206456
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008694171905517578
PCA fit: 1.5043971538543701
[[ 9.80392158e-01 -2.26594987e-09]
 [-2.26594996e-09  9.80392157e-01]]
PCA Transform: 0.017041683197021484
total iterations: 315
TLDA fit: 173.54377460479736
Whitened factor: 
[[ 0.92230856 -0.94646364]
 [ 0.38645437 -0.32281053]]
PCA Reverse Transform: 0.00023627281188964844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03229998291273318
Fit RMSE: 0.03257864503002624
 Test Against Ground Truth
[(' decentering', 0.0013551712036132812), (' smoothing and normalization', 0.0003559589385986328)]
Smoothing and Normalization: 0.0005114078521728516
Fit RMSE: 0.032262747087556394
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011482715606689453
PCA fit: 2.634091377258301
[[ 9.80392152e-01 -2.44186959e-10]
 [-2.44186672e-10  9.80392124e-01]]
PCA Transform: 0.02220320701599121
total iterations: 334
TLDA fit: 183.86790537834167
Whitened factor: 
[[ 0.99436295 -0.9976239 ]
 [ 0.10603017 -0.06889591]]
PCA Reverse Transform: 0.0002999305725097656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02887783851322312
Fit RMSE: 0.02894343011378367
 Test Against Ground Truth
[(' decentering', 0.0014226436614990234), (' smoothing and normalization', 0.0003600120544433594)]
Smoothing and Normalization: 0.0005280971527099609
Fit RMSE: 0.028845377465566088
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013596534729003906
PCA fit: 4.640881299972534
[[9.80392156e-01 9.28845455e-10]
 [9.28845582e-10 9.80392170e-01]]
PCA Transform: 0.02793717384338379
total iterations: 308
TLDA fit: 170.1056616306305
Whitened factor: 
[[ 0.05457452  0.05316602]
 [ 0.99850965 -0.99858564]]
PCA Reverse Transform: 0.0002493858337402344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02704386012617057
Fit RMSE: 0.02704297024129193
 Test Against Ground Truth
[(' decentering', 0.0015842914581298828), (' smoothing and normalization', 0.00035762786865234375)]
Smoothing and Normalization: 0.0005428791046142578
Fit RMSE: 0.025875551570099728
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014224052429199219
PCA fit: 0.04472780227661133
[[9.80392165e-01 2.12990088e-09]
 [2.12990083e-09 9.80392126e-01]]
PCA Transform: 0.0016314983367919922
total iterations: 309
TLDA fit: 171.90659713745117
Whitened factor: 
[[ 0.90979874 -0.88586265]
 [-0.4150496   0.46394747]]
PCA Reverse Transform: 0.0002636909484863281
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1219849583725796
Fit RMSE: 0.12277254773271058
 Test Against Ground Truth
[(' decentering', 0.0013988018035888672), (' smoothing and normalization', 0.00033783912658691406)]
Smoothing and Normalization: 0.0004639625549316406
Fit RMSE: 0.12192433466297406
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0033445358276367188
PCA fit: 0.26163506507873535
[[9.80392159e-01 4.05216083e-09]
 [4.05216101e-09 9.80392141e-01]]
PCA Transform: 0.005898475646972656
total iterations: 319
TLDA fit: 173.57393407821655
Whitened factor: 
[[-0.24197163  0.29738483]
 [ 0.97028327 -0.95475775]]
PCA Reverse Transform: 0.0002574920654296875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05807598877213388
Fit RMSE: 0.05891516250873767
 Test Against Ground Truth
[(' decentering', 0.0013508796691894531), (' smoothing and normalization', 0.00038361549377441406)]
Smoothing and Normalization: 0.0004971027374267578
Fit RMSE: 0.05573074134467058
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.0052852630615234375
PCA fit: 0.769256591796875
[[ 9.80392160e-01 -3.12965011e-09]
 [-3.12965023e-09  9.80392143e-01]]
PCA Transform: 0.01127767562866211
total iterations: 310
TLDA fit: 169.8615367412567
Whitened factor: 
[[-0.02740687  0.13585548]
 [-0.9996243   0.9907287 ]]
PCA Reverse Transform: 0.0002760887145996094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04207197189488317
Fit RMSE: 0.042299214266135525
 Test Against Ground Truth
[(' decentering', 0.0014081001281738281), (' smoothing and normalization', 0.00034618377685546875)]
Smoothing and Normalization: 0.0005323886871337891
Fit RMSE: 0.040088190864182124
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008857488632202148
PCA fit: 1.4352366924285889
[[ 9.80392153e-01 -6.05302437e-09]
 [-6.05302438e-09  9.80392144e-01]]
PCA Transform: 0.01693558692932129
total iterations: 315
TLDA fit: 170.93812441825867
Whitened factor: 
[[ 0.9219936  -0.94736844]
 [ 0.38720495 -0.32014537]]
PCA Reverse Transform: 0.0002503395080566406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.032787251042169695
Fit RMSE: 0.03303176418595041
 Test Against Ground Truth
[(' decentering', 0.0013637542724609375), (' smoothing and normalization', 0.0003726482391357422)]
Smoothing and Normalization: 0.0005357265472412109
Fit RMSE: 0.03274651296694936
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011623144149780273
PCA fit: 1.6998391151428223
[[ 9.80392155e-01 -3.50565250e-09]
 [-3.50565250e-09  9.80392152e-01]]
PCA Transform: 0.02219414710998535
total iterations: 310
TLDA fit: 168.3023133277893
Whitened factor: 
[[ 0.01817102  0.06613201]
 [ 0.99983495 -0.99781084]]
PCA Reverse Transform: 0.00023889541625976562
decenter with new strategy:
[0.00696278 0.00306674]
decenter with old strategy:
[0.0291537  0.02536105]
Fit RMSE new decenter: 0.02850597927871048
Fit RMSE: 0.02941338833248068
 Test Against Ground Truth
[(' decentering', 0.0013935565948486328), (' smoothing and normalization', 0.00035572052001953125)]
Smoothing and Normalization: 0.0005204677581787109
Fit RMSE: 0.0278593309318891
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014175891876220703
PCA fit: 4.66081976890564
[[9.80392160e-01 2.47057960e-09]
 [2.47057915e-09 9.80392162e-01]]
PCA Transform: 0.027939319610595703
total iterations: 315
TLDA fit: 170.2439305782318
Whitened factor: 
[[ 0.9410373  -0.9592987 ]
 [ 0.33830273 -0.2823935 ]]
PCA Reverse Transform: 0.000255584716796875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.025735148844884155
Fit RMSE: 0.025902332028495043
 Test Against Ground Truth
[(' decentering', 0.0014972686767578125), (' smoothing and normalization', 0.0003521442413330078)]
Smoothing and Normalization: 0.0005385875701904297
Fit RMSE: 0.025705339564558984
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013742446899414062
PCA fit: 0.04531383514404297
[[9.80392158e-01 1.08416925e-08]
 [1.08416923e-08 9.80392212e-01]]
PCA Transform: 0.001501321792602539
total iterations: 315
TLDA fit: 171.16650915145874
Whitened factor: 
[[-0.8320007   0.86576766]
 [ 0.5547746  -0.50044614]]
PCA Reverse Transform: 0.0002410411834716797
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12145954117052316
Fit RMSE: 0.12262371189607199
 Test Against Ground Truth
[(' decentering', 0.0013151168823242188), (' smoothing and normalization', 0.0003514289855957031)]
Smoothing and Normalization: 0.0004749298095703125
Fit RMSE: 0.12114942688222163
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0032241344451904297
PCA fit: 0.2796294689178467
[[9.80392157e-01 7.12960365e-09]
 [7.12960352e-09 9.80392192e-01]]
PCA Transform: 0.005896568298339844
total iterations: 310
TLDA fit: 173.30896854400635
Whitened factor: 
[[ 0.06680293  0.0278412 ]
 [-0.9977662   0.9996124 ]]
PCA Reverse Transform: 0.0002815723419189453
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05962779939861342
Fit RMSE: 0.05966355626298565
 Test Against Ground Truth
[(' decentering', 0.001359701156616211), (' smoothing and normalization', 0.00036334991455078125)]
Smoothing and Normalization: 0.0005333423614501953
Fit RMSE: 0.056295516639905736
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005875349044799805
PCA fit: 0.7700886726379395
[[9.80392161e-01 2.89563760e-09]
 [2.89563767e-09 9.80392144e-01]]
PCA Transform: 0.01198720932006836
total iterations: 338
TLDA fit: 182.92571091651917
Whitened factor: 
[[ 0.22646578 -0.18687986]
 [ 0.97401917 -0.9823827 ]]
PCA Reverse Transform: 0.00025272369384765625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04153356507694547
Fit RMSE: 0.04205666112263344
 Test Against Ground Truth
[(' decentering', 0.001336812973022461), (' smoothing and normalization', 0.00036263465881347656)]
Smoothing and Normalization: 0.0005078315734863281
Fit RMSE: 0.04015617391652369
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008984088897705078
PCA fit: 1.4796595573425293
[[9.80392158e-01 5.17314681e-09]
 [5.17314700e-09 9.80392153e-01]]
PCA Transform: 0.017033815383911133
total iterations: 319
TLDA fit: 175.16792011260986
Whitened factor: 
[[-0.03305565  0.09143578]
 [-0.99945354  0.995811  ]]
PCA Reverse Transform: 0.00024580955505371094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03468152479787067
Fit RMSE: 0.03464993635803161
 Test Against Ground Truth
[(' decentering', 0.0014126300811767578), (' smoothing and normalization', 0.0003502368927001953)]
Smoothing and Normalization: 0.0004870891571044922
Fit RMSE: 0.03293828400363724
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.01173543930053711
PCA fit: 2.646134376525879
[[9.80392158e-01 2.21592309e-09]
 [2.21592317e-09 9.80392175e-01]]
PCA Transform: 0.022218704223632812
total iterations: 308
TLDA fit: 169.80297207832336
Whitened factor: 
[[ 0.04333304  0.03756761]
 [-0.9990607   0.9992941 ]]
PCA Reverse Transform: 0.0002586841583251953
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028848712068172643
Fit RMSE: 0.029568218028275084
 Test Against Ground Truth
[(' decentering', 0.0014920234680175781), (' smoothing and normalization', 0.0004105567932128906)]
Smoothing and Normalization: 0.0005023479461669922
Fit RMSE: 0.027970110743448854
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013646841049194336
PCA fit: 4.6409502029418945
[[ 9.80392159e-01 -2.48114473e-10]
 [-2.48114324e-10  9.80392148e-01]]
PCA Transform: 0.027979135513305664
total iterations: 309
TLDA fit: 169.68412685394287
Whitened factor: 
[[ 0.05859811  0.02780897]
 [-0.9982816   0.9996132 ]]
PCA Reverse Transform: 0.0002503395080566406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026263983212891497
Fit RMSE: 0.026758733351924868
 Test Against Ground Truth
[(' decentering', 0.0015048980712890625), (' smoothing and normalization', 0.0003542900085449219)]
Smoothing and Normalization: 0.0005009174346923828
Fit RMSE: 0.025560644888690573
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0013730525970458984
PCA fit: 0.04609560966491699
[[9.80392153e-01 3.27700682e-08]
 [3.27700678e-08 9.80392118e-01]]
PCA Transform: 0.0015423297882080078
total iterations: 337
TLDA fit: 184.2097086906433
Whitened factor: 
[[-0.9535849   0.9626632 ]
 [ 0.30112424 -0.270702  ]]
PCA Reverse Transform: 0.0002460479736328125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12004361832200805
Fit RMSE: 0.12061351098439206
 Test Against Ground Truth
[(' decentering', 0.0013844966888427734), (' smoothing and normalization', 0.0003383159637451172)]
Smoothing and Normalization: 0.00048470497131347656
Fit RMSE: 0.11985354988707794
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003256559371948242
PCA fit: 0.29592466354370117
[[9.80392159e-01 2.71975825e-09]
 [2.71975752e-09 9.80392227e-01]]
PCA Transform: 0.005844593048095703
total iterations: 404
TLDA fit: 222.01196336746216
Whitened factor: 
[[ 0.98475254 -0.988141  ]
 [ 0.17396094 -0.15354943]]
PCA Reverse Transform: 0.00024628639221191406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056285385499970926
Fit RMSE: 0.05641490375276321
 Test Against Ground Truth
[(' decentering', 0.0013527870178222656), (' smoothing and normalization', 0.00035452842712402344)]
Smoothing and Normalization: 0.0005557537078857422
Fit RMSE: 0.05620571157520369
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005311250686645508
PCA fit: 0.7761204242706299
[[9.80392157e-01 3.58407160e-09]
 [3.58407112e-09 9.80392122e-01]]
PCA Transform: 0.011314630508422852
total iterations: 308
TLDA fit: 171.211434841156
Whitened factor: 
[[ 0.05071696  0.05677333]
 [ 0.99871314 -0.99838704]]
PCA Reverse Transform: 0.0002455711364746094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0423691185503207
Fit RMSE: 0.04237127716552795
 Test Against Ground Truth
[(' decentering', 0.0013287067413330078), (' smoothing and normalization', 0.0003535747528076172)]
Smoothing and Normalization: 0.0006930828094482422
Fit RMSE: 0.04019110510555418
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008668899536132812
PCA fit: 1.5055692195892334
[[ 9.80392157e-01 -3.43487198e-09]
 [-3.43487206e-09  9.80392148e-01]]
PCA Transform: 0.01694965362548828
total iterations: 308
TLDA fit: 172.1092975139618
Whitened factor: 
[[ 0.01680986  0.08355436]
 [ 0.99985874 -0.99650323]]
PCA Reverse Transform: 0.00024318695068359375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03397500507834309
Fit RMSE: 0.03445782202240534
 Test Against Ground Truth
[(' decentering', 0.001383066177368164), (' smoothing and normalization', 0.00037479400634765625)]
Smoothing and Normalization: 0.0005497932434082031
Fit RMSE: 0.032743507680354514
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011626482009887695
PCA fit: 1.6799519062042236
[[ 9.80392157e-01 -3.77765149e-09]
 [-3.77765172e-09  9.80392162e-01]]
PCA Transform: 0.02259206771850586
total iterations: 308
TLDA fit: 171.09427857398987
Whitened factor: 
[[ 0.03851805  0.07027049]
 [-0.99925786  0.997528  ]]
PCA Reverse Transform: 0.0002429485321044922
decenter with new strategy:
[0.00126788 0.00144857]
decenter with old strategy:
[0.0052998  0.00549772]
Fit RMSE new decenter: 0.029731536829498324
Fit RMSE: 0.029869815267486423
 Test Against Ground Truth
[(' decentering', 0.0014061927795410156), (' smoothing and normalization', 0.00035190582275390625)]
Smoothing and Normalization: 0.0004899501800537109
Fit RMSE: 0.028285423278950797
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014627695083618164
PCA fit: 4.648263931274414
[[ 9.80392156e-01 -6.08186162e-09]
 [-6.08186183e-09  9.80392141e-01]]
PCA Transform: 0.02774357795715332
total iterations: 310
TLDA fit: 171.57597064971924
Whitened factor: 
[[ 0.0386274   0.03984272]
 [ 0.99925363 -0.99920595]]
PCA Reverse Transform: 0.0002684593200683594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026073361069577552
Fit RMSE: 0.02664425307763367
 Test Against Ground Truth
[(' decentering', 0.001569509506225586), (' smoothing and normalization', 0.0003490447998046875)]
Smoothing and Normalization: 0.0005421638488769531
Fit RMSE: 0.025392735043588417
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.001379251480102539
PCA fit: 0.045210838317871094
[[ 9.80392155e-01 -1.05420210e-08]
 [-1.05420207e-08  9.80392247e-01]]
PCA Transform: 0.0015556812286376953
total iterations: 319
TLDA fit: 177.19299602508545
Whitened factor: 
[[-0.90494126  0.9273039 ]
 [ 0.4255366  -0.3743093 ]]
PCA Reverse Transform: 0.00026154518127441406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1195190686344168
Fit RMSE: 0.12073555436353228
 Test Against Ground Truth
[(' decentering', 0.0013566017150878906), (' smoothing and normalization', 0.0003333091735839844)]
Smoothing and Normalization: 0.0004680156707763672
Fit RMSE: 0.11572715572892772
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003318309783935547
PCA fit: 0.28641414642333984
[[9.80392156e-01 4.76921203e-09]
 [4.76921237e-09 9.80392192e-01]]
PCA Transform: 0.005850553512573242
total iterations: 311
TLDA fit: 166.44651699066162
Whitened factor: 
[[ 0.07035268  0.01452392]
 [ 0.9975222  -0.9998945 ]]
PCA Reverse Transform: 0.0002410411834716797
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.059748441469637224
Fit RMSE: 0.05994603345798123
 Test Against Ground Truth
[(' decentering', 0.0013184547424316406), (' smoothing and normalization', 0.0003807544708251953)]
Smoothing and Normalization: 0.0005278587341308594
Fit RMSE: 0.05691525204773607
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005792379379272461
PCA fit: 0.7555391788482666
[[ 9.80392154e-01 -2.20954136e-09]
 [-2.20954135e-09  9.80392179e-01]]
PCA Transform: 0.011252403259277344
total iterations: 310
TLDA fit: 170.50855803489685
Whitened factor: 
[[-0.03678956  0.14177318]
 [-0.999323    0.98989916]]
PCA Reverse Transform: 0.0002429485321044922
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04166183134249494
Fit RMSE: 0.04205388221743035
 Test Against Ground Truth
[(' decentering', 0.001338958740234375), (' smoothing and normalization', 0.0003650188446044922)]
Smoothing and Normalization: 0.0005283355712890625
Fit RMSE: 0.03967118374025425
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008023262023925781
PCA fit: 1.4871649742126465
[[ 9.80392155e-01 -7.54047915e-10]
 [-7.54048189e-10  9.80392167e-01]]
PCA Transform: 0.016948223114013672
total iterations: 310
TLDA fit: 170.61536264419556
Whitened factor: 
[[ 0.00529318  0.085563  ]
 [-0.999986    0.99633276]]
PCA Reverse Transform: 0.00026035308837890625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.033700748009137575
Fit RMSE: 0.0341882852378634
 Test Against Ground Truth
[(' decentering', 0.0013968944549560547), (' smoothing and normalization', 0.0003483295440673828)]
Smoothing and Normalization: 0.0005517005920410156
Fit RMSE: 0.03233598823957661
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011607885360717773
PCA fit: 2.638889789581299
[[ 9.80392155e-01 -4.65752447e-09]
 [-4.65752424e-09  9.80392134e-01]]
PCA Transform: 0.022404909133911133
total iterations: 318
TLDA fit: 176.06075143814087
Whitened factor: 
[[ 0.9625464  -0.97349167]
 [ 0.27111694 -0.2287226 ]]
PCA Reverse Transform: 0.0002751350402832031
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028679856577842402
Fit RMSE: 0.028829665620343583
 Test Against Ground Truth
[(' decentering', 0.0015079975128173828), (' smoothing and normalization', 0.00035643577575683594)]
Smoothing and Normalization: 0.0005154609680175781
Fit RMSE: 0.02863901040816626
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.01422572135925293
PCA fit: 4.660831928253174
[[9.80392158e-01 1.84594433e-09]
 [1.84594425e-09 9.80392158e-01]]
PCA Transform: 0.027766942977905273
total iterations: 320
TLDA fit: 176.90106010437012
Whitened factor: 
[[ 0.14805134 -0.09143355]
 [ 0.98897964 -0.99581116]]
PCA Reverse Transform: 0.00023937225341796875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.026479167185253186
Fit RMSE: 0.026705613984843106
 Test Against Ground Truth
[(' decentering', 0.001531839370727539), (' smoothing and normalization', 0.0003688335418701172)]
Smoothing and Normalization: 0.0005319118499755859
Fit RMSE: 0.025259781482173893
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014028549194335938
PCA fit: 0.04541635513305664
[[ 9.80392155e-01 -3.13678861e-10]
 [-3.13679027e-10  9.80392136e-01]]
PCA Transform: 0.0015270709991455078
total iterations: 312
TLDA fit: 172.81812453269958
Whitened factor: 
[[ 0.89811474 -0.8765163 ]
 [-0.43976122  0.48137242]]
PCA Reverse Transform: 0.00024700164794921875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11921998629854212
Fit RMSE: 0.12021419602953093
 Test Against Ground Truth
[(' decentering', 0.0013077259063720703), (' smoothing and normalization', 0.000339508056640625)]
Smoothing and Normalization: 0.0004858970642089844
Fit RMSE: 0.1189792074873511
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0030956268310546875
PCA fit: 0.23666691780090332
[[9.80392162e-01 4.04293693e-09]
 [4.04293718e-09 9.80392170e-01]]
PCA Transform: 0.00587916374206543
total iterations: 310
TLDA fit: 168.05292510986328
Whitened factor: 
[[ 0.05437999  0.03528883]
 [ 0.9985204  -0.9993772 ]]
PCA Reverse Transform: 0.00029540061950683594
decenter with new strategy:
[ 1.19786716e-04 -6.73702014e-05]
decenter with old strategy:
[1.98383075e-04 1.09465597e-05]
Fit RMSE new decenter: 0.05808917001404069
Fit RMSE: 0.0585524672111507
 Test Against Ground Truth
[(' decentering', 0.001302957534790039), (' smoothing and normalization', 0.0003542900085449219)]
Smoothing and Normalization: 0.0005707740783691406
Fit RMSE: 0.054402101510336004
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.005554676055908203
PCA fit: 0.7671878337860107
[[ 9.80392155e-01 -7.00830273e-09]
 [-7.00830298e-09  9.80392169e-01]]
PCA Transform: 0.011208295822143555
total iterations: 311
TLDA fit: 171.70954418182373
Whitened factor: 
[[ 0.0260836   0.06093878]
 [-0.9996598   0.99814147]]
PCA Reverse Transform: 0.0002598762512207031
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04149535703746942
Fit RMSE: 0.042126893635320815
 Test Against Ground Truth
[(' decentering', 0.0013895034790039062), (' smoothing and normalization', 0.0003609657287597656)]
Smoothing and Normalization: 0.0005161762237548828
Fit RMSE: 0.039950621449619125
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008306741714477539
PCA fit: 1.478074312210083
[[ 9.80392161e-01 -1.70229403e-08]
 [-1.70229407e-08  9.80392210e-01]]
PCA Transform: 0.016917943954467773
total iterations: 314
TLDA fit: 174.73302507400513
Whitened factor: 
[[ 0.27010655 -0.20031367]
 [-0.9628304   0.97973186]]
PCA Reverse Transform: 0.00024175643920898438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03456559054839577
Fit RMSE: 0.03493023609892386
 Test Against Ground Truth
[(' decentering', 0.0013685226440429688), (' smoothing and normalization', 0.0003666877746582031)]
Smoothing and Normalization: 0.0005619525909423828
Fit RMSE: 0.03367633099381426
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011563539505004883
PCA fit: 2.6290602684020996
[[9.80392154e-01 5.84789107e-09]
 [5.84789113e-09 9.80392151e-01]]
PCA Transform: 0.02217411994934082
total iterations: 308
TLDA fit: 170.78860688209534
Whitened factor: 
[[ 0.21528997 -0.12780602]
 [-0.9765502   0.99179924]]
PCA Reverse Transform: 0.0002880096435546875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03007749580343512
Fit RMSE: 0.030087708806591947
 Test Against Ground Truth
[(' decentering', 0.0014863014221191406), (' smoothing and normalization', 0.00035452842712402344)]
Smoothing and Normalization: 0.0005161762237548828
Fit RMSE: 0.028688356088041437
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.013402462005615234
PCA fit: 4.683278560638428
[[9.80392158e-01 7.75492113e-10]
 [7.75492207e-10 9.80392153e-01]]
PCA Transform: 0.027738332748413086
total iterations: 307
TLDA fit: 169.10788559913635
Whitened factor: 
[[ 0.09675502 -0.01379512]
 [-0.9953083   0.9999048 ]]
PCA Reverse Transform: 0.00023984909057617188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02599083230553579
Fit RMSE: 0.026627098535837918
 Test Against Ground Truth
[(' decentering', 0.001489877700805664), (' smoothing and normalization', 0.00037360191345214844)]
Smoothing and Normalization: 0.0005395412445068359
Fit RMSE: 0.025355068783680174
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014617443084716797
PCA fit: 0.046878814697265625
[[ 9.80392154e-01 -3.50170937e-09]
 [-3.50170973e-09  9.80392237e-01]]
PCA Transform: 0.0016016960144042969
total iterations: 339
TLDA fit: 187.03132581710815
Whitened factor: 
[[ 0.944744   -0.9309799 ]
 [-0.32780904  0.36507034]]
PCA Reverse Transform: 0.0002491474151611328
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12129615778523423
Fit RMSE: 0.12190654292565917
 Test Against Ground Truth
[(' decentering', 0.001371622085571289), (' smoothing and normalization', 0.00034618377685546875)]
Smoothing and Normalization: 0.0005066394805908203
Fit RMSE: 0.1210325333254989
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003318309783935547
PCA fit: 0.29152655601501465
[[9.80392153e-01 3.17766113e-09]
 [3.17766140e-09 9.80392173e-01]]
PCA Transform: 0.005858421325683594
total iterations: 327
TLDA fit: 179.26262879371643
Whitened factor: 
[[ 0.08447961 -0.02327024]
 [ 0.9964253  -0.9997292 ]]
PCA Reverse Transform: 0.00024437904357910156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05920788143521825
Fit RMSE: 0.05922150995492384
 Test Against Ground Truth
[(' decentering', 0.0013728141784667969), (' smoothing and normalization', 0.00035572052001953125)]
Smoothing and Normalization: 0.0005326271057128906
Fit RMSE: 0.05564973899977324
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
Centering time: 0.0057277679443359375
PCA fit: 0.758167028427124
[[9.80392156e-01 2.23534974e-09]
 [2.23534980e-09 9.80392183e-01]]
PCA Transform: 0.011205673217773438
total iterations: 351
TLDA fit: 190.60697603225708
Whitened factor: 
[[ 0.15494266 -0.10505895]
 [ 0.9879235  -0.99446607]]
PCA Reverse Transform: 0.00024127960205078125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.041765298673162296
Fit RMSE: 0.04212150453511332
 Test Against Ground Truth
[(' decentering', 0.0013871192932128906), (' smoothing and normalization', 0.0003654956817626953)]
Smoothing and Normalization: 0.0005517005920410156
Fit RMSE: 0.03998815463124248
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 20000
density: 15
Centering time: 0.008697032928466797
PCA fit: 1.495908260345459
[[ 9.80392160e-01 -6.80940234e-10]
 [-6.80940246e-10  9.80392154e-01]]
PCA Transform: 0.01696920394897461
total iterations: 309
TLDA fit: 168.81134009361267
Whitened factor: 
[[ 0.02064754  0.06453349]
 [-0.99978685  0.99791557]]
PCA Reverse Transform: 0.00025177001953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03407208873624917
Fit RMSE: 0.034475184504696894
 Test Against Ground Truth
[(' decentering', 0.0014472007751464844), (' smoothing and normalization', 0.0003762245178222656)]
Smoothing and Normalization: 0.0005471706390380859
Fit RMSE: 0.03269300279558572
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 20000
density: 15
Centering time: 0.011449337005615234
PCA fit: 2.6263530254364014
[[ 9.80392157e-01 -1.38414635e-09]
 [-1.38414638e-09  9.80392159e-01]]
PCA Transform: 0.02238607406616211
total iterations: 306
TLDA fit: 167.4193344116211
Whitened factor: 
[[-0.0316573   0.13091101]
 [-0.9994988   0.9913941 ]]
PCA Reverse Transform: 0.00026297569274902344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.029441177600968274
Fit RMSE: 0.02984018676825125
 Test Against Ground Truth
[(' decentering', 0.0014777183532714844), (' smoothing and normalization', 0.0004088878631591797)]
Smoothing and Normalization: 0.0005781650543212891
Fit RMSE: 0.028309328501092607
sklearn Test Against Ground Truth
Vocab: 2500
num_tweets: 20000
density: 15
Centering time: 0.014206647872924805
PCA fit: 2.5966029167175293
[[9.80392154e-01 6.57619257e-10]
 [6.57618901e-10 9.80392163e-01]]
PCA Transform: 0.027991771697998047
total iterations: 309
TLDA fit: 170.85472345352173
Whitened factor: 
[[ 0.07675809  0.01530667]
 [-0.99704975  0.9998829 ]]
PCA Reverse Transform: 0.00023937225341796875
decenter with new strategy:
[0.00182149 0.00178516]
decenter with old strategy:
[0.00733495 0.0072561 ]
Fit RMSE new decenter: 0.026393655495334786
Fit RMSE: 0.026692396098772653
 Test Against Ground Truth
[(' decentering', 0.0015025138854980469), (' smoothing and normalization', 0.0003466606140136719)]
Smoothing and Normalization: 0.0005192756652832031
Fit RMSE: 0.025333808183922963
sklearn Test Against Ground Truth
Done!
  File "generate_tables.py", line 454
    print(str(res_sklearn) + '\n' + str(accuracy_sklearn), file=outFile)
    ^
SyntaxError: invalid syntax
  File "generate_tables.py", line 454
    print(str(res_sklearn) + '\n' + str(accuracy_sklearn), file=outFile)
    ^
SyntaxError: invalid syntax
  File "generate_tables.py", line 454
    print(str(res_sklearn) + '\n' + str(accuracy_sklearn), file=outFile)
    ^
SyntaxError: invalid syntax
  File "generate_tables.py", line 454
    print(str(res_sklearn) + '\n' + str(accuracy_sklearn), file=outFile)
    ^
SyntaxError: invalid syntax
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.33280396461486816
PCA fit: 8.050404787063599
[[9.80392161e-01 1.59093691e-08]
 [1.59093687e-08 9.80392087e-01]]
PCA Transform: 0.06653976440429688
Traceback (most recent call last):
  File "generate_tables.py", line 502, in <module>
    main()
  File "generate_tables.py", line 452, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 378, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 123, in fit
    self.factors_ -= lr*cumulant_gradient(self.factors_, y, self.alpha_0,self.theta)
NameError: name 'lr' is not defined
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08176231384277344
PCA fit: 0.5419845581054688
[[ 9.80392158e-01 -7.54854218e-09]
 [-7.54854230e-09  9.80392119e-01]]
PCA Transform: 0.002197742462158203
Traceback (most recent call last):
  File "generate_tables.py", line 504, in <module>
    main()
  File "generate_tables.py", line 454, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 375, in gen_fit_0_20
    M3 = tlda_mid.get_M3(x_whit, tl.zeros((2, 1)), alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 196, in get_M3
    sum_ -= alpha_0*(alpha_0 + 1)/(2*ns)*tot2 #rescale
  File "cupy/_core/core.pyx", line 1193, in cupy._core.core.ndarray.__isub__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 586, in cupy._core._kernel._get_out_args
ValueError: Out shape is mismatched
  File "generate_tables.py", line 138
    plt.savefig('results/est_map_adjusted_postprocessing'+name+'.pdf')
    ^
SyntaxError: invalid syntax
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0797734260559082
PCA fit: 0.5404775142669678
[[9.80392154e-01 2.11507234e-09]
 [2.11507231e-09 9.80392179e-01]]
PCA Transform: 0.0022521018981933594
total iterations: 55
TLDA fit: 25.62181043624878
Whitened factor: 
[[ 0.95399609 -0.81150872]
 [-0.29981903  0.58434031]]
PCA Reverse Transform: 0.0009829998016357422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Traceback (most recent call last):
  File "generate_tables.py", line 525, in <module>
    main()
  File "generate_tables.py", line 475, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 422, in gen_fit_0_20
    res2, accuracy = postprocess(factors_unwhitened, mu, x, vocab, num_tops, smoothing, True, alpha_0 = alpha_0)
  File "generate_tables.py", line 137, in postprocess
    plt.scatter(cp.asnumpy((adjusted_factors.T[0,:]), cp.asnumpy((adjusted_factors.T[1,:]))))
TypeError: scatter() missing 1 required positional argument: 'y'
  File "generate_tables.py", line 144
    factors_no_M1 = [factors_no_M1  < 0.] = 0.
                    ^
SyntaxError: cannot assign to comparison
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08071422576904297
PCA fit: 0.5306031703948975
[[9.80392155e-01 1.56715649e-08]
 [1.56715646e-08 9.80392112e-01]]
PCA Transform: 0.0021936893463134766
total iterations: 107
TLDA fit: 51.04413318634033
Whitened factor: 
[[-0.02949993  0.30646637]
 [-0.99956478  0.95188149]]
PCA Reverse Transform: 0.001005411148071289
Traceback (most recent call last):
  File "generate_tables.py", line 554, in <module>
    main()
  File "generate_tables.py", line 504, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 451, in gen_fit_0_20
    res2, accuracy = postprocess(factors_unwhitened, mu, x, vocab, num_tops, smoothing, True, alpha_0 = alpha_0)
  File "generate_tables.py", line 93, in postprocess
    plt.scatter(cp.asnumpy(mu[0,:]), cp.asnumpy(tl.mean(x, axis=0)))
TypeError: list indices must be integers or slices, not tuple
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.08183002471923828
PCA fit: 0.545107364654541
[[ 9.80392160e-01 -6.39484765e-09]
 [-6.39484729e-09  9.80392188e-01]]
PCA Transform: 0.002273082733154297
total iterations: 11
TLDA fit: 5.513107061386108
Whitened factor: 
[[ 0.9585862  -0.7971969 ]
 [-0.2848025   0.60371935]]
PCA Reverse Transform: 0.001041412353515625
decenter with new strategy:
[-0.83397514  1.07753634]
decenter with old strategy:
[0.1057972  1.93486066]
Fit RMSE new decenter: 0.12200770166447925
Fit RMSE: 0.12277766604876968
 Test Against Ground Truth
[(' decentering', 0.002652883529663086), (' smoothing and normalization', 0.00025081634521484375)]
Smoothing and Normalization: 0.0004646778106689453
Fit RMSE: 0.12143429901580788
sklearn Test Against Ground Truth
Centering time: 0.001359701156616211
PCA fit: 0.04168844223022461
[[ 9.80392160e-01 -6.39484765e-09]
 [-6.39484729e-09  9.80392188e-01]]
PCA Transform: 0.0015540122985839844
total iterations: 11
TLDA fit: 5.633352756500244
Whitened factor: 
[[ 0.9197898  -0.8743518 ]
 [-0.39241138  0.48529276]]
PCA Reverse Transform: 0.00024700164794921875
decenter with new strategy:
[-0.73737035  1.16003836]
decenter with old strategy:
[0.16657886 2.04044012]
Fit RMSE new decenter: 0.12195958161048343
Fit RMSE: 0.12270781525744527
 Test Against Ground Truth
[(' decentering', 0.001268625259399414), (' smoothing and normalization', 0.00025081634521484375)]
Smoothing and Normalization: 0.0004520416259765625
Fit RMSE: 0.12143429901580788
sklearn Test Against Ground Truth
Centering time: 0.001352071762084961
PCA fit: 0.04153752326965332
[[ 9.80392160e-01 -6.39484765e-09]
 [-6.39484729e-09  9.80392188e-01]]
PCA Transform: 0.0015347003936767578
total iterations: 11
TLDA fit: 5.576500415802002
Whitened factor: 
[[ 0.9149684  -0.89728016]
 [-0.40352556  0.44146168]]
PCA Reverse Transform: 0.0002465248107910156
decenter with new strategy:
[-0.72247619  1.1860572 ]
decenter with old strategy:
[0.17377094 2.07319805]
Fit RMSE new decenter: 0.12194778782759381
Fit RMSE: 0.12264762191888573
 Test Against Ground Truth
[(' decentering', 0.001268148422241211), (' smoothing and normalization', 0.00028705596923828125)]
Smoothing and Normalization: 0.00046253204345703125
Fit RMSE: 0.12143429901580788
sklearn Test Against Ground Truth
Centering time: 0.001363515853881836
PCA fit: 0.04179525375366211
[[ 9.80392160e-01 -6.39484765e-09]
 [-6.39484729e-09  9.80392188e-01]]
PCA Transform: 0.0015952587127685547
total iterations: 11
TLDA fit: 5.561922311782837
Whitened factor: 
[[ 0.02306054 -0.02866452]
 [-0.99973404  0.99958915]]
PCA Reverse Transform: 0.00025463104248046875
decenter with new strategy:
[0.37982467 0.11616569]
decenter with old strategy:
[1.27057388 1.00864119]
Fit RMSE new decenter: 0.13109970162804951
Fit RMSE: 0.1307310091529653
 Test Against Ground Truth
[(' decentering', 0.0012621879577636719), (' smoothing and normalization', 0.0002849102020263672)]
Smoothing and Normalization: 0.0004513263702392578
Fit RMSE: 0.12143429901580788
sklearn Test Against Ground Truth
Centering time: 0.0013496875762939453
PCA fit: 0.041567087173461914
[[ 9.80392160e-01 -6.39484765e-09]
 [-6.39484729e-09  9.80392188e-01]]
PCA Transform: 0.0015385150909423828
total iterations: 37
TLDA fit: 19.841528177261353
Whitened factor: 
[[ 0.9517323  -0.81457126]
 [-0.30692944  0.58006346]]
PCA Reverse Transform: 0.0002391338348388672
decenter with new strategy:
[-0.81483856  1.09668636]
decenter with old strategy:
[0.11703393 1.95815372]
Fit RMSE new decenter: 0.12199877764629859
Fit RMSE: 0.12276081954246884
 Test Against Ground Truth
[(' decentering', 0.0012583732604980469), (' smoothing and normalization', 0.00025153160095214844)]
Smoothing and Normalization: 0.00045800209045410156
Fit RMSE: 0.12143429901580788
sklearn Test Against Ground Truth
Centering time: 0.001348733901977539
PCA fit: 0.041559696197509766
[[ 9.80392160e-01 -6.39484765e-09]
 [-6.39484729e-09  9.80392188e-01]]
PCA Transform: 0.0015246868133544922
total iterations: 80
TLDA fit: 43.432817459106445
Whitened factor: 
[[ 0.9155895  -0.8686325 ]
 [-0.4021142   0.49545702]]
PCA Reverse Transform: 0.00023865699768066406
decenter with new strategy:
[-0.73159776  1.15239784]
decenter with old strategy:
[0.17284767 2.03239199]
Fit RMSE new decenter: 0.12195908543995546
Fit RMSE: 0.12274292922247325
 Test Against Ground Truth
[(' decentering', 0.0012750625610351562), (' smoothing and normalization', 0.0002522468566894531)]
Smoothing and Normalization: 0.0004639625549316406
Fit RMSE: 0.12143429901580788
sklearn Test Against Ground Truth
Centering time: 0.0013632774353027344
PCA fit: 0.04153728485107422
[[ 9.80392160e-01 -6.39484765e-09]
 [-6.39484729e-09  9.80392188e-01]]
PCA Transform: 0.0015292167663574219
total iterations: 153
TLDA fit: 84.81024765968323
Whitened factor: 
[[ 0.9060363  -0.88000286]
 [-0.42320007  0.47496852]]
PCA Reverse Transform: 0.00023937225341796875
decenter with new strategy:
[-0.71163574  1.16342638]
decenter with old strategy:
[0.18694831 2.04843722]
Fit RMSE new decenter: 0.12194892586071014
Fit RMSE: 0.12273910025765347
 Test Against Ground Truth
[(' decentering', 0.001287221908569336), (' smoothing and normalization', 0.0002868175506591797)]
Smoothing and Normalization: 0.00045943260192871094
Fit RMSE: 0.12143429901580788
sklearn Test Against Ground Truth
Centering time: 0.0013854503631591797
PCA fit: 0.04158616065979004
[[ 9.80392160e-01 -6.39484765e-09]
 [-6.39484729e-09  9.80392188e-01]]
PCA Transform: 0.0015153884887695312
total iterations: 309
TLDA fit: 169.65005159378052
Whitened factor: 
[[ 0.89924335 -0.88788724]
 [-0.4374488   0.4600611 ]]
PCA Reverse Transform: 0.00025844573974609375
decenter with new strategy:
[-0.69775061  1.1709913 ]
decenter with old strategy:
[0.19685561 2.05967533]
Fit RMSE new decenter: 0.12194161569225821
Fit RMSE: 0.12273544607253485
 Test Against Ground Truth
[(' decentering', 0.001298666000366211), (' smoothing and normalization', 0.00027942657470703125)]
Smoothing and Normalization: 0.00045752525329589844
Fit RMSE: 0.12143429901580788
sklearn Test Against Ground Truth
Centering time: 0.0013568401336669922
PCA fit: 0.04167771339416504
[[ 9.80392160e-01 -6.39484765e-09]
 [-6.39484729e-09  9.80392188e-01]]
PCA Transform: 0.001566171646118164
total iterations: 1847
TLDA fit: 1031.3510916233063
Whitened factor: 
[[ 0.9517132  -0.81425035]
 [-0.30698875  0.58051395]]
PCA Reverse Transform: 0.00024819374084472656
decenter with new strategy:
[-0.81490973  1.09631043]
decenter with old strategy:
[0.11706484 1.95772132]
Fit RMSE new decenter: 0.12199885860990893
Fit RMSE: 0.1227618989321592
 Test Against Ground Truth
[(' decentering', 0.0012845993041992188), (' smoothing and normalization', 0.0002598762512207031)]
Smoothing and Normalization: 0.00046515464782714844
Fit RMSE: 0.12143429901580788
sklearn Test Against Ground Truth
Centering time: 0.0013587474822998047
PCA fit: 0.04153037071228027
[[ 9.80392160e-01 -6.39484765e-09]
 [-6.39484729e-09  9.80392188e-01]]
PCA Transform: 0.0015606880187988281
total iterations: 2066
TLDA fit: 1156.4944553375244
Whitened factor: 
[[ 0.9155045  -0.8687956 ]
 [-0.40230772  0.49517104]]
PCA Reverse Transform: 0.00024700164794921875
decenter with new strategy:
[-0.73140027  1.15256849]
decenter with old strategy:
[0.17297407 2.03262086]
Fit RMSE new decenter: 0.1219589736306192
Fit RMSE: 0.12274269882021197
 Test Against Ground Truth
[(' decentering', 0.0013012886047363281), (' smoothing and normalization', 0.0002543926239013672)]
Smoothing and Normalization: 0.0004725456237792969
Fit RMSE: 0.12143429901580788
sklearn Test Against Ground Truth
Centering time: 0.0013880729675292969
PCA fit: 0.04165148735046387
[[ 9.80392160e-01 -6.39484765e-09]
 [-6.39484729e-09  9.80392188e-01]]
PCA Transform: 0.0015361309051513672
total iterations: 5000
TLDA fit: 2803.4978721141815
Whitened factor: 
[[ 0.90473163 -0.8790805 ]
 [-0.42598212  0.47667345]]
PCA Reverse Transform: 0.0002338886260986328
decenter with new strategy:
[-0.70962862  1.16203102]
decenter with old strategy:
[0.18885841 2.04712874]
Fit RMSE new decenter: 0.12194846065399512
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07887673377990723
PCA fit: 0.5245852470397949
[[9.80392153e-01 4.81524465e-09]
 [4.81524458e-09 9.80392289e-01]]
PCA Transform: 0.002249479293823242
total iterations: 11
TLDA fit: 5.37883186340332
Whitened factor: 
[[ 0.13566208  0.35346293]
 [ 0.9907552  -0.9354485 ]]
PCA Reverse Transform: 0.0009310245513916016
decenter with new strategy:
[5.43513613e-04 5.16694232e-05]
decenter with old strategy:
[0.00105918 0.00066747]
Fit RMSE new decenter: 0.12609524553773543
Fit RMSE: 0.12877383543015414
 Test Against Ground Truth
[(' decentering', 0.0038614273071289062), (' smoothing and normalization', 0.0002582073211669922)]
Smoothing and Normalization: 0.0004458427429199219
Fit RMSE: 0.1177677077443904
sklearn Test Against Ground Truth
Centering time: 0.0013606548309326172
PCA fit: 0.041600942611694336
[[9.80392153e-01 4.81524465e-09]
 [4.81524458e-09 9.80392289e-01]]
PCA Transform: 0.0015480518341064453
total iterations: 11
TLDA fit: 5.481704950332642
Whitened factor: 
[[-0.04082493  0.22019951]
 [ 0.9991663  -0.97545475]]
PCA Reverse Transform: 0.0002014636993408203
decenter with new strategy:
[ 4.26684187e-04 -2.03113785e-05]
decenter with old strategy:
[0.00094913 0.0005716 ]
Fit RMSE new decenter: 0.12439391803420799
Fit RMSE: 0.12856058497596712
 Test Against Ground Truth
[(' decentering', 0.0013003349304199219), (' smoothing and normalization', 0.00025177001953125)]
Smoothing and Normalization: 0.00046896934509277344
Fit RMSE: 0.1177677077443904
sklearn Test Against Ground Truth
Centering time: 0.0013582706451416016
PCA fit: 0.04159092903137207
[[9.80392153e-01 4.81524465e-09]
 [4.81524458e-09 9.80392289e-01]]
PCA Transform: 0.0015277862548828125
total iterations: 11
TLDA fit: 5.476556062698364
Whitened factor: 
[[-0.08760872  0.20149334]
 [ 0.9961549  -0.97948986]]
PCA Reverse Transform: 0.0002090930938720703
decenter with new strategy:
[ 3.90390770e-04 -2.01660584e-05]
decenter with old strategy:
[0.00091852 0.00055858]
Fit RMSE new decenter: 0.12432256908481484
Fit RMSE: 0.1283934353657507
 Test Against Ground Truth
[(' decentering', 0.0013151168823242188), (' smoothing and normalization', 0.0002493858337402344)]
Smoothing and Normalization: 0.00043654441833496094
Fit RMSE: 0.1177677077443904
sklearn Test Against Ground Truth
Centering time: 0.001360177993774414
PCA fit: 0.04169297218322754
[[9.80392153e-01 4.81524465e-09]
 [4.81524458e-09 9.80392289e-01]]
PCA Transform: 0.001550436019897461
total iterations: 11
TLDA fit: 5.475989818572998
Whitened factor: 
[[ 0.13287345  0.34685645]
 [ 0.99113303 -0.9379182 ]]
PCA Reverse Transform: 0.00020384788513183594
decenter with new strategy:
[5.41588670e-04 4.79790318e-05]
decenter with old strategy:
[0.00105751 0.00066258]
Fit RMSE new decenter: 0.12608740282819234
Fit RMSE: 0.1288024381587438
 Test Against Ground Truth
[(' decentering', 0.001256704330444336), (' smoothing and normalization', 0.0002474784851074219)]
Smoothing and Normalization: 0.0004482269287109375
Fit RMSE: 0.1177677077443904
sklearn Test Against Ground Truth
Centering time: 0.0013413429260253906
PCA fit: 0.0416414737701416
[[9.80392153e-01 4.81524465e-09]
 [4.81524458e-09 9.80392289e-01]]
PCA Transform: 0.0015518665313720703
total iterations: 14
TLDA fit: 7.1640589237213135
Whitened factor: 
[[-0.03274401  0.18709695]
 [ 0.9994638  -0.9823414 ]]
PCA Reverse Transform: 0.0002040863037109375
decenter with new strategy:
[ 4.27790180e-04 -3.33585217e-05]
decenter with old strategy:
[0.00095436 0.00054863]
Fit RMSE new decenter: 0.12515917267746327
Fit RMSE: 0.12882468812466424
 Test Against Ground Truth
[(' decentering', 0.00125885009765625), (' smoothing and normalization', 0.00024819374084472656)]
Smoothing and Normalization: 0.0004889965057373047
Fit RMSE: 0.1177677077443904
sklearn Test Against Ground Truth
Centering time: 0.0013604164123535156
PCA fit: 0.04160666465759277
[[9.80392153e-01 4.81524465e-09]
 [4.81524458e-09 9.80392289e-01]]
PCA Transform: 0.0015087127685546875
total iterations: 23
TLDA fit: 12.213184595108032
Whitened factor: 
[[-0.06985521  0.15298925]
 [ 0.99755716 -0.9882279 ]]
PCA Reverse Transform: 0.00021719932556152344
decenter with new strategy:
[ 3.95462574e-04 -4.18151305e-05]
decenter with old strategy:
[0.0009302 0.0005253]
Fit RMSE new decenter: 0.1254464585110621
Fit RMSE: 0.12881246124857604
 Test Against Ground Truth
[(' decentering', 0.0012471675872802734), (' smoothing and normalization', 0.00024366378784179688)]
Smoothing and Normalization: 0.000453948974609375
Fit RMSE: 0.1177677077443904
sklearn Test Against Ground Truth
Centering time: 0.0013473033905029297
PCA fit: 0.04157209396362305
[[9.80392153e-01 4.81524465e-09]
 [4.81524458e-09 9.80392289e-01]]
PCA Transform: 0.0015690326690673828
total iterations: 46
TLDA fit: 24.806167125701904
Whitened factor: 
[[ 0.12481751  0.3373582 ]
 [ 0.99217975 -0.9413763 ]]
PCA Reverse Transform: 0.00020813941955566406
decenter with new strategy:
[5.36563206e-04 4.18271583e-05]
decenter with old strategy:
[0.00105267 0.00065558]
Fit RMSE new decenter: 0.12606097256167037
Fit RMSE: 0.1288168615780293
 Test Against Ground Truth
[(' decentering', 0.0012729167938232422), (' smoothing and normalization', 0.0002887248992919922)]
Smoothing and Normalization: 0.00045752525329589844
Fit RMSE: 0.1177677077443904
sklearn Test Against Ground Truth
Centering time: 0.0013608932495117188
PCA fit: 0.041571855545043945
[[9.80392153e-01 4.81524465e-09]
 [4.81524458e-09 9.80392289e-01]]
PCA Transform: 0.0015597343444824219
total iterations: 87
TLDA fit: 48.270484924316406
Whitened factor: 
[[-0.03385296  0.18389378]
 [ 0.9994268  -0.9829461 ]]
PCA Reverse Transform: 0.0002276897430419922
decenter with new strategy:
[ 4.26557470e-04 -3.44626429e-05]
decenter with old strategy:
[0.00095364 0.00054643]
Fit RMSE new decenter: 0.12517851537301167
Fit RMSE: 0.12883838274574802
 Test Against Ground Truth
[(' decentering', 0.001260995864868164), (' smoothing and normalization', 0.0002512931823730469)]
Smoothing and Normalization: 0.00045561790466308594
Fit RMSE: 0.1177677077443904
sklearn Test Against Ground Truth
Centering time: 0.0013492107391357422
PCA fit: 0.041637420654296875
[[9.80392153e-01 4.81524465e-09]
 [4.81524458e-09 9.80392289e-01]]
PCA Transform: 0.0015587806701660156
total iterations: 5000
TLDA fit: 2693.635839700699
Whitened factor: 
[[-0.06793159  0.1503459 ]
 [ 0.99769    -0.98863345]]
PCA Reverse Transform: 0.00021147727966308594
decenter with new strategy:
[ 3.96438829e-04 -4.31710996e-05]
decenter with old strategy:
[0.00093147 0.0005235 ]
Fit RMSE new decenter: 0.1255399944967208
Fit RMSE: 0.12884112412985843
 Test Against Ground Truth
[(' decentering', 0.0012660026550292969), (' smoothing and normalization', 0.0002467632293701172)]
Smoothing and Normalization: 0.0004544258117675781
Fit RMSE: 0.1177677077443904
sklearn Test Against Ground Truth
Centering time: 0.0013437271118164062
PCA fit: 0.041539907455444336
[[9.80392153e-01 4.81524465e-09]
 [4.81524458e-09 9.80392289e-01]]
PCA Transform: 0.0015430450439453125
total iterations: 341
TLDA fit: 184.19097352027893
Whitened factor: 
[[ 0.12301235  0.33575258]
 [ 0.9924052  -0.9419503 ]]
PCA Reverse Transform: 0.00021266937255859375
decenter with new strategy:
[5.35464087e-04 4.07007255e-05]
decenter with old strategy:
[0.00105158 0.0006544 ]
Fit RMSE new decenter: 0.1260547366171871
Fit RMSE: 0.1288163896698226
 Test Against Ground Truth
[(' decentering', 0.0012848377227783203), (' smoothing and normalization', 0.00025081634521484375)]
Smoothing and Normalization: 0.0004487037658691406
Fit RMSE: 0.1177677077443904
sklearn Test Against Ground Truth
Centering time: 0.0013532638549804688
PCA fit: 0.04169464111328125
[[9.80392153e-01 4.81524465e-09]
 [4.81524458e-09 9.80392289e-01]]
PCA Transform: 0.0015583038330078125
total iterations: 672
TLDA fit: 362.5406982898712
Whitened factor: 
[[-0.03558422  0.18218957]
 [ 0.99936664 -0.9832635 ]]
PCA Reverse Transform: 0.00021266937255859375
decenter with new strategy:
[ 4.25111543e-04 -3.49510226e-05]
decenter with old strategy:
[0.00095252 0.00054525]
Fit RMSE new decenter: 0.1251633723784896
Fit RMSE: 0.12883856889913245
 Test Against Ground Truth
[(' decentering', 0.0012540817260742188), (' smoothing and normalization', 0.00024771690368652344)]
Smoothing and Normalization: 0.0004475116729736328
Fit RMSE: 0.1177677077443904
sklearn Test Against Ground Truth
Centering time: 0.0013430118560791016
PCA fit: 0.04154062271118164
[[9.80392153e-01 4.81524465e-09]
 [4.81524458e-09 9.80392289e-01]]
PCA Transform: 0.0015387535095214844
total iterations: 1155
TLDA fit: 627.9720344543457
Whitened factor: 
[[-0.06911722  0.14900258]
 [ 0.9976086  -0.9888369 ]]
PCA Reverse Transform: 0.00021147727966308594
decenter with new strategy:
[ 3.95292636e-04 -4.35212792e-05]
decenter with old strategy:
[0.00093069 0.00052259]
Fit RMSE new decenter: 0.1255769327248911
Fit RMSE: 0.12884230074163686
 Test Against Ground Truth
[(' decentering', 0.0012848377227783203), (' smoothing and normalization', 0.00024771690368652344)]
Smoothing and Normalization: 0.0004525184631347656
Fit RMSE: 0.1177677077443904
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003323793411254883
PCA fit: 0.24128127098083496
[[9.80392149e-01 9.35263081e-09]
 [9.35263045e-09 9.80392130e-01]]
PCA Transform: 0.005769491195678711
total iterations: 11
TLDA fit: 5.511105060577393
Whitened factor: 
[[ 0.3932031   0.11047556]
 [-0.91945165  0.99387884]]
PCA Reverse Transform: 0.00021004676818847656
decenter with new strategy:
[-0.00176924  0.00103037]
decenter with old strategy:
[0.00542761 0.00712147]
Fit RMSE new decenter: 0.05743876187619658
Fit RMSE: 0.059091410901802706
 Test Against Ground Truth
[(' decentering', 0.0012881755828857422), (' smoothing and normalization', 0.0002865791320800781)]
Smoothing and Normalization: 0.0005156993865966797
Fit RMSE: 0.055521566915404046
sklearn Test Against Ground Truth
Centering time: 0.003397703170776367
PCA fit: 0.2358548641204834
[[9.80392149e-01 9.35263081e-09]
 [9.35263045e-09 9.80392130e-01]]
PCA Transform: 0.005789756774902344
total iterations: 11
TLDA fit: 5.556565523147583
Whitened factor: 
[[ 0.22582178 -0.06078639]
 [-0.9741687   0.99815077]]
PCA Reverse Transform: 0.00020432472229003906
decenter with new strategy:
[-3.79553942e-05  2.21492418e-03]
decenter with old strategy:
[0.00674861 0.00845694]
Fit RMSE new decenter: 0.05820715577779579
Fit RMSE: 0.059106326628859465
 Test Against Ground Truth
[(' decentering', 0.0013058185577392578), (' smoothing and normalization', 0.00032258033752441406)]
Smoothing and Normalization: 0.00047469139099121094
Fit RMSE: 0.055521566915404046
sklearn Test Against Ground Truth
Centering time: 0.003461122512817383
PCA fit: 0.24184751510620117
[[9.80392149e-01 9.35263081e-09]
 [9.35263045e-09 9.80392130e-01]]
PCA Transform: 0.005742311477661133
total iterations: 11
TLDA fit: 5.60680627822876
Whitened factor: 
[[ 0.20411053 -0.10102293]
 [-0.97894776  0.99488413]]
PCA Reverse Transform: 0.0002048015594482422
decenter with new strategy:
[0.00024232 0.00246511]
decenter with old strategy:
[0.00691933 0.00877185]
Fit RMSE new decenter: 0.05842081412331795
Fit RMSE: 0.059070057729306194
 Test Against Ground Truth
[(' decentering', 0.001295328140258789), (' smoothing and normalization', 0.00032591819763183594)]
Smoothing and Normalization: 0.0004930496215820312
Fit RMSE: 0.055521566915404046
sklearn Test Against Ground Truth
Centering time: 0.0034482479095458984
PCA fit: 0.22510099411010742
[[9.80392149e-01 9.35263081e-09]
 [9.35263045e-09 9.80392130e-01]]
PCA Transform: 0.005773782730102539
total iterations: 11
TLDA fit: 5.461222887039185
Whitened factor: 
[[ 0.40243098  0.11353698]
 [-0.91545033  0.99353385]]
PCA Reverse Transform: 0.00020551681518554688
decenter with new strategy:
[-0.00186205  0.00101116]
decenter with old strategy:
[0.00535452 0.00709767]
Fit RMSE new decenter: 0.05744412171545838
Fit RMSE: 0.0590759583864394
 Test Against Ground Truth
[(' decentering', 0.001340627670288086), (' smoothing and normalization', 0.000286102294921875)]
Smoothing and Normalization: 0.0005197525024414062
Fit RMSE: 0.055521566915404046
sklearn Test Against Ground Truth
Centering time: 0.003414154052734375
PCA fit: 0.238816499710083
[[9.80392149e-01 9.35263081e-09]
 [9.35263045e-09 9.80392130e-01]]
PCA Transform: 0.005771160125732422
total iterations: 13
TLDA fit: 6.579521656036377
Whitened factor: 
[[ 0.22990587 -0.06771114]
 [-0.97321284  0.99770504]]
PCA Reverse Transform: 0.00020623207092285156
decenter with new strategy:
[-7.39255485e-05  2.27112015e-03]
decenter with old strategy:
[0.00671648 0.00851111]
Fit RMSE new decenter: 0.05809443428200222
Fit RMSE: 0.05908350936199359
 Test Against Ground Truth
[(' decentering', 0.0012829303741455078), (' smoothing and normalization', 0.0002956390380859375)]
Smoothing and Normalization: 0.000476837158203125
Fit RMSE: 0.055521566915404046
sklearn Test Against Ground Truth
Centering time: 0.0034151077270507812
PCA fit: 0.23757696151733398
[[9.80392149e-01 9.35263081e-09]
 [9.35263045e-09 9.80392130e-01]]
PCA Transform: 0.005809307098388672
total iterations: 19
TLDA fit: 9.80567216873169
Whitened factor: 
[[ 0.19176127 -0.10628324]
 [-0.9814416   0.9943359 ]]
PCA Reverse Transform: 0.0002105236053466797
decenter with new strategy:
[0.00037831 0.0024798 ]
decenter with old strategy:
[0.00701638 0.00881305]
Fit RMSE new decenter: 0.0585703543084232
Fit RMSE: 0.05908515380139692
 Test Against Ground Truth
[(' decentering', 0.0012874603271484375), (' smoothing and normalization', 0.0002856254577636719)]
Smoothing and Normalization: 0.0005207061767578125
Fit RMSE: 0.055521566915404046
sklearn Test Against Ground Truth
Centering time: 0.0033066272735595703
PCA fit: 0.20570063591003418
[[9.80392149e-01 9.35263081e-09]
 [9.35263045e-09 9.80392130e-01]]
PCA Transform: 0.005786895751953125
total iterations: 46
TLDA fit: 24.538684368133545
Whitened factor: 
[[ 0.39238134  0.1046062 ]
 [-0.9198026   0.9945138 ]]
PCA Reverse Transform: 0.00021409988403320312
decenter with new strategy:
[-0.00176935  0.00107757]
decenter with old strategy:
[0.00543412 0.00716711]
Fit RMSE new decenter: 0.057431015689765004
Fit RMSE: 0.05908062919234413
 Test Against Ground Truth
[(' decentering', 0.0012867450714111328), (' smoothing and normalization', 0.00027942657470703125)]
Smoothing and Normalization: 0.0005254745483398438
Fit RMSE: 0.055521566915404046
sklearn Test Against Ground Truth
Centering time: 0.003361940383911133
PCA fit: 0.2391815185546875
[[9.80392149e-01 9.35263081e-09]
 [9.35263045e-09 9.80392130e-01]]
PCA Transform: 0.0057871341705322266
total iterations: 86
TLDA fit: 46.36376619338989
Whitened factor: 
[[ 0.22752133 -0.06853407]
 [-0.9737731   0.9976488 ]]
PCA Reverse Transform: 0.00021696090698242188
decenter with new strategy:
[-4.76933590e-05  2.27358979e-03]
decenter with old strategy:
[0.00673524 0.00851755]
Fit RMSE new decenter: 0.05814084302800097
Fit RMSE: 0.059086885958916015
 Test Against Ground Truth
[(' decentering', 0.0012919902801513672), (' smoothing and normalization', 0.0002932548522949219)]
Smoothing and Normalization: 0.0005161762237548828
Fit RMSE: 0.055521566915404046
sklearn Test Against Ground Truth
Centering time: 0.0034253597259521484
PCA fit: 0.23894262313842773
[[9.80392149e-01 9.35263081e-09]
 [9.35263045e-09 9.80392130e-01]]
PCA Transform: 0.005794525146484375
total iterations: 139
TLDA fit: 74.77838206291199
Whitened factor: 
[[ 0.19203067 -0.10456295]
 [-0.9813889   0.9945183 ]]
PCA Reverse Transform: 0.00021386146545410156
decenter with new strategy:
[0.00037281 0.00246871]
decenter with old strategy:
[0.00701427 0.00879957]
Fit RMSE new decenter: 0.05857399876460461
Fit RMSE: 0.05908808455141493
 Test Against Ground Truth
[(' decentering', 0.0012996196746826172), (' smoothing and normalization', 0.0002887248992919922)]
Smoothing and Normalization: 0.0005121231079101562
Fit RMSE: 0.055521566915404046
sklearn Test Against Ground Truth
Centering time: 0.0034062862396240234
PCA fit: 0.2246856689453125
[[9.80392149e-01 9.35263081e-09]
 [9.35263045e-09 9.80392130e-01]]
PCA Transform: 0.005804300308227539
total iterations: 366
TLDA fit: 197.31279134750366
Whitened factor: 
[[ 0.39094886  0.10375385]
 [-0.9204124   0.99460304]]
PCA Reverse Transform: 0.00022149085998535156
decenter with new strategy:
[-0.00175544  0.00108359]
decenter with old strategy:
[0.00544547 0.00717374]
Fit RMSE new decenter: 0.057429684217403146
Fit RMSE: 0.059082195326245854
 Test Against Ground Truth
[(' decentering', 0.0012960433959960938), (' smoothing and normalization', 0.00028634071350097656)]
Smoothing and Normalization: 0.0005245208740234375
Fit RMSE: 0.055521566915404046
sklearn Test Against Ground Truth
Centering time: 0.003320455551147461
PCA fit: 0.20821332931518555
[[9.80392149e-01 9.35263081e-09]
 [9.35263045e-09 9.80392130e-01]]
PCA Transform: 0.00580143928527832
total iterations: 643
TLDA fit: 351.4436972141266
Whitened factor: 
[[ 0.2260445  -0.06985524]
 [-0.9741171   0.99755716]]
PCA Reverse Transform: 0.00021505355834960938
decenter with new strategy:
[-3.05335150e-05  2.28094585e-03]
decenter with old strategy:
[0.00674686 0.00852788]
Fit RMSE new decenter: 0.058165713526095
Fit RMSE: 0.05908731752657427
 Test Against Ground Truth
[(' decentering', 0.001331329345703125), (' smoothing and normalization', 0.0003237724304199219)]
Smoothing and Normalization: 0.0004744529724121094
Fit RMSE: 0.055521566915404046
sklearn Test Against Ground Truth
Centering time: 0.0033822059631347656
PCA fit: 0.22902655601501465
[[9.80392149e-01 9.35263081e-09]
 [9.35263045e-09 9.80392130e-01]]
PCA Transform: 0.005796670913696289
total iterations: 1111
TLDA fit: 599.1150918006897
Whitened factor: 
[[ 0.19077039 -0.10580897]
 [-0.98163474  0.9943865 ]]
PCA Reverse Transform: 0.000213623046875
decenter with new strategy:
[0.00038773 0.00247492]
decenter with old strategy:
[0.00702417 0.00880933]
Fit RMSE new decenter: 0.05858507450701515
Fit RMSE: 0.05908817829936013
 Test Against Ground Truth
[(' decentering', 0.0012869834899902344), (' smoothing and normalization', 0.00028204917907714844)]
Smoothing and Normalization: 0.00048041343688964844
Fit RMSE: 0.055521566915404046
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014040470123291016
PCA fit: 0.045723676681518555
[[9.80392152e-01 1.21600443e-08]
 [1.21600444e-08 9.80392094e-01]]
PCA Transform: 0.0015728473663330078
total iterations: 11
TLDA fit: 5.9023144245147705
Whitened factor: 
[[ 0.12434551  0.25218955]
 [ 0.992239   -0.96767783]]
PCA Reverse Transform: 0.0002200603485107422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.13088185250452855
Fit RMSE: 0.1300574898676556
 Test Against Ground Truth
[(' decentering', 0.0012769699096679688), (' smoothing and normalization', 0.00028634071350097656)]
Smoothing and Normalization: 0.0004982948303222656
Fit RMSE: 0.11973903431984927
sklearn Test Against Ground Truth
Centering time: 0.001377105712890625
PCA fit: 0.04373502731323242
[[9.80392152e-01 1.21600443e-08]
 [1.21600444e-08 9.80392094e-01]]
PCA Transform: 0.0015265941619873047
total iterations: 11
TLDA fit: 5.581895112991333
Whitened factor: 
[[-0.05378705  0.1450233 ]
 [ 0.99855244 -0.9894282 ]]
PCA Reverse Transform: 0.00020623207092285156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1275685379134385
Fit RMSE: 0.1296724856760176
 Test Against Ground Truth
[(' decentering', 0.0012705326080322266), (' smoothing and normalization', 0.0002503395080566406)]
Smoothing and Normalization: 0.0005030632019042969
Fit RMSE: 0.11973903431984927
sklearn Test Against Ground Truth
Centering time: 0.0013649463653564453
PCA fit: 0.04378962516784668
[[9.80392152e-01 1.21600443e-08]
 [1.21600444e-08 9.80392094e-01]]
PCA Transform: 0.0015799999237060547
total iterations: 11
TLDA fit: 5.650028944015503
Whitened factor: 
[[-0.10170491  0.13452424]
 [ 0.9948146  -0.9909103 ]]
PCA Reverse Transform: 0.00020265579223632812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12732810240403705
Fit RMSE: 0.12946429725075184
 Test Against Ground Truth
[(' decentering', 0.00124359130859375), (' smoothing and normalization', 0.0002601146697998047)]
Smoothing and Normalization: 0.0004546642303466797
Fit RMSE: 0.11973903431984927
sklearn Test Against Ground Truth
Centering time: 0.0013456344604492188
PCA fit: 0.043519020080566406
[[9.80392152e-01 1.21600443e-08]
 [1.21600444e-08 9.80392094e-01]]
PCA Transform: 0.0015835762023925781
total iterations: 11
TLDA fit: 5.7777252197265625
Whitened factor: 
[[ 0.07249902  0.29018304]
 [ 0.9973685  -0.9569712 ]]
PCA Reverse Transform: 0.0002009868621826172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12415264921251996
Fit RMSE: 0.12953948138581578
 Test Against Ground Truth
[(' decentering', 0.0013234615325927734), (' smoothing and normalization', 0.00026297569274902344)]
Smoothing and Normalization: 0.0004870891571044922
Fit RMSE: 0.11973903431984927
sklearn Test Against Ground Truth
Centering time: 0.0013628005981445312
PCA fit: 0.04373669624328613
[[9.80392152e-01 1.21600443e-08]
 [1.21600444e-08 9.80392094e-01]]
PCA Transform: 0.0015234947204589844
total iterations: 18
TLDA fit: 9.399473905563354
Whitened factor: 
[[-0.05695596  0.17553324]
 [ 0.9983767  -0.9844735 ]]
PCA Reverse Transform: 0.00020265579223632812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12711020601792916
Fit RMSE: 0.12948294985960057
 Test Against Ground Truth
[(' decentering', 0.0012655258178710938), (' smoothing and normalization', 0.0002491474151611328)]
Smoothing and Normalization: 0.0004756450653076172
Fit RMSE: 0.11973903431984927
sklearn Test Against Ground Truth
Centering time: 0.0013420581817626953
PCA fit: 0.04357719421386719
[[9.80392152e-01 1.21600443e-08]
 [1.21600444e-08 9.80392094e-01]]
PCA Transform: 0.0015358924865722656
total iterations: 28
TLDA fit: 14.717292785644531
Whitened factor: 
[[-0.08603949  0.15043455]
 [ 0.99629176 -0.9886199 ]]
PCA Reverse Transform: 0.00020265579223632812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12724105589541757
Fit RMSE: 0.1294624883330231
 Test Against Ground Truth
[(' decentering', 0.0012667179107666016), (' smoothing and normalization', 0.0002503395080566406)]
Smoothing and Normalization: 0.00048613548278808594
Fit RMSE: 0.11973903431984927
sklearn Test Against Ground Truth
Centering time: 0.0013704299926757812
PCA fit: 0.043801307678222656
[[9.80392152e-01 1.21600443e-08]
 [1.21600444e-08 9.80392094e-01]]
PCA Transform: 0.0015110969543457031
total iterations: 49
TLDA fit: 26.483131408691406
Whitened factor: 
[[ 0.07023316  0.29915315]
 [ 0.99753064 -0.95420516]]
PCA Reverse Transform: 0.00020170211791992188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12414153563500656
Fit RMSE: 0.12947330041475913
 Test Against Ground Truth
[(' decentering', 0.0012705326080322266), (' smoothing and normalization', 0.0003039836883544922)]
Smoothing and Normalization: 0.0004525184631347656
Fit RMSE: 0.11973903431984927
sklearn Test Against Ground Truth
Centering time: 0.0013647079467773438
PCA fit: 0.043744802474975586
[[9.80392152e-01 1.21600443e-08]
 [1.21600444e-08 9.80392094e-01]]
PCA Transform: 0.0015323162078857422
total iterations: 120
TLDA fit: 66.0103669166565
Whitened factor: 
[[-0.05786712  0.17600936]
 [ 0.9983243  -0.9843885 ]]
PCA Reverse Transform: 0.0002002716064453125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12709452542058186
Fit RMSE: 0.12947519455608383
 Test Against Ground Truth
[(' decentering', 0.0012798309326171875), (' smoothing and normalization', 0.0002567768096923828)]
Smoothing and Normalization: 0.0004961490631103516
Fit RMSE: 0.11973903431984927
sklearn Test Against Ground Truth
Centering time: 0.001363515853881836
PCA fit: 0.04351353645324707
[[9.80392152e-01 1.21600443e-08]
 [1.21600444e-08 9.80392094e-01]]
PCA Transform: 0.001552581787109375
total iterations: 207
TLDA fit: 114.60998010635376
Whitened factor: 
[[-0.08547483  0.14932337]
 [ 0.99634033 -0.9887884 ]]
PCA Reverse Transform: 0.00021886825561523438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12725909890846993
Fit RMSE: 0.12947185229769317
 Test Against Ground Truth
[(' decentering', 0.0012879371643066406), (' smoothing and normalization', 0.000244140625)]
Smoothing and Normalization: 0.0004646778106689453
Fit RMSE: 0.11973903431984927
sklearn Test Against Ground Truth
Centering time: 0.0013689994812011719
PCA fit: 0.043573617935180664
[[9.80392152e-01 1.21600443e-08]
 [1.21600444e-08 9.80392094e-01]]
PCA Transform: 0.0015575885772705078
total iterations: 376
TLDA fit: 205.8197319507599
Whitened factor: 
[[ 0.07059661  0.30048364]
 [ 0.99750495 -0.953787  ]]
PCA Reverse Transform: 0.00020456314086914062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12414500027844814
Fit RMSE: 0.1294673654295683
 Test Against Ground Truth
[(' decentering', 0.001260995864868164), (' smoothing and normalization', 0.0002529621124267578)]
Smoothing and Normalization: 0.0004553794860839844
Fit RMSE: 0.11973903431984927
sklearn Test Against Ground Truth
Centering time: 0.0013484954833984375
PCA fit: 0.044173240661621094
[[9.80392152e-01 1.21600443e-08]
 [1.21600444e-08 9.80392094e-01]]
PCA Transform: 0.0015556812286376953
total iterations: 872
TLDA fit: 481.75623416900635
Whitened factor: 
[[-0.05696554  0.17684564]
 [ 0.9983762  -0.9842387 ]]
PCA Reverse Transform: 0.000209808349609375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12708889276877663
Fit RMSE: 0.12947547990522665
 Test Against Ground Truth
[(' decentering', 0.0013179779052734375), (' smoothing and normalization', 0.0002529621124267578)]
Smoothing and Normalization: 0.0004782676696777344
Fit RMSE: 0.11973903431984927
sklearn Test Against Ground Truth
Centering time: 0.0013599395751953125
PCA fit: 0.04357409477233887
[[9.80392152e-01 1.21600443e-08]
 [1.21600444e-08 9.80392094e-01]]
PCA Transform: 0.0015408992767333984
total iterations: 1546
TLDA fit: 847.3784346580505
Whitened factor: 
[[-0.08452065  0.14963828]
 [ 0.99642175 -0.9887409 ]]
PCA Reverse Transform: 0.0002086162567138672
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12726182864803706
Fit RMSE: 0.1294753936877516
 Test Against Ground Truth
[(' decentering', 0.0012805461883544922), (' smoothing and normalization', 0.00025391578674316406)]
Smoothing and Normalization: 0.00046062469482421875
Fit RMSE: 0.11973903431984927
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.002893209457397461
PCA fit: 0.27662014961242676
[[9.80392153e-01 1.54919716e-08]
 [1.54919718e-08 9.80392097e-01]]
PCA Transform: 0.005797624588012695
total iterations: 11
TLDA fit: 5.529084920883179
Whitened factor: 
[[ 0.29730788  0.00212216]
 [-0.9547816   0.9999978 ]]
PCA Reverse Transform: 0.0002009868621826172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05965889277243465
Fit RMSE: 0.06013229073042134
 Test Against Ground Truth
[(' decentering', 0.0012898445129394531), (' smoothing and normalization', 0.0002894401550292969)]
Smoothing and Normalization: 0.0005257129669189453
Fit RMSE: 0.05779542025179471
sklearn Test Against Ground Truth
Centering time: 0.0028104782104492188
PCA fit: 0.24605298042297363
[[9.80392153e-01 1.54919716e-08]
 [1.54919718e-08 9.80392097e-01]]
PCA Transform: 0.005778074264526367
total iterations: 11
TLDA fit: 5.4493796825408936
Whitened factor: 
[[ 0.20768127 -0.11379433]
 [-0.9781966   0.99350435]]
PCA Reverse Transform: 0.00019431114196777344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05945949390921717
Fit RMSE: 0.06010319175172725
 Test Against Ground Truth
[(' decentering', 0.0013260841369628906), (' smoothing and normalization', 0.00029015541076660156)]
Smoothing and Normalization: 0.0005271434783935547
Fit RMSE: 0.05779542025179471
sklearn Test Against Ground Truth
Centering time: 0.0029144287109375
PCA fit: 0.26090550422668457
[[9.80392153e-01 1.54919716e-08]
 [1.54919718e-08 9.80392097e-01]]
PCA Transform: 0.00581049919128418
total iterations: 11
TLDA fit: 5.448359251022339
Whitened factor: 
[[ 0.19954488 -0.14574106]
 [-0.9798887   0.9893228 ]]
PCA Reverse Transform: 0.000202178955078125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0594067966439367
Fit RMSE: 0.06007018271949182
 Test Against Ground Truth
[(' decentering', 0.0012831687927246094), (' smoothing and normalization', 0.0002930164337158203)]
Smoothing and Normalization: 0.0005388259887695312
Fit RMSE: 0.05779542025179471
sklearn Test Against Ground Truth
Centering time: 0.0028219223022460938
PCA fit: 0.26099085807800293
[[9.80392153e-01 1.54919716e-08]
 [1.54919718e-08 9.80392097e-01]]
PCA Transform: 0.005868434906005859
total iterations: 11
TLDA fit: 5.5105085372924805
Whitened factor: 
[[ 0.3463115   0.01030936]
 [-0.9381196   0.9999469 ]]
PCA Reverse Transform: 0.0002014636993408203
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0586864166252471
Fit RMSE: 0.060067204139836176
 Test Against Ground Truth
[(' decentering', 0.0012917518615722656), (' smoothing and normalization', 0.00029850006103515625)]
Smoothing and Normalization: 0.0005021095275878906
Fit RMSE: 0.05779542025179471
sklearn Test Against Ground Truth
Centering time: 0.0028777122497558594
PCA fit: 0.27927422523498535
[[9.80392153e-01 1.54919716e-08]
 [1.54919718e-08 9.80392097e-01]]
PCA Transform: 0.005836009979248047
total iterations: 15
TLDA fit: 7.767192363739014
Whitened factor: 
[[ 0.22967757 -0.1226055 ]
 [-0.9732668   0.9924555 ]]
PCA Reverse Transform: 0.00019884109497070312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05937056952106104
Fit RMSE: 0.06005834069293413
 Test Against Ground Truth
[(' decentering', 0.001276254653930664), (' smoothing and normalization', 0.0002923011779785156)]
Smoothing and Normalization: 0.0005207061767578125
Fit RMSE: 0.05779542025179471
sklearn Test Against Ground Truth
Centering time: 0.0029630661010742188
PCA fit: 0.2721688747406006
[[9.80392153e-01 1.54919716e-08]
 [1.54919718e-08 9.80392097e-01]]
PCA Transform: 0.0057718753814697266
total iterations: 27
TLDA fit: 14.006321430206299
Whitened factor: 
[[ 0.20614913 -0.15168987]
 [-0.9785205   0.9884282 ]]
PCA Reverse Transform: 0.0002071857452392578
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05937652498464654
Fit RMSE: 0.06005214793115494
 Test Against Ground Truth
[(' decentering', 0.0012748241424560547), (' smoothing and normalization', 0.00029349327087402344)]
Smoothing and Normalization: 0.0005109310150146484
Fit RMSE: 0.05779542025179471
sklearn Test Against Ground Truth
Centering time: 0.0029342174530029297
PCA fit: 0.2851219177246094
[[9.80392153e-01 1.54919716e-08]
 [1.54919718e-08 9.80392097e-01]]
PCA Transform: 0.005840301513671875
total iterations: 44
TLDA fit: 24.230777978897095
Whitened factor: 
[[ 0.35192853  0.00268059]
 [-0.93602675  0.9999964 ]]
PCA Reverse Transform: 0.00022172927856445312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058673932933863394
Fit RMSE: 0.06004742341579237
 Test Against Ground Truth
[(' decentering', 0.0012950897216796875), (' smoothing and normalization', 0.0002970695495605469)]
Smoothing and Normalization: 0.0005118846893310547
Fit RMSE: 0.05779542025179471
sklearn Test Against Ground Truth
Centering time: 0.0029473304748535156
PCA fit: 0.28595876693725586
[[9.80392153e-01 1.54919716e-08]
 [1.54919718e-08 9.80392097e-01]]
PCA Transform: 0.005807638168334961
total iterations: 113
TLDA fit: 59.922245264053345
Whitened factor: 
[[ 0.23584832 -0.12016106]
 [-0.9717899   0.9927544 ]]
PCA Reverse Transform: 0.00021195411682128906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05935437471630481
Fit RMSE: 0.06005262582498084
 Test Against Ground Truth
[(' decentering', 0.0012755393981933594), (' smoothing and normalization', 0.00029730796813964844)]
Smoothing and Normalization: 0.0005152225494384766
Fit RMSE: 0.05779542025179471
sklearn Test Against Ground Truth
Centering time: 0.002996683120727539
PCA fit: 0.2567892074584961
[[9.80392153e-01 1.54919716e-08]
 [1.54919718e-08 9.80392097e-01]]
PCA Transform: 0.005791902542114258
total iterations: 188
TLDA fit: 100.74566435813904
Whitened factor: 
[[ 0.20989285 -0.14686662]
 [-0.97772443  0.9891563 ]]
PCA Reverse Transform: 0.0002086162567138672
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05937642961635245
Fit RMSE: 0.06005345403474295
 Test Against Ground Truth
[(' decentering', 0.0013599395751953125), (' smoothing and normalization', 0.0002968311309814453)]
Smoothing and Normalization: 0.0005159378051757812
Fit RMSE: 0.05779542025179471
sklearn Test Against Ground Truth
Centering time: 0.002918243408203125
PCA fit: 0.26816511154174805
[[9.80392153e-01 1.54919716e-08]
 [1.54919718e-08 9.80392097e-01]]
PCA Transform: 0.005817413330078125
total iterations: 301
TLDA fit: 161.09446096420288
Whitened factor: 
[[ 0.35240534  0.00173624]
 [-0.9358476   0.9999985 ]]
PCA Reverse Transform: 0.00021386146545410156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05867235213332704
Fit RMSE: 0.06004532567313636
 Test Against Ground Truth
[(' decentering', 0.0013315677642822266), (' smoothing and normalization', 0.00029754638671875)]
Smoothing and Normalization: 0.0005183219909667969
Fit RMSE: 0.05779542025179471
sklearn Test Against Ground Truth
Centering time: 0.0029146671295166016
PCA fit: 0.2597012519836426
[[9.80392153e-01 1.54919716e-08]
 [1.54919718e-08 9.80392097e-01]]
PCA Transform: 0.0058214664459228516
total iterations: 826
TLDA fit: 443.56929087638855
Whitened factor: 
[[ 0.23577642 -0.12063032]
 [-0.97180736  0.99269754]]
PCA Reverse Transform: 0.0002143383026123047
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.059353539348766846
Fit RMSE: 0.06005207498411087
 Test Against Ground Truth
[(' decentering', 0.0013859272003173828), (' smoothing and normalization', 0.00029349327087402344)]
Smoothing and Normalization: 0.0005147457122802734
Fit RMSE: 0.05779542025179471
sklearn Test Against Ground Truth
Centering time: 0.002824544906616211
PCA fit: 0.24454283714294434
[[9.80392153e-01 1.54919716e-08]
 [1.54919718e-08 9.80392097e-01]]
PCA Transform: 0.005811929702758789
total iterations: 1318
TLDA fit: 704.7058589458466
Whitened factor: 
[[ 0.20980905 -0.14711921]
 [-0.9777424   0.9891188 ]]
PCA Reverse Transform: 0.0002148151397705078
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.059376123739438395
Fit RMSE: 0.0600532213959485
 Test Against Ground Truth
[(' decentering', 0.0013501644134521484), (' smoothing and normalization', 0.0002942085266113281)]
Smoothing and Normalization: 0.0005238056182861328
Fit RMSE: 0.05779542025179471
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0016772747039794922
PCA fit: 0.04486393928527832
[[ 9.80392158e-01 -8.73445097e-09]
 [-8.73445112e-09  9.80392043e-01]]
PCA Transform: 0.00156402587890625
total iterations: 11
TLDA fit: 5.575831174850464
Whitened factor: 
[[ 0.9563324  -0.81815195]
 [-0.29228136  0.575002  ]]
PCA Reverse Transform: 0.0002257823944091797
decenter with new strategy:
[ 1.10941454e-04 -5.45922726e-05]
decenter with old strategy:
[1.92853708e-04 2.12591719e-05]
Fit RMSE new decenter: 0.1191879347560647
Fit RMSE: 0.12036727237409826
 Test Against Ground Truth
[(' decentering', 0.0012688636779785156), (' smoothing and normalization', 0.0002586841583251953)]
Smoothing and Normalization: 0.0004820823669433594
Fit RMSE: 0.11892376202158689
sklearn Test Against Ground Truth
Centering time: 0.0013892650604248047
PCA fit: 0.04114031791687012
[[ 9.80392158e-01 -8.73445097e-09]
 [-8.73445112e-09  9.80392043e-01]]
PCA Transform: 0.0015392303466796875
total iterations: 11
TLDA fit: 5.554732322692871
Whitened factor: 
[[ 0.9248913  -0.87618476]
 [-0.38023165  0.4819754 ]]
PCA Reverse Transform: 0.0002124309539794922
decenter with new strategy:
[ 1.10003681e-04 -6.20439243e-05]
decenter with old strategy:
[1.89591885e-04 1.53871669e-05]
Fit RMSE new decenter: 0.11917115223853282
Fit RMSE: 0.12027511123243526
 Test Against Ground Truth
[(' decentering', 0.0012960433959960938), (' smoothing and normalization', 0.0003116130828857422)]
Smoothing and Normalization: 0.00046324729919433594
Fit RMSE: 0.11892376202158689
sklearn Test Against Ground Truth
Centering time: 0.0013604164123535156
PCA fit: 0.04117465019226074
[[ 9.80392158e-01 -8.73445097e-09]
 [-8.73445112e-09  9.80392043e-01]]
PCA Transform: 0.0015716552734375
total iterations: 11
TLDA fit: 5.612616777420044
Whitened factor: 
[[ 0.9219569 -0.8940123]
 [-0.3872924  0.4480426]]
PCA Reverse Transform: 0.00021195411682128906
decenter with new strategy:
[ 1.10200895e-04 -6.42856557e-05]
decenter with old strategy:
[1.89289919e-04 1.35718263e-05]
Fit RMSE new decenter: 0.11916608261500845
Fit RMSE: 0.12020313154029756
 Test Against Ground Truth
[(' decentering', 0.0012791156768798828), (' smoothing and normalization', 0.0002620220184326172)]
Smoothing and Normalization: 0.00045943260192871094
Fit RMSE: 0.11892376202158689
sklearn Test Against Ground Truth
Centering time: 0.0013604164123535156
PCA fit: 0.041077613830566406
[[ 9.80392158e-01 -8.73445097e-09]
 [-8.73445112e-09  9.80392043e-01]]
PCA Transform: 0.001519918441772461
total iterations: 12
TLDA fit: 6.161565780639648
Whitened factor: 
[[ 0.95550394 -0.8124488 ]
 [-0.29497847  0.5830326 ]]
PCA Reverse Transform: 0.00021123886108398438
decenter with new strategy:
[ 1.10712333e-04 -5.39386024e-05]
decenter with old strategy:
[1.92766948e-04 2.18338585e-05]
Fit RMSE new decenter: 0.1191885037355959
Fit RMSE: 0.12039755869746849
 Test Against Ground Truth
[(' decentering', 0.0012924671173095703), (' smoothing and normalization', 0.0002601146697998047)]
Smoothing and Normalization: 0.00047087669372558594
Fit RMSE: 0.11892376202158689
sklearn Test Against Ground Truth
Centering time: 0.0013527870178222656
PCA fit: 0.041326045989990234
[[ 9.80392158e-01 -8.73445097e-09]
 [-8.73445112e-09  9.80392043e-01]]
PCA Transform: 0.0015184879302978516
total iterations: 19
TLDA fit: 9.871242046356201
Whitened factor: 
[[ 0.9187455  -0.8715666 ]
 [-0.39485034  0.49027705]]
PCA Reverse Transform: 0.00021696090698242188
decenter with new strategy:
[ 1.09402681e-04 -6.16001949e-05]
decenter with old strategy:
[1.88959813e-04 1.58563788e-05]
Fit RMSE new decenter: 0.11917062837818237
Fit RMSE: 0.12032654992746561
 Test Against Ground Truth
[(' decentering', 0.0012674331665039062), (' smoothing and normalization', 0.00026035308837890625)]
Smoothing and Normalization: 0.0004680156707763672
Fit RMSE: 0.11892376202158689
sklearn Test Against Ground Truth
Centering time: 0.0013675689697265625
PCA fit: 0.04123830795288086
[[ 9.80392158e-01 -8.73445097e-09]
 [-8.73445112e-09  9.80392043e-01]]
PCA Transform: 0.0015070438385009766
total iterations: 26
TLDA fit: 13.682584524154663
Whitened factor: 
[[ 0.9094885  -0.8841511 ]
 [-0.4157291   0.46720105]]
PCA Reverse Transform: 0.00021791458129882812
decenter with new strategy:
[ 1.08975398e-04 -6.33292654e-05]
decenter with old strategy:
[1.88010204e-04 1.45768033e-05]
Fit RMSE new decenter: 0.11916507868723432
Fit RMSE: 0.12030806382231934
 Test Against Ground Truth
[(' decentering', 0.0012989044189453125), (' smoothing and normalization', 0.0002734661102294922)]
Smoothing and Normalization: 0.0004687309265136719
Fit RMSE: 0.11892376202158689
sklearn Test Against Ground Truth
Centering time: 0.0013632774353027344
PCA fit: 0.04125475883483887
[[ 9.80392158e-01 -8.73445097e-09]
 [-8.73445112e-09  9.80392043e-01]]
PCA Transform: 0.002070903778076172
total iterations: 88
TLDA fit: 46.76766633987427
Whitened factor: 
[[ 0.9554222  -0.81416374]
 [-0.2952428   0.58063537]]
PCA Reverse Transform: 0.00021147727966308594
decenter with new strategy:
[ 1.10754978e-04 -5.41395787e-05]
decenter with old strategy:
[1.92758393e-04 2.16610871e-05]
Fit RMSE new decenter: 0.11918828589039314
Fit RMSE: 0.12039056391258648
 Test Against Ground Truth
[(' decentering', 0.0012772083282470703), (' smoothing and normalization', 0.0002582073211669922)]
Smoothing and Normalization: 0.0004565715789794922
Fit RMSE: 0.11892376202158689
sklearn Test Against Ground Truth
Centering time: 0.0013427734375
PCA fit: 0.041239261627197266
[[ 9.80392158e-01 -8.73445097e-09]
 [-8.73445112e-09  9.80392043e-01]]
PCA Transform: 0.0015382766723632812
total iterations: 119
TLDA fit: 63.395039796829224
Whitened factor: 
[[ 0.91861403 -0.8706057 ]
 [-0.3951561   0.49198148]]
PCA Reverse Transform: 0.00020623207092285156
decenter with new strategy:
[ 1.09368408e-04 -6.14855726e-05]
decenter with old strategy:
[1.88946305e-04 1.59539571e-05]
Fit RMSE new decenter: 0.11917081480906762
Fit RMSE: 0.1203317734266207
 Test Against Ground Truth
[(' decentering', 0.0012743473052978516), (' smoothing and normalization', 0.00026226043701171875)]
Smoothing and Normalization: 0.00048542022705078125
Fit RMSE: 0.11892376202158689
sklearn Test Against Ground Truth
Centering time: 0.001356363296508789
PCA fit: 0.041173458099365234
[[ 9.80392158e-01 -8.73445097e-09]
 [-8.73445112e-09  9.80392043e-01]]
PCA Transform: 0.0015604496002197266
total iterations: 193
TLDA fit: 104.58865022659302
Whitened factor: 
[[ 0.9087385 -0.8824637]
 [-0.417366   0.4703805]]
PCA Reverse Transform: 0.0002143383026123047
decenter with new strategy:
[ 1.08875908e-04 -6.31371365e-05]
decenter with old strategy:
[1.87933384e-04 1.47485593e-05]
Fit RMSE new decenter: 0.11916529613450647
Fit RMSE: 0.12031966216332703
 Test Against Ground Truth
[(' decentering', 0.0013098716735839844), (' smoothing and normalization', 0.0002529621124267578)]
Smoothing and Normalization: 0.0004971027374267578
Fit RMSE: 0.11892376202158689
sklearn Test Against Ground Truth
Centering time: 0.0013530254364013672
PCA fit: 0.04120182991027832
[[ 9.80392158e-01 -8.73445097e-09]
 [-8.73445112e-09  9.80392043e-01]]
PCA Transform: 0.0015435218811035156
total iterations: 774
TLDA fit: 417.83084082603455
Whitened factor: 
[[ 0.9553684  -0.81438637]
 [-0.29541713  0.58032304]]
PCA Reverse Transform: 0.00021910667419433594
decenter with new strategy:
[ 1.10757059e-04 -5.41662784e-05]
decenter with old strategy:
[1.92752759e-04 2.16386566e-05]
Fit RMSE new decenter: 0.11918825077076149
Fit RMSE: 0.12038993002647827
 Test Against Ground Truth
[(' decentering', 0.0013654232025146484), (' smoothing and normalization', 0.0002532005310058594)]
Smoothing and Normalization: 0.00047969818115234375
Fit RMSE: 0.11892376202158689
sklearn Test Against Ground Truth
Centering time: 0.0013568401336669922
PCA fit: 0.04111289978027344
[[ 9.80392158e-01 -8.73445097e-09]
 [-8.73445112e-09  9.80392043e-01]]
PCA Transform: 0.0015559196472167969
total iterations: 871
TLDA fit: 468.31937623023987
Whitened factor: 
[[ 0.9184291 -0.8706406]
 [-0.3955858  0.4919197]]
PCA Reverse Transform: 0.0002117156982421875
decenter with new strategy:
[ 1.09354673e-04 -6.14934288e-05]
decenter with old strategy:
[1.88927305e-04 1.59504164e-05]
Fit RMSE new decenter: 0.11917075923552543
Fit RMSE: 0.12033247918500753
 Test Against Ground Truth
[(' decentering', 0.00128936767578125), (' smoothing and normalization', 0.0002579689025878906)]
Smoothing and Normalization: 0.0004558563232421875
Fit RMSE: 0.11892376202158689
sklearn Test Against Ground Truth
Centering time: 0.0013477802276611328
PCA fit: 0.04119729995727539
[[ 9.80392158e-01 -8.73445097e-09]
 [-8.73445112e-09  9.80392043e-01]]
PCA Transform: 0.0015401840209960938
total iterations: 1466
TLDA fit: 780.7780706882477
Whitened factor: 
[[ 0.9084603  -0.8823526 ]
 [-0.4179711   0.47058883]]
PCA Reverse Transform: 0.00021266937255859375
decenter with new strategy:
[ 1.08851226e-04 -6.31292452e-05]
decenter with old strategy:
[1.87904896e-04 1.47598660e-05]
Fit RMSE new decenter: 0.11916524601865625
Fit RMSE: 0.1203214441736554
 Test Against Ground Truth
[(' decentering', 0.0012671947479248047), (' smoothing and normalization', 0.00025081634521484375)]
Smoothing and Normalization: 0.0004553794860839844
Fit RMSE: 0.11892376202158689
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0015468597412109375
PCA fit: 0.2554936408996582
[[ 9.80392155e-01 -5.50506975e-09]
 [-5.50506934e-09  9.80392196e-01]]
PCA Transform: 0.006043434143066406
total iterations: 11
TLDA fit: 5.550685167312622
Whitened factor: 
[[ 0.24787828  0.2665989 ]
 [ 0.9687912  -0.9638076 ]]
PCA Reverse Transform: 0.00021791458129882812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05886563789285319
Fit RMSE: 0.05933429908641701
 Test Against Ground Truth
[(' decentering', 0.0013401508331298828), (' smoothing and normalization', 0.000293731689453125)]
Smoothing and Normalization: 0.0005307197570800781
Fit RMSE: 0.055979587509240133
sklearn Test Against Ground Truth
Centering time: 0.002895355224609375
PCA fit: 0.2723672389984131
[[ 9.80392155e-01 -5.50506975e-09]
 [-5.50506934e-09  9.80392196e-01]]
PCA Transform: 0.006079912185668945
total iterations: 11
TLDA fit: 5.4588782787323
Whitened factor: 
[[ 0.07643855  0.11636901]
 [ 0.9970743  -0.993206  ]]
PCA Reverse Transform: 0.0002028942108154297
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05859783882707092
Fit RMSE: 0.05930433325183696
 Test Against Ground Truth
[(' decentering', 0.0013492107391357422), (' smoothing and normalization', 0.00033211708068847656)]
Smoothing and Normalization: 0.0005352497100830078
Fit RMSE: 0.055979587509240133
sklearn Test Against Ground Truth
Centering time: 0.0029892921447753906
PCA fit: 0.2687687873840332
[[ 9.80392155e-01 -5.50506975e-09]
 [-5.50506934e-09  9.80392196e-01]]
PCA Transform: 0.0059969425201416016
total iterations: 11
TLDA fit: 5.403945207595825
Whitened factor: 
[[ 0.01430615  0.08874185]
 [ 0.99989766 -0.9960547 ]]
PCA Reverse Transform: 0.0002052783966064453
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058359629659870634
Fit RMSE: 0.05924145011115133
 Test Against Ground Truth
[(' decentering', 0.0012888908386230469), (' smoothing and normalization', 0.0002951622009277344)]
Smoothing and Normalization: 0.00052642822265625
Fit RMSE: 0.055979587509240133
sklearn Test Against Ground Truth
Centering time: 0.0029897689819335938
PCA fit: 0.2728300094604492
[[ 9.80392155e-01 -5.50506975e-09]
 [-5.50506934e-09  9.80392196e-01]]
PCA Transform: 0.006044626235961914
total iterations: 13
TLDA fit: 6.469269037246704
Whitened factor: 
[[ 0.23829493  0.23017828]
 [ 0.97119284 -0.9731486 ]]
PCA Reverse Transform: 0.00020551681518554688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05881337655822675
Fit RMSE: 0.0593906844516044
 Test Against Ground Truth
[(' decentering', 0.0013036727905273438), (' smoothing and normalization', 0.00029587745666503906)]
Smoothing and Normalization: 0.0005736351013183594
Fit RMSE: 0.055979587509240133
sklearn Test Against Ground Truth
Centering time: 0.0029561519622802734
PCA fit: 0.24483227729797363
[[ 9.80392155e-01 -5.50506975e-09]
 [-5.50506934e-09  9.80392196e-01]]
PCA Transform: 0.006094217300415039
total iterations: 15
TLDA fit: 7.575829267501831
Whitened factor: 
[[ 0.08327705  0.0693446 ]
 [ 0.9965264  -0.99759275]]
PCA Reverse Transform: 0.0002048015594482422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05894470892203145
Fit RMSE: 0.059408698655056415
 Test Against Ground Truth
[(' decentering', 0.00128173828125), (' smoothing and normalization', 0.0003402233123779297)]
Smoothing and Normalization: 0.0005421638488769531
Fit RMSE: 0.055979587509240133
sklearn Test Against Ground Truth
Centering time: 0.0029554367065429688
PCA fit: 0.28380537033081055
[[ 9.80392155e-01 -5.50506975e-09]
 [-5.50506934e-09  9.80392196e-01]]
PCA Transform: 0.006035804748535156
total iterations: 23
TLDA fit: 11.908406019210815
Whitened factor: 
[[ 0.04849448  0.03677949]
 [ 0.99882346 -0.9993234 ]]
PCA Reverse Transform: 0.00021958351135253906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058821283529767586
Fit RMSE: 0.059405700773338135
 Test Against Ground Truth
[(' decentering', 0.001291513442993164), (' smoothing and normalization', 0.0002899169921875)]
Smoothing and Normalization: 0.0005536079406738281
Fit RMSE: 0.055979587509240133
sklearn Test Against Ground Truth
Centering time: 0.0029535293579101562
PCA fit: 0.25637197494506836
[[ 9.80392155e-01 -5.50506975e-09]
 [-5.50506934e-09  9.80392196e-01]]
PCA Transform: 0.006063699722290039
total iterations: 107
TLDA fit: 56.83221507072449
Whitened factor: 
[[ 0.23753248  0.22298793]
 [ 0.97137964 -0.97482127]]
PCA Reverse Transform: 0.0002067089080810547
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05881123913777063
Fit RMSE: 0.059403926189949
 Test Against Ground Truth
[(' decentering', 0.0013189315795898438), (' smoothing and normalization', 0.00029349327087402344)]
Smoothing and Normalization: 0.000522613525390625
Fit RMSE: 0.055979587509240133
sklearn Test Against Ground Truth
Centering time: 0.0029516220092773438
PCA fit: 0.25668764114379883
[[ 9.80392155e-01 -5.50506975e-09]
 [-5.50506934e-09  9.80392196e-01]]
PCA Transform: 0.005782604217529297
total iterations: 104
TLDA fit: 54.89094066619873
Whitened factor: 
[[ 0.08215661  0.06560023]
 [ 0.99661946 -0.997846  ]]
PCA Reverse Transform: 0.00020265579223632812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05895190857414732
Fit RMSE: 0.059413815322816636
 Test Against Ground Truth
[(' decentering', 0.0012857913970947266), (' smoothing and normalization', 0.0002903938293457031)]
Smoothing and Normalization: 0.0005383491516113281
Fit RMSE: 0.055979587509240133
sklearn Test Against Ground Truth
Centering time: 0.0029685497283935547
PCA fit: 0.2714691162109375
[[ 9.80392155e-01 -5.50506975e-09]
 [-5.50506934e-09  9.80392196e-01]]
PCA Transform: 0.005803823471069336
total iterations: 178
TLDA fit: 95.53562140464783
Whitened factor: 
[[ 0.04862399  0.03208252]
 [ 0.99881715 -0.99948525]]
PCA Reverse Transform: 0.00020647048950195312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058841860072422696
Fit RMSE: 0.059414971787621396
 Test Against Ground Truth
[(' decentering', 0.0013191699981689453), (' smoothing and normalization', 0.0002796649932861328)]
Smoothing and Normalization: 0.0005555152893066406
Fit RMSE: 0.055979587509240133
sklearn Test Against Ground Truth
Centering time: 0.002905130386352539
PCA fit: 0.2701680660247803
[[ 9.80392155e-01 -5.50506975e-09]
 [-5.50506934e-09  9.80392196e-01]]
PCA Transform: 0.0058019161224365234
total iterations: 991
TLDA fit: 533.5933003425598
Whitened factor: 
[[ 0.23661828  0.2222482 ]
 [ 0.9716027  -0.9749901 ]]
PCA Reverse Transform: 0.0002143383026123047
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05881247662191919
Fit RMSE: 0.05940360713315184
 Test Against Ground Truth
[(' decentering', 0.0012774467468261719), (' smoothing and normalization', 0.0002899169921875)]
Smoothing and Normalization: 0.0005204677581787109
Fit RMSE: 0.055979587509240133
sklearn Test Against Ground Truth
Centering time: 0.0030024051666259766
PCA fit: 0.26447010040283203
[[ 9.80392155e-01 -5.50506975e-09]
 [-5.50506934e-09  9.80392196e-01]]
PCA Transform: 0.00578761100769043
total iterations: 801
TLDA fit: 432.277019739151
Whitened factor: 
[[ 0.08141573  0.06486687]
 [ 0.99668026 -0.997894  ]]
PCA Reverse Transform: 0.00021028518676757812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058948161901154955
Fit RMSE: 0.05941382699854569
 Test Against Ground Truth
[(' decentering', 0.0012919902801513672), (' smoothing and normalization', 0.0002791881561279297)]
Smoothing and Normalization: 0.0005788803100585938
Fit RMSE: 0.055979587509240133
sklearn Test Against Ground Truth
Centering time: 0.0024673938751220703
PCA fit: 0.2533304691314697
[[ 9.80392155e-01 -5.50506975e-09]
 [-5.50506934e-09  9.80392196e-01]]
PCA Transform: 0.005785226821899414
total iterations: 1488
TLDA fit: 810.1221904754639
Whitened factor: 
[[ 0.04814785  0.03144382]
 [ 0.9988403  -0.9995055 ]]
PCA Reverse Transform: 0.00020742416381835938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05884131975095937
Fit RMSE: 0.05941530092056416
 Test Against Ground Truth
[(' decentering', 0.0012903213500976562), (' smoothing and normalization', 0.00029277801513671875)]
Smoothing and Normalization: 0.0005652904510498047
Fit RMSE: 0.055979587509240133
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014226436614990234
PCA fit: 0.04635810852050781
[[ 9.80392155e-01 -4.85614438e-10]
 [-4.85614715e-10  9.80392053e-01]]
PCA Transform: 0.0015857219696044922
total iterations: 11
TLDA fit: 5.632293224334717
Whitened factor: 
[[ 0.92169714 -0.7737622 ]
 [-0.38791016  0.63347614]]
PCA Reverse Transform: 0.00022840499877929688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12348966941511942
Fit RMSE: 0.1244408358939895
 Test Against Ground Truth
[(' decentering', 0.0012903213500976562), (' smoothing and normalization', 0.0002593994140625)]
Smoothing and Normalization: 0.00046253204345703125
Fit RMSE: 0.12330823564437891
sklearn Test Against Ground Truth
Centering time: 0.0013594627380371094
PCA fit: 0.04401254653930664
[[ 9.80392155e-01 -4.85614438e-10]
 [-4.85614715e-10  9.80392053e-01]]
PCA Transform: 0.0015578269958496094
total iterations: 11
TLDA fit: 5.634361743927002
Whitened factor: 
[[ 0.88896173 -0.8301387 ]
 [-0.4579815   0.55755705]]
PCA Reverse Transform: 0.00021386146545410156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1234947174919414
Fit RMSE: 0.1243838032887171
 Test Against Ground Truth
[(' decentering', 0.0012958049774169922), (' smoothing and normalization', 0.00024628639221191406)]
Smoothing and Normalization: 0.000484466552734375
Fit RMSE: 0.12330823564437891
sklearn Test Against Ground Truth
Centering time: 0.0013730525970458984
PCA fit: 0.04388594627380371
[[ 9.80392155e-01 -4.85614438e-10]
 [-4.85614715e-10  9.80392053e-01]]
PCA Transform: 0.0015325546264648438
total iterations: 11
TLDA fit: 5.545188903808594
Whitened factor: 
[[ 0.8858061  -0.84666425]
 [-0.46405557  0.53212744]]
PCA Reverse Transform: 0.00023031234741210938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.123494375050083
Fit RMSE: 0.12433816108174349
 Test Against Ground Truth
[(' decentering', 0.0012807846069335938), (' smoothing and normalization', 0.00024700164794921875)]
Smoothing and Normalization: 0.0004680156707763672
Fit RMSE: 0.12330823564437891
sklearn Test Against Ground Truth
Centering time: 0.001352548599243164
PCA fit: 0.04397988319396973
[[ 9.80392155e-01 -4.85614438e-10]
 [-4.85614715e-10  9.80392053e-01]]
PCA Transform: 0.0015611648559570312
total iterations: 11
TLDA fit: 5.568459987640381
Whitened factor: 
[[ 0.92579013 -0.7595628 ]
 [-0.3780379   0.65043396]]
PCA Reverse Transform: 0.00020766258239746094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12349064260232472
Fit RMSE: 0.1244689242679252
 Test Against Ground Truth
[(' decentering', 0.0012946128845214844), (' smoothing and normalization', 0.0002467632293701172)]
Smoothing and Normalization: 0.0004601478576660156
Fit RMSE: 0.12330823564437891
sklearn Test Against Ground Truth
Centering time: 0.0013620853424072266
PCA fit: 0.04405331611633301
[[ 9.80392155e-01 -4.85614438e-10]
 [-4.85614715e-10  9.80392053e-01]]
PCA Transform: 0.0015153884887695312
total iterations: 13
TLDA fit: 6.640547275543213
Whitened factor: 
[[ 0.8782027  -0.8262608 ]
 [-0.47828856  0.5632878 ]]
PCA Reverse Transform: 0.00020694732666015625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12348094311281181
Fit RMSE: 0.12443765482717889
 Test Against Ground Truth
[(' decentering', 0.0013384819030761719), (' smoothing and normalization', 0.0002818107604980469)]
Smoothing and Normalization: 0.0004668235778808594
Fit RMSE: 0.12330823564437891
sklearn Test Against Ground Truth
Centering time: 0.00135040283203125
PCA fit: 0.04395866394042969
[[ 9.80392155e-01 -4.85614438e-10]
 [-4.85614715e-10  9.80392053e-01]]
PCA Transform: 0.0015900135040283203
total iterations: 16
TLDA fit: 8.291121244430542
Whitened factor: 
[[ 0.8672358  -0.83994424]
 [-0.49789765  0.54267275]]
PCA Reverse Transform: 0.00021696090698242188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12347547564916948
Fit RMSE: 0.1244291517045483
 Test Against Ground Truth
[(' decentering', 0.001277923583984375), (' smoothing and normalization', 0.000244140625)]
Smoothing and Normalization: 0.0004832744598388672
Fit RMSE: 0.12330823564437891
sklearn Test Against Ground Truth
Centering time: 0.001359701156616211
PCA fit: 0.04381561279296875
[[ 9.80392155e-01 -4.85614438e-10]
 [-4.85614715e-10  9.80392053e-01]]
PCA Transform: 0.0015835762023925781
total iterations: 39
TLDA fit: 21.260178089141846
Whitened factor: 
[[ 0.9249672  -0.7581095 ]
 [-0.380047    0.65212727]]
PCA Reverse Transform: 0.00020837783813476562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12349054205375777
Fit RMSE: 0.1244773772810619
 Test Against Ground Truth
[(' decentering', 0.0012860298156738281), (' smoothing and normalization', 0.0002446174621582031)]
Smoothing and Normalization: 0.0004680156707763672
Fit RMSE: 0.12330823564437891
sklearn Test Against Ground Truth
Centering time: 0.0013546943664550781
PCA fit: 0.04398202896118164
[[ 9.80392155e-01 -4.85614438e-10]
 [-4.85614715e-10  9.80392053e-01]]
PCA Transform: 0.0015380382537841797
total iterations: 95
TLDA fit: 52.029175996780396
Whitened factor: 
[[ 0.8789532  -0.82239044]
 [-0.47690797  0.56892353]]
PCA Reverse Transform: 0.00021338462829589844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12348087239133991
Fit RMSE: 0.12444846278967583
 Test Against Ground Truth
[(' decentering', 0.0012760162353515625), (' smoothing and normalization', 0.0002453327178955078)]
Smoothing and Normalization: 0.0004622936248779297
Fit RMSE: 0.12330823564437891
sklearn Test Against Ground Truth
Centering time: 0.0013918876647949219
PCA fit: 0.04395699501037598
[[ 9.80392155e-01 -4.85614438e-10]
 [-4.85614715e-10  9.80392053e-01]]
PCA Transform: 0.0015423297882080078
total iterations: 123
TLDA fit: 66.95165538787842
Whitened factor: 
[[ 0.86705416 -0.83613825]
 [-0.49821395  0.5485188 ]]
PCA Reverse Transform: 0.00020933151245117188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12347517565387568
Fit RMSE: 0.12444339097535823
 Test Against Ground Truth
[(' decentering', 0.0012693405151367188), (' smoothing and normalization', 0.00024437904357910156)]
Smoothing and Normalization: 0.00041556358337402344
Fit RMSE: 0.12330823564437891
sklearn Test Against Ground Truth
Centering time: 0.0013530254364013672
PCA fit: 0.043828725814819336
[[ 9.80392155e-01 -4.85614438e-10]
 [-4.85614715e-10  9.80392053e-01]]
PCA Transform: 0.0015637874603271484
total iterations: 280
TLDA fit: 154.67289876937866
Whitened factor: 
[[ 0.9249819  -0.7577465 ]
 [-0.38001126  0.65254915]]
PCA Reverse Transform: 0.0002186298370361328
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12349054884641357
Fit RMSE: 0.12447849206178295
 Test Against Ground Truth
[(' decentering', 0.0013060569763183594), (' smoothing and normalization', 0.0002460479736328125)]
Smoothing and Normalization: 0.00046062469482421875
Fit RMSE: 0.12330823564437891
sklearn Test Against Ground Truth
Centering time: 0.0013620853424072266
PCA fit: 0.04397010803222656
[[ 9.80392155e-01 -4.85614438e-10]
 [-4.85614715e-10  9.80392053e-01]]
PCA Transform: 0.0015540122985839844
total iterations: 534
TLDA fit: 294.7920801639557
Whitened factor: 
[[ 0.8788965  -0.82218343]
 [-0.47701263  0.5692226 ]]
PCA Reverse Transform: 0.00021457672119140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12348079826398752
Fit RMSE: 0.12444940249226887
 Test Against Ground Truth
[(' decentering', 0.0012726783752441406), (' smoothing and normalization', 0.0002484321594238281)]
Smoothing and Normalization: 0.00048422813415527344
Fit RMSE: 0.12330823564437891
sklearn Test Against Ground Truth
Centering time: 0.0013461112976074219
PCA fit: 0.04383230209350586
[[ 9.80392155e-01 -4.85614438e-10]
 [-4.85614715e-10  9.80392053e-01]]
PCA Transform: 0.0015380382537841797
total iterations: 939
TLDA fit: 517.6401958465576
Whitened factor: 
[[ 0.86684984 -0.83589655]
 [-0.49856937  0.548887  ]]
PCA Reverse Transform: 0.0002295970916748047
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12347506597264452
Fit RMSE: 0.12444498844816863
 Test Against Ground Truth
[(' decentering', 0.0012958049774169922), (' smoothing and normalization', 0.0002448558807373047)]
Smoothing and Normalization: 0.0004749298095703125
Fit RMSE: 0.12330823564437891
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.0036118030548095703
PCA fit: 0.2758452892303467
[[9.80392149e-01 2.43882416e-08]
 [2.43882412e-08 9.80392068e-01]]
PCA Transform: 0.0058116912841796875
total iterations: 11
TLDA fit: 5.540710926055908
Whitened factor: 
[[-0.07529423  0.41153145]
 [ 0.9971613  -0.9113956 ]]
PCA Reverse Transform: 0.00020933151245117188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058731940765597006
Fit RMSE: 0.05955368643759955
 Test Against Ground Truth
[(' decentering', 0.001291036605834961), (' smoothing and normalization', 0.00029087066650390625)]
Smoothing and Normalization: 0.0005276203155517578
Fit RMSE: 0.05697887453109377
sklearn Test Against Ground Truth
Centering time: 0.0030100345611572266
PCA fit: 0.2813549041748047
[[9.80392149e-01 2.43882416e-08]
 [2.43882412e-08 9.80392068e-01]]
PCA Transform: 0.005800962448120117
total iterations: 11
TLDA fit: 5.481799602508545
Whitened factor: 
[[-0.22106683  0.30727386]
 [ 0.97525865 -0.9516211 ]]
PCA Reverse Transform: 0.0002009868621826172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058435348112941196
Fit RMSE: 0.059511142390082726
 Test Against Ground Truth
[(' decentering', 0.0012965202331542969), (' smoothing and normalization', 0.000308990478515625)]
Smoothing and Normalization: 0.0005307197570800781
Fit RMSE: 0.05697887453109377
sklearn Test Against Ground Truth
Centering time: 0.00299072265625
PCA fit: 0.262176513671875
[[9.80392149e-01 2.43882416e-08]
 [2.43882412e-08 9.80392068e-01]]
PCA Transform: 0.005798816680908203
total iterations: 11
TLDA fit: 5.448890686035156
Whitened factor: 
[[-0.25457466  0.294502  ]
 [ 0.9670532  -0.95565087]]
PCA Reverse Transform: 0.00020170211791992188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0584032485960157
Fit RMSE: 0.05948013725540024
 Test Against Ground Truth
[(' decentering', 0.0012927055358886719), (' smoothing and normalization', 0.0003414154052734375)]
Smoothing and Normalization: 0.0005359649658203125
Fit RMSE: 0.05697887453109377
sklearn Test Against Ground Truth
Centering time: 0.0030035972595214844
PCA fit: 0.24616003036499023
[[9.80392149e-01 2.43882416e-08]
 [2.43882412e-08 9.80392068e-01]]
PCA Transform: 0.006415367126464844
total iterations: 11
TLDA fit: 5.46866250038147
Whitened factor: 
[[-0.0307058   0.40448254]
 [ 0.99952847 -0.9145458 ]]
PCA Reverse Transform: 0.0002167224884033203
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058781089382062585
Fit RMSE: 0.05963992834521427
 Test Against Ground Truth
[(' decentering', 0.001279592514038086), (' smoothing and normalization', 0.0002903938293457031)]
Smoothing and Normalization: 0.0005300045013427734
Fit RMSE: 0.05697887453109377
sklearn Test Against Ground Truth
Centering time: 0.002970457077026367
PCA fit: 0.2581310272216797
[[9.80392149e-01 2.43882416e-08]
 [2.43882412e-08 9.80392068e-01]]
PCA Transform: 0.0058324337005615234
total iterations: 17
TLDA fit: 8.877675533294678
Whitened factor: 
[[-0.15973371  0.27936795]
 [ 0.98716015 -0.9601842 ]]
PCA Reverse Transform: 0.00020170211791992188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058634683989817846
Fit RMSE: 0.05965787080964117
 Test Against Ground Truth
[(' decentering', 0.001374959945678711), (' smoothing and normalization', 0.00028514862060546875)]
Smoothing and Normalization: 0.0005269050598144531
Fit RMSE: 0.05697887453109377
sklearn Test Against Ground Truth
Centering time: 0.003013134002685547
PCA fit: 0.28923749923706055
[[9.80392149e-01 2.43882416e-08]
 [2.43882412e-08 9.80392068e-01]]
PCA Transform: 0.005803823471069336
total iterations: 28
TLDA fit: 14.791511297225952
Whitened factor: 
[[-0.18986657  0.2513308 ]
 [ 0.9818099  -0.96790135]]
PCA Reverse Transform: 0.0002009868621826172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058634705002521006
Fit RMSE: 0.05965776434673422
 Test Against Ground Truth
[(' decentering', 0.0012919902801513672), (' smoothing and normalization', 0.00028705596923828125)]
Smoothing and Normalization: 0.0005421638488769531
Fit RMSE: 0.05697887453109377
sklearn Test Against Ground Truth
Centering time: 0.0029981136322021484
PCA fit: 0.24488425254821777
[[9.80392149e-01 2.43882416e-08]
 [2.43882412e-08 9.80392068e-01]]
PCA Transform: 0.005789756774902344
total iterations: 51
TLDA fit: 27.552663564682007
Whitened factor: 
[[-0.03592531  0.3914537 ]
 [ 0.9993545  -0.9201978 ]]
PCA Reverse Transform: 0.00020575523376464844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05878198105172646
Fit RMSE: 0.05965613007333319
 Test Against Ground Truth
[(' decentering', 0.0013124942779541016), (' smoothing and normalization', 0.00029778480529785156)]
Smoothing and Normalization: 0.0005276203155517578
Fit RMSE: 0.05697887453109377
sklearn Test Against Ground Truth
Centering time: 0.0030226707458496094
PCA fit: 0.2514500617980957
[[9.80392149e-01 2.43882416e-08]
 [2.43882412e-08 9.80392068e-01]]
PCA Transform: 0.005795717239379883
total iterations: 121
TLDA fit: 65.53211164474487
Whitened factor: 
[[-0.15845999  0.27515063]
 [ 0.9873654  -0.9614012 ]]
PCA Reverse Transform: 0.00021028518676757812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058643862665436586
Fit RMSE: 0.05966726743689364
 Test Against Ground Truth
[(' decentering', 0.0012934207916259766), (' smoothing and normalization', 0.0002884864807128906)]
Smoothing and Normalization: 0.0005288124084472656
Fit RMSE: 0.05697887453109377
sklearn Test Against Ground Truth
Centering time: 0.00292205810546875
PCA fit: 0.24587631225585938
[[9.80392149e-01 2.43882416e-08]
 [2.43882412e-08 9.80392068e-01]]
PCA Transform: 0.005791902542114258
total iterations: 222
TLDA fit: 118.41111087799072
Whitened factor: 
[[-0.18478009  0.24945341]
 [ 0.98277986 -0.9683868 ]]
PCA Reverse Transform: 0.00020575523376464844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05864892990049476
Fit RMSE: 0.059669173173269724
 Test Against Ground Truth
[(' decentering', 0.0012888908386230469), (' smoothing and normalization', 0.0003261566162109375)]
Smoothing and Normalization: 0.0005164146423339844
Fit RMSE: 0.05697887453109377
sklearn Test Against Ground Truth
Centering time: 0.0029137134552001953
PCA fit: 0.27027320861816406
[[9.80392149e-01 2.43882416e-08]
 [2.43882412e-08 9.80392068e-01]]
PCA Transform: 0.0058100223541259766
total iterations: 380
TLDA fit: 202.72372150421143
Whitened factor: 
[[-0.03607798  0.39070424]
 [ 0.999349   -0.9205163 ]]
PCA Reverse Transform: 0.0002148151397705078
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058782225924986095
Fit RMSE: 0.05965729807614696
 Test Against Ground Truth
[(' decentering', 0.0013060569763183594), (' smoothing and normalization', 0.00028824806213378906)]
Smoothing and Normalization: 0.0005624294281005859
Fit RMSE: 0.05697887453109377
sklearn Test Against Ground Truth
Centering time: 0.003000974655151367
PCA fit: 0.24881267547607422
[[9.80392149e-01 2.43882416e-08]
 [2.43882412e-08 9.80392068e-01]]
PCA Transform: 0.0058519840240478516
total iterations: 859
TLDA fit: 452.2092318534851
Whitened factor: 
[[-0.159145    0.27414995]
 [ 0.9872553  -0.9616871 ]]
PCA Reverse Transform: 0.0002129077911376953
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05864326253529876
Fit RMSE: 0.059667901785314614
 Test Against Ground Truth
[(' decentering', 0.0013005733489990234), (' smoothing and normalization', 0.0002903938293457031)]
Smoothing and Normalization: 0.0005238056182861328
Fit RMSE: 0.05697887453109377
sklearn Test Against Ground Truth
Centering time: 0.0029892921447753906
PCA fit: 0.26255273818969727
[[9.80392149e-01 2.43882416e-08]
 [2.43882412e-08 9.80392068e-01]]
PCA Transform: 0.005768775939941406
total iterations: 1592
TLDA fit: 842.4093346595764
Whitened factor: 
[[-0.18564315  0.24827269]
 [ 0.9826172  -0.9686902 ]]
PCA Reverse Transform: 0.00022220611572265625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05865360211875264
Fit RMSE: 0.05966980421895936
 Test Against Ground Truth
[(' decentering', 0.0013022422790527344), (' smoothing and normalization', 0.00032639503479003906)]
Smoothing and Normalization: 0.0005295276641845703
Fit RMSE: 0.05697887453109377
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.001644134521484375
PCA fit: 0.04586386680603027
[[9.80392156e-01 9.31082356e-09]
 [9.31082370e-09 9.80392070e-01]]
PCA Transform: 0.001562356948852539
total iterations: 11
TLDA fit: 5.599274158477783
Whitened factor: 
[[-0.8139059   0.9296598 ]
 [ 0.58099675 -0.36841926]]
PCA Reverse Transform: 0.00022792816162109375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1204189994645046
Fit RMSE: 0.12157129602373111
 Test Against Ground Truth
[(' decentering', 0.0012714862823486328), (' smoothing and normalization', 0.0002493858337402344)]
Smoothing and Normalization: 0.00045943260192871094
Fit RMSE: 0.12003615015860288
sklearn Test Against Ground Truth
Centering time: 0.0013458728790283203
PCA fit: 0.04316520690917969
[[9.80392156e-01 9.31082356e-09]
 [9.31082370e-09 9.80392070e-01]]
PCA Transform: 0.0015370845794677734
total iterations: 11
TLDA fit: 5.551363229751587
Whitened factor: 
[[-0.8722686   0.89870983]
 [ 0.48902702 -0.43854383]]
PCA Reverse Transform: 0.00021004676818847656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12035716879732482
Fit RMSE: 0.12151721681942854
 Test Against Ground Truth
[(' decentering', 0.001295328140258789), (' smoothing and normalization', 0.00025343894958496094)]
Smoothing and Normalization: 0.0004589557647705078
Fit RMSE: 0.12003615015860288
sklearn Test Against Ground Truth
Centering time: 0.0013530254364013672
PCA fit: 0.04319596290588379
[[9.80392156e-01 9.31082356e-09]
 [9.31082370e-09 9.80392070e-01]]
PCA Transform: 0.0015113353729248047
total iterations: 11
TLDA fit: 5.578406810760498
Whitened factor: 
[[-0.888042    0.89996   ]
 [ 0.45976245 -0.43597248]]
PCA Reverse Transform: 0.00022339820861816406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12034697116986207
Fit RMSE: 0.12144580799149672
 Test Against Ground Truth
[(' decentering', 0.0013189315795898438), (' smoothing and normalization', 0.00026106834411621094)]
Smoothing and Normalization: 0.00046133995056152344
Fit RMSE: 0.12003615015860288
sklearn Test Against Ground Truth
Centering time: 0.0013756752014160156
PCA fit: 0.043112754821777344
[[9.80392156e-01 9.31082356e-09]
 [9.31082370e-09 9.80392070e-01]]
PCA Transform: 0.0015311241149902344
total iterations: 11
TLDA fit: 5.5768046379089355
Whitened factor: 
[[-0.7916839   0.9428243 ]
 [ 0.61093086 -0.33329016]]
PCA Reverse Transform: 0.00021982192993164062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1204421450382428
Fit RMSE: 0.12156813509418243
 Test Against Ground Truth
[(' decentering', 0.0013308525085449219), (' smoothing and normalization', 0.0002567768096923828)]
Smoothing and Normalization: 0.0004940032958984375
Fit RMSE: 0.12003615015860288
sklearn Test Against Ground Truth
Centering time: 0.0013492107391357422
PCA fit: 0.043196916580200195
[[9.80392156e-01 9.31082356e-09]
 [9.31082370e-09 9.80392070e-01]]
PCA Transform: 0.001558065414428711
total iterations: 12
TLDA fit: 6.099827289581299
Whitened factor: 
[[-0.85236466  0.9009153 ]
 [ 0.5229479  -0.43399492]]
PCA Reverse Transform: 0.00023221969604492188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12037262925457981
Fit RMSE: 0.1215865316225964
 Test Against Ground Truth
[(' decentering', 0.0013086795806884766), (' smoothing and normalization', 0.00025272369384765625)]
Smoothing and Normalization: 0.0004987716674804688
Fit RMSE: 0.12003615015860288
sklearn Test Against Ground Truth
Centering time: 0.0013890266418457031
PCA fit: 0.043799638748168945
[[9.80392156e-01 9.31082356e-09]
 [9.31082370e-09 9.80392070e-01]]
PCA Transform: 0.0015647411346435547
total iterations: 18
TLDA fit: 9.35045599937439
Whitened factor: 
[[-0.86595553  0.8901521 ]
 [ 0.50012094 -0.45566356]]
PCA Reverse Transform: 0.00020694732666015625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12035495204403708
Fit RMSE: 0.12158725971169261
 Test Against Ground Truth
[(' decentering', 0.001268625259399414), (' smoothing and normalization', 0.0002522468566894531)]
Smoothing and Normalization: 0.00045752525329589844
Fit RMSE: 0.12003615015860288
sklearn Test Against Ground Truth
Centering time: 0.0013806819915771484
PCA fit: 0.04342985153198242
[[9.80392156e-01 9.31082356e-09]
 [9.31082370e-09 9.80392070e-01]]
PCA Transform: 0.0015354156494140625
total iterations: 39
TLDA fit: 21.143290042877197
Whitened factor: 
[[-0.78721184  0.9433536 ]
 [ 0.61668277 -0.33178902]]
PCA Reverse Transform: 0.0002219676971435547
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12044527387767917
Fit RMSE: 0.12158086326335982
 Test Against Ground Truth
[(' decentering', 0.001306772232055664), (' smoothing and normalization', 0.0002608299255371094)]
Smoothing and Normalization: 0.0004563331604003906
Fit RMSE: 0.12003615015860288
sklearn Test Against Ground Truth
Centering time: 0.0013461112976074219
PCA fit: 0.043145179748535156
[[9.80392156e-01 9.31082356e-09]
 [9.31082370e-09 9.80392070e-01]]
PCA Transform: 0.0015416145324707031
total iterations: 82
TLDA fit: 44.431936264038086
Whitened factor: 
[[-0.8489559   0.90117496]
 [ 0.5284638  -0.43345553]]
PCA Reverse Transform: 0.00021457672119140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12037514692636889
Fit RMSE: 0.12159888317099217
 Test Against Ground Truth
[(' decentering', 0.0012676715850830078), (' smoothing and normalization', 0.0002524852752685547)]
Smoothing and Normalization: 0.00045680999755859375
Fit RMSE: 0.12003615015860288
sklearn Test Against Ground Truth
Centering time: 0.0013616085052490234
PCA fit: 0.0432133674621582
[[9.80392156e-01 9.31082356e-09]
 [9.31082370e-09 9.80392070e-01]]
PCA Transform: 0.0015430450439453125
total iterations: 116
TLDA fit: 63.6291286945343
Whitened factor: 
[[-0.86185807  0.8901372 ]
 [ 0.5071495  -0.45569274]]
PCA Reverse Transform: 0.00021266937255859375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12035781570094681
Fit RMSE: 0.12160422665773113
 Test Against Ground Truth
[(' decentering', 0.0013127326965332031), (' smoothing and normalization', 0.0002567768096923828)]
Smoothing and Normalization: 0.0004551410675048828
Fit RMSE: 0.12003615015860288
sklearn Test Against Ground Truth
Centering time: 0.0013492107391357422
PCA fit: 0.043244123458862305
[[9.80392156e-01 9.31082356e-09]
 [9.31082370e-09 9.80392070e-01]]
PCA Transform: 0.0015170574188232422
total iterations: 259
TLDA fit: 142.31879258155823
Whitened factor: 
[[-0.78617376  0.94367975]
 [ 0.6180055  -0.33086026]]
PCA Reverse Transform: 0.0002086162567138672
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12044613677761082
Fit RMSE: 0.12158240021493234
 Test Against Ground Truth
[(' decentering', 0.0013179779052734375), (' smoothing and normalization', 0.00025200843811035156)]
Smoothing and Normalization: 0.00045418739318847656
Fit RMSE: 0.12003615015860288
sklearn Test Against Ground Truth
Centering time: 0.0013682842254638672
PCA fit: 0.043492794036865234
[[9.80392156e-01 9.31082356e-09]
 [9.31082370e-09 9.80392070e-01]]
PCA Transform: 0.0015993118286132812
total iterations: 508
TLDA fit: 279.074999332428
Whitened factor: 
[[-0.84821534  0.9015494 ]
 [ 0.52965164 -0.43267608]]
PCA Reverse Transform: 0.00021123886108398438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12037593207429577
Fit RMSE: 0.12159984982205432
 Test Against Ground Truth
[(' decentering', 0.001294851303100586), (' smoothing and normalization', 0.00025081634521484375)]
Smoothing and Normalization: 0.000453948974609375
Fit RMSE: 0.12003615015860288
sklearn Test Against Ground Truth
Centering time: 0.001355886459350586
PCA fit: 0.04317331314086914
[[9.80392156e-01 9.31082356e-09]
 [9.31082370e-09 9.80392070e-01]]
PCA Transform: 0.00151824951171875
total iterations: 623
TLDA fit: 338.7680585384369
Whitened factor: 
[[-0.86126006  0.89031726]
 [ 0.5081645  -0.4553408 ]]
PCA Reverse Transform: 0.0002105236053466797
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12035837350157763
Fit RMSE: 0.12160576566757056
 Test Against Ground Truth
[(' decentering', 0.0012688636779785156), (' smoothing and normalization', 0.00025391578674316406)]
Smoothing and Normalization: 0.0004885196685791016
Fit RMSE: 0.12003615015860288
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
Centering time: 0.003529787063598633
PCA fit: 0.28647899627685547
[[9.80392162e-01 2.54778936e-09]
 [2.54778897e-09 9.80392152e-01]]
PCA Transform: 0.0057981014251708984
total iterations: 11
TLDA fit: 5.3972063064575195
Whitened factor: 
[[-0.9818496   0.9987094 ]
 [ 0.18966137  0.05078973]]
PCA Reverse Transform: 0.00020933151245117188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057187235188965936
Fit RMSE: 0.05724198126177498
 Test Against Ground Truth
[(' decentering', 0.001291036605834961), (' smoothing and normalization', 0.0002884864807128906)]
Smoothing and Normalization: 0.0005242824554443359
Fit RMSE: 0.05709442625978878
sklearn Test Against Ground Truth
Centering time: 0.002911090850830078
PCA fit: 0.28516077995300293
[[9.80392162e-01 2.54778936e-09]
 [2.54778897e-09 9.80392152e-01]]
PCA Transform: 0.005822420120239258
total iterations: 11
TLDA fit: 5.302320957183838
Whitened factor: 
[[-0.99189305  0.99983925]
 [ 0.12707575 -0.01793137]]
PCA Reverse Transform: 0.00020051002502441406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05718828432694705
Fit RMSE: 0.05724548492283147
 Test Against Ground Truth
[(' decentering', 0.0012860298156738281), (' smoothing and normalization', 0.0003046989440917969)]
Smoothing and Normalization: 0.0005359649658203125
Fit RMSE: 0.05709442625978878
sklearn Test Against Ground Truth
Centering time: 0.002984285354614258
PCA fit: 0.2768230438232422
[[9.80392162e-01 2.54778936e-09]
 [2.54778897e-09 9.80392152e-01]]
PCA Transform: 0.005797386169433594
total iterations: 11
TLDA fit: 5.375610828399658
Whitened factor: 
[[-0.9942943   0.9995219 ]
 [ 0.10667168 -0.03091848]]
PCA Reverse Transform: 0.00020313262939453125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057188452959687525
Fit RMSE: 0.057248279540319455
 Test Against Ground Truth
[(' decentering', 0.0012853145599365234), (' smoothing and normalization', 0.0003483295440673828)]
Smoothing and Normalization: 0.0005276203155517578
Fit RMSE: 0.05709442625978878
sklearn Test Against Ground Truth
Centering time: 0.0029709339141845703
PCA fit: 0.2719380855560303
[[9.80392162e-01 2.54778936e-09]
 [2.54778897e-09 9.80392152e-01]]
PCA Transform: 0.00579380989074707
total iterations: 21
TLDA fit: 10.68941068649292
Whitened factor: 
[[-0.98294735  0.9994995 ]
 [ 0.18388696  0.03163517]]
PCA Reverse Transform: 0.00020623207092285156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057187638958513955
Fit RMSE: 0.05724248356559227
 Test Against Ground Truth
[(' decentering', 0.0012934207916259766), (' smoothing and normalization', 0.0002923011779785156)]
Smoothing and Normalization: 0.0005352497100830078
Fit RMSE: 0.05709442625978878
sklearn Test Against Ground Truth
Centering time: 0.0029075145721435547
PCA fit: 0.2442779541015625
[[9.80392162e-01 2.54778936e-09]
 [2.54778897e-09 9.80392152e-01]]
PCA Transform: 0.005776643753051758
total iterations: 48
TLDA fit: 24.870237588882446
Whitened factor: 
[[-0.9936457   0.99920744]
 [ 0.11255246 -0.03980574]]
PCA Reverse Transform: 0.00020384788513183594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057188510715112446
Fit RMSE: 0.05724912750209353
 Test Against Ground Truth
[(' decentering', 0.0013699531555175781), (' smoothing and normalization', 0.0002956390380859375)]
Smoothing and Normalization: 0.00052642822265625
Fit RMSE: 0.05709442625978878
sklearn Test Against Ground Truth
Centering time: 0.0029075145721435547
PCA fit: 0.24598145484924316
[[9.80392162e-01 2.54778936e-09]
 [2.54778897e-09 9.80392152e-01]]
PCA Transform: 0.00581049919128418
total iterations: 87
TLDA fit: 45.649940490722656
Whitened factor: 
[[-0.99534863  0.9984633 ]
 [ 0.09633935 -0.05541807]]
PCA Reverse Transform: 0.00021004676818847656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057188655381350754
Fit RMSE: 0.05725234094071236
 Test Against Ground Truth
[(' decentering', 0.001271963119506836), (' smoothing and normalization', 0.00031375885009765625)]
Smoothing and Normalization: 0.0005280971527099609
Fit RMSE: 0.05709442625978878
sklearn Test Against Ground Truth
Centering time: 0.0028564929962158203
PCA fit: 0.2612576484680176
[[9.80392162e-01 2.54778936e-09]
 [2.54778897e-09 9.80392152e-01]]
PCA Transform: 0.005800724029541016
total iterations: 138
TLDA fit: 72.73502612113953
Whitened factor: 
[[-0.983801    0.9996431 ]
 [ 0.17926365  0.0267161 ]]
PCA Reverse Transform: 0.00020956993103027344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057187713379123435
Fit RMSE: 0.05724205670270169
 Test Against Ground Truth
[(' decentering', 0.0012979507446289062), (' smoothing and normalization', 0.0002925395965576172)]
Smoothing and Normalization: 0.0005619525909423828
Fit RMSE: 0.05709442625978878
sklearn Test Against Ground Truth
Centering time: 0.0030481815338134766
PCA fit: 0.2550961971282959
[[9.80392162e-01 2.54778936e-09]
 [2.54778897e-09 9.80392152e-01]]
PCA Transform: 0.005809783935546875
total iterations: 402
TLDA fit: 213.96076798439026
Whitened factor: 
[[-0.9938059   0.999119  ]
 [ 0.11113033 -0.04196851]]
PCA Reverse Transform: 0.00021314620971679688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05718852953653083
Fit RMSE: 0.05724950855547004
 Test Against Ground Truth
[(' decentering', 0.0014176368713378906), (' smoothing and normalization', 0.0002970695495605469)]
Smoothing and Normalization: 0.0005233287811279297
Fit RMSE: 0.05709442625978878
sklearn Test Against Ground Truth
Centering time: 0.003144502639770508
PCA fit: 0.26359033584594727
[[9.80392162e-01 2.54778936e-09]
 [2.54778897e-09 9.80392152e-01]]
PCA Transform: 0.0058040618896484375
total iterations: 669
TLDA fit: 357.01794624328613
Whitened factor: 
[[-0.995416    0.99835247]
 [ 0.09564093 -0.05737954]]
PCA Reverse Transform: 0.00020575523376464844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057188666731694246
Fit RMSE: 0.057252672479347864
 Test Against Ground Truth
[(' decentering', 0.0012903213500976562), (' smoothing and normalization', 0.0002913475036621094)]
Smoothing and Normalization: 0.0005333423614501953
Fit RMSE: 0.05709442625978878
sklearn Test Against Ground Truth
Centering time: 0.0029571056365966797
PCA fit: 0.2829713821411133
[[9.80392162e-01 2.54778936e-09]
 [2.54778897e-09 9.80392152e-01]]
PCA Transform: 0.0058498382568359375
total iterations: 1111
TLDA fit: 593.9495797157288
Whitened factor: 
[[-0.9839251   0.9996623 ]
 [ 0.17858197  0.02598697]]
PCA Reverse Transform: 0.00021076202392578125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057187724213780315
Fit RMSE: 0.05724199716859772
 Test Against Ground Truth
[(' decentering', 0.0012848377227783203), (' smoothing and normalization', 0.00032639503479003906)]
Smoothing and Normalization: 0.0005552768707275391
Fit RMSE: 0.05709442625978878
sklearn Test Against Ground Truth
Centering time: 0.002846956253051758
PCA fit: 0.24963164329528809
[[9.80392162e-01 2.54778936e-09]
 [2.54778897e-09 9.80392152e-01]]
PCA Transform: 0.005794048309326172
total iterations: 3179
TLDA fit: 1719.8226096630096
Whitened factor: 
[[-0.9938779   0.9990787 ]
 [ 0.11048504 -0.04291651]]
PCA Reverse Transform: 0.00021028518676757812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057188537638814124
Fit RMSE: 0.05724967811027908
 Test Against Ground Truth
[(' decentering', 0.0012938976287841797), (' smoothing and normalization', 0.00028896331787109375)]
Smoothing and Normalization: 0.0005373954772949219
Fit RMSE: 0.05709442625978878
sklearn Test Against Ground Truth
Centering time: 0.003062009811401367
PCA fit: 0.2686281204223633
[[9.80392162e-01 2.54778936e-09]
 [2.54778897e-09 9.80392152e-01]]
PCA Transform: 0.005793571472167969
total iterations: 5000
TLDA fit: 2669.7564659118652
Whitened factor: 
[[-0.9954608   0.99830025]
 [ 0.09517337 -0.05828092]]
PCA Reverse Transform: 0.00022125244140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057188672366135146
Fit RMSE: 0.05725283446285355
 Test Against Ground Truth
[(' decentering', 0.0012812614440917969), (' smoothing and normalization', 0.00030350685119628906)]
Smoothing and Normalization: 0.0005211830139160156
Fit RMSE: 0.05709442625978878
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0014483928680419922
PCA fit: 0.045867919921875
[[ 9.80392159e-01 -1.73001898e-08]
 [-1.73001911e-08  9.80392131e-01]]
PCA Transform: 0.0015375614166259766
total iterations: 11
TLDA fit: 5.5628838539123535
Whitened factor: 
[[ 0.9982341  -0.98002076]
 [-0.05940415  0.19889544]]
PCA Reverse Transform: 0.00022077560424804688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12091171615008772
Fit RMSE: 0.12102960549569912
 Test Against Ground Truth
[(' decentering', 0.0012819766998291016), (' smoothing and normalization', 0.0002465248107910156)]
Smoothing and Normalization: 0.0004634857177734375
Fit RMSE: 0.12053107326299113
sklearn Test Against Ground Truth
Centering time: 0.001356363296508789
PCA fit: 0.04405784606933594
[[ 9.80392159e-01 -1.73001898e-08]
 [-1.73001911e-08  9.80392131e-01]]
PCA Transform: 0.0015408992767333984
total iterations: 11
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0786890983581543
PCA fit: 0.5833637714385986
[[ 9.80392162e-01 -6.40081854e-09]
 [-6.40081801e-09  9.80392131e-01]]
PCA Transform: 0.00223541259765625
Traceback (most recent call last):
  File "generate_tables.py", line 577, in <module>
    main()
  File "generate_tables.py", line 514, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 427, in gen_fit_0_20
    mu_cent = mu.T - tl.mean(x, axis=0)
AttributeError: 'list' object has no attribute 'T'
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.07996320724487305
PCA fit: 0.5495798587799072
[[ 9.80392143e-01 -3.59768235e-08]
 [-3.59768231e-08  9.80392122e-01]]
PCA Transform: 0.0022449493408203125
Traceback (most recent call last):
  File "generate_tables.py", line 577, in <module>
    main()
  File "generate_tables.py", line 514, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 427, in gen_fit_0_20
    mu_cent = np.asarray(mu)[:, 0, :].T - tl.mean(x, axis=0)
  File "cupy/_core/core.pyx", line 1466, in cupy._core.core.ndarray.__array_ufunc__
  File "cupy/_core/_kernel.pyx", line 1061, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 109, in cupy._core._kernel._preprocess_args
TypeError: Unsupported type <class 'numpy.ndarray'>
new version
Vocab: 100
num_tweets: 20000
density: 15
Centering time: 0.0800774097442627
PCA fit: 0.5333054065704346
[[9.80392162e-01 2.94367050e-08]
 [2.94367052e-08 9.80392239e-01]]
PCA Transform: 0.0022363662719726562
Traceback (most recent call last):
  File "generate_tables.py", line 577, in <module>
    main()
  File "generate_tables.py", line 514, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 427, in gen_fit_0_20
    mu_cent = tl.tensor(cp.asarray(mu)[:, 0, :].T) - tl.mean(x, axis=0)
  File "cupy/_core/core.pyx", line 1078, in cupy._core.core.ndarray.__sub__
  File "cupy/_core/_kernel.pyx", line 1079, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/internal.pyx", line 359, in cupy._core.internal._broadcast_core
ValueError: operands could not be broadcast together with shapes (100, 2), (100,)
new version
Vocab: 100
num_tweets: 20000
density: 15
Traceback (most recent call last):
  File "generate_tables.py", line 579, in <module>
    main()
  File "generate_tables.py", line 510, in main
    create_data(vocab=vocab, seed=seed_arr[j])
  File "generate_tables.py", line 68, in create_data
    x, mu, _, alpha_0 = test_util.get_mu(num_tops, vocab, num_tweets, density, seed)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/test_util.py", line 42, in get_mu
    mu.append(topic/np.norm(topic))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/numpy/__init__.py", line 313, in __getattr__
    raise AttributeError("module {!r} has no attribute "
AttributeError: module 'numpy' has no attribute 'norm'
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.08260679244995117
PCA fit: 0.5500144958496094
[[9.80392160e-01 9.47231249e-09]
 [9.47231266e-09 9.80392138e-01]]
PCA Transform: 0.0022695064544677734
total iterations: 11
TLDA fit: 5.368100643157959
Whitened factor: 
[[-0.94228965  0.9982829 ]
 [ 0.33479872 -0.05857653]]
PCA Reverse Transform: 0.0010259151458740234
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11912107374002388
Fit RMSE: 0.11949209347465571
 Test Against Ground Truth
[(' decentering', 0.0038759708404541016), (' smoothing and normalization', 0.00025153160095214844)]
Smoothing and Normalization: 0.00043320655822753906
Fit RMSE: 0.11882230655085847
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
[1.0, 1.0000000000000002]
Centering time: 0.003326892852783203
PCA fit: 0.2869374752044678
[[9.80392154e-01 7.24807503e-09]
 [7.24807490e-09 9.80392143e-01]]
PCA Transform: 0.005750417709350586
total iterations: 11
TLDA fit: 5.236816883087158
Whitened factor: 
[[ 0.9783476  -0.79327995]
 [-0.20696859  0.6088571 ]]
PCA Reverse Transform: 0.00023746490478515625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.054770200631243146
Fit RMSE: 0.05517509157459965
 Test Against Ground Truth
[(' decentering', 0.0012733936309814453), (' smoothing and normalization', 0.0002841949462890625)]
Smoothing and Normalization: 0.0005304813385009766
Fit RMSE: 0.0546822120010573
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0013537406921386719
PCA fit: 0.04539966583251953
[[9.80392171e-01 1.50790875e-08]
 [1.50790874e-08 9.80392161e-01]]
PCA Transform: 0.0015833377838134766
total iterations: 11
TLDA fit: 5.379284143447876
Whitened factor: 
[[-0.7080775   0.97288275]
 [ 0.70613474 -0.23129879]]
PCA Reverse Transform: 0.00023293495178222656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12033452973554265
Fit RMSE: 0.12127178806708874
 Test Against Ground Truth
[(' decentering', 0.0012502670288085938), (' smoothing and normalization', 0.00024509429931640625)]
Smoothing and Normalization: 0.0004467964172363281
Fit RMSE: 0.11664500040230047
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0038034915924072266
PCA fit: 0.29735255241394043
[[ 9.80392157e-01 -4.03075072e-09]
 [-4.03075121e-09  9.80392162e-01]]
PCA Transform: 0.005766153335571289
total iterations: 28
TLDA fit: 14.217742919921875
Whitened factor: 
[[0.61025065 0.6103042 ]
 [0.7922084  0.7921672 ]]
PCA Reverse Transform: 0.00025081634521484375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.059618281335390294
Fit RMSE: 0.05961816648714705
 Test Against Ground Truth
[(' decentering', 0.001276254653930664), (' smoothing and normalization', 0.0002751350402832031)]
Smoothing and Normalization: 0.0005085468292236328
Fit RMSE: 0.05623422423777611
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014600753784179688
PCA fit: 0.04617667198181152
[[ 9.80392155e-01 -9.62546198e-09]
 [-9.62546202e-09  9.80392207e-01]]
PCA Transform: 0.0015401840209960938
total iterations: 11
TLDA fit: 5.559112787246704
Whitened factor: 
[[-0.75042933  0.9820423 ]
 [ 0.6609507  -0.18866082]]
PCA Reverse Transform: 0.00023436546325683594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12133242837762477
Fit RMSE: 0.1222221720084855
 Test Against Ground Truth
[(' decentering', 0.0012462139129638672), (' smoothing and normalization', 0.00025582313537597656)]
Smoothing and Normalization: 0.0005123615264892578
Fit RMSE: 0.12077145505373993
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0019140243530273438
PCA fit: 0.30328798294067383
[[9.80392156e-01 1.76176651e-09]
 [1.76176601e-09 9.80392164e-01]]
PCA Transform: 0.005753993988037109
total iterations: 29
TLDA fit: 15.014926195144653
Whitened factor: 
[[0.6120063  0.61195254]
 [0.79085284 0.79089445]]
PCA Reverse Transform: 0.0002453327178955078
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05947303200098412
Fit RMSE: 0.05947291532179575
 Test Against Ground Truth
[(' decentering', 0.0014085769653320312), (' smoothing and normalization', 0.00029587745666503906)]
Smoothing and Normalization: 0.0005543231964111328
Fit RMSE: 0.055998333790963105
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.0013623237609863281
PCA fit: 0.0456082820892334
[[9.80392157e-01 1.76034605e-08]
 [1.76034608e-08 9.80392181e-01]]
PCA Transform: 0.0015380382537841797
total iterations: 11
TLDA fit: 5.543231248855591
Whitened factor: 
[[-0.7496855   0.9843442 ]
 [ 0.6617943  -0.17625695]]
PCA Reverse Transform: 0.00023055076599121094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12123325267468651
Fit RMSE: 0.1221544651417669
 Test Against Ground Truth
[(' decentering', 0.0012428760528564453), (' smoothing and normalization', 0.0002448558807373047)]
Smoothing and Normalization: 0.0005218982696533203
Fit RMSE: 0.12078421744169236
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999999, 0.9999999999999999]
Centering time: 0.003862142562866211
PCA fit: 0.22366023063659668
[[ 9.80392157e-01 -7.08628715e-09]
 [-7.08628705e-09  9.80392128e-01]]
PCA Transform: 0.005761861801147461
total iterations: 27
TLDA fit: 14.093700408935547
Whitened factor: 
[[0.23102996 0.9408955 ]
 [0.97294664 0.338697  ]]
PCA Reverse Transform: 0.00022530555725097656
decenter with new strategy:
[ 0.00115983 -0.0476618 ]
decenter with old strategy:
[0.04215912 0.00626722]
Fit RMSE new decenter: 0.05843955102676306
Fit RMSE: 0.05833086817077
 Test Against Ground Truth
[(' decentering', 0.0012705326080322266), (' smoothing and normalization', 0.00028634071350097656)]
Smoothing and Normalization: 0.0005192756652832031
Fit RMSE: 0.05700372171662684
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001764059066772461
PCA fit: 0.04776167869567871
[[ 9.80392151e-01 -2.23659102e-08]
 [-2.23659103e-08  9.80391993e-01]]
PCA Transform: 0.0015304088592529297
total iterations: 11
TLDA fit: 5.514100074768066
Whitened factor: 
[[ 0.9836084  -0.77597743]
 [-0.18031752  0.6307606 ]]
PCA Reverse Transform: 0.00023651123046875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12264632272512008
Fit RMSE: 0.12359307735065693
 Test Against Ground Truth
[(' decentering', 0.0012409687042236328), (' smoothing and normalization', 0.000244140625)]
Smoothing and Normalization: 0.0004875659942626953
Fit RMSE: 0.12231561205597369
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003779172897338867
PCA fit: 0.24661660194396973
[[9.80392161e-01 3.33967927e-09]
 [3.33967938e-09 9.80392181e-01]]
PCA Transform: 0.005768537521362305
total iterations: 26
TLDA fit: 13.379481554031372
Whitened factor: 
[[0.65048164 0.65050954]
 [0.759522   0.7594981 ]]
PCA Reverse Transform: 0.0002560615539550781
decenter with new strategy:
[1.74892793e-05 1.74924530e-05]
decenter with old strategy:
[9.59442293e-05 9.59488081e-05]
Fit RMSE new decenter: 0.05921381102940711
Fit RMSE: 0.05921373696261412
 Test Against Ground Truth
[(' decentering', 0.0012738704681396484), (' smoothing and normalization', 0.0002903938293457031)]
Smoothing and Normalization: 0.0005166530609130859
Fit RMSE: 0.055535961384060036
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.001653432846069336
PCA fit: 0.046912431716918945
[[ 9.80392153e-01 -1.09410985e-08]
 [-1.09410984e-08  9.80392219e-01]]
PCA Transform: 0.0015752315521240234
total iterations: 98new version
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.07944798469543457
PCA fit: 0.5363357067108154
[[8.33333337e-01 1.04583363e-08]
 [1.04583364e-08 8.33333397e-01]]
PCA Transform: 0.0022780895233154297
total iterations: 11
TLDA fit: 5.407322645187378
Whitened factor: 
[[ 0.9409334  -0.7342836 ]
 [-0.33859175  0.6788429 ]]
PCA Reverse Transform: 0.001009225845336914
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12364780892750038
Fit RMSE: 0.1249916408160763
 Test Against Ground Truth
[(' decentering', 0.0038597583770751953), (' smoothing and normalization', 0.00026726722717285156)]
Smoothing and Normalization: 0.00044274330139160156
Fit RMSE: 0.12365091689427452
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0033845901489257812
PCA fit: 0.22145581245422363
[[8.33333334e-01 5.17745646e-09]
 [5.17745653e-09 8.33333402e-01]]
PCA Transform: 0.005749225616455078
total iterations: 11
TLDA fit: 5.40283989906311
Whitened factor: 
[[-0.65842336  0.9196212 ]
 [ 0.7526478  -0.39280638]]
PCA Reverse Transform: 0.0002422332763671875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05983070848707161
Fit RMSE: 0.06013698522731651
 Test Against Ground Truth
[(' decentering', 0.0012569427490234375), (' smoothing and normalization', 0.000286102294921875)]
Smoothing and Normalization: 0.0005805492401123047
Fit RMSE: 0.05491631516836004
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0013842582702636719
PCA fit: 0.04630255699157715
[[ 8.33333339e-01 -2.55859192e-08]
 [-2.55859190e-08  8.33332894e-01]]
PCA Transform: 0.0015895366668701172
total iterations: 11
TLDA fit: 5.51679253578186
Whitened factor: 
[[-0.58974665  0.9366112 ]
 [ 0.80758834 -0.35037062]]
PCA Reverse Transform: 0.0002486705780029297
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12625303815641992
Fit RMSE: 0.12784751353543297
 Test Against Ground Truth
[(' decentering', 0.0012726783752441406), (' smoothing and normalization', 0.00028824806213378906)]
Smoothing and Normalization: 0.00048732757568359375
Fit RMSE: 0.12630915068721787
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001954793930053711
PCA fit: 0.26280713081359863
[[8.33333335e-01 1.36672972e-09]
 [1.36673011e-09 8.33333134e-01]]
PCA Transform: 0.005763530731201172
total iterations: 11
TLDA fit: 5.419041395187378
Whitened factor: 
[[ 0.900428   -0.6665189 ]
 [-0.43500513  0.74548805]]
PCA Reverse Transform: 0.00027632713317871094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05999128607009844
Fit RMSE: 0.06029746702806982
 Test Against Ground Truth
[(' decentering', 0.0012440681457519531), (' smoothing and normalization', 0.0002872943878173828)]
Smoothing and Normalization: 0.0004973411560058594
Fit RMSE: 0.05998206500649802
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0016109943389892578
PCA fit: 0.046173095703125
[[8.33333334e-01 1.73739452e-08]
 [1.73739453e-08 8.33333436e-01]]
PCA Transform: 0.0015540122985839844
total iterations: 11
TLDA fit: 5.636816501617432
Whitened factor: 
[[-0.12794092 -0.6780559 ]
 [ 0.99178183  0.7350103 ]]
PCA Reverse Transform: 0.0002739429473876953
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12804764816179923
Fit RMSE: 0.13041626816493707
 Test Against Ground Truth
[(' decentering', 0.0012598037719726562), (' smoothing and normalization', 0.00024819374084472656)]
Smoothing and Normalization: 0.0004572868347167969
Fit RMSE: 0.12523033438881315
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0034186840057373047
PCA fit: 0.25699925422668457
[[8.33333336e-01 4.58951943e-09]
 [4.58951921e-09 8.33333339e-01]]
PCA Transform: 0.005753993988037109
total iterations: 11
TLDA fit: 5.678828001022339
Whitened factor: 
[[-0.8759882   0.98061854]
 [ 0.48233247 -0.19592646]]
PCA Reverse Transform: 0.00035071372985839844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05968419238140499
Fit RMSE: 0.059855752921587006
 Test Against Ground Truth
[(' decentering', 0.0012750625610351562), (' smoothing and normalization', 0.00028777122497558594)]
Smoothing and Normalization: 0.000545501708984375
Fit RMSE: 0.05967344991567602
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001600503921508789
PCA fit: 0.04606938362121582
[[8.33333336e-01 6.38885400e-09]
 [6.38885404e-09 8.33333386e-01]]
PCA Transform: 0.0015418529510498047
total iterations: 11
TLDA fit: 5.595434188842773
Whitened factor: 
[[-0.86646324  0.97445875]
 [ 0.49924102 -0.22456644]]
PCA Reverse Transform: 0.00023245811462402344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11983310756295598
Fit RMSE: 0.1209407498938658
 Test Against Ground Truth
[(' decentering', 0.001714468002319336), (' smoothing and normalization', 0.00025463104248046875)]
Smoothing and Normalization: 0.0004601478576660156
Fit RMSE: 0.11980300970122817
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003926992416381836
PCA fit: 0.25802135467529297
[[ 8.33333332e-01 -1.93322928e-08]
 [-1.93322930e-08  8.33333398e-01]]
PCA Transform: 0.005760908126831055
total iterations: 11
TLDA fit: 5.587192058563232
Whitened factor: 
[[ 0.92276025 -0.68497324]
 [-0.3853745   0.72856826]]
PCA Reverse Transform: 0.0002415180206298828
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05943183216835272
Fit RMSE: 0.05977625891028955
 Test Against Ground Truth
[(' decentering', 0.0012712478637695312), (' smoothing and normalization', 0.0002830028533935547)]
Smoothing and Normalization: 0.0005314350128173828
Fit RMSE: 0.0594270792760661
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001562356948852539
PCA fit: 0.044011831283569336
[[ 8.33333339e-01 -5.92734082e-07]
 [-5.92734064e-07  8.33301672e-01]]
PCA Transform: 0.0015361309051513672
total iterations: 11
TLDA fit: 5.664456844329834
Whitened factor: 
[[-0.02513759 -0.02513759]
 [ 0.999684    0.999684  ]]
PCA Reverse Transform: 0.00024271011352539062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1341637022641058
Fit RMSE: 0.1341637022641058
 Test Against Ground Truth
[(' decentering', 0.001291513442993164), (' smoothing and normalization', 0.00028204917907714844)]
Smoothing and Normalization: 0.00046181678771972656
Fit RMSE: 0.12669996862669047
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003935098648071289
PCA fit: 0.27755093574523926
[[8.33333331e-01 7.32477856e-10]
 [7.32477662e-10 8.33333409e-01]]
PCA Transform: 0.005799055099487305
total iterations: 11
TLDA fit: 5.55758810043335
Whitened factor: 
[[-0.6558235   0.9090848 ]
 [ 0.75491434 -0.41661108]]
PCA Reverse Transform: 0.00024056434631347656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.06003042469667328
Fit RMSE: 0.06033837855302016
 Test Against Ground Truth
[(' decentering', 0.0012927055358886719), (' smoothing and normalization', 0.00029659271240234375)]
Smoothing and Normalization: 0.0005724430084228516
Fit RMSE: 0.06001830941121533
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0018815994262695312
PCA fit: 0.046690940856933594
[[ 8.33333344e-01 -1.97202561e-08]
 [-1.97202563e-08  8.33333522e-01]]
PCA Transform: 0.0015347003936767578
total iterations: 11
TLDA fit: 5.675673007965088
Whitened factor: 
[[-0.6980918  -0.18526733]
 [ 0.7160083   0.9826882 ]]
PCA Reverse Transform: 0.0002391338348388672
new version
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.08343839645385742
PCA fit: 0.5400540828704834
[[9.80392152e-01 2.61158171e-08]
 [2.61158175e-08 9.80392469e-01]]
PCA Transform: 0.002244234085083008
total iterations: 11
TLDA fit: 5.428329706192017
Whitened factor: 
[[-0.697421    0.8901471 ]
 [-0.71666163  0.45567337]]
PCA Reverse Transform: 0.001336812973022461
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1259005523105465
Fit RMSE: 0.12688795314290977
 Test Against Ground Truth
[(' decentering', 0.0032923221588134766), (' smoothing and normalization', 0.0002510547637939453)]
Smoothing and Normalization: 0.0004296302795410156
Fit RMSE: 0.1256848596067639
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003854036331176758
PCA fit: 0.25716710090637207
[[ 9.80392160e-01 -6.66310396e-09]
 [-6.66310397e-09  9.80392143e-01]]
PCA Transform: 0.005880117416381836
total iterations: 17
TLDA fit: 8.588785409927368
Whitened factor: 
[[ 0.9864367  -0.99955916]
 [ 0.16414183  0.0296872 ]]
PCA Reverse Transform: 0.00023603439331054688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05576010566378664
Fit RMSE: 0.05590437656396034
 Test Against Ground Truth
[(' decentering', 0.001298666000366211), (' smoothing and normalization', 0.0002818107604980469)]
Smoothing and Normalization: 0.000499725341796875
Fit RMSE: 0.055715442583613085
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0017156600952148438
PCA fit: 0.04668879508972168
[[9.80392165e-01 7.54965646e-10]
 [7.54966409e-10 9.80392112e-01]]
PCA Transform: 0.0015587806701660156
total iterations: 11
TLDA fit: 5.577446699142456
Whitened factor: 
[[ 0.9550257 -0.7736309]
 [-0.2965232  0.6336365]]
PCA Reverse Transform: 0.00024175643920898438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11896980913699244
Fit RMSE: 0.12009943114227245
 Test Against Ground Truth
[(' decentering', 0.001264810562133789), (' smoothing and normalization', 0.00025463104248046875)]
Smoothing and Normalization: 0.0004534721374511719
Fit RMSE: 0.11845994759863639
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0039217472076416016
PCA fit: 0.25545692443847656
[[9.80392156e-01 1.32900757e-09]
 [1.32900751e-09 9.80392153e-01]]
PCA Transform: 0.005865573883056641
total iterations: 11
TLDA fit: 5.3797948360443115
Whitened factor: 
[[-0.79434776  0.9478012 ]
 [ 0.6074631  -0.31886196]]
PCA Reverse Transform: 0.0002353191375732422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05710900569714399
Fit RMSE: 0.05743699962268747
 Test Against Ground Truth
[(' decentering', 0.001264810562133789), (' smoothing and normalization', 0.0002808570861816406)]
Smoothing and Normalization: 0.0005042552947998047
Fit RMSE: 0.05700536122258186
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014421939849853516
PCA fit: 0.04544377326965332
[[9.80392153e-01 9.68600089e-09]
 [9.68600036e-09 9.80392192e-01]]
PCA Transform: 0.0016138553619384766
total iterations: 11
TLDA fit: 5.68665075302124
Whitened factor: 
[[ 0.9683229  -0.87914604]
 [-0.24970149  0.47655243]]
PCA Reverse Transform: 0.00024199485778808594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12554696342814375
Fit RMSE: 0.1260434634787357
 Test Against Ground Truth
[(' decentering', 0.001253366470336914), (' smoothing and normalization', 0.00024509429931640625)]
Smoothing and Normalization: 0.00044846534729003906
Fit RMSE: 0.1252017373217647
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.002124786376953125
PCA fit: 0.27558279037475586
[[ 9.80392153e-01 -3.39974415e-09]
 [-3.39974412e-09  9.80392135e-01]]
PCA Transform: 0.005765199661254883
total iterations: 11
TLDA fit: 5.486422538757324
Whitened factor: 
[[ 0.263042    0.28451267]
 [-0.9647844   0.9586723 ]]
PCA Reverse Transform: 0.00023794174194335938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05876433112922899
Fit RMSE: 0.05888623999952849
 Test Against Ground Truth
[(' decentering', 0.0012607574462890625), (' smoothing and normalization', 0.00028133392333984375)]
Smoothing and Normalization: 0.0005383491516113281
Fit RMSE: 0.05517679667405738
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0017628669738769531
PCA fit: 0.04523134231567383
[[ 9.80392155e-01 -3.21076787e-09]
 [-3.21076752e-09  9.80392179e-01]]
PCA Transform: 0.001531839370727539
total iterations: 11
TLDA fit: 5.560470819473267
Whitened factor: 
[[ 0.9477207  -0.82527065]
 [-0.319101    0.56473744]]
PCA Reverse Transform: 0.00023865699768066406
decenter with new strategy:
[-0.70014675  0.96981533]
decenter with old strategy:
[0.10444896 1.72027742]
Fit RMSE new decenter: 0.11774369617998545
Fit RMSE: 0.1189535044640932
 Test Against Ground Truth
[(' decentering', 0.0012669563293457031), (' smoothing and normalization', 0.00025200843811035156)]
Smoothing and Normalization: 0.0004558563232421875
Fit RMSE: 0.11718396186974404
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0022284984588623047
PCA fit: 0.24406909942626953
[[9.80392152e-01 1.65085806e-10]
 [1.65085999e-10 9.80392187e-01]]
PCA Transform: 0.00576019287109375
total iterations: 11
TLDA fit: 5.64205002784729
Whitened factor: 
[[ 0.38847482  0.13934661]
 [-0.9214593   0.9902437 ]]
PCA Reverse Transform: 0.00024628639221191406
decenter with new strategy:
[0.0018437  0.00288538]
decenter with old strategy:
[0.00657369 0.00686783]
Fit RMSE new decenter: 0.058495859885585116
Fit RMSE: 0.05980431268233479
 Test Against Ground Truth
[(' decentering', 0.0013492107391357422), (' smoothing and normalization', 0.00030875205993652344)]
Smoothing and Normalization: 0.0005512237548828125
Fit RMSE: 0.05674857500989516
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014488697052001953
PCA fit: 0.04358196258544922
[[ 9.80392170e-01 -3.77292875e-09]
 [-3.77292876e-09  9.80392140e-01]]
PCA Transform: 0.001522064208984375
total iterations: 11
TLDA fit: 5.629442453384399
Whitened factor: 
[[ 0.9444034 -0.808852 ]
 [-0.3287889  0.5880123]]
PCA Reverse Transform: 0.00023436546325683594
decenter with new strategy:
[-0.00209707  0.00299688]
decenter with old strategy:
[-0.0001698   0.00477809]
Fit RMSE new decenter: 0.11988520285616226
Fit RMSE: 0.12106815959683123
 Test Against Ground Truth
[(' decentering', 0.00127410888671875), (' smoothing and normalization', 0.0002429485321044922)]
Smoothing and Normalization: 0.0005021095275878906
Fit RMSE: 0.11981283730130565
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0019533634185791016
PCA fit: 0.2689039707183838
[[ 9.80392155e-01 -3.48131071e-09]
 [-3.48131058e-09  9.80392172e-01]]
PCA Transform: 0.005830049514770508
total iterations: 11
TLDA fit: 5.598074197769165
Whitened factor: 
[[ 0.3408394   0.24385831]
 [-0.9401216   0.96981096]]
PCA Reverse Transform: 0.00024271011352539062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05688683315403693
Fit RMSE: 0.05869202366117958
 Test Against Ground Truth
[(' decentering', 0.0013115406036376953), (' smoothing and normalization', 0.0003039836883544922)]
Smoothing and Normalization: 0.000469207763671875
Fit RMSE: 0.05497279885206149
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001417398452758789
PCA fit: 0.04271268844604492
[[9.80392160e-01 1.39336885e-08]
 [1.39336888e-08 9.80392113e-01]]
PCA Transform: 0.0015273094177246094
total iterations: 11
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0000000000000002]
Traceback (most recent call last):
  File "generate_tables.py", line 579, in <module>
    main()
  File "generate_tables.py", line 513, in main
    for lr in lr_arr:
TypeError: 'float' object is not iterable
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0825049877166748
PCA fit: 0.5500359535217285
[[ 9.80392160e-01 -2.52915565e-09]
 [-2.52915574e-09  9.80392088e-01]]
PCA Transform: 0.0025322437286376953
total iterations: 66
TLDA fit: 34.436097145080566
Whitened factor: 
[[ 0.2875035   0.24558946]
 [-0.95777965  0.96937394]]
PCA Reverse Transform: 0.0010671615600585938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12710799008022405
Fit RMSE: 0.12707339270044807
 Test Against Ground Truth
[(' decentering', 0.0033812522888183594), (' smoothing and normalization', 0.0002856254577636719)]
Smoothing and Normalization: 0.0004839897155761719
Fit RMSE: 0.11348986308473884
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999998, 0.9999999999999999]
Centering time: 0.0019328594207763672
PCA fit: 0.29068422317504883
[[ 9.80392161e-01 -6.18141671e-09]
 [-6.18141622e-09  9.80392141e-01]]
PCA Transform: 0.005743980407714844
total iterations: 85
TLDA fit: 45.05395221710205
Whitened factor: 
[[-0.83855855  0.95531976]
 [ 0.54481155 -0.29557425]]
PCA Reverse Transform: 0.00023865699768066406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056974715242036116
Fit RMSE: 0.05725069119738847
 Test Against Ground Truth
[(' decentering', 0.0012843608856201172), (' smoothing and normalization', 0.0002951622009277344)]
Smoothing and Normalization: 0.00047659873962402344
Fit RMSE: 0.056891538092741674
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.001489400863647461
PCA fit: 0.04655194282531738
[[ 9.80392157e-01 -6.06762618e-09]
 [-6.06762614e-09  9.80392202e-01]]
PCA Transform: 0.0015647411346435547
total iterations: 51
TLDA fit: 27.32186532020569
Whitened factor: 
[[ 0.31300184  0.15136321]
 [-0.94975257  0.9884782 ]]
PCA Reverse Transform: 0.00025153160095214844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12685400589983006
Fit RMSE: 0.1286636711487283
 Test Against Ground Truth
[(' decentering', 0.0013055801391601562), (' smoothing and normalization', 0.00024819374084472656)]
Smoothing and Normalization: 0.0004467964172363281
Fit RMSE: 0.11840733835322588
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0019338130950927734
PCA fit: 0.280745267868042
[[ 9.80392146e-01 -6.16218876e-09]
 [-6.16218861e-09  9.80392100e-01]]
PCA Transform: 0.005847454071044922
total iterations: 52
TLDA fit: 27.46063470840454
Whitened factor: 
[[ 0.9543407  -0.7973537 ]
 [-0.29872042  0.6035123 ]]
PCA Reverse Transform: 0.00025725364685058594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05635279243583479
Fit RMSE: 0.05671806594138956
 Test Against Ground Truth
[(' decentering', 0.0013003349304199219), (' smoothing and normalization', 0.00027632713317871094)]
Smoothing and Normalization: 0.0005767345428466797
Fit RMSE: 0.05626201471164282
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001430511474609375
PCA fit: 0.04765486717224121
[[9.80392154e-01 2.44458276e-09]
 [2.44458249e-09 9.80392156e-01]]
PCA Transform: 0.001535177230834961
total iterations: 72
TLDA fit: 38.98274207115173
Whitened factor: 
[[ 0.29538682  0.18707108]
 [ 0.95537776 -0.9823464 ]]
PCA Reverse Transform: 0.00024199485778808594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12599418004196838
Fit RMSE: 0.1274227512131443
 Test Against Ground Truth
[(' decentering', 0.0012693405151367188), (' smoothing and normalization', 0.0002486705780029297)]
Smoothing and Normalization: 0.00046372413635253906
Fit RMSE: 0.11433054883738057
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.0032279491424560547
PCA fit: 0.23273658752441406
[[9.80392159e-01 3.30255685e-09]
 [3.30255662e-09 9.80392143e-01]]
PCA Transform: 0.005744457244873047
total iterations: 96
TLDA fit: 50.922985315322876
Whitened factor: 
[[ 0.09971086  0.29539922]
 [-0.9950165   0.9553739 ]]
PCA Reverse Transform: 0.0002446174621582031
decenter with new strategy:
[ 0.00473421 -0.00325052]
decenter with old strategy:
[0.02759003 0.02230646]
Fit RMSE new decenter: 0.058075881188417305
Fit RMSE: 0.05895560789761382
 Test Against Ground Truth
[(' decentering', 0.0012645721435546875), (' smoothing and normalization', 0.0002830028533935547)]
Smoothing and Normalization: 0.0005133152008056641
Fit RMSE: 0.055215206287144485
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999998]
Centering time: 0.0014276504516601562
PCA fit: 0.04412055015563965
[[9.80392161e-01 4.60608911e-08]
 [4.60608910e-08 9.80392154e-01]]
PCA Transform: 0.0015192031860351562
total iterations: 63
TLDA fit: 34.34611415863037
Whitened factor: 
[[-0.6884934   0.859973  ]
 [ 0.72524273 -0.51033956]]
PCA Reverse Transform: 0.00023436546325683594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12553202899566063
Fit RMSE: 0.12713384465852806
 Test Against Ground Truth
[(' decentering', 0.0012652873992919922), (' smoothing and normalization', 0.00025010108947753906)]
Smoothing and Normalization: 0.000484466552734375
Fit RMSE: 0.12545462125657095
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0020818710327148438
PCA fit: 0.21713590621948242
[[9.80392152e-01 3.70137825e-09]
 [3.70137858e-09 9.80392138e-01]]
PCA Transform: 0.005761623382568359
total iterations: 57
TLDA fit: 31.088521003723145
Whitened factor: 
[[-0.8388208   0.96392184]
 [ 0.5444076  -0.2661855 ]]
PCA Reverse Transform: 0.0002410411834716797
decenter with new strategy:
[ 0.20813637 -0.14534884]
decenter with old strategy:
[0.36940974 0.02677035]
Fit RMSE new decenter: 0.05657800083487319
Fit RMSE: 0.05689745866957555
 Test Against Ground Truth
[(' decentering', 0.0013074874877929688), (' smoothing and normalization', 0.0002949237823486328)]
Smoothing and Normalization: 0.0005548000335693359
Fit RMSE: 0.056498466832672026
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001592397689819336
PCA fit: 0.0481419563293457
[[ 9.80392159e-01 -7.85934517e-09]
 [-7.85934492e-09  9.80392178e-01]]
PCA Transform: 0.001561880111694336
total iterations: 72
TLDA fit: 38.99510169029236
Whitened factor: 
[[ 0.36253452  0.1412191 ]
 [-0.93197036  0.9899783 ]]
PCA Reverse Transform: 0.0002593994140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12560886845442057
Fit RMSE: 0.1288041195582087
 Test Against Ground Truth
[(' decentering', 0.0012562274932861328), (' smoothing and normalization', 0.00025081634521484375)]
Smoothing and Normalization: 0.0004932880401611328
Fit RMSE: 0.1177910631778912
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0020422935485839844
PCA fit: 0.29395008087158203
[[ 9.80392156e-01 -3.85851588e-09]
 [-3.85851580e-09  9.80392179e-01]]
PCA Transform: 0.005761861801147461
total iterations: 83
TLDA fit: 44.73618936538696
Whitened factor: 
[[0.28155512 0.8810625 ]
 [0.9595451  0.47299978]]
PCA Reverse Transform: 0.0002384185791015625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05649320121888833
Fit RMSE: 0.0569017984606572
 Test Against Ground Truth
[(' decentering', 0.0013072490692138672), (' smoothing and normalization', 0.0002892017364501953)]
Smoothing and Normalization: 0.0005216598510742188
Fit RMSE: 0.054793238782312806
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014345645904541016
PCA fit: 0.04479479789733887
[[9.80392153e-01 5.41987633e-09]
 [5.41987616e-09 9.80392155e-01]]
PCA Transform: 0.001589059829711914
total iterations: 52
Traceback (most recent call last):
  File "generate_tables.py", line 25, in <module>
    from version0_20.tlda_final import TLDA
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 113
    while (i <= 10 or rec_loss >= 0.1 or rec_loss < prev_rec_loss)) and i < max_train_iter:
                                                                  ^
SyntaxError: unmatched ')'
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.08215689659118652
PCA fit: 0.5387454032897949
[[9.80392155e-01 3.21767131e-08]
 [3.21767129e-08 9.80392152e-01]]
PCA Transform: 0.002231597900390625
Traceback (most recent call last):
  File "generate_tables.py", line 579, in <module>
    main()
  File "generate_tables.py", line 516, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 441, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 146, in fit
    if curr_max_rec == -1 or max_rec > curr_max_rec:
  File "cupy/_core/core.pyx", line 1031, in cupy._core.core.ndarray.__richcmp__
TypeError: operand type(s) all returned NotImplemented from __array_ufunc__(<ufunc 'greater'>, '__call__', array([16.44769454, 11.22154061]), None): 'ndarray', 'NoneType'
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.07881379127502441
PCA fit: 0.5361251831054688
[[ 9.80392156e-01 -1.81518330e-08]
 [-1.81518331e-08  9.80392098e-01]]
PCA Transform: 0.0022215843200683594
Traceback (most recent call last):
  File "generate_tables.py", line 579, in <module>
    main()
  File "generate_tables.py", line 516, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 441, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 146, in fit
    if curr_max_rec == -1 or max_rec > curr_max_rec:
  File "cupy/_core/core.pyx", line 1031, in cupy._core.core.ndarray.__richcmp__
TypeError: operand type(s) all returned NotImplemented from __array_ufunc__(<ufunc 'greater'>, '__call__', array([5.33656184, 5.32079257]), None): 'ndarray', 'NoneType'
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.07948517799377441
PCA fit: 0.5463051795959473
[[ 9.80392151e-01 -1.81657065e-08]
 [-1.81657067e-08  9.80392000e-01]]
PCA Transform: 0.0023338794708251953
total iterations: 16
TLDA fit: 8.286473274230957
Whitened factor: 
[[-0.9245888   0.9828991 ]
 [ 0.3809668  -0.18414497]]
PCA Reverse Transform: 0.002602100372314453
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1203095076067939
Fit RMSE: 0.12076012315481138
 Test Against Ground Truth
[(' decentering', 0.0033156871795654297), (' smoothing and normalization', 0.00025725364685058594)]
Smoothing and Normalization: 0.00044465065002441406
Fit RMSE: 0.12019905200632511
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003902435302734375
PCA fit: 0.28197264671325684
[[9.80392155e-01 7.37770853e-09]
 [7.37770863e-09 9.80392196e-01]]
PCA Transform: 0.005782127380371094
total iterations: 16
TLDA fit: 8.21233344078064
Whitened factor: 
[[0.14196247 0.93484324]
 [0.98987204 0.35506085]]
PCA Reverse Transform: 0.00025177001953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05636091018534677
Fit RMSE: 0.05682521666206504
 Test Against Ground Truth
[(' decentering', 0.0012493133544921875), (' smoothing and normalization', 0.0002892017364501953)]
Smoothing and Normalization: 0.0005061626434326172
Fit RMSE: 0.055203598092113074
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.0013599395751953125
PCA fit: 0.04195570945739746
[[ 9.80392142e-01 -7.46511786e-09]
 [-7.46511761e-09  9.80392132e-01]]
PCA Transform: 0.0015649795532226562
total iterations: 16
TLDA fit: 8.235535383224487
Whitened factor: 
[[-0.807936    0.95487905]
 [ 0.5892703  -0.29699486]]
PCA Reverse Transform: 0.00024318695068359375
decenter with new strategy:
[ 0.01432421 -0.01037664]
decenter with old strategy:
[0.0253718  0.00163145]
Fit RMSE new decenter: 0.12025591717194173
Fit RMSE: 0.12119184639220176
 Test Against Ground Truth
[(' decentering', 0.0012679100036621094), (' smoothing and normalization', 0.00024771690368652344)]
Smoothing and Normalization: 0.0004470348358154297
Fit RMSE: 0.11966263484191764
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0039033889770507812
PCA fit: 0.2699306011199951
[[9.80392168e-01 1.11664303e-08]
 [1.11664303e-08 9.80392140e-01]]
PCA Transform: 0.005758762359619141
total iterations: 16
TLDA fit: 8.103580713272095
Whitened factor: 
[[ 0.9901754  -0.9279029 ]
 [-0.13983034  0.37282196]]
PCA Reverse Transform: 0.0002415180206298828
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056754684126464665
Fit RMSE: 0.056873484093483154
 Test Against Ground Truth
[(' decentering', 0.001279592514038086), (' smoothing and normalization', 0.0002834796905517578)]
Smoothing and Normalization: 0.0005044937133789062
Fit RMSE: 0.05666319896859368
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014522075653076172
PCA fit: 0.046823740005493164
[[ 9.80392168e-01 -2.78700096e-09]
 [-2.78700091e-09  9.80392182e-01]]
PCA Transform: 0.0016071796417236328
total iterations: 16
TLDA fit: 8.493074417114258
Whitened factor: 
[[-0.0486826   0.29668042]
 [ 0.9988143  -0.95497686]]
PCA Reverse Transform: 0.00024628639221191406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12444785217984146
Fit RMSE: 0.12915558484433923
 Test Against Ground Truth
[(' decentering', 0.0012586116790771484), (' smoothing and normalization', 0.00025010108947753906)]
Smoothing and Normalization: 0.0003895759582519531
Fit RMSE: 0.12082730616362969
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.003779172897338867
PCA fit: 0.26213884353637695
[[ 9.80392159e-01 -5.36561766e-09]
 [-5.36561764e-09  9.80392197e-01]]
PCA Transform: 0.005772113800048828
total iterations: 16
TLDA fit: 8.242098808288574
Whitened factor: 
[[-0.868857    0.97098666]
 [ 0.49506307 -0.23913361]]
PCA Reverse Transform: 0.00024580955505371094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057903005912159355
Fit RMSE: 0.05817552650015303
 Test Against Ground Truth
[(' decentering', 0.0012774467468261719), (' smoothing and normalization', 0.00028133392333984375)]
Smoothing and Normalization: 0.0005135536193847656
Fit RMSE: 0.05786908453406602
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0017049312591552734
PCA fit: 0.04460954666137695
[[ 9.80392154e-01 -8.43326298e-10]
 [-8.43325396e-10  9.80392487e-01]]
PCA Transform: 0.0015194416046142578
total iterations: 16
TLDA fit: 8.500789880752563
Whitened factor: 
[[-0.9522133   0.9899829 ]
 [ 0.30543363 -0.14118738]]
PCA Reverse Transform: 0.0002570152282714844
decenter with new strategy:
[-0.00501176  0.00774585]
decenter with old strategy:
[0.00011483 0.01297943]
Fit RMSE new decenter: 0.12316632703125721
Fit RMSE: 0.12334232382448512
 Test Against Ground Truth
[(' decentering', 0.0013074874877929688), (' smoothing and normalization', 0.00026226043701171875)]
Smoothing and Normalization: 0.00040078163146972656
Fit RMSE: 0.12294350546118364
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0037834644317626953
PCA fit: 0.2541186809539795
[[ 9.80392151e-01 -9.56093193e-09]
 [-9.56093179e-09  9.80392178e-01]]
PCA Transform: 0.005757808685302734
total iterations: 17
TLDA fit: 8.83525013923645
Whitened factor: 
[[ 0.95397395 -0.7939957 ]
 [-0.29988945  0.6079233 ]]
PCA Reverse Transform: 0.000244140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05851469399370702
Fit RMSE: 0.05873866708300174
 Test Against Ground Truth
[(' decentering', 0.0012917518615722656), (' smoothing and normalization', 0.00029468536376953125)]
Smoothing and Normalization: 0.0005252361297607422
Fit RMSE: 0.058376250750734326
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0015063285827636719
PCA fit: 0.0448150634765625
[[9.80392149e-01 1.20127744e-08]
 [1.20127743e-08 9.80392309e-01]]
PCA Transform: 0.0015342235565185547
total iterations: 16
TLDA fit: 8.259889125823975
Whitened factor: 
[[ 0.9807452  -0.93192166]
 [-0.1952918   0.36265963]]
PCA Reverse Transform: 0.0002353191375732422
decenter with new strategy:
[-0.31999696  0.48289902]
decenter with old strategy:
[0.05741478 0.85035905]
Fit RMSE new decenter: 0.12269920169251128
Fit RMSE: 0.12310821624539653
 Test Against Ground Truth
[(' decentering', 0.0012700557708740234), (' smoothing and normalization', 0.0002560615539550781)]
Smoothing and Normalization: 0.0004634857177734375
Fit RMSE: 0.1223400215135703
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0036623477935791016
PCA fit: 0.27637267112731934
[[ 9.80392157e-01 -4.95201455e-09]
 [-4.95201431e-09  9.80392136e-01]]
PCA Transform: 0.005799055099487305
total iterations: 2000
TLDA fit: 1073.3131830692291
Whitened factor: 
[[ 0.16952218  0.36860442]
 [ 0.9855264  -0.92958635]]
PCA Reverse Transform: 0.00025177001953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05782269901565876
Fit RMSE: 0.0592318642422236
 Test Against Ground Truth
[(' decentering', 0.0013244152069091797), (' smoothing and normalization', 0.0002911090850830078)]
Smoothing and Normalization: 0.0005240440368652344
Fit RMSE: 0.05583944132204408
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0013611316680908203
PCA fit: 0.04353904724121094
[[ 9.80392155e-01 -2.24987606e-09]
 [-2.24987612e-09  9.80392165e-01]]
PCA Transform: 0.0015587806701660156
total iterations: 17
TLDA fit: 8.964310884475708
Whitened factor: 
[[-0.79216677  0.9487675 ]
 [ 0.6103047  -0.31597495]]
PCA Reverse Transform: 0.0002512931823730469
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1188100858258698
Fit RMSE: 0.12016255424704593
 Test Against Ground Truth
[(' decentering', 0.001252889633178711), (' smoothing and normalization', 0.00028896331787109375)]
Smoothing and Normalization: 0.0004634857177734375
Fit RMSE: 0.11871991769301579
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.003593921661376953
PCA fit: 0.2520866394042969
[[ 9.80392159e-01 -4.18679201e-09]
 [-4.18679190e-09  9.80392183e-01]]
PCA Transform: 0.005805253982543945
total iterations: 2000
TLDA fit: 1074.7516450881958
Whitened factor: 
[[ 0.35909405  0.22213188]
 [-0.9333013   0.97501665]]
PCA Reverse Transform: 0.0002541542053222656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.059097358140661596
Fit RMSE: 0.059024693703829685
 Test Against Ground Truth
[(' decentering', 0.0013887882232666016), (' smoothing and normalization', 0.0002989768981933594)]
Smoothing and Normalization: 0.0005168914794921875
Fit RMSE: 0.0550637530366312
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0013608932495117188
PCA fit: 0.04527997970581055
[[9.80392149e-01 6.93746764e-09]
 [6.93746748e-09 9.80392147e-01]]
PCA Transform: 0.00154876708984375
total iterations: 16
TLDA fit: 8.385054349899292
Whitened factor: 
[[ 0.9739822  -0.8980077 ]
 [-0.22662428  0.43997985]]
PCA Reverse Transform: 0.0005698204040527344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11805445503745975
Fit RMSE: 0.11876401222859284
 Test Against Ground Truth
[(' decentering', 0.0012650489807128906), (' smoothing and normalization', 0.00025916099548339844)]
Smoothing and Normalization: 0.0004467964172363281
Fit RMSE: 0.11784748723513166
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.003696441650390625
PCA fit: 0.27710938453674316
[[9.80392157e-01 4.65103289e-10]
 [4.65103775e-10 9.80392169e-01]]
PCA Transform: 0.00576329231262207
total iterations: 2000
TLDA fit: 1074.8871607780457
Whitened factor: 
[[ 0.23434256  0.3135733 ]
 [-0.9721541   0.94956404]]
PCA Reverse Transform: 0.00024700164794921875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05912198147270036
Fit RMSE: 0.05922195240067894
 Test Against Ground Truth
[(' decentering', 0.0012843608856201172), (' smoothing and normalization', 0.00029754638671875)]
Smoothing and Normalization: 0.0005061626434326172
Fit RMSE: 0.05543676997032645
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999998]
Centering time: 0.001661539077758789
PCA fit: 0.04614996910095215
[[9.80392164e-01 1.46389828e-08]
 [1.46389827e-08 9.80392296e-01]]
PCA Transform: 0.0015554428100585938
total iterations: 17
TLDA fit: 8.888120889663696
Whitened factor: 
[[ 0.9557817  -0.8105889 ]
 [-0.294077    0.58561575]]
PCA Reverse Transform: 0.0002503395080566406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1195398278392544
Fit RMSE: 0.12048176562965592
 Test Against Ground Truth
[(' decentering', 0.0012657642364501953), (' smoothing and normalization', 0.0002465248107910156)]
Smoothing and Normalization: 0.0004665851593017578
Fit RMSE: 0.11913369681098149
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003461599349975586
PCA fit: 0.24920392036437988
[[9.80392164e-01 3.95656341e-09]
 [3.95656331e-09 9.80392143e-01]]
PCA Transform: 0.0057485103607177734
total iterations: 18
TLDA fit: 9.35632038116455
Whitened factor: 
[[ 0.97675014 -0.8708905 ]
 [-0.21438076  0.49147722]]
PCA Reverse Transform: 0.0002570152282714844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055802013874625855
Fit RMSE: 0.05616017916392705
 Test Against Ground Truth
[(' decentering', 0.0013027191162109375), (' smoothing and normalization', 0.00029158592224121094)]
Smoothing and Normalization: 0.0005261898040771484
Fit RMSE: 0.055783166967828236
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0016849040985107422
PCA fit: 0.04558610916137695
[[ 9.80392159e-01 -4.90474550e-09]
 [-4.90474601e-09  9.80392203e-01]]
PCA Transform: 0.0015616416931152344
total iterations: 2000
TLDA fit: 1089.997845172882
Whitened factor: 
[[ 0.3494005   0.04887325]
 [-0.93697345  0.99880505]]
PCA Reverse Transform: 0.0002551078796386719
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.124608928875113
Fit RMSE: 0.12783913628655072
 Test Against Ground Truth
[(' decentering', 0.0012755393981933594), (' smoothing and normalization', 0.00025391578674316406)]
Smoothing and Normalization: 0.0004837512969970703
Fit RMSE: 0.11738081596161529
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.003445148468017578
PCA fit: 0.29343605041503906
[[9.80392158e-01 4.20388276e-09]
 [4.20388273e-09 9.80392161e-01]]
PCA Transform: 0.0057752132415771484
total iterations: 2000
TLDA fit: 1067.9735360145569
Whitened factor: 
[[ 0.3847471   0.14260837]
 [ 0.92302203 -0.9897792 ]]
PCA Reverse Transform: 0.00025177001953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058975580607582796
Fit RMSE: 0.059488113828684665
 Test Against Ground Truth
[(' decentering', 0.0013034343719482422), (' smoothing and normalization', 0.0002942085266113281)]
Smoothing and Normalization: 0.0004951953887939453
Fit RMSE: 0.05602124333897814
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.0013594627380371094
PCA fit: 0.046210527420043945
[[9.80392160e-01 2.16119669e-08]
 [2.16119669e-08 9.80392155e-01]]
PCA Transform: 0.0015549659729003906
total iterations: 16
TLDA fit: 8.362711668014526
Whitened factor: 
[[-0.741251    0.9013939 ]
 [ 0.67122793 -0.43299997]]
PCA Reverse Transform: 0.0005764961242675781
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12221325923224718
Fit RMSE: 0.12328437715343925
 Test Against Ground Truth
[(' decentering', 0.001337289810180664), (' smoothing and normalization', 0.0002532005310058594)]
Smoothing and Normalization: 0.0004563331604003906
Fit RMSE: 0.12191904138880318
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0034601688385009766
PCA fit: 0.2675595283508301
[[ 9.80392157e-01 -1.24863869e-11]
 [-1.24863158e-11  9.80392152e-01]]
PCA Transform: 0.005769014358520508
total iterations: 2000
TLDA fit: 1080.9749960899353
Whitened factor: 
[[ 0.22420609  0.31905672]
 [-0.9745418   0.9477356 ]]
PCA Reverse Transform: 0.00025010108947753906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05841797764375011
Fit RMSE: 0.05850200869509044
 Test Against Ground Truth
[(' decentering', 0.0012960433959960938), (' smoothing and normalization', 0.00029015541076660156)]
Smoothing and Normalization: 0.0005083084106445312
Fit RMSE: 0.05412634415994265
sklearn Test Against Ground Truth
Done!
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.08025741577148438
PCA fit: 0.5378711223602295
[[ 9.80392167e-01 -9.33465047e-10]
 [-9.33464305e-10  9.80392219e-01]]
PCA Transform: 0.0022394657135009766
total iterations: 11
TLDA fit: 5.653799533843994
Whitened factor: 
[[ 0.62283397 -0.23974277]
 [ 0.78235406  0.97083646]]
PCA Reverse Transform: 0.0014688968658447266
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11970324454337619
Fit RMSE: 0.12373624600122816
 Test Against Ground Truth
[(' decentering', 0.0033388137817382812), (' smoothing and normalization', 0.0002579689025878906)]
Smoothing and Normalization: 0.0004756450653076172
Fit RMSE: 0.11852660181337106
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003293752670288086
PCA fit: 0.26120662689208984
[[ 9.80392155e-01 -6.68555062e-09]
 [-6.68555060e-09  9.80392214e-01]]
PCA Transform: 0.005774021148681641
total iterations: 11
TLDA fit: 5.4183244705200195
Whitened factor: 
[[0.99979717 0.25970912]
 [0.02013933 0.965687  ]]
PCA Reverse Transform: 0.00024390220642089844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05829522771335529
Fit RMSE: 0.05777540250840548
 Test Against Ground Truth
[(' decentering', 0.0012984275817871094), (' smoothing and normalization', 0.00028824806213378906)]
Smoothing and Normalization: 0.0005035400390625
Fit RMSE: 0.05651215599843271
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0013554096221923828
PCA fit: 0.04464602470397949
[[ 9.80392168e-01 -1.35535608e-08]
 [-1.35535604e-08  9.80392148e-01]]
PCA Transform: 0.0015940666198730469
total iterations: 11
TLDA fit: 5.517126083374023
Whitened factor: 
[[-0.01422155  0.79443747]
 [ 0.9998989   0.6073459 ]]
PCA Reverse Transform: 0.00023937225341796875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1251014648280684
Fit RMSE: 0.12721849111961797
 Test Against Ground Truth
[(' decentering', 0.0012536048889160156), (' smoothing and normalization', 0.0002503395080566406)]
Smoothing and Normalization: 0.0004508495330810547
Fit RMSE: 0.1231666034133189
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003935575485229492
PCA fit: 0.22736501693725586
[[9.80392157e-01 1.48389079e-10]
 [1.48389370e-10 9.80392190e-01]]
PCA Transform: 0.005761623382568359
total iterations: 26
TLDA fit: 13.446165800094604
Whitened factor: 
[[0.35514054 0.94763964]
 [0.934813   0.31934166]]
PCA Reverse Transform: 0.00023984909057617188
decenter with new strategy:
[-0.00019967 -0.00217555]
decenter with old strategy:
[0.00173467 0.0002964 ]
Fit RMSE new decenter: 0.05781553238259194
Fit RMSE: 0.056882100433738814
 Test Against Ground Truth
[(' decentering', 0.0012755393981933594), (' smoothing and normalization', 0.00027823448181152344)]
Smoothing and Normalization: 0.0005080699920654297
Fit RMSE: 0.055109508782865795
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0016827583312988281
PCA fit: 0.04772758483886719
[[ 9.80392176e-01 -5.82973780e-10]
 [-5.82973832e-10  9.80392157e-01]]
PCA Transform: 0.0015485286712646484
total iterations: 11
TLDA fit: 5.578891038894653
Whitened factor: 
[[-0.08198208  0.9843047 ]
 [ 0.9966339   0.17647736]]
PCA Reverse Transform: 0.0002429485321044922
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12422179695659584
Fit RMSE: 0.12390253862304132
 Test Against Ground Truth
[(' decentering', 0.0012664794921875), (' smoothing and normalization', 0.0002503395080566406)]
Smoothing and Normalization: 0.00044846534729003906
Fit RMSE: 0.12091058013010364
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003920555114746094
PCA fit: 0.2923407554626465
[[ 9.80392154e-01 -7.40465148e-10]
 [-7.40465102e-10  9.80392188e-01]]
PCA Transform: 0.005768299102783203
total iterations: 12
TLDA fit: 5.999714374542236
Whitened factor: 
[[0.9763487  0.18245699]
 [0.21620187 0.9832138 ]]
PCA Reverse Transform: 0.0002827644348144531
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058073286020024414
Fit RMSE: 0.05781371387626129
 Test Against Ground Truth
[(' decentering', 0.0012602806091308594), (' smoothing and normalization', 0.0002834796905517578)]
Smoothing and Normalization: 0.0005207061767578125
Fit RMSE: 0.05660891961395988
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0016601085662841797
PCA fit: 0.04613375663757324
[[ 9.80392153e-01 -1.92254202e-08]
 [-1.92254208e-08  9.80392100e-01]]
PCA Transform: 0.001573801040649414
total iterations: 12
TLDA fit: 6.1454432010650635
Whitened factor: 
[[0.74390835 0.9983525 ]
 [0.6682817  0.0573779 ]]
PCA Reverse Transform: 0.00025725364685058594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12913397227976509
Fit RMSE: 0.12299997797277279
 Test Against Ground Truth
[(' decentering', 0.0012633800506591797), (' smoothing and normalization', 0.00025010108947753906)]
Smoothing and Normalization: 0.000461578369140625
Fit RMSE: 0.11857557579311997
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.002098560333251953
PCA fit: 0.29770374298095703
[[ 9.80392161e-01 -3.96597656e-09]
 [-3.96597664e-09  9.80392180e-01]]
PCA Transform: 0.005753755569458008
total iterations: 11
TLDA fit: 5.433201789855957
Whitened factor: 
[[0.9946745  0.13378838]
 [0.10306634 0.99100995]]
PCA Reverse Transform: 0.00024080276489257812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05670245728968978
Fit RMSE: 0.05689796988691038
 Test Against Ground Truth
[(' decentering', 0.0012769699096679688), (' smoothing and normalization', 0.0003561973571777344)]
Smoothing and Normalization: 0.0004906654357910156
Fit RMSE: 0.055568194121098556
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.001420736312866211
PCA fit: 0.04673504829406738
[[9.80392142e-01 7.71209713e-09]
 [7.71209676e-09 9.80392205e-01]]
PCA Transform: 0.0016169548034667969
total iterations: 11
TLDA fit: 5.491239547729492
Whitened factor: 
[[0.97516644 0.2773686 ]
 [0.22147346 0.96076363]]
PCA Reverse Transform: 0.00023603439331054688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12715111333640264
Fit RMSE: 0.12433354265853197
 Test Against Ground Truth
[(' decentering', 0.0013077259063720703), (' smoothing and normalization', 0.000247955322265625)]
Smoothing and Normalization: 0.0004532337188720703
Fit RMSE: 0.12077675166670356
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 0.9999999999999998]
Centering time: 0.001928091049194336
PCA fit: 0.2675623893737793
[[ 9.80392157e-01 -5.80659399e-10]
 [-5.80658795e-10  9.80392179e-01]]
PCA Transform: 0.0057675838470458984
total iterations: 14
TLDA fit: 7.035080671310425
Whitened factor: 
[[0.9701666  0.3690428 ]
 [0.24243905 0.9294124 ]]
PCA Reverse Transform: 0.0002446174621582031
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05851120107062358
Fit RMSE: 0.05731114834386228
 Test Against Ground Truth
[(' decentering', 0.00128173828125), (' smoothing and normalization', 0.00029730796813964844)]
Smoothing and Normalization: 0.0005283355712890625
Fit RMSE: 0.05569809307074358
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001432180404663086
PCA fit: 0.04555392265319824
[[ 9.80392142e-01 -4.01524636e-09]
 [-4.01524688e-09  9.80392177e-01]]
PCA Transform: 0.0015292167663574219
total iterations: 12
TLDA fit: 6.115307807922363
Whitened factor: 
[[0.9164148  0.53465056]
 [0.40023    0.8450733 ]]
new version
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 0.9999999999999999]
Centering time: 0.08167624473571777
PCA fit: 0.5449035167694092
[[ 9.80392146e-01 -1.46055537e-08]
 [-1.46055535e-08  9.80392246e-01]]
PCA Transform: 0.0022394657135009766
total iterations: 2000
TLDA fit: 1062.4063591957092
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0010793209075927734
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.09999999999999999
Fit RMSE: 0.09999999999999999
 Test Against Ground Truth
[(' decentering', 0.003844738006591797), (' smoothing and normalization', 0.00022029876708984375)]
Smoothing and Normalization: 0.0004649162292480469
Fit RMSE: 0.12386705791004596
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.0032999515533447266
PCA fit: 0.28232407569885254
[[ 9.80392154e-01 -4.40674231e-09]
 [-4.40674252e-09  9.80392191e-01]]
PCA Transform: 0.005768299102783203
total iterations: 2000
TLDA fit: 1071.0690376758575
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002732276916503906
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.04472135954999579
Fit RMSE: 0.04472135954999579
 Test Against Ground Truth
[(' decentering', 0.0013282299041748047), (' smoothing and normalization', 0.00025010108947753906)]
Smoothing and Normalization: 0.0005085468292236328
Fit RMSE: 0.05543841384113954
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0013988018035888672
PCA fit: 0.045725107192993164
[[ 9.80392147e-01 -1.16102515e-08]
 [-1.16102515e-08  9.80392133e-01]]
PCA Transform: 0.0015654563903808594
total iterations: 2000
TLDA fit: 1079.933653831482
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002548694610595703
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.1
Fit RMSE: 0.1
 Test Against Ground Truth
[(' decentering', 0.0012841224670410156), (' smoothing and normalization', 0.0002048015594482422)]
Smoothing and Normalization: 0.0004570484161376953
Fit RMSE: 0.12367584843007905
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.0039157867431640625
PCA fit: 0.28162479400634766
[[ 9.80392159e-01 -4.34052305e-09]
 [-4.34052342e-09  9.80392109e-01]]
PCA Transform: 0.005753993988037109
total iterations: 2000
TLDA fit: 1076.7554533481598
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.00025200843811035156
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.044721359549995794
Fit RMSE: 0.044721359549995794
 Test Against Ground Truth
[(' decentering', 0.0012600421905517578), (' smoothing and normalization', 0.00024080276489257812)]
Smoothing and Normalization: 0.0004885196685791016
Fit RMSE: 0.05702978189886183
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0017299652099609375
PCA fit: 0.04598879814147949
[[ 9.80392153e-01 -1.61135862e-08]
 [-1.61135860e-08  9.80392172e-01]]
PCA Transform: 0.0015645027160644531
total iterations: 2000
TLDA fit: 1085.1949863433838
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002460479736328125
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.09999999999999999
Fit RMSE: 0.09999999999999999
 Test Against Ground Truth
[(' decentering', 0.0012812614440917969), (' smoothing and normalization', 0.00021147727966308594)]
Smoothing and Normalization: 0.0004620552062988281
Fit RMSE: 0.1204917107559789
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0033190250396728516
PCA fit: 0.2322676181793213
[[9.80392165e-01 1.71886725e-08]
 [1.71886724e-08 9.80392199e-01]]
PCA Transform: 0.0057544708251953125
total iterations: 2000
TLDA fit: 1077.3330328464508
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002465248107910156
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.044721359549995794
Fit RMSE: 0.044721359549995794
 Test Against Ground Truth
[(' decentering', 0.0012774467468261719), (' smoothing and normalization', 0.0002410411834716797)]
Smoothing and Normalization: 0.0005326271057128906
Fit RMSE: 0.056883134790622424
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014410018920898438
PCA fit: 0.04715466499328613
[[9.80392148e-01 4.32628866e-08]
 [4.32628866e-08 9.80392058e-01]]
PCA Transform: 0.0015549659729003906
total iterations: 2000
TLDA fit: 1065.6993470191956
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002524852752685547
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.1
Fit RMSE: 0.1
 Test Against Ground Truth
[(' decentering', 0.001346588134765625), (' smoothing and normalization', 0.0002357959747314453)]
Smoothing and Normalization: 0.00046443939208984375
Fit RMSE: 0.12087277713049942
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.0020198822021484375
PCA fit: 0.2608363628387451
[[ 9.80392153e-01 -1.97170270e-08]
 [-1.97170273e-08  9.80392089e-01]]
PCA Transform: 0.0057947635650634766
total iterations: 2000
TLDA fit: 1090.9236872196198
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002474784851074219
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.04472135954999579
Fit RMSE: 0.04472135954999579
 Test Against Ground Truth
[(' decentering', 0.001321554183959961), (' smoothing and normalization', 0.0002613067626953125)]
Smoothing and Normalization: 0.0005385875701904297
Fit RMSE: 0.05503153497433885
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0013616085052490234
PCA fit: 0.04357481002807617
[[ 9.80392153e-01 -9.27631876e-09]
 [-9.27631911e-09  9.80392072e-01]]
PCA Transform: 0.0015294551849365234
total iterations: 2000
TLDA fit: 1073.3655984401703
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.00024771690368652344
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.1
Fit RMSE: 0.1
 Test Against Ground Truth
[(' decentering', 0.0012674331665039062), (' smoothing and normalization', 0.0002079010009765625)]
Smoothing and Normalization: 0.0004947185516357422
Fit RMSE: 0.12079795192937332
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000002, 0.9999999999999999]
Centering time: 0.0019145011901855469
PCA fit: 0.27584218978881836
[[9.80392154e-01 4.16746099e-09]
 [4.16746075e-09 9.80392130e-01]]
PCA Transform: 0.005755901336669922
total iterations: 2000
TLDA fit: 1054.6385810375214
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002589225769042969
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.04472135954999579
Fit RMSE: 0.04472135954999579
 Test Against Ground Truth
[(' decentering', 0.001294851303100586), (' smoothing and normalization', 0.0002512931823730469)]
Smoothing and Normalization: 0.0005125999450683594
Fit RMSE: 0.05709801343524908
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014009475708007812
PCA fit: 0.042719364166259766
[[9.80392159e-01 3.13152013e-08]
 [3.13152011e-08 9.80391716e-01]]
PCA Transform: 0.0015413761138916016
total iterations: 2000
TLDA fit: 1093.4895038604736
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002503395080566406
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.09999999999999999
Fit RMSE: 0.09999999999999999
 Test Against Ground Truth
[(' decentering', 0.0012826919555664062), (' smoothing and normalization', 0.000209808349609375)]
Smoothing and Normalization: 0.00047707557678222656
Fit RMSE: 0.12403391477674042
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.0033910274505615234
PCA fit: 0.28769588470458984
[[ 9.80392155e-01 -8.22150864e-09]
 [-8.22150877e-09  9.80392171e-01]]
PCA Transform: 0.005780220031738281
total iterations: 2000
TLDA fit: 1056.6763911247253
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002510547637939453
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.04472135954999579
Fit RMSE: 0.04472135954999579
 Test Against Ground Truth
[(' decentering', 0.0013391971588134766), (' smoothing and normalization', 0.00025272369384765625)]
Smoothing and Normalization: 0.000522613525390625
Fit RMSE: 0.0568766609752547
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0016446113586425781
PCA fit: 0.04653215408325195
[[9.80392158e-01 6.43092425e-09]
 [6.43092448e-09 9.80392192e-01]]
PCA Transform: 0.0015430450439453125
total iterations: 2000
TLDA fit: 1070.9914243221283
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002422332763671875
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.1
Fit RMSE: 0.1
 Test Against Ground Truth
[(' decentering', 0.001272439956665039), (' smoothing and normalization', 0.00021457672119140625)]
Smoothing and Normalization: 0.00045299530029296875
Fit RMSE: 0.1141782059485745
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.002307415008544922
PCA fit: 0.25193095207214355
[[9.80392159e-01 6.40367182e-09]
 [6.40367079e-09 9.80392180e-01]]
PCA Transform: 0.005789279937744141
total iterations: 2000
TLDA fit: 1075.0881707668304
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.00024580955505371094
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.044721359549995794
Fit RMSE: 0.044721359549995794
 Test Against Ground Truth
[(' decentering', 0.0013072490692138672), (' smoothing and normalization', 0.0002899169921875)]
Smoothing and Normalization: 0.0005147457122802734
Fit RMSE: 0.05540082333963608
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0013813972473144531
PCA fit: 0.0446012020111084
[[ 9.80392157e-01 -3.38856438e-08]
 [-3.38856444e-08  9.80392021e-01]]
PCA Transform: 0.001651763916015625
total iterations: 2000
TLDA fit: 1081.949856042862
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.00027251243591308594
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.1
Fit RMSE: 0.1
 Test Against Ground Truth
[(' decentering', 0.0012679100036621094), (' smoothing and normalization', 0.00020503997802734375)]
Smoothing and Normalization: 0.0004703998565673828
Fit RMSE: 0.12322643729328447
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001989603042602539
PCA fit: 0.22191977500915527
[[ 9.80392158e-01 -7.29133234e-10]
 [-7.29133262e-10  9.80392157e-01]]
PCA Transform: 0.005779743194580078
total iterations: 2000
TLDA fit: 1075.3882682323456
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002465248107910156
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.0447213595499958
Fit RMSE: 0.0447213595499958
 Test Against Ground Truth
[(' decentering', 0.0013346672058105469), (' smoothing and normalization', 0.00025153160095214844)]
Smoothing and Normalization: 0.0005092620849609375
Fit RMSE: 0.05642643374509453
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.001678466796875
PCA fit: 0.04670000076293945
[[9.80392155e-01 2.21899749e-09]
 [2.21899723e-09 9.80392138e-01]]
PCA Transform: 0.0015571117401123047
total iterations: 2000
TLDA fit: 1073.5429277420044
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.00025773048400878906
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.1
Fit RMSE: 0.1
 Test Against Ground Truth
[(' decentering', 0.0013225078582763672), (' smoothing and normalization', 0.0002086162567138672)]
Smoothing and Normalization: 0.0004019737243652344
Fit RMSE: 0.11275005674134492
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999998, 0.9999999999999998]
Centering time: 0.0019085407257080078
PCA fit: 0.2781665325164795
[[ 9.80392154e-01 -9.61576530e-09]
 [-9.61576536e-09  9.80392106e-01]]
PCA Transform: 0.005827903747558594
total iterations: 2000
TLDA fit: 1070.8465132713318
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002503395080566406
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.04472135954999579
Fit RMSE: 0.04472135954999579
 Test Against Ground Truth
[(' decentering', 0.001285552978515625), (' smoothing and normalization', 0.00027298927307128906)]
Smoothing and Normalization: 0.0005183219909667969
Fit RMSE: 0.055932579180555185
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.0016870498657226562
PCA fit: 0.04662036895751953
[[ 9.80392156e-01 -2.70675984e-09]
 [-2.70675965e-09  9.80392094e-01]]
PCA Transform: 0.0015940666198730469
total iterations: 2000
TLDA fit: 1102.5735065937042
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002455711364746094
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.1
Fit RMSE: 0.1
 Test Against Ground Truth
[(' decentering', 0.0012791156768798828), (' smoothing and normalization', 0.0002460479736328125)]
Smoothing and Normalization: 0.0004951953887939453
Fit RMSE: 0.11889860241962623
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
[1.0, 1.0]
Centering time: 0.0035238265991210938
PCA fit: 0.2547340393066406
[[9.80392146e-01 1.50029547e-08]
 [1.50029543e-08 9.80392153e-01]]
PCA Transform: 0.005776166915893555
total iterations: 2000
TLDA fit: 1071.79753947258
Whitened factor: 
[[nan nan]
 [nan nan]]
PCA Reverse Transform: 0.0002541542053222656
decenter with new strategy:
[nan nan]
decenter with old strategy:
[nan nan]
Fit RMSE new decenter: 0.044721359549995794
Fit RMSE: 0.044721359549995794
 Test Against Ground Truth
[(' decentering', 0.0012631416320800781), (' smoothing and normalization', 0.000255584716796875)]
Smoothing and Normalization: 0.0005421638488769531
Fit RMSE: 0.05698495435262885
sklearn Test Against Ground Truth
Done!
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.08152270317077637
PCA fit: 0.541649341583252
[[9.80392167e-01 1.23222508e-08]
 [1.23222510e-08 9.80392212e-01]]
PCA Transform: 0.0022203922271728516
total iterations: 2000
TLDA fit: 1056.95366024971
Whitened factor: 
[[-0.84916544  0.9620161 ]
 [ 0.52812696 -0.27299258]]
PCA Reverse Transform: 0.001119375228881836
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12045035362171594
Fit RMSE: 0.12119044591182412
 Test Against Ground Truth
[(' decentering', 0.0038743019104003906), (' smoothing and normalization', 0.00026154518127441406)]
Smoothing and Normalization: 0.0004329681396484375
Fit RMSE: 0.12015744588210092
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003257274627685547
PCA fit: 0.2975316047668457
[[9.80392161e-01 1.42521894e-08]
 [1.42521896e-08 9.80392224e-01]]
PCA Transform: 0.005803108215332031
total iterations: 31
TLDA fit: 16.44558095932007
Whitened factor: 
[[ 0.97359586 -0.99990237]
 [ 0.22827832 -0.01397391]]
PCA Reverse Transform: 0.00024175643920898438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05650364342704332
Fit RMSE: 0.0566055118826864
 Test Against Ground Truth
[(' decentering', 0.0012865066528320312), (' smoothing and normalization', 0.0002853870391845703)]
Smoothing and Normalization: 0.0004961490631103516
Fit RMSE: 0.056423735460723726
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0013780593872070312
PCA fit: 0.04553818702697754
[[ 9.80392158e-01 -1.02749051e-08]
 [-1.02749050e-08  9.80392125e-01]]
PCA Transform: 0.0016150474548339844
total iterations: 2000
TLDA fit: 1105.0119018554688
Whitened factor: 
[[ 0.92209923 -0.74816775]
 [-0.3869537   0.66350967]]
PCA Reverse Transform: 0.00025081634521484375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12328928173025777
Fit RMSE: 0.1243577326558847
 Test Against Ground Truth
[(' decentering', 0.0012798309326171875), (' smoothing and normalization', 0.00025773048400878906)]
Smoothing and Normalization: 0.0004572868347167969
Fit RMSE: 0.12290905131824742
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0021524429321289062
PCA fit: 0.24675822257995605
[[9.80392151e-01 3.20503145e-09]
 [3.20503138e-09 9.80392173e-01]]
PCA Transform: 0.005933284759521484
total iterations: 16
TLDA fit: 8.313038349151611
Whitened factor: 
[[ 0.3830018   0.13470237]
 [-0.92374754  0.9908861 ]]
PCA Reverse Transform: 0.0005621910095214844
decenter with new strategy:
[0.03448262 0.0525203 ]
decenter with old strategy:
[0.12113967 0.12688615]
Fit RMSE new decenter: 0.05793208508172358
Fit RMSE: 0.059169982131414264
 Test Against Ground Truth
[(' decentering', 0.00124359130859375), (' smoothing and normalization', 0.0002923011779785156)]
Smoothing and Normalization: 0.0010707378387451172
Fit RMSE: 0.05570035683463193
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.0013928413391113281
PCA fit: 0.046874284744262695
[[9.80392174e-01 1.36105769e-08]
 [1.36105774e-08 9.80392078e-01]]
PCA Transform: 0.0015475749969482422
total iterations: 2000
TLDA fit: 1093.2649683952332
Whitened factor: 
[[-0.78868127  0.9523808 ]
 [ 0.6148023  -0.30491143]]
PCA Reverse Transform: 0.0002467632293701172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11890977606648016
Fit RMSE: 0.11988294891688375
 Test Against Ground Truth
[(' decentering', 0.0012924671173095703), (' smoothing and normalization', 0.00025081634521484375)]
Smoothing and Normalization: 0.00044918060302734375
Fit RMSE: 0.11843056094959382
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0019659996032714844
PCA fit: 0.24912595748901367
[[9.80392157e-01 8.59970249e-09]
 [8.59970243e-09 9.80392208e-01]]
PCA Transform: 0.0059356689453125
total iterations: 2000
TLDA fit: 1096.597347021103
Whitened factor: 
[[0.29557455 0.9106639 ]
 [0.9553197  0.41314802]]
PCA Reverse Transform: 0.0002529621124267578
decenter with new strategy:
[0.00029659 0.00164669]
decenter with old strategy:
[0.002085   0.00393102]
Fit RMSE new decenter: 0.05607878169258007
Fit RMSE: 0.05529585454126969
 Test Against Ground Truth
[(' decentering', 0.001279592514038086), (' smoothing and normalization', 0.0002875328063964844)]
Smoothing and Normalization: 0.0005421638488769531
Fit RMSE: 0.052756663521282235
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0013704299926757812
PCA fit: 0.044991493225097656
[[9.80392175e-01 1.96064779e-08]
 [1.96064779e-08 9.80392078e-01]]
PCA Transform: 0.0015454292297363281
total iterations: 2000
TLDA fit: 1107.6790401935577
Whitened factor: 
[[ 0.9692813  -0.88072366]
 [-0.24595459  0.4736306 ]]
PCA Reverse Transform: 0.0002548694610595703
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12444753657389401
Fit RMSE: 0.12495864535670582
 Test Against Ground Truth
[(' decentering', 0.001256704330444336), (' smoothing and normalization', 0.0002498626708984375)]
Smoothing and Normalization: 0.0004622936248779297
Fit RMSE: 0.12431074285628702
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.0032525062561035156
PCA fit: 0.2616450786590576
[[ 9.80392148e-01 -1.66308927e-08]
 [-1.66308929e-08  9.80392144e-01]]
PCA Transform: 0.0057506561279296875
total iterations: 45
TLDA fit: 24.04104709625244
Whitened factor: 
[[ 0.37960175 -0.22637145]
 [ 0.9251499  -0.97404116]]
PCA Reverse Transform: 0.0002429485321044922
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05881053080627632
Fit RMSE: 0.05950714367704311
 Test Against Ground Truth
[(' decentering', 0.0013661384582519531), (' smoothing and normalization', 0.00029921531677246094)]
Smoothing and Normalization: 0.0005183219909667969
Fit RMSE: 0.05742094360316928
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001455068588256836
PCA fit: 0.04455208778381348
[[9.80392153e-01 7.30714417e-09]
 [7.30714470e-09 9.80392150e-01]]
PCA Transform: 0.0015566349029541016
total iterations: 16
TLDA fit: 8.48554277420044
Whitened factor: 
[[ 0.40171126  0.13829213]
 [-0.91576636  0.99039155]]
PCA Reverse Transform: 0.00024962425231933594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1220284416320486
Fit RMSE: 0.1263965969070989
 Test Against Ground Truth
[(' decentering', 0.0012919902801513672), (' smoothing and normalization', 0.00024962425231933594)]
Smoothing and Normalization: 0.0004630088806152344
Fit RMSE: 0.11388860018873817
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0033283233642578125
PCA fit: 0.295640230178833
[[9.80392157e-01 5.41506559e-09]
 [5.41506552e-09 9.80392170e-01]]
PCA Transform: 0.005769014358520508
total iterations: 33
TLDA fit: 17.489407539367676
Whitened factor: 
[[-0.9890653   0.9975559 ]
 [ 0.14747827  0.06987318]]
PCA Reverse Transform: 0.0007557868957519531
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056665238839311606
Fit RMSE: 0.05670945245799324
 Test Against Ground Truth
[(' decentering', 0.0013015270233154297), (' smoothing and normalization', 0.0002980232238769531)]
Smoothing and Normalization: 0.0004916191101074219
Fit RMSE: 0.056601375721752695
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.0013926029205322266
PCA fit: 0.04516243934631348
[[ 9.80392157e-01 -4.86800655e-09]
 [-4.86800675e-09  9.80392151e-01]]
PCA Transform: 0.0015702247619628906
total iterations: 2000
TLDA fit: 1099.453937292099
Whitened factor: 
[[-0.61116403  0.7104205 ]
 [ 0.79150397 -0.70377743]]
PCA Reverse Transform: 0.00024271011352539062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11723615698604382
Fit RMSE: 0.12066261777531476
 Test Against Ground Truth
[(' decentering', 0.0012769699096679688), (' smoothing and normalization', 0.00025343894958496094)]
Smoothing and Normalization: 0.0004432201385498047
Fit RMSE: 0.11749833731757463
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003723621368408203
PCA fit: 0.21525788307189941
[[ 9.80392158e-01 -1.08330423e-08]
 [-1.08330424e-08  9.80392175e-01]]
PCA Transform: 0.005776166915893555
total iterations: 2000
TLDA fit: 1102.1734273433685
Whitened factor: 
[[-0.03327763  0.42485774]
 [ 0.99944615 -0.9052602 ]]
PCA Reverse Transform: 0.0002593994140625
decenter with new strategy:
[6.36906118e-04 9.17117438e-06]
decenter with old strategy:
[0.00129482 0.00083601]
Fit RMSE new decenter: 0.057567402429672385
Fit RMSE: 0.0590610116291336
 Test Against Ground Truth
[(' decentering', 0.0012941360473632812), (' smoothing and normalization', 0.0003056526184082031)]
Smoothing and Normalization: 0.0005245208740234375
Fit RMSE: 0.05558075446353924
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0013651847839355469
PCA fit: 0.04311871528625488
[[ 9.80392158e-01 -1.48783993e-09]
 [-1.48783963e-09  9.80392163e-01]]
PCA Transform: 0.0015716552734375
total iterations: 2000
TLDA fit: 1112.3821206092834
Whitened factor: 
[[ 0.8212291   0.95150906]
 [ 0.57059854 -0.30762053]]
PCA Reverse Transform: 0.0002574920654296875
decenter with new strategy:
[1.06313921 1.17753949]
decenter with old strategy:
[1.86810788 2.04181201]
Fit RMSE new decenter: 0.12716216876029954
Fit RMSE: 0.12668011234916748
 Test Against Ground Truth
[(' decentering', 0.0012683868408203125), (' smoothing and normalization', 0.0002543926239013672)]
Smoothing and Normalization: 0.0003883838653564453
Fit RMSE: 0.11469329937646543
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.002069711685180664
PCA fit: 0.2576305866241455
[[9.80392155e-01 2.97416508e-09]
 [2.97416453e-09 9.80392171e-01]]
PCA Transform: 0.00580596923828125
total iterations: 33
TLDA fit: 17.67680549621582
Whitened factor: 
[[ 0.99997646 -0.98866737]
 [ 0.00686823  0.15012322]]
PCA Reverse Transform: 0.00024509429931640625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055626618551986924
Fit RMSE: 0.05571855949847757
 Test Against Ground Truth
[(' decentering', 0.0013489723205566406), (' smoothing and normalization', 0.00028777122497558594)]
Smoothing and Normalization: 0.0005192756652832031
Fit RMSE: 0.05554110875497172
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.0017354488372802734
PCA fit: 0.04611396789550781
[[ 9.80392158e-01 -1.94558825e-09]
 [-1.94558798e-09  9.80392148e-01]]
PCA Transform: 0.0015702247619628906
total iterations: 20
TLDA fit: 10.739087343215942
Whitened factor: 
[[ 0.29266262  0.07364332]
 [-0.9562158   0.9972847 ]]
PCA Reverse Transform: 0.00024271011352539062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12804603942534226
Fit RMSE: 0.12765534441504386
 Test Against Ground Truth
[(' decentering', 0.001255035400390625), (' smoothing and normalization', 0.00024962425231933594)]
Smoothing and Normalization: 0.00045871734619140625
Fit RMSE: 0.11708048810978423
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000002, 0.9999999999999999]
Centering time: 0.0019161701202392578
PCA fit: 0.25121521949768066
[[ 9.80392148e-01 -9.26519352e-09]
 [-9.26519369e-09  9.80392119e-01]]
PCA Transform: 0.005769252777099609
total iterations: 2000
TLDA fit: 1088.4690434932709
Whitened factor: 
[[ 0.94083536 -0.8342985 ]
 [-0.33886418  0.55131304]]
PCA Reverse Transform: 0.0002491474151611328
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056070487192596385
Fit RMSE: 0.05663056835998568
 Test Against Ground Truth
[(' decentering', 0.001312255859375), (' smoothing and normalization', 0.0002911090850830078)]
Smoothing and Normalization: 0.0004978179931640625
Fit RMSE: 0.056186825068651886
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001384735107421875
PCA fit: 0.044475555419921875
[[9.80392156e-01 3.05591982e-08]
 [3.05591982e-08 9.80392198e-01]]
PCA Transform: 0.0015461444854736328
total iterations: 2000
TLDA fit: 1094.8943228721619
Whitened factor: 
[[ 0.9631027  -0.83814394]
 [-0.26913393  0.54544914]]
PCA Reverse Transform: 0.000247955322265625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11851223297483585
Fit RMSE: 0.11947438011262379
 Test Against Ground Truth
[(' decentering', 0.001262664794921875), (' smoothing and normalization', 0.0002498626708984375)]
Smoothing and Normalization: 0.00045609474182128906
Fit RMSE: 0.11822549189880109
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0037097930908203125
PCA fit: 0.2662777900695801
[[ 9.80392156e-01 -2.23408936e-09]
 [-2.23408950e-09  9.80392164e-01]]
PCA Transform: 0.005819797515869141
total iterations: 30
TLDA fit: 15.96841287612915
Whitened factor: 
[[ 0.18571633  0.05373385]
 [ 0.98260343 -0.9985553 ]]
PCA Reverse Transform: 0.00023984909057617188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058214416559699785
Fit RMSE: 0.058951651155499475
 Test Against Ground Truth
[(' decentering', 0.0013232231140136719), (' smoothing and normalization', 0.0002951622009277344)]
Smoothing and Normalization: 0.0005035400390625
Fit RMSE: 0.055465315922578674
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0016927719116210938
PCA fit: 0.046468257904052734
[[9.80392155e-01 2.16447187e-08]
 [2.16447184e-08 9.80392113e-01]]
PCA Transform: 0.0015320777893066406
total iterations: 16
TLDA fit: 8.410213708877563
Whitened factor: 
[[ 0.30990404  0.1315211 ]
 [-0.9507678   0.99131334]]
PCA Reverse Transform: 0.0008199214935302734
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12253591007668013
Fit RMSE: 0.128083071330678
 Test Against Ground Truth
[(' decentering', 0.0012736320495605469), (' smoothing and normalization', 0.0002560615539550781)]
Smoothing and Normalization: 0.00047898292541503906
Fit RMSE: 0.11757498972631238
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.002673625946044922
PCA fit: 0.2700991630554199
[[ 9.80392159e-01 -3.68773087e-09]
 [-3.68773070e-09  9.80392117e-01]]
PCA Transform: 0.005766391754150391
total iterations: 16
TLDA fit: 8.298217058181763
Whitened factor: 
[[ 0.2232411   0.32833216]
 [ 0.97476333 -0.94456226]]
PCA Reverse Transform: 0.000244140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05945605048342081
Fit RMSE: 0.05966368822348681
 Test Against Ground Truth
[(' decentering', 0.0012683868408203125), (' smoothing and normalization', 0.00028324127197265625)]
Smoothing and Normalization: 0.000492095947265625
Fit RMSE: 0.05648038456450248
sklearn Test Against Ground Truth
Done!
Traceback (most recent call last):
  File "generate_tables.py", line 25, in <module>
    from version0_20.tlda_final import TLDA
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 111
    while (i <= 10 or max_diff >= tol) and i < max_train_iter:
    ^
IndentationError: unexpected indent
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0835723876953125
PCA fit: 0.5486524105072021
[[9.80392162e-01 1.91482577e-08]
 [1.91482583e-08 9.80392229e-01]]
PCA Transform: 0.0022385120391845703
total iterations: 200
TLDA fit: 106.15029263496399
Whitened factor: 
[[ 0.04281448  0.04281448]
 [-0.99908304 -0.99908304]]
PCA Reverse Transform: 0.0011179447174072266
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.13131178221465006
Fit RMSE: 0.13131178221465006
 Test Against Ground Truth
[(' decentering', 0.003469705581665039), (' smoothing and normalization', 0.00028443336486816406)]
Smoothing and Normalization: 0.0003833770751953125
Fit RMSE: 0.12200936904488183
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 0.9999999999999998]
Centering time: 0.003351449966430664
PCA fit: 0.276928186416626
[[ 9.80392158e-01 -3.32292934e-09]
 [-3.32292940e-09  9.80392146e-01]]
PCA Transform: 0.005800485610961914
total iterations: 11
TLDA fit: 5.497847080230713
Whitened factor: 
[[ 0.93306375 -0.9330595 ]
 [-0.3597111   0.35972205]]
PCA Reverse Transform: 0.0002467632293701172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05550025044288536
Fit RMSE: 0.055678755695473724
 Test Against Ground Truth
[(' decentering', 0.0014138221740722656), (' smoothing and normalization', 0.0003247261047363281)]
Smoothing and Normalization: 0.0005066394805908203
Fit RMSE: 0.05539160437747277
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0015494823455810547
PCA fit: 0.04635906219482422
[[9.80392159e-01 1.31761746e-08]
 [1.31761750e-08 9.80392033e-01]]
PCA Transform: 0.001580953598022461
total iterations: 200
TLDA fit: 108.83603525161743
Whitened factor: 
[[-0.07940813 -0.07940813]
 [-0.9968422  -0.9968422 ]]
PCA Reverse Transform: 0.0002639293670654297
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12802868302335077
Fit RMSE: 0.12802868302335077
 Test Against Ground Truth
[(' decentering', 0.0013151168823242188), (' smoothing and normalization', 0.00024580955505371094)]
Smoothing and Normalization: 0.00049591064453125
Fit RMSE: 0.11625371961714696
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0037720203399658203
PCA fit: 0.2473454475402832
[[9.80392157e-01 9.62550240e-10]
 [9.62550123e-10 9.80392183e-01]]
PCA Transform: 0.005766630172729492
total iterations: 11
TLDA fit: 5.414533615112305
Whitened factor: 
[[ 0.5217149 -0.5217449]
 [-0.8531199  0.8531016]]
PCA Reverse Transform: 0.00027108192443847656
decenter with new strategy:
[ 0.4596393  -0.19896949]
decenter with old strategy:
[0.93429112 0.27569215]
Fit RMSE new decenter: 0.0547905476728568
Fit RMSE: 0.056493032262387784
 Test Against Ground Truth
[(' decentering', 0.0012636184692382812), (' smoothing and normalization', 0.0002830028533935547)]
Smoothing and Normalization: 0.0004849433898925781
Fit RMSE: 0.054460241035322786
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.0014302730560302734
PCA fit: 0.04488706588745117
[[ 9.80392148e-01 -1.45135950e-08]
 [-1.45135949e-08  9.80392175e-01]]
PCA Transform: 0.0015459060668945312
total iterations: 11
TLDA fit: 5.52993106842041
Whitened factor: 
[[ 0.25949854 -0.2595767 ]
 [ 0.96574354 -0.96572256]]
PCA Reverse Transform: 0.0002377033233642578
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12507939582686242
Fit RMSE: 0.12789252821105768
 Test Against Ground Truth
[(' decentering', 0.0012576580047607422), (' smoothing and normalization', 0.00024771690368652344)]
Smoothing and Normalization: 0.00044846534729003906
Fit RMSE: 0.12082185309421868
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0038268566131591797
PCA fit: 0.2944941520690918
[[ 9.80392155e-01 -9.52607809e-10]
 [-9.52608006e-10  9.80392137e-01]]
PCA Transform: 0.005858898162841797
total iterations: 11
TLDA fit: 5.484341621398926
Whitened factor: 
[[-0.9620662   0.96207297]
 [-0.2728162   0.27279243]]
PCA Reverse Transform: 0.000240325927734375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05715403845431523
Fit RMSE: 0.057300854351025395
 Test Against Ground Truth
[(' decentering', 0.0012726783752441406), (' smoothing and normalization', 0.0002818107604980469)]
Smoothing and Normalization: 0.0005183219909667969
Fit RMSE: 0.057067424589922575
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.0016641616821289062
PCA fit: 0.047820329666137695
[[9.80392162e-01 2.06396330e-08]
 [2.06396334e-08 9.80392200e-01]]
PCA Transform: 0.0015511512756347656
total iterations: 11
TLDA fit: 5.557301998138428
Whitened factor: 
[[ 0.99759746 -0.9975964 ]
 [-0.06927843  0.06929275]]
PCA Reverse Transform: 0.00023651123046875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1191277941067505
Fit RMSE: 0.11911055847950204
 Test Against Ground Truth
[(' decentering', 0.0012722015380859375), (' smoothing and normalization', 0.0002536773681640625)]
Smoothing and Normalization: 0.0004551410675048828
Fit RMSE: 0.11877553170043348
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0019190311431884766
PCA fit: 0.2955501079559326
[[ 9.80392160e-01 -2.49891885e-09]
 [-2.49891881e-09  9.80392179e-01]]
PCA Transform: 0.005788087844848633
total iterations: 200
TLDA fit: 108.34121513366699
Whitened factor: 
[[0.9993123  0.9993123 ]
 [0.03707984 0.03707984]]
PCA Reverse Transform: 0.00025177001953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.059556108415382523
Fit RMSE: 0.059556108415382523
 Test Against Ground Truth
[(' decentering', 0.001279592514038086), (' smoothing and normalization', 0.0003731250762939453)]
Smoothing and Normalization: 0.0005459785461425781
Fit RMSE: 0.05619155646764682
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0013632774353027344
PCA fit: 0.04598212242126465
[[ 9.80392148e-01 -1.58216481e-08]
 [-1.58216471e-08  9.80391918e-01]]
PCA Transform: 0.0015349388122558594
total iterations: 11
TLDA fit: 5.568056344985962
Whitened factor: 
[[-0.91874963  0.9187844 ]
 [ 0.39484063 -0.39475986]]
PCA Reverse Transform: 0.00024008750915527344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12412066697221058
Fit RMSE: 0.12462735134731177
 Test Against Ground Truth
[(' decentering', 0.0012590885162353516), (' smoothing and normalization', 0.0002474784851074219)]
Smoothing and Normalization: 0.00045180320739746094
Fit RMSE: 0.12393077546209814
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0019521713256835938
PCA fit: 0.29608893394470215
[[ 9.80392154e-01 -1.27677848e-08]
 [-1.27677852e-08  9.80392122e-01]]
PCA Transform: 0.005890607833862305
total iterations: 11
TLDA fit: 5.396361589431763
Whitened factor: 
[[-0.309008    0.30897403]
 [ 0.9510595  -0.9510705 ]]
PCA Reverse Transform: 0.0002415180206298828
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056969669298280144
Fit RMSE: 0.05858767777093593
 Test Against Ground Truth
[(' decentering', 0.0012798309326171875), (' smoothing and normalization', 0.00029015541076660156)]
Smoothing and Normalization: 0.0005278587341308594
Fit RMSE: 0.05663919550431098
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999998]
Centering time: 0.0014574527740478516
PCA fit: 0.04494285583496094
[[9.80392164e-01 3.82720322e-08]
 [3.82720318e-08 9.80392117e-01]]
PCA Transform: 0.0015528202056884766
total iterations: 200
TLDA fit: 109.69104242324829
Whitened factor: 
[[-0.70119965 -0.70119965]
 [-0.712965   -0.712965  ]]
PCA Reverse Transform: 0.0002617835998535156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.13274952666885825
Fit RMSE: 0.13274952666885825
 Test Against Ground Truth
[(' decentering', 0.0012774467468261719), (' smoothing and normalization', 0.00025081634521484375)]
Smoothing and Normalization: 0.00041484832763671875
Fit RMSE: 0.12523688800623725
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0037457942962646484
PCA fit: 0.2731912136077881
[[9.80392158e-01 7.29902673e-09]
 [7.29902678e-09 9.80392154e-01]]
PCA Transform: 0.005757331848144531
total iterations: 11
TLDA fit: 5.449418783187866
Whitened factor: 
[[-0.6039359   0.60400087]
 [-0.7970329   0.79698366]]
PCA Reverse Transform: 0.00023889541625976562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05650744966040427
Fit RMSE: 0.057613396975700296
 Test Against Ground Truth
[(' decentering', 0.0012919902801513672), (' smoothing and normalization', 0.0002961158752441406)]
Smoothing and Normalization: 0.0005164146423339844
Fit RMSE: 0.05639792635115449
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.0013988018035888672
PCA fit: 0.04845476150512695
[[9.80392150e-01 2.79475393e-09]
 [2.79475378e-09 9.80392264e-01]]
PCA Transform: 0.0015943050384521484
total iterations: 11
TLDA fit: 5.530141592025757
Whitened factor: 
[[-0.5501067   0.55012137]
 [ 0.83509445 -0.83508474]]
PCA Reverse Transform: 0.00024056434631347656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12090363140867577
Fit RMSE: 0.12461225380531386
 Test Against Ground Truth
[(' decentering', 0.0013155937194824219), (' smoothing and normalization', 0.0002491474151611328)]
Smoothing and Normalization: 0.0004410743713378906
Fit RMSE: 0.12063354901941996
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0038199424743652344
PCA fit: 0.28142881393432617
[[ 9.80392155e-01 -1.03594460e-08]
 [-1.03594462e-08  9.80392147e-01]]
PCA Transform: 0.005759477615356445
total iterations: 200
TLDA fit: 107.27858877182007
Whitened factor: 
[[-0.12202416 -0.12202416]
 [-0.9925271  -0.9925271 ]]
PCA Reverse Transform: 0.0002574920654296875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05982099631117709
Fit RMSE: 0.05982099631117709
 Test Against Ground Truth
[(' decentering', 0.0013091564178466797), (' smoothing and normalization', 0.0003218650817871094)]
Smoothing and Normalization: 0.0005252361297607422
Fit RMSE: 0.05706843604875843
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.0013856887817382812
PCA fit: 0.04523015022277832
[[9.80392163e-01 8.69210726e-09]
 [8.69210758e-09 9.80392243e-01]]
PCA Transform: 0.0015752315521240234
total iterations: 200
TLDA fit: 107.08323192596436
Whitened factor: 
[[-0.87176657 -0.87176657]
 [-0.48992136 -0.48992136]]
PCA Reverse Transform: 0.0002613067626953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.13061059233138875
Fit RMSE: 0.13061059233138875
 Test Against Ground Truth
[(' decentering', 0.0013179779052734375), (' smoothing and normalization', 0.0002753734588623047)]
Smoothing and Normalization: 0.00046181678771972656
Fit RMSE: 0.12052802114952292
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001912832260131836
PCA fit: 0.22489356994628906
[[9.80392161e-01 1.36187006e-08]
 [1.36187005e-08 9.80392164e-01]]
PCA Transform: 0.005756378173828125
total iterations: 15
TLDA fit: 7.691497802734375
Whitened factor: 
[[-0.9878363  -0.9878363 ]
 [-0.15549766 -0.15549766]]
PCA Reverse Transform: 0.00024247169494628906
decenter with new strategy:
[0.0561394 0.0561394]
decenter with old strategy:
[0.09509263 0.09509263]
Fit RMSE new decenter: 0.05950069347791623
Fit RMSE: 0.05950069347791623
 Test Against Ground Truth
[(' decentering', 0.0012497901916503906), (' smoothing and normalization', 0.00028967857360839844)]
Smoothing and Normalization: 0.0005214214324951172
Fit RMSE: 0.056134865442495484
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0016484260559082031
PCA fit: 0.047777652740478516
[[9.80392155e-01 6.99974545e-09]
 [6.99974530e-09 9.80392194e-01]]
PCA Transform: 0.0015633106231689453
total iterations: 11
TLDA fit: 5.55117130279541
Whitened factor: 
[[ 0.04608909 -0.04621743]
 [-0.99893737  0.9989314 ]]
PCA Reverse Transform: 0.0002429485321044922
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12857096015033653
Fit RMSE: 0.12845961910750162
 Test Against Ground Truth
[(' decentering', 0.0012669563293457031), (' smoothing and normalization', 0.0002474784851074219)]
Smoothing and Normalization: 0.00040411949157714844
Fit RMSE: 0.11756308390459293
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0035431385040283203
PCA fit: 0.2962806224822998
[[ 9.80392157e-01 -6.51432558e-09]
 [-6.51432544e-09  9.80392103e-01]]
PCA Transform: 0.005766868591308594
total iterations: 200
TLDA fit: 107.41447615623474
Whitened factor: 
[[-0.6483075 -0.6483075]
 [-0.7613786 -0.7613786]]
PCA Reverse Transform: 0.0002434253692626953
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.06008820004921118
Fit RMSE: 0.06008820004921118
 Test Against Ground Truth
[(' decentering', 0.0012803077697753906), (' smoothing and normalization', 0.0002892017364501953)]
Smoothing and Normalization: 0.0005259513854980469
Fit RMSE: 0.057218799470535375
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014536380767822266
PCA fit: 0.04453158378601074
[[ 9.80392161e-01 -7.25148212e-09]
 [-7.25148210e-09  9.80392163e-01]]
PCA Transform: 0.0015740394592285156
total iterations: 200
TLDA fit: 108.63865447044373
Whitened factor: 
[[-0.6669678  -0.6669678 ]
 [ 0.74508655  0.74508655]]
PCA Reverse Transform: 0.0002512931823730469
decenter with new strategy:
[0.50715007 0.50715007]
decenter with old strategy:
[1.0989803 1.0989803]
Fit RMSE new decenter: 0.12651572018341192
Fit RMSE: 0.12651572018341192
 Test Against Ground Truth
[(' decentering', 0.001283407211303711), (' smoothing and normalization', 0.0002522468566894531)]
Smoothing and Normalization: 0.0004527568817138672
Fit RMSE: 0.12067215301442717
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003101348876953125
PCA fit: 0.25447940826416016
[[ 9.80392153e-01 -4.53410751e-09]
 [-4.53410727e-09  9.80392195e-01]]
PCA Transform: 0.00577235221862793
total iterations: 11
TLDA fit: 5.478078126907349
Whitened factor: 
[[-0.37286913  0.37295872]
 [-0.927884    0.9278479 ]]
PCA Reverse Transform: 0.00024199485778808594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055005002161148966
Fit RMSE: 0.05716918146309481
 Test Against Ground Truth
[(' decentering', 0.0012917518615722656), (' smoothing and normalization', 0.0002903938293457031)]
Smoothing and Normalization: 0.0005619525909423828
Fit RMSE: 0.05444354700746779
sklearn Test Against Ground Truth
Done!
new version
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0000000000000002]
Centering time: 0.07664799690246582
PCA fit: 0.5250484943389893
[[9.80392157e-01 3.35051414e-09]
 [3.35051416e-09 9.80392183e-01]]
PCA Transform: 0.0022559165954589844
total iterations: 11
TLDA fit: 6.6982386112213135
Whitened factor: 
[[ 0.21897711 -0.21906567]
 [-0.97573     0.97571015]]
PCA Reverse Transform: 0.0009639263153076172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12212538662915123
Fit RMSE: 0.1247743096406086
 Test Against Ground Truth
[(' decentering', 0.0032978057861328125), (' smoothing and normalization', 0.0002923011779785156)]
Smoothing and Normalization: 0.0004322528839111328
Fit RMSE: 0.11559460746718642
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0037071704864501953
PCA fit: 0.28194260597229004
[[9.80392154e-01 2.30419514e-09]
 [2.30419525e-09 9.80392145e-01]]
PCA Transform: 0.005785942077636719
total iterations: 11
TLDA fit: 5.370170593261719
Whitened factor: 
[[-0.9988382   0.998837  ]
 [-0.04819127  0.04821526]]
PCA Reverse Transform: 0.0002148151397705078
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05371411931038511
Fit RMSE: 0.05378762017341772
 Test Against Ground Truth
[(' decentering', 0.0012776851654052734), (' smoothing and normalization', 0.0003185272216796875)]
Smoothing and Normalization: 0.0005061626434326172
Fit RMSE: 0.053604883712561786
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.001638650894165039
PCA fit: 0.04581308364868164
[[9.80392156e-01 3.92128596e-09]
 [3.92128568e-09 9.80392244e-01]]
PCA Transform: 0.00164794921875
total iterations: 200
TLDA fit: 109.88721942901611
Whitened factor: 
[[0.17986102 0.17986102]
 [0.98369205 0.98369205]]
PCA Reverse Transform: 0.00023174285888671875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.13319137775513815
Fit RMSE: 0.13319137775513815
 Test Against Ground Truth
[(' decentering', 0.0012946128845214844), (' smoothing and normalization', 0.0002510547637939453)]
Smoothing and Normalization: 0.00044226646423339844
Fit RMSE: 0.1253715135985033
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003921985626220703
PCA fit: 0.30286717414855957
[[ 9.80392158e-01 -2.36365027e-09]
 [-2.36365017e-09  9.80392148e-01]]
PCA Transform: 0.0057544708251953125
total iterations: 11
TLDA fit: 5.408509731292725
Whitened factor: 
[[-0.90917784  0.90922   ]
 [ 0.4164082  -0.41631606]]
PCA Reverse Transform: 0.0002086162567138672
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05684387684645753
Fit RMSE: 0.05713451280125626
 Test Against Ground Truth
[(' decentering', 0.0012602806091308594), (' smoothing and normalization', 0.0002837181091308594)]
Smoothing and Normalization: 0.0005061626434326172
Fit RMSE: 0.056762719953387324
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014438629150390625
PCA fit: 0.04711627960205078
[[9.80392154e-01 2.16445950e-09]
 [2.16445975e-09 9.80392197e-01]]
PCA Transform: 0.0015797615051269531
total iterations: 11
TLDA fit: 5.524089336395264
Whitened factor: 
[[-0.99028516  0.9902859 ]
 [ 0.1390516  -0.13904665]]
PCA Reverse Transform: 0.00021409988403320312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11892158077127174
Fit RMSE: 0.11891119215173807
 Test Against Ground Truth
[(' decentering', 0.0012514591217041016), (' smoothing and normalization', 0.0002522468566894531)]
Smoothing and Normalization: 0.0004057884216308594
Fit RMSE: 0.11704098938435885
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0037164688110351562
PCA fit: 0.29654955863952637
[[9.80392159e-01 1.05281422e-09]
 [1.05281402e-09 9.80392145e-01]]
PCA Transform: 0.005766153335571289
total iterations: 200
TLDA fit: 106.108069896698
Whitened factor: 
[[0.64924794 0.64924794]
 [0.76057684 0.76057684]]
PCA Reverse Transform: 0.00022268295288085938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05924636929973852
Fit RMSE: 0.05924636929973852
 Test Against Ground Truth
[(' decentering', 0.001268625259399414), (' smoothing and normalization', 0.0002872943878173828)]
Smoothing and Normalization: 0.0005061626434326172
Fit RMSE: 0.055613447926274796
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 0.9999999999999998]
Centering time: 0.0016512870788574219
PCA fit: 0.0477910041809082
[[ 9.80392152e-01 -2.31959811e-08]
 [-2.31959814e-08  9.80392227e-01]]
PCA Transform: 0.0015566349029541016
total iterations: 11
TLDA fit: 5.494005441665649
Whitened factor: 
[[-0.92180496  0.9218168 ]
 [ 0.38765404 -0.38762596]]
PCA Reverse Transform: 0.00022101402282714844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12323214983162571
Fit RMSE: 0.12375981962786724
 Test Against Ground Truth
[(' decentering', 0.0012700557708740234), (' smoothing and normalization', 0.00025343894958496094)]
Smoothing and Normalization: 0.00045871734619140625
Fit RMSE: 0.12307168479233814
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.002049684524536133
PCA fit: 0.2970132827758789
[[ 9.80392158e-01 -1.28511881e-08]
 [-1.28511880e-08  9.80392270e-01]]
PCA Transform: 0.005752086639404297
total iterations: 11
TLDA fit: 5.449100017547607
Whitened factor: 
[[-0.61126953  0.6112363 ]
 [-0.7914226   0.7914482 ]]
PCA Reverse Transform: 0.0002186298370361328
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05721256850713947
Fit RMSE: 0.057985721326711266
 Test Against Ground Truth
[(' decentering', 0.0013661384582519531), (' smoothing and normalization', 0.00029659271240234375)]
Smoothing and Normalization: 0.0005247592926025391
Fit RMSE: 0.05711216590777696
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014481544494628906
PCA fit: 0.04474472999572754
[[ 9.80392158e-01 -1.20528203e-08]
 [-1.20528201e-08  9.80392103e-01]]
PCA Transform: 0.00154876708984375
total iterations: 11
TLDA fit: 5.485872268676758
Whitened factor: 
[[-0.17941704  0.1794259 ]
 [-0.9837732   0.9837715 ]]
PCA Reverse Transform: 0.00021076202392578125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12715325179528963
Fit RMSE: 0.12837115265480298
 Test Against Ground Truth
[(' decentering', 0.0012462139129638672), (' smoothing and normalization', 0.00024771690368652344)]
Smoothing and Normalization: 0.00045108795166015625
Fit RMSE: 0.11970646773796584
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003710031509399414
PCA fit: 0.25234055519104004
[[9.80392159e-01 9.52654832e-09]
 [9.52654812e-09 9.80392160e-01]]
PCA Transform: 0.005764961242675781
total iterations: 200
TLDA fit: 106.51736354827881
Whitened factor: 
[[0.7614653 0.7614653]
 [0.6482057 0.6482057]]
PCA Reverse Transform: 0.00021386146545410156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05919665913802754
Fit RMSE: 0.05919665913802754
 Test Against Ground Truth
[(' decentering', 0.001348733901977539), (' smoothing and normalization', 0.0002906322479248047)]
Smoothing and Normalization: 0.0005240440368652344
Fit RMSE: 0.05549915200030128
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014657974243164062
PCA fit: 0.04443049430847168
[[ 9.80392153e-01 -1.39265580e-08]
 [-1.39265578e-08  9.80392161e-01]]
PCA Transform: 0.001537322998046875
total iterations: 11
TLDA fit: 5.490076780319214
Whitened factor: 
[[ 0.42968982 -0.4296875 ]
 [-0.9029766   0.90297765]]
PCA Reverse Transform: 0.0002162456512451172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12216522665659804
Fit RMSE: 0.12642401241810503
 Test Against Ground Truth
[(' decentering', 0.0013005733489990234), (' smoothing and normalization', 0.0002510547637939453)]
Smoothing and Normalization: 0.00045943260192871094
Fit RMSE: 0.12108650157939628
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0031769275665283203
PCA fit: 0.2682363986968994
[[9.80392152e-01 1.01078278e-09]
 [1.01078254e-09 9.80392175e-01]]
PCA Transform: 0.005820035934448242
total iterations: 200
TLDA fit: 106.35791802406311
Whitened factor: 
[[0.37555474 0.37555474]
 [0.9268002  0.9268002 ]]
PCA Reverse Transform: 0.0002384185791015625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05956356230969066
Fit RMSE: 0.05956356230969066
 Test Against Ground Truth
[(' decentering', 0.0012688636779785156), (' smoothing and normalization', 0.0002944469451904297)]
Smoothing and Normalization: 0.0005195140838623047
Fit RMSE: 0.056143092925861386
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014564990997314453
PCA fit: 0.04489445686340332
[[ 9.80392169e-01 -2.13033535e-09]
 [-2.13033521e-09  9.80392179e-01]]
PCA Transform: 0.0015673637390136719
total iterations: 200
TLDA fit: 108.93941330909729
Whitened factor: 
[[-0.2342121 -0.2342121]
 [ 0.9721855  0.9721855]]
PCA Reverse Transform: 0.0002155303955078125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.13016144583112246
Fit RMSE: 0.13016144583112246
 Test Against Ground Truth
[(' decentering', 0.0012841224670410156), (' smoothing and normalization', 0.0002741813659667969)]
Smoothing and Normalization: 0.00037550926208496094
Fit RMSE: 0.12134569608282765
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.002057313919067383
PCA fit: 0.24613308906555176
[[ 9.80392157e-01 -6.34961035e-09]
 [-6.34961057e-09  9.80392141e-01]]
PCA Transform: 0.005774736404418945
total iterations: 11
TLDA fit: 5.450239658355713
Whitened factor: 
[[-0.9452388   0.94521236]
 [-0.32637942  0.3264561 ]]
PCA Reverse Transform: 0.00021195411682128906
decenter with new strategy:
[-0.00021034  0.00027499]
decenter with old strategy:
[-9.26514726e-05  3.92670855e-04]
Fit RMSE new decenter: 0.05532059281702505
Fit RMSE: 0.05565666502849515
 Test Against Ground Truth
[(' decentering', 0.001283884048461914), (' smoothing and normalization', 0.0002853870391845703)]
Smoothing and Normalization: 0.0004892349243164062
Fit RMSE: 0.05528112695917151
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014128684997558594
PCA fit: 0.047039031982421875
[[ 9.80392158e-01 -3.75847910e-09]
 [-3.75847915e-09  9.80392135e-01]]
PCA Transform: 0.0015804767608642578
total iterations: 200
TLDA fit: 109.10704970359802
Whitened factor: 
[[0.32736382 0.32736382]
 [0.9448984  0.9448984 ]]
PCA Reverse Transform: 0.0002205371856689453
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.13045892515319552
Fit RMSE: 0.13045892515319552
 Test Against Ground Truth
[(' decentering', 0.001270294189453125), (' smoothing and normalization', 0.000244140625)]
Smoothing and Normalization: 0.00048351287841796875
Fit RMSE: 0.12003612811871013
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0035932064056396484
PCA fit: 0.2758183479309082
[[9.80392160e-01 2.26115404e-09]
 [2.26115442e-09 9.80392186e-01]]
PCA Transform: 0.00577998161315918
total iterations: 11
TLDA fit: 5.543996810913086
Whitened factor: 
[[ 0.38142866 -0.3813653 ]
 [-0.92439836  0.92442447]]
PCA Reverse Transform: 0.00021219253540039062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05584949789752184
Fit RMSE: 0.05715384135102149
 Test Against Ground Truth
[(' decentering', 0.0012786388397216797), (' smoothing and normalization', 0.0002887248992919922)]
Smoothing and Normalization: 0.0005195140838623047
Fit RMSE: 0.055110748609507855
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.001424551010131836
PCA fit: 0.044769287109375
[[9.80392154e-01 5.26661149e-09]
 [5.26661161e-09 9.80392090e-01]]
PCA Transform: 0.0015804767608642578
total iterations: 200
TLDA fit: 110.46659874916077
Whitened factor: 
[[-0.9950372  -0.9950372 ]
 [-0.09950338 -0.09950338]]
PCA Reverse Transform: 0.0002276897430419922
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1296247747145852
Fit RMSE: 0.1296247747145852
 Test Against Ground Truth
[(' decentering', 0.0013055801391601562), (' smoothing and normalization', 0.000263214111328125)]
Smoothing and Normalization: 0.00039696693420410156
Fit RMSE: 0.1185973523921906
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0036079883575439453
PCA fit: 0.28254127502441406
[[ 9.80392161e-01 -1.33662437e-08]
 [-1.33662441e-08  9.80392160e-01]]
PCA Transform: 0.005767107009887695
total iterations: 11
TLDA fit: 5.606137037277222
Whitened factor: 
[[-0.14599144  0.14594004]
 [-0.9892859   0.9892934 ]]
PCA Reverse Transform: 0.0002167224884033203
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.058764098684629285
Fit RMSE: 0.059836600590068484
 Test Against Ground Truth
[(' decentering', 0.0012664794921875), (' smoothing and normalization', 0.0002892017364501953)]
Smoothing and Normalization: 0.0005233287811279297
Fit RMSE: 0.05758634731847803
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.0014657974243164062
PCA fit: 0.04415488243103027
[[ 9.80392151e-01 -1.83471571e-09]
 [-1.83471556e-09  9.80392175e-01]]
PCA Transform: 0.0015773773193359375
total iterations: 11
TLDA fit: 5.576122999191284
Whitened factor: 
[[-0.9701988   0.97020483]
 [ 0.24231043 -0.24228635]]
PCA Reverse Transform: 0.0002193450927734375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11898458169329341
Fit RMSE: 0.11922784987001364
 Test Against Ground Truth
[(' decentering', 0.001256704330444336), (' smoothing and normalization', 0.0002446174621582031)]
Smoothing and Normalization: 0.0004622936248779297
Fit RMSE: 0.11861795911257664
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0033893585205078125
PCA fit: 0.2546513080596924
[[ 9.80392151e-01 -1.26341863e-08]
 [-1.26341856e-08  9.80392155e-01]]
PCA Transform: 0.005784511566162109
total iterations: 11
TLDA fit: 5.453790903091431
Whitened factor: 
[[-0.99087137  0.9908774 ]
 [ 0.13481092 -0.13476668]]
PCA Reverse Transform: 0.00021147727966308594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05840762446951747
Fit RMSE: 0.05843259044836649
 Test Against Ground Truth
[(' decentering', 0.00127410888671875), (' smoothing and normalization', 0.00032210350036621094)]
Smoothing and Normalization: 0.0005223751068115234
Fit RMSE: 0.05832971846285101
sklearn Test Against Ground Truth
Done!
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.07989645004272461
PCA fit: 0.5423357486724854
[[ 9.80392167e-01 -5.60099017e-09]
 [-5.60098987e-09  9.80392155e-01]]
PCA Transform: 0.0022101402282714844
Traceback (most recent call last):
  File "generate_tables.py", line 579, in <module>
    main()
  File "generate_tables.py", line 516, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 441, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 115, in fit
    while (i <= 10 or curr_max_rec <= tol) and i < max_train_iter:
NameError: name 'curr_max_rec' is not defined
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.07765626907348633
PCA fit: 0.5311174392700195
[[ 9.80392155e-01 -9.20037380e-10]
 [-9.20037435e-10  9.80392150e-01]]
PCA Transform: 0.0021953582763671875
total iterations: 200
TLDA fit: 113.02899098396301
Whitened factor: 
[[-0.07157963  0.07955422]
 [-0.9974349   0.9968305 ]]
PCA Reverse Transform: 0.0025212764739990234
decenter with new strategy:
[0.02070568 0.01626703]
decenter with old strategy:
[0.08690951 0.08249636]
Fit RMSE new decenter: 0.13010942680672877
Fit RMSE: 0.13104337694167373
 Test Against Ground Truth
[(' decentering', 0.003313779830932617), (' smoothing and normalization', 0.0002560615539550781)]
Smoothing and Normalization: 0.0004260540008544922
Fit RMSE: 0.13031158507250787
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0032906532287597656
PCA fit: 0.23357915878295898
[[ 9.80392158e-01 -8.97413918e-09]
 [-8.97413911e-09  9.80392154e-01]]
PCA Transform: 0.005817890167236328
total iterations: 200
TLDA fit: 114.56831097602844
Whitened factor: 
[[ 0.7453804  -0.88904613]
 [-0.6666394   0.4578177 ]]
PCA Reverse Transform: 0.0002338886260986328
decenter with new strategy:
[-0.01107591  0.02491524]
decenter with old strategy:
[0.00765637 0.04515298]
Fit RMSE new decenter: 0.05605882901373918
Fit RMSE: 0.056746202255567646
 Test Against Ground Truth
[(' decentering', 0.0013217926025390625), (' smoothing and normalization', 0.00027871131896972656)]
Smoothing and Normalization: 0.0005161762237548828
Fit RMSE: 0.05606497495254067
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.0017273426055908203
PCA fit: 0.04653644561767578
[[ 9.80392157e-01 -9.18475429e-09]
 [-9.18475435e-09  9.80392203e-01]]
PCA Transform: 0.0015411376953125
total iterations: 200
TLDA fit: 115.81743168830872
Whitened factor: 
[[0.967084   0.32027406]
 [0.2544575  0.94732493]]
PCA Reverse Transform: 0.0002143383026123047
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1269126580874337
Fit RMSE: 0.12195749713825133
 Test Against Ground Truth
[(' decentering', 0.0013279914855957031), (' smoothing and normalization', 0.0002815723419189453)]
Smoothing and Normalization: 0.00043892860412597656
Fit RMSE: 0.11763770642049642
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0019676685333251953
PCA fit: 0.24808239936828613
[[9.80392159e-01 1.04433716e-08]
 [1.04433717e-08 9.80392228e-01]]
PCA Transform: 0.0057773590087890625
total iterations: 200
TLDA fit: 115.00638961791992
Whitened factor: 
[[-0.9470557   0.95343816]
 [ 0.32106936 -0.30158845]]
PCA Reverse Transform: 0.0002372264862060547
decenter with new strategy:
[ 0.00018344 -0.00011842]
decenter with old strategy:
[ 3.00932977e-04 -5.45696177e-07]
Fit RMSE new decenter: 0.05669823741933544
Fit RMSE: 0.05682996500894821
 Test Against Ground Truth
[(' decentering', 0.0012979507446289062), (' smoothing and normalization', 0.0002868175506591797)]
Smoothing and Normalization: 0.00051116943359375
Fit RMSE: 0.05658710237213821
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.0016324520111083984
PCA fit: 0.04535269737243652
[[9.80392158e-01 1.38120000e-08]
 [1.38119998e-08 9.80392121e-01]]
PCA Transform: 0.0015544891357421875
total iterations: 200
TLDA fit: 117.07584929466248
Whitened factor: 
[[0.97551674 0.35484168]
 [0.21992536 0.9349264 ]]
PCA Reverse Transform: 0.0002357959747314453
decenter with new strategy:
[-0.24261496 -0.01585026]
decenter with old strategy:
[0.02143905 0.18765034]
Fit RMSE new decenter: 0.1277994780221785
Fit RMSE: 0.12304585755599672
 Test Against Ground Truth
[(' decentering', 0.0012879371643066406), (' smoothing and normalization', 0.0002651214599609375)]
Smoothing and Normalization: 0.00045871734619140625
Fit RMSE: 0.11817844172972401
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0020258426666259766
PCA fit: 0.2675638198852539
[[9.80392158e-01 6.71974611e-09]
 [6.71974656e-09 9.80392161e-01]]
PCA Transform: 0.005757331848144531
total iterations: 200
TLDA fit: 114.9974217414856
Whitened factor: 
[[ 0.7715155 -0.8659659]
 [-0.6362105  0.5001031]]
PCA Reverse Transform: 0.0002231597900390625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05458008534293598
Fit RMSE: 0.05517697435985047
 Test Against Ground Truth
[(' decentering', 0.0012645721435546875), (' smoothing and normalization', 0.0003199577331542969)]
Smoothing and Normalization: 0.0005116462707519531
Fit RMSE: 0.05445027008466739
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0016236305236816406
PCA fit: 0.048330068588256836
[[9.80392154e-01 6.23178775e-09]
 [6.23178773e-09 9.80392241e-01]]
PCA Transform: 0.001537322998046875
total iterations: 200
TLDA fit: 117.78622484207153
Whitened factor: 
[[ 0.5689377  -0.5947827 ]
 [-0.82238066  0.80388653]]
PCA Reverse Transform: 0.0002377033233642578
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12108525754750488
Fit RMSE: 0.12445934174455718
 Test Against Ground Truth
[(' decentering', 0.0012896060943603516), (' smoothing and normalization', 0.0002536773681640625)]
Smoothing and Normalization: 0.0004649162292480469
Fit RMSE: 0.12064078970565438
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0019843578338623047
PCA fit: 0.25757312774658203
[[ 9.80392159e-01 -1.64664760e-09]
 [-1.64664701e-09  9.80392157e-01]]
PCA Transform: 0.00577092170715332
total iterations: 200
TLDA fit: 114.8582992553711
Whitened factor: 
[[0.94623613 0.36727092]
 [0.32347655 0.930114  ]]
PCA Reverse Transform: 0.0002353191375732422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05816332719025875
Fit RMSE: 0.05686893398589881
 Test Against Ground Truth
[(' decentering', 0.0012898445129394531), (' smoothing and normalization', 0.0002872943878173828)]
Smoothing and Normalization: 0.0005259513854980469
Fit RMSE: 0.054966257553441436
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0015146732330322266
PCA fit: 0.04526209831237793
[[9.80392149e-01 3.72471455e-09]
 [3.72471415e-09 9.80392208e-01]]
PCA Transform: 0.0015299320220947266
total iterations: 200
TLDA fit: 116.61557602882385
Whitened factor: 
[[-0.7512957   0.881101  ]
 [ 0.65996575 -0.47292814]]
PCA Reverse Transform: 0.00024390220642089844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11751372213614535
Fit RMSE: 0.11935739909953406
 Test Against Ground Truth
[(' decentering', 0.0012955665588378906), (' smoothing and normalization', 0.00024819374084472656)]
Smoothing and Normalization: 0.0003979206085205078
Fit RMSE: 0.11727823815134743
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.0027761459350585938
PCA fit: 0.2632002830505371
[[ 9.80392153e-01 -2.54803565e-09]
 [-2.54803620e-09  9.80392155e-01]]
PCA Transform: 0.005774021148681641
total iterations: 200
TLDA fit: 115.42915868759155
Whitened factor: 
[[-0.62997764  0.7251784 ]
 [-0.7766133   0.688561  ]]
PCA Reverse Transform: 0.00024771690368652344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05656636597460446
Fit RMSE: 0.05760835089136951
 Test Against Ground Truth
[(' decentering', 0.0013184547424316406), (' smoothing and normalization', 0.0003275871276855469)]
Smoothing and Normalization: 0.0005273818969726562
Fit RMSE: 0.05647396932897936
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.0017197132110595703
PCA fit: 0.04609203338623047
[[9.80392151e-01 1.40742018e-08]
 [1.40742012e-08 9.80392170e-01]]
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.07889270782470703
PCA fit: 0.5320181846618652
[[9.80392157e-01 3.64898767e-09]
 [3.64898767e-09 9.80392122e-01]]
PCA Transform: 0.0022356510162353516
Traceback (most recent call last):
  File "generate_tables.py", line 579, in <module>
    main()
  File "generate_tables.py", line 516, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 20, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 441, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 180, in fit
    if i == last_init + 1 and ortho_loss > prev_ortho_loss:
NameError: name 'prev_ortho_loss' is not defined
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.08098912239074707
PCA fit: 0.5352535247802734
[[9.80392153e-01 8.52156878e-09]
 [8.52156880e-09 9.80392127e-01]]
PCA Transform: 0.0022106170654296875
total iterations: 200
TLDA fit: 112.04916262626648
Whitened factor: 
[[-0.7973517   0.8005815 ]
 [ 0.6035149  -0.59922385]]
PCA Reverse Transform: 0.0011093616485595703
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11648821493620003
Fit RMSE: 0.11797284996249781
 Test Against Ground Truth
[(' decentering', 0.0033278465270996094), (' smoothing and normalization', 0.0002906322479248047)]
Smoothing and Normalization: 0.0003826618194580078
Fit RMSE: 0.1162398712466231
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0000000000000002]
Centering time: 0.003282308578491211
PCA fit: 0.283402681350708
[[ 9.80392154e-01 -1.09552527e-08]
 [-1.09552521e-08  9.80392141e-01]]
PCA Transform: 0.005781412124633789
total iterations: 200
TLDA fit: 113.19660687446594
Whitened factor: 
[[-0.7606214   0.80557406]
 [ 0.64919573 -0.5924951 ]]
PCA Reverse Transform: 0.0005359649658203125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05675505333176278
Fit RMSE: 0.05729791631698015
 Test Against Ground Truth
[(' decentering', 0.0013082027435302734), (' smoothing and normalization', 0.00028252601623535156)]
Smoothing and Normalization: 0.0005354881286621094
Fit RMSE: 0.05665762375999324
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.0016231536865234375
PCA fit: 0.04815030097961426
[[9.80392161e-01 1.20000112e-08]
 [1.20000109e-08 9.80392261e-01]]
PCA Transform: 0.0016188621520996094
total iterations: 200
TLDA fit: 114.29758310317993
Whitened factor: 
[[ 0.6597876  -0.6693914 ]
 [-0.75145215  0.74290997]]
PCA Reverse Transform: 0.00021576881408691406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11996377076360153
Fit RMSE: 0.12246195491441143
 Test Against Ground Truth
[(' decentering', 0.0013098716735839844), (' smoothing and normalization', 0.0002467632293701172)]
Smoothing and Normalization: 0.0004215240478515625
Fit RMSE: 0.11951974002142035
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001983642578125
PCA fit: 0.28547120094299316
[[ 9.80392159e-01 -1.39397760e-09]
 [-1.39397811e-09  9.80392138e-01]]
PCA Transform: 0.005803108215332031
total iterations: 200
TLDA fit: 113.47794985771179
Whitened factor: 
[[-0.42731783  0.83022183]
 [-0.9041015   0.55743325]]
PCA Reverse Transform: 0.0002181529998779297
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05558472404931807
Fit RMSE: 0.0566365635616532
 Test Against Ground Truth
[(' decentering', 0.0012629032135009766), (' smoothing and normalization', 0.00028443336486816406)]
Smoothing and Normalization: 0.0005056858062744141
Fit RMSE: 0.055232267703532685
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.0014078617095947266
PCA fit: 0.046621084213256836
[[ 9.80392163e-01 -7.83541107e-09]
 [-7.83541088e-09  9.80392137e-01]]
PCA Transform: 0.0015683174133300781
total iterations: 200
TLDA fit: 116.49726986885071
Whitened factor: 
[[ 0.47412854 -0.4671278 ]
 [-0.8804556   0.8841898 ]]
PCA Reverse Transform: 0.0002200603485107422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12115648864288993
Fit RMSE: 0.12417179379462588
 Test Against Ground Truth
[(' decentering', 0.001268625259399414), (' smoothing and normalization', 0.0002505779266357422)]
Smoothing and Normalization: 0.00044989585876464844
Fit RMSE: 0.11944884473751027
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.003928422927856445
PCA fit: 0.280397891998291
[[ 9.80392153e-01 -1.56992914e-10]
 [-1.56993270e-10  9.80392175e-01]]
PCA Transform: 0.005767822265625
total iterations: 200
TLDA fit: 113.34503221511841
Whitened factor: 
[[ 0.71982074 -0.8835266 ]
 [-0.69416     0.46838084]]
PCA Reverse Transform: 0.000213623046875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05547322756511666
Fit RMSE: 0.05615908079107186
 Test Against Ground Truth
[(' decentering', 0.001264333724975586), (' smoothing and normalization', 0.0002841949462890625)]
Smoothing and Normalization: 0.0005095005035400391
Fit RMSE: 0.05537380176013907
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.0017642974853515625
PCA fit: 0.047807931900024414
[[ 9.80392159e-01 -5.96568328e-09]
 [-5.96568346e-09  9.80392212e-01]]
PCA Transform: 0.0015730857849121094
total iterations: 200
TLDA fit: 115.54608130455017
Whitened factor: 
[[ 0.40707117 -0.41361848]
 [-0.9133964   0.91045034]]
PCA Reverse Transform: 0.0002105236053466797
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12311074597106675
Fit RMSE: 0.12661667578911295
 Test Against Ground Truth
[(' decentering', 0.001260995864868164), (' smoothing and normalization', 0.0002503395080566406)]
Smoothing and Normalization: 0.0004608631134033203
Fit RMSE: 0.12120270757974168
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.003787994384765625
PCA fit: 0.2804751396179199
[[9.80392155e-01 4.15118062e-09]
 [4.15118040e-09 9.80392190e-01]]
PCA Transform: 0.0067255496978759766
total iterations: 200
TLDA fit: 114.00307941436768
Whitened factor: 
[[ 0.67890775 -0.8540378 ]
 [-0.73422366  0.5202109 ]]
PCA Reverse Transform: 0.0002269744873046875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055555617320317455
Fit RMSE: 0.05620180293855266
 Test Against Ground Truth
[(' decentering', 0.0013151168823242188), (' smoothing and normalization', 0.00028395652770996094)]
Smoothing and Normalization: 0.0005424022674560547
Fit RMSE: 0.055468449637586646
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0016584396362304688
PCA fit: 0.04647684097290039
[[ 9.80392165e-01 -1.11658527e-08]
 [-1.11658526e-08  9.80392147e-01]]
PCA Transform: 0.0016245841979980469
total iterations: 200
TLDA fit: 113.85293650627136
Whitened factor: 
[[-0.78503543  0.8771479 ]
 [ 0.6194509  -0.48022023]]
PCA Reverse Transform: 0.00021648406982421875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1234327177041676
Fit RMSE: 0.12448630177178198
 Test Against Ground Truth
[(' decentering', 0.001262664794921875), (' smoothing and normalization', 0.00025081634521484375)]
Smoothing and Normalization: 0.0004832744598388672
Fit RMSE: 0.12315367248921189
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.003236055374145508
PCA fit: 0.2986323833465576
[[ 9.80392159e-01 -8.57053302e-09]
 [-8.57053296e-09  9.80392104e-01]]
PCA Transform: 0.00577545166015625
total iterations: 200
TLDA fit: 114.36751222610474
Whitened factor: 
[[-0.78014773  0.8039194 ]
 [ 0.62559533 -0.59473836]]
PCA Reverse Transform: 0.00021123886108398438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05690292885327028
Fit RMSE: 0.057472227448725294
 Test Against Ground Truth
[(' decentering', 0.0012862682342529297), (' smoothing and normalization', 0.0002913475036621094)]
Smoothing and Normalization: 0.0005300045013427734
Fit RMSE: 0.056819543254009425
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.0013532638549804688
PCA fit: 0.04301619529724121
[[ 9.80392163e-01 -8.80639117e-10]
 [-8.80639200e-10  9.80392074e-01]]
PCA Transform: 0.001565694808959961
total iterations: 200
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.07750344276428223
PCA fit: 0.5786211490631104
[[ 9.80392153e-01 -1.32459195e-08]
 [-1.32459197e-08  9.80392082e-01]]
PCA Transform: 0.0022394657135009766
total iterations: 11
TLDA fit: 5.875867128372192
Whitened factor: 
[[ 0.47407165 -0.47474253]
 [-0.8804863   0.8801247 ]]
PCA Reverse Transform: 0.0009360313415527344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12212535223139646
Fit RMSE: 0.12595126170332843
 Test Against Ground Truth
[(' decentering', 0.0032935142517089844), (' smoothing and normalization', 0.0002524852752685547)]
Smoothing and Normalization: 0.0004303455352783203
Fit RMSE: 0.1213384465049545
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.0038275718688964844
PCA fit: 0.27776265144348145
[[ 9.80392151e-01 -7.84458821e-09]
 [-7.84458830e-09  9.80392159e-01]]
PCA Transform: 0.005759239196777344
total iterations: 200
TLDA fit: 114.28244113922119
Whitened factor: 
[[-0.753992    0.8805797 ]
 [ 0.65688354 -0.47389814]]
PCA Reverse Transform: 0.0002224445343017578
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055461310588150445
Fit RMSE: 0.05609543704763019
 Test Against Ground Truth
[(' decentering', 0.0013108253479003906), (' smoothing and normalization', 0.000286102294921875)]
Smoothing and Normalization: 0.0005192756652832031
Fit RMSE: 0.05532557077663648
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0016260147094726562
PCA fit: 0.046732425689697266
[[9.80392151e-01 4.09121848e-10]
 [4.09121181e-10 9.80392156e-01]]
PCA Transform: 0.0015835762023925781
total iterations: 200
TLDA fit: 116.64738392829895
Whitened factor: 
[[ 0.7919427  -0.87185663]
 [-0.6105954   0.48976117]]
PCA Reverse Transform: 0.00021696090698242188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12022310804986214
Fit RMSE: 0.12168563195607052
 Test Against Ground Truth
[(' decentering', 0.0013318061828613281), (' smoothing and normalization', 0.0002567768096923828)]
Smoothing and Normalization: 0.0004410743713378906
Fit RMSE: 0.1198827211137412
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.002017974853515625
PCA fit: 0.24298739433288574
[[9.80392163e-01 3.16716390e-09]
 [3.16716422e-09 9.80392163e-01]]
PCA Transform: 0.0058367252349853516
total iterations: 200
TLDA fit: 114.597012758255
Whitened factor: 
[[-0.56976473  0.7566346 ]
 [-0.82180786  0.65383804]]
PCA Reverse Transform: 0.000217437744140625
decenter with new strategy:
[ 0.00017741 -0.00012212]
decenter with old strategy:
[0.00040217 0.00012937]
Fit RMSE new decenter: 0.05578876893662347
Fit RMSE: 0.056913387833016145
 Test Against Ground Truth
[(' decentering', 0.0013051033020019531), (' smoothing and normalization', 0.0002875328063964844)]
Smoothing and Normalization: 0.0005192756652832031
Fit RMSE: 0.05552698252629811
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.001720428466796875
PCA fit: 0.04808330535888672
[[9.80392158e-01 6.27922310e-09]
 [6.27922381e-09 9.80392066e-01]]
PCA Transform: 0.0015506744384765625
total iterations: 200
TLDA fit: 116.25713300704956
Whitened factor: 
[[ 0.7884027  -0.87185764]
 [-0.6151596   0.48975942]]
PCA Reverse Transform: 0.00021648406982421875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12068162374314263
Fit RMSE: 0.12236589318928436
 Test Against Ground Truth
[(' decentering', 0.0012671947479248047), (' smoothing and normalization', 0.0002570152282714844)]
Smoothing and Normalization: 0.00045990943908691406
Fit RMSE: 0.12064337547222356
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001970052719116211
PCA fit: 0.246168851852417
[[ 9.80392152e-01 -7.13029159e-10]
 [-7.13029136e-10  9.80392169e-01]]
PCA Transform: 0.0057833194732666016
total iterations: 200
TLDA fit: 113.92620301246643
Whitened factor: 
[[ 0.7334792  -0.86504865]
 [-0.6797119   0.50168806]]
PCA Reverse Transform: 0.00032639503479003906
decenter with new strategy:
[-1.86760498e-05  4.74495478e-05]
decenter with old strategy:
[1.92833193e-05 8.83462598e-05]
Fit RMSE new decenter: 0.05658334214826953
Fit RMSE: 0.0571028597981195
 Test Against Ground Truth
[(' decentering', 0.0013065338134765625), (' smoothing and normalization', 0.00028634071350097656)]
Smoothing and Normalization: 0.0005178451538085938
Fit RMSE: 0.05647097178107007
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.0016629695892333984
PCA fit: 0.04691934585571289
[[9.80392163e-01 2.59748160e-09]
 [2.59748098e-09 9.80392253e-01]]
PCA Transform: 0.0015366077423095703
total iterations: 200
TLDA fit: 115.27288722991943
Whitened factor: 
[[-0.7213227   0.88649535]
 [ 0.69259906 -0.46273756]]
PCA Reverse Transform: 0.00021004676818847656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11986472345597467
Fit RMSE: 0.12149328396731748
 Test Against Ground Truth
[(' decentering', 0.0012729167938232422), (' smoothing and normalization', 0.00024962425231933594)]
Smoothing and Normalization: 0.0004622936248779297
Fit RMSE: 0.1196221860744271
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001861572265625
PCA fit: 0.28775978088378906
[[ 9.80392153e-01 -3.50403892e-09]
 [-3.50403829e-09  9.80392137e-01]]
PCA Transform: 0.0058100223541259766
total iterations: 200
TLDA fit: 116.40253019332886
Whitened factor: 
[[-0.75539625  0.884306  ]
 [ 0.6552683  -0.46690774]]
PCA Reverse Transform: 0.0004897117614746094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056627078931732
Fit RMSE: 0.05720720958499317
 Test Against Ground Truth
[(' decentering', 0.0014216899871826172), (' smoothing and normalization', 0.000324249267578125)]
Smoothing and Normalization: 0.0005238056182861328
Fit RMSE: 0.05655688761637621
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001470804214477539
PCA fit: 0.04534125328063965
[[ 9.80392168e-01 -1.54833211e-08]
 [-1.54833213e-08  9.80392020e-01]]
PCA Transform: 0.0015845298767089844
total iterations: 200
TLDA fit: 116.49400901794434
Whitened factor: 
[[ 0.6138506  -0.69283944]
 [-0.7894223   0.7210919 ]]
PCA Reverse Transform: 0.0002353191375732422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12407685544127071
Fit RMSE: 0.12640428699755793
 Test Against Ground Truth
[(' decentering', 0.0012769699096679688), (' smoothing and normalization', 0.00024771690368652344)]
Smoothing and Normalization: 0.00045800209045410156
Fit RMSE: 0.12368645125205191
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0032029151916503906
PCA fit: 0.29586267471313477
[[9.80392158e-01 7.17130388e-09]
 [7.17130434e-09 9.80392137e-01]]
PCA Transform: 0.005764007568359375
total iterations: 200
TLDA fit: 115.69038152694702
Whitened factor: 
[[-0.75209284  0.8820125 ]
 [ 0.65905714 -0.47122607]]
PCA Reverse Transform: 0.00022530555725097656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056046074114236996
Fit RMSE: 0.05667028311671059
 Test Against Ground Truth
[(' decentering', 0.0013272762298583984), (' smoothing and normalization', 0.0002949237823486328)]
Smoothing and Normalization: 0.0005435943603515625
Fit RMSE: 0.05596720332590234
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[0.9999999999999999, 0.9999999999999999]
Centering time: 0.001379251480102539
PCA fit: 0.04597067832946777
[[ 9.80392151e-01 -4.89742116e-09]
 [-4.89742101e-09  9.80392121e-01]]
PCA Transform: 0.0015387535095214844
new version
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.07985305786132812
PCA fit: 0.5444214344024658
[[9.80392164e-01 5.40449367e-09]
 [5.40449433e-09 9.80392095e-01]]
PCA Transform: 0.0022521018981933594
total iterations: 200
TLDA fit: 111.39516878128052
Whitened factor: 
[[-0.08396526 -0.08709096]
 [-0.9964687   0.9962004 ]]
PCA Reverse Transform: 0.0025153160095214844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12977371200024454
Fit RMSE: 0.12972707840247874
 Test Against Ground Truth
[(' decentering', 0.003315448760986328), (' smoothing and normalization', 0.0003180503845214844)]
Smoothing and Normalization: 0.0004336833953857422
Fit RMSE: 0.11888641546482873
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999999, 0.9999999999999999]
Centering time: 0.0033190250396728516
PCA fit: 0.27643585205078125
[[9.80392160e-01 8.80912387e-09]
 [8.80912369e-09 9.80392177e-01]]
PCA Transform: 0.006573200225830078
total iterations: 200
TLDA fit: 112.78179121017456
Whitened factor: 
[[-0.9342007   0.8833409 ]
 [-0.35674793  0.46873108]]
PCA Reverse Transform: 0.00022983551025390625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05372354748584811
Fit RMSE: 0.05423185144871047
 Test Against Ground Truth
[(' decentering', 0.0012905597686767578), (' smoothing and normalization', 0.0003325939178466797)]
Smoothing and Normalization: 0.001087188720703125
Fit RMSE: 0.053565110249895895
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.0013461112976074219
PCA fit: 0.04697275161743164
[[9.80392160e-01 3.33337202e-09]
 [3.33337168e-09 9.80392125e-01]]
PCA Transform: 0.0015971660614013672
total iterations: 200
TLDA fit: 114.25039076805115
Whitened factor: 
[[-0.06860995 -0.11377758]
 [-0.9976436   0.99350625]]
PCA Reverse Transform: 0.00023365020751953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1308565915121951
Fit RMSE: 0.1309235385406987
 Test Against Ground Truth
[(' decentering', 0.001264810562133789), (' smoothing and normalization', 0.0002460479736328125)]
Smoothing and Normalization: 0.0004429817199707031
Fit RMSE: 0.12123712310237872
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003870248794555664
PCA fit: 0.22647857666015625
[[ 9.80392152e-01 -1.12058977e-08]
 [-1.12058970e-08  9.80392126e-01]]
PCA Transform: 0.005785703659057617
total iterations: 200
TLDA fit: 113.38577699661255
Whitened factor: 
[[ 0.69447374 -0.7163827 ]
 [-0.7195181   0.6977076 ]]
PCA Reverse Transform: 0.0002224445343017578
decenter with new strategy:
[-4.92845567e-05  7.10963054e-05]
decenter with old strategy:
[-1.03240152e-05  1.10605383e-04]
Fit RMSE new decenter: 0.05639973099942723
Fit RMSE: 0.05711978518730853
 Test Against Ground Truth
[(' decentering', 0.0012717247009277344), (' smoothing and normalization', 0.00028133392333984375)]
Smoothing and Normalization: 0.0005567073822021484
Fit RMSE: 0.05633505928280409
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014803409576416016
PCA fit: 0.047785043716430664
[[9.80392160e-01 2.03842259e-08]
 [2.03842258e-08 9.80392226e-01]]
PCA Transform: 0.0015513896942138672
total iterations: 200
TLDA fit: 115.45024061203003
Whitened factor: 
[[-0.20457187  0.03635943]
 [-0.97885156  0.99933875]]
PCA Reverse Transform: 0.00021600723266601562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12761388081026068
Fit RMSE: 0.13209320041319927
 Test Against Ground Truth
[(' decentering', 0.0012617111206054688), (' smoothing and normalization', 0.0002484321594238281)]
Smoothing and Normalization: 0.0004534721374511719
Fit RMSE: 0.12485269920565538
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.002263784408569336
PCA fit: 0.2877678871154785
[[ 9.80392157e-01 -3.03596033e-09]
 [-3.03596028e-09  9.80392197e-01]]
PCA Transform: 0.00584864616394043
total iterations: 200
TLDA fit: 113.75444960594177
Whitened factor: 
[[ 0.8296968  -0.8662898 ]
 [-0.5582143   0.49954185]]
PCA Reverse Transform: 0.00022792816162109375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056304161478048925
Fit RMSE: 0.056688041683842116
 Test Against Ground Truth
[(' decentering', 0.0012829303741455078), (' smoothing and normalization', 0.0002830028533935547)]
Smoothing and Normalization: 0.0005102157592773438
Fit RMSE: 0.05619867242848064
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.001440286636352539
PCA fit: 0.046811819076538086
[[ 9.80392154e-01 -1.59319788e-08]
 [-1.59319787e-08  9.80392204e-01]]
PCA Transform: 0.0015940666198730469
total iterations: 200
TLDA fit: 114.94155550003052
Whitened factor: 
[[ 0.01270598 -0.12206524]
 [-0.99991924  0.99252206]]
PCA Reverse Transform: 0.00021958351135253906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12736897347446324
Fit RMSE: 0.12656531754269926
 Test Against Ground Truth
[(' decentering', 0.001285552978515625), (' smoothing and normalization', 0.0002498626708984375)]
Smoothing and Normalization: 0.0003910064697265625
Fit RMSE: 0.11475859210256263
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.0037391185760498047
PCA fit: 0.2642707824707031
[[ 9.80392158e-01 -4.66297201e-09]
 [-4.66297279e-09  9.80392127e-01]]
PCA Transform: 0.005774497985839844
total iterations: 200
TLDA fit: 112.794602394104
Whitened factor: 
[[-0.9822409   0.9920671 ]
 [ 0.18762422 -0.12570925]]
PCA Reverse Transform: 0.0002257823944091797
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055873993092978205
Fit RMSE: 0.055973906086106744
 Test Against Ground Truth
[(' decentering', 0.0013535022735595703), (' smoothing and normalization', 0.00028514862060546875)]
Smoothing and Normalization: 0.0005435943603515625
Fit RMSE: 0.0557868492026957
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0014357566833496094
PCA fit: 0.0466616153717041
[[9.80392156e-01 3.71827327e-09]
 [3.71827312e-09 9.80392090e-01]]
PCA Transform: 0.001546621322631836
total iterations: 200
TLDA fit: 116.78728747367859
Whitened factor: 
[[ 0.48686248 -0.51346403]
 [-0.87347865  0.85811114]]
PCA Reverse Transform: 0.0002238750457763672
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11995590154482365
Fit RMSE: 0.1245527396639217
 Test Against Ground Truth
[(' decentering', 0.0012619495391845703), (' smoothing and normalization', 0.00024819374084472656)]
Smoothing and Normalization: 0.0004646778106689453
Fit RMSE: 0.11958191276274607
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003787994384765625
PCA fit: 0.2665736675262451
[[9.80392160e-01 8.37657885e-09]
 [8.37657898e-09 9.80392206e-01]]
PCA Transform: 0.0057637691497802734
total iterations: 200
TLDA fit: 116.53238463401794
Whitened factor: 
[[ 0.81241935 -0.8668293 ]
 [-0.58307356  0.49860513]]
PCA Reverse Transform: 0.00020885467529296875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05637068986699539
Fit RMSE: 0.056813140567083205
 Test Against Ground Truth
[(' decentering', 0.0012841224670410156), (' smoothing and normalization', 0.00028634071350097656)]
Smoothing and Normalization: 0.0005314350128173828
Fit RMSE: 0.056263166977125696
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0015079975128173828
PCA fit: 0.04706454277038574
[[9.80392153e-01 4.28486724e-09]
 [4.28486724e-09 9.80392187e-01]]
PCA Transform: 0.0015625953674316406
total iterations: 200
TLDA fit: 115.60950803756714
Whitened factor: 
[[-0.10401143 -0.07250714]
 [-0.9945761   0.99736786]]
PCA Reverse Transform: 0.00023102760314941406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12454121958700724
Fit RMSE: 0.12686019778721647
 Test Against Ground Truth
[(' decentering', 0.0012786388397216797), (' smoothing and normalization', 0.0002415180206298828)]
Smoothing and Normalization: 0.0004968643188476562
Fit RMSE: 0.11394331247938289
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0037758350372314453
PCA fit: 0.2739989757537842
[[ 9.80392155e-01 -4.87609508e-09]
 [-4.87609542e-09  9.80392126e-01]]
PCA Transform: 0.0057942867279052734
total iterations: 200
TLDA fit: 114.5458779335022
Whitened factor: 
[[-0.95675933  0.92932045]
 [-0.29088056  0.3692742 ]]
PCA Reverse Transform: 0.00021123886108398438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05502733505597998
Fit RMSE: 0.05546117117146576
 Test Against Ground Truth
[(' decentering', 0.0013518333435058594), (' smoothing and normalization', 0.0003032684326171875)]
Smoothing and Normalization: 0.0004680156707763672
Fit RMSE: 0.05489755708108066
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.001367330551147461
PCA fit: 0.04158306121826172
[[ 9.80392158e-01 -6.14829813e-09]
 [-6.14829876e-09  9.80392205e-01]]
PCA Transform: 0.0015380382537841797
total iterations: 200
TLDA fit: 119.3027856349945
Whitened factor: 
[[-0.07140432 -0.1025103 ]
 [-0.9974475   0.99473196]]
PCA Reverse Transform: 0.0005095005035400391
decenter with new strategy:
[ 0.00021571 -0.00012588]
decenter with old strategy:
[ 3.31625532e-04 -6.21402275e-06]
Fit RMSE new decenter: 0.12972295277467719
Fit RMSE: 0.12944988969656507
 Test Against Ground Truth
[(' decentering', 0.0013327598571777344), (' smoothing and normalization', 0.0002498626708984375)]
Smoothing and Normalization: 0.00045418739318847656
Fit RMSE: 0.11925039379661356
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0037364959716796875
PCA fit: 0.25165820121765137
[[ 9.80392153e-01 -1.24119123e-09]
 [-1.24119156e-09  9.80392171e-01]]
PCA Transform: 0.005776882171630859
total iterations: 200
TLDA fit: 114.90155339241028
Whitened factor: 
[[-0.05947192 -0.10652851]
 [-0.99823004  0.9943096 ]]
PCA Reverse Transform: 0.00021839141845703125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05975673525440634
Fit RMSE: 0.05988741561745674
 Test Against Ground Truth
[(' decentering', 0.0013473033905029297), (' smoothing and normalization', 0.00028896331787109375)]
Smoothing and Normalization: 0.0005211830139160156
Fit RMSE: 0.056792622315610164
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0017473697662353516
PCA fit: 0.04642844200134277
[[9.80392149e-01 2.23291490e-08]
 [2.23291492e-08 9.80392190e-01]]
PCA Transform: 0.0016431808471679688
total iterations: 200
TLDA fit: 117.69910097122192
Whitened factor: 
[[-0.87319916  0.9177846 ]
 [ 0.48736358 -0.39707875]]
PCA Reverse Transform: 0.0002422332763671875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11908346193337574
Fit RMSE: 0.1201225177433622
 Test Against Ground Truth
[(' decentering', 0.001272439956665039), (' smoothing and normalization', 0.00024771690368652344)]
Smoothing and Normalization: 0.00045299530029296875
Fit RMSE: 0.11870996066032688
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003587961196899414
PCA fit: 0.2877326011657715
[[ 9.80392157e-01 -1.06644797e-08]
 [-1.06644797e-08  9.80392160e-01]]
PCA Transform: 0.0057811737060546875
total iterations: 200
TLDA fit: 117.38719964027405
Whitened factor: 
[[-0.9989544   0.99462456]
 [-0.04571794  0.10354741]]
PCA Reverse Transform: 0.00024271011352539062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05593037420912568
Fit RMSE: 0.055993363032557975
 Test Against Ground Truth
[(' decentering', 0.0013270378112792969), (' smoothing and normalization', 0.0003070831298828125)]
Smoothing and Normalization: 0.0005438327789306641
Fit RMSE: 0.05582012113295941
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.001615762710571289
PCA fit: 0.04757547378540039
[[ 9.80392157e-01 -1.83229820e-09]
 [-1.83229823e-09  9.80392185e-01]]
PCA Transform: 0.0015559196472167969
total iterations: 200
TLDA fit: 118.5328779220581
Whitened factor: 
[[-0.14900678  0.26961586]
 [ 0.98883617 -0.962968  ]]
PCA Reverse Transform: 0.0002155303955078125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12221823540319433
Fit RMSE: 0.12698078748214814
 Test Against Ground Truth
[(' decentering', 0.0012776851654052734), (' smoothing and normalization', 0.0002493858337402344)]
Smoothing and Normalization: 0.00047969818115234375
Fit RMSE: 0.11880657238483534
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.003729581832885742
PCA fit: 0.2761211395263672
[[ 9.80392156e-01 -7.34884575e-09]
 [-7.34884652e-09  9.80392190e-01]]
PCA Transform: 0.005793333053588867
total iterations: 200
TLDA fit: 115.2367889881134
Whitened factor: 
[[-0.8726834   0.92230487]
 [ 0.48828638 -0.38646322]]
PCA Reverse Transform: 0.00021123886108398438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05649374998412735
Fit RMSE: 0.056849239989379845
 Test Against Ground Truth
[(' decentering', 0.0012760162353515625), (' smoothing and normalization', 0.00038433074951171875)]
Smoothing and Normalization: 0.0005486011505126953
Fit RMSE: 0.05640747840586031
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0017101764678955078
PCA fit: 0.046630144119262695
[[ 9.80392163e-01 -5.47427992e-09]
 [-5.47427959e-09  9.80392185e-01]]
PCA Transform: 0.001556396484375
total iterations: 200
TLDA fit: 119.21082592010498
Whitened factor: 
[[-0.26194966  0.30070186]
 [ 0.9650816  -0.9537181 ]]
PCA Reverse Transform: 0.0005011558532714844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12352229164161538
Fit RMSE: 0.1262558070422891
 Test Against Ground Truth
[(' decentering', 0.0012862682342529297), (' smoothing and normalization', 0.0002467632293701172)]
Smoothing and Normalization: 0.0004680156707763672
Fit RMSE: 0.11786970482561969
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.0019812583923339844
PCA fit: 0.2544867992401123
[[9.80392160e-01 8.25763309e-09]
 [8.25763258e-09 9.80392113e-01]]
PCA Transform: 0.005755901336669922
total iterations: 200
TLDA fit: 115.80529069900513
Whitened factor: 
[[-0.8911182   0.8478583 ]
 [-0.4537711   0.53022283]]
PCA Reverse Transform: 0.0002117156982421875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.055900024042796474
Fit RMSE: 0.05631269470184024
 Test Against Ground Truth
[(' decentering', 0.0012717247009277344), (' smoothing and normalization', 0.0002899169921875)]
Smoothing and Normalization: 0.0005207061767578125
Fit RMSE: 0.05581752729696775
sklearn Test Against Ground Truth
Done!
new version
Vocab: 100
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.07903051376342773
PCA fit: 0.6065220832824707
[[ 9.80392178e-01 -3.51377446e-07]
 [-3.51377446e-07  9.80400770e-01]]
PCA Transform: 0.0059587955474853516
total iterations: 200
TLDA fit: 123.48110032081604
Whitened factor: 
[[ 0.04502943 -0.16199479]
 [-0.99898565  0.9867916 ]]
PCA Reverse Transform: 0.0002155303955078125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12713465898941842
Fit RMSE: 0.129145756598885
 Test Against Ground Truth
[(' decentering', 0.0038847923278808594), (' smoothing and normalization', 0.00024771690368652344)]
Smoothing and Normalization: 0.0004527568817138672
Fit RMSE: 0.1196724568557064
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.007893085479736328
PCA fit: 0.5511438846588135
[[9.80393152e-01 2.27910363e-06]
 [2.27910363e-06 9.80425180e-01]]
PCA Transform: 0.026714086532592773
total iterations: 200
TLDA fit: 124.38247346878052
Whitened factor: 
[[-0.94736105  0.90520984]
 [-0.3201673   0.42496482]]
PCA Reverse Transform: 0.0003178119659423828
decenter with new strategy:
[-2.78136722e-06  9.53590110e-06]
decenter with old strategy:
[1.30818391e-05 2.50677002e-05]
Fit RMSE new decenter: 0.054187128397195465
Fit RMSE: 0.05473455521089236
 Test Against Ground Truth
[(' decentering', 0.0019376277923583984), (' smoothing and normalization', 0.0002982616424560547)]
Smoothing and Normalization: 0.0011563301086425781
Fit RMSE: 0.05410921560689548
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.015007257461547852
PCA fit: 1.9399094581604004
[[9.80392383e-01 2.02088436e-06]
 [2.02088436e-06 9.80469662e-01]]
PCA Transform: 0.0523686408996582
total iterations: 200
TLDA fit: 123.60828518867493
Whitened factor: 
[[-0.01099936  0.17873262]
 [ 0.9999395  -0.9838977 ]]
PCA Reverse Transform: 0.00022220611572265625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04250257400285163
Fit RMSE: 0.042688856460123895
 Test Against Ground Truth
[(' decentering', 0.0020079612731933594), (' smoothing and normalization', 0.00028634071350097656)]
Smoothing and Normalization: 0.000579833984375
Fit RMSE: 0.040875214304803134
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.003969430923461914
PCA fit: 0.12205219268798828
[[ 9.80392153e-01 -4.70711902e-08]
 [-4.70711913e-08  9.80396803e-01]]
PCA Transform: 0.005354404449462891
total iterations: 200
TLDA fit: 127.03858947753906
Whitened factor: 
[[ 0.03814628 -0.17403618]
 [-0.9992722   0.9847393 ]]
PCA Reverse Transform: 0.00021600723266601562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1286743950234851
Fit RMSE: 0.13095613871258824
 Test Against Ground Truth
[(' decentering', 0.0019054412841796875), (' smoothing and normalization', 0.0002605915069580078)]
Smoothing and Normalization: 0.00047779083251953125
Fit RMSE: 0.12240381180346774
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.008108854293823242
PCA fit: 0.6198348999023438
[[ 9.80392361e-01 -6.62739107e-07]
 [-6.62739107e-07  9.80409764e-01]]
PCA Transform: 0.026683807373046875
total iterations: 200
TLDA fit: 126.5745689868927
Whitened factor: 
[[-0.07903543 -0.07849101]
 [-0.99687177  0.99691486]]
PCA Reverse Transform: 0.00022363662719726562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.059453275224925946
Fit RMSE: 0.05944616602456332
 Test Against Ground Truth
[(' decentering', 0.0019497871398925781), (' smoothing and normalization', 0.0003075599670410156)]
Smoothing and Normalization: 0.0011646747589111328
Fit RMSE: 0.056043338455281776
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.015405416488647461
PCA fit: 1.9487652778625488
[[ 9.80392976e-01 -4.23943045e-06]
 [-4.23943045e-06  9.80654629e-01]]
PCA Transform: 0.05237388610839844
total iterations: 200
TLDA fit: 126.37226796150208
Whitened factor: 
[[-0.9324343   0.87911546]
 [-0.3613394   0.47660878]]
PCA Reverse Transform: 0.00021767616271972656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03903289652999062
Fit RMSE: 0.03940465999031815
 Test Against Ground Truth
[(' decentering', 0.002010822296142578), (' smoothing and normalization', 0.00029468536376953125)]
Smoothing and Normalization: 0.0005326271057128906
Fit RMSE: 0.038966364541912865
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.0032777786254882812
PCA fit: 0.12185239791870117
[[ 9.80392275e-01 -2.96641103e-08]
 [-2.96641105e-08  9.80392288e-01]]
PCA Transform: 0.005336284637451172
total iterations: 200
TLDA fit: 129.20733523368835
Whitened factor: 
[[-0.9025998  0.8519046]
 [-0.4304805  0.5236969]]
PCA Reverse Transform: 0.0002143383026123047
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11760466726926845
Fit RMSE: 0.1189482922237844
 Test Against Ground Truth
[(' decentering', 0.0019125938415527344), (' smoothing and normalization', 0.00024247169494628906)]
Smoothing and Normalization: 0.00046324729919433594
Fit RMSE: 0.11752207186062985
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.007928848266601562
PCA fit: 0.6442880630493164
[[9.80393079e-01 7.75117198e-06]
 [7.75117198e-06 9.80547268e-01]]
PCA Transform: 0.026464223861694336
total iterations: 200
TLDA fit: 126.74567985534668
Whitened factor: 
[[-0.9047407   0.8482391 ]
 [-0.42596284  0.5296134 ]]
PCA Reverse Transform: 0.00022292137145996094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05517956739934054
Fit RMSE: 0.055689026685177494
 Test Against Ground Truth
[(' decentering', 0.002012014389038086), (' smoothing and normalization', 0.0002841949462890625)]
Smoothing and Normalization: 0.0005245208740234375
Fit RMSE: 0.05506501467321397
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.015242815017700195
PCA fit: 1.4150469303131104
[[ 9.80392811e-01 -2.40898198e-06]
 [-2.40898198e-06  9.80470210e-01]]
PCA Transform: 0.05235552787780762
total iterations: 200
TLDA fit: 124.50693941116333
Whitened factor: 
[[ 0.816698   -0.86561733]
 [-0.5770654   0.5007062 ]]
PCA Reverse Transform: 0.0002472400665283203
decenter with new strategy:
[ 0.0002889 -0.0001758]
decenter with old strategy:
[5.05866258e-04 4.68560921e-05]
Fit RMSE new decenter: 0.03992173673457776
Fit RMSE: 0.04021457100561124
 Test Against Ground Truth
[(' decentering', 0.002014636993408203), (' smoothing and normalization', 0.0003192424774169922)]
Smoothing and Normalization: 0.0005369186401367188
Fit RMSE: 0.039863020582631664
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.003607511520385742
PCA fit: 0.1263744831085205
[[9.80392171e-01 4.77579983e-07]
 [4.77579983e-07 9.80410349e-01]]
PCA Transform: 0.005280971527099609
total iterations: 200
TLDA fit: 126.8036699295044
Whitened factor: 
[[-0.90313923  0.94189954]
 [ 0.4293478  -0.33589482]]
PCA Reverse Transform: 0.0002167224884033203
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1211273627834914
Fit RMSE: 0.1218810615643214
 Test Against Ground Truth
[(' decentering', 0.001901865005493164), (' smoothing and normalization', 0.00025200843811035156)]
Smoothing and Normalization: 0.000476837158203125
Fit RMSE: 0.1207001395922331
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.006702423095703125
PCA fit: 0.6377050876617432
[[ 9.80392284e-01 -1.13183666e-06]
 [-1.13183666e-06  9.80453444e-01]]
PCA Transform: 0.02639007568359375
total iterations: 200
TLDA fit: 127.16636633872986
Whitened factor: 
[[ 0.8003694  -0.8446511 ]
 [-0.5995072   0.53531724]]
PCA Reverse Transform: 0.00022530555725097656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056406246502294165
Fit RMSE: 0.05682685463591524
 Test Against Ground Truth
[(' decentering', 0.0019428730010986328), (' smoothing and normalization', 0.00028967857360839844)]
Smoothing and Normalization: 0.0005159378051757812
Fit RMSE: 0.05628710621135112
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[1.0, 0.9999999999999998]
Centering time: 0.00957036018371582
PCA fit: 1.3950061798095703
[[9.80392598e-01 1.31371157e-06]
 [1.31371157e-06 9.80447274e-01]]
PCA Transform: 0.05232858657836914
total iterations: 200
TLDA fit: 127.6057710647583
Whitened factor: 
[[ 0.01342067 -0.1085601 ]
 [-0.99990994  0.99408996]]
PCA Reverse Transform: 0.0002560615539550781
decenter with new strategy:
[0.00739217 0.00729385]
decenter with old strategy:
[0.02950694 0.02981836]
Fit RMSE new decenter: 0.04135997226378057
Fit RMSE: 0.041909163044585515
 Test Against Ground Truth
[(' decentering', 0.002023935317993164), (' smoothing and normalization', 0.0002925395965576172)]
Smoothing and Normalization: 0.0005185604095458984
Fit RMSE: 0.03976250697451498
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.0033996105194091797
PCA fit: 0.12545299530029297
[[ 9.80392164e-01 -4.62773617e-08]
 [-4.62773615e-08  9.80392923e-01]]
PCA Transform: 0.005346059799194336
total iterations: 200
TLDA fit: 130.69231534004211
Whitened factor: 
[[-0.13505036 -0.02670334]
 [-0.99083877  0.99964345]]
PCA Reverse Transform: 0.0002219676971435547
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12896592036564758
Fit RMSE: 0.13102564225388702
 Test Against Ground Truth
[(' decentering', 0.0019211769104003906), (' smoothing and normalization', 0.0002675056457519531)]
Smoothing and Normalization: 0.00046133995056152344
Fit RMSE: 0.12173403339303505
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.0056362152099609375
PCA fit: 0.5421772003173828
[[9.80392380e-01 2.36657594e-06]
 [2.36657594e-06 9.80619963e-01]]
PCA Transform: 0.026532888412475586
total iterations: 200
TLDA fit: 127.68218851089478
Whitened factor: 
[[ 0.6997515 -0.7326169]
 [-0.7143863  0.6806412]]
PCA Reverse Transform: 0.00022745132446289062
decenter with new strategy:
[ 0.00029696 -0.00018999]
decenter with old strategy:
[4.98875264e-04 1.62226930e-05]
Fit RMSE new decenter: 0.05568319333607366
Fit RMSE: 0.056398874793024786
 Test Against Ground Truth
[(' decentering', 0.0019443035125732422), (' smoothing and normalization', 0.0002949237823486328)]
Smoothing and Normalization: 0.0005033016204833984
Fit RMSE: 0.05559267138957955
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.0063381195068359375
PCA fit: 1.9508473873138428
[[ 9.80392364e-01 -3.46584058e-07]
 [-3.46584058e-07  9.80404802e-01]]
PCA Transform: 0.052484989166259766
total iterations: 200
TLDA fit: 126.55248188972473
Whitened factor: 
[[-0.8038761  0.7393659]
 [-0.5947968  0.673304 ]]
PCA Reverse Transform: 0.0002186298370361328
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04020119473840009
Fit RMSE: 0.0404988268267565
 Test Against Ground Truth
[(' decentering', 0.002008676528930664), (' smoothing and normalization', 0.00029468536376953125)]
Smoothing and Normalization: 0.0005371570587158203
Fit RMSE: 0.040154070390758086
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.003437519073486328
PCA fit: 0.11610031127929688
[[ 9.80392190e-01 -1.35430394e-07]
 [-1.35430394e-07  9.80398330e-01]]
PCA Transform: 0.005384922027587891
total iterations: 200
TLDA fit: 128.27862405776978
Whitened factor: 
[[-0.8936045   0.8423895 ]
 [-0.44885516  0.5388691 ]]
PCA Reverse Transform: 0.0002193450927734375
decenter with new strategy:
[ 2.30011436 -1.30114223]
decenter with old strategy:
[3.95887359 0.31084209]
Fit RMSE new decenter: 0.11720614475010847
Fit RMSE: 0.11814031847648816
 Test Against Ground Truth
[(' decentering', 0.001888275146484375), (' smoothing and normalization', 0.0002903938293457031)]
Smoothing and Normalization: 0.00046181678771972656
Fit RMSE: 0.11676442268403596
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.0034096240997314453
PCA fit: 0.6591541767120361
[[ 9.80392540e-01 -1.01123462e-06]
 [-1.01123462e-06  9.80495684e-01]]
PCA Transform: 0.02642202377319336
total iterations: 200
TLDA fit: 126.47773718833923
Whitened factor: 
[[-0.94136727  0.9095143 ]
 [-0.33738366  0.4156725 ]]
PCA Reverse Transform: 0.00021791458129882812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05535704676462046
Fit RMSE: 0.05576844463118431
 Test Against Ground Truth
[(' decentering', 0.0019443035125732422), (' smoothing and normalization', 0.00029397010803222656)]
Smoothing and Normalization: 0.001089334487915039
Fit RMSE: 0.05524192599424533
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.013949155807495117
PCA fit: 1.9433166980743408
[[ 9.80393198e-01 -1.91856718e-06]
 [-1.91856718e-06  9.80457766e-01]]
PCA Transform: 0.0524141788482666
total iterations: 200
TLDA fit: 126.68068861961365
Whitened factor: 
[[-0.9994499  0.999224 ]
 [ 0.0331655  0.0393888]]
PCA Reverse Transform: 0.000217437744140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03997717308195064
Fit RMSE: 0.04003820222830002
 Test Against Ground Truth
[(' decentering', 0.002035379409790039), (' smoothing and normalization', 0.0002944469451904297)]
Smoothing and Normalization: 0.0005180835723876953
Fit RMSE: 0.03991891118481587
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.0036356449127197266
PCA fit: 0.1157228946685791
[[ 9.80392167e-01 -8.13572252e-08]
 [-8.13572251e-08  9.80404195e-01]]
PCA Transform: 0.005338430404663086
total iterations: 200
TLDA fit: 128.32946109771729
Whitened factor: 
[[-0.9049086   0.94276315]
 [ 0.42560583 -0.33346316]]
PCA Reverse Transform: 0.00022363662719726562
decenter with new strategy:
[-9.28613141e-05  1.57353019e-04]
decenter with old strategy:
[1.58536771e-05 2.68404350e-04]
Fit RMSE new decenter: 0.12066497637620364
Fit RMSE: 0.12149212301694078
 Test Against Ground Truth
[(' decentering', 0.0019354820251464844), (' smoothing and normalization', 0.0002453327178955078)]
Smoothing and Normalization: 0.0004711151123046875
Fit RMSE: 0.12039491599189929
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.004575252532958984
PCA fit: 0.6570131778717041
[[ 9.80392201e-01 -4.97519491e-08]
 [-4.97519490e-08  9.80437257e-01]]
PCA Transform: 0.026546239852905273
total iterations: 200
TLDA fit: 127.61675930023193
Whitened factor: 
[[-0.8415368   0.9016893 ]
 [ 0.5401998  -0.43238458]]
PCA Reverse Transform: 0.00024080276489257812
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.057605616750624984
Fit RMSE: 0.05795153141953008
 Test Against Ground Truth
[(' decentering', 0.0020515918731689453), (' smoothing and normalization', 0.00030803680419921875)]
Smoothing and Normalization: 0.0011222362518310547
Fit RMSE: 0.05750066124219557
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.013715505599975586
PCA fit: 1.961651086807251
[[9.80392406e-01 2.35735491e-07]
 [2.35735491e-07 9.80447464e-01]]
PCA Transform: 0.052446603775024414
total iterations: 200
TLDA fit: 125.87319302558899
Whitened factor: 
[[-0.9970103   0.9999857 ]
 [ 0.07726789  0.00535639]]
PCA Reverse Transform: 0.00022411346435546875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0408501181106577
Fit RMSE: 0.04090531120256178
 Test Against Ground Truth
[(' decentering', 0.0020203590393066406), (' smoothing and normalization', 0.0002942085266113281)]
Smoothing and Normalization: 0.0005133152008056641
Fit RMSE: 0.040784407386190887
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.0031676292419433594
PCA fit: 0.12358856201171875
[[ 9.80392217e-01 -6.01160187e-07]
 [-6.01160188e-07  9.80405101e-01]]
PCA Transform: 0.005316495895385742
total iterations: 200
TLDA fit: 125.66523885726929
Whitened factor: 
[[ 0.10794964 -0.1684559 ]
 [-0.9941564   0.9857092 ]]
PCA Reverse Transform: 0.00022459030151367188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12630778234824883
Fit RMSE: 0.12742507292995547
 Test Against Ground Truth
[(' decentering', 0.0019025802612304688), (' smoothing and normalization', 0.0002543926239013672)]
Smoothing and Normalization: 0.0004634857177734375
Fit RMSE: 0.11812026287315232
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.005877017974853516
PCA fit: 0.529975175857544
[[9.80392250e-01 1.41628595e-08]
 [1.41628596e-08 9.80432980e-01]]
PCA Transform: 0.026445627212524414
total iterations: 200
TLDA fit: 126.58440637588501
Whitened factor: 
[[-0.9809013   0.9651341 ]
 [-0.19450627  0.26175615]]
PCA Reverse Transform: 0.00022411346435546875
decenter with new strategy:
[-0.00455068  0.00748611]
decenter with old strategy:
[0.00123098 0.0132224 ]
Fit RMSE new decenter: 0.055694115114432265
Fit RMSE: 0.05592898222161688
 Test Against Ground Truth
[(' decentering', 0.0019373893737792969), (' smoothing and normalization', 0.00033020973205566406)]
Smoothing and Normalization: 0.0005488395690917969
Fit RMSE: 0.05563347315545321
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[1.0000000000000002, 0.9999999999999999]
Centering time: 0.013565301895141602
PCA fit: 1.9708232879638672
[[9.80392696e-01 1.66218415e-06]
 [1.66218415e-06 9.80422022e-01]]
PCA Transform: 0.05230832099914551
total iterations: 200
TLDA fit: 125.20372176170349
Whitened factor: 
[[-9.9999970e-01  9.9838775e-01]
 [-7.1909092e-04  5.6761276e-02]]
PCA Reverse Transform: 0.00021767616271972656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.040877091795552996
Fit RMSE: 0.04091193030733798
 Test Against Ground Truth
[(' decentering', 0.0020248889923095703), (' smoothing and normalization', 0.0002853870391845703)]
Smoothing and Normalization: 0.0009925365447998047
Fit RMSE: 0.04081523615586459
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.0036013126373291016
PCA fit: 0.11988401412963867
[[9.80392168e-01 1.20307744e-07]
 [1.20307744e-07 9.80400894e-01]]
PCA Transform: 0.0053708553314208984
total iterations: 200
TLDA fit: 126.97029638290405
Whitened factor: 
[[-0.05512946 -0.11343337]
 [-0.9984792   0.99354565]]
PCA Reverse Transform: 0.0002231597900390625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.13019168591539618
Fit RMSE: 0.13071407660281473
 Test Against Ground Truth
[(' decentering', 0.0019125938415527344), (' smoothing and normalization', 0.00027251243591308594)]
Smoothing and Normalization: 0.0004723072052001953
Fit RMSE: 0.12058683538329527
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.006798744201660156
PCA fit: 0.6598763465881348
[[ 9.80392349e-01 -1.06375163e-06]
 [-1.06375163e-06  9.80435087e-01]]
PCA Transform: 0.02639150619506836
total iterations: 200
TLDA fit: 125.58754539489746
Whitened factor: 
[[-0.98173887  0.95465565]
 [-0.1902337   0.29771206]]
PCA Reverse Transform: 0.0002193450927734375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05547996266075898
Fit RMSE: 0.055870404442798685
 Test Against Ground Truth
[(' decentering', 0.0019497871398925781), (' smoothing and normalization', 0.0002911090850830078)]
Smoothing and Normalization: 0.001140594482421875
Fit RMSE: 0.05543722417812209
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.013707637786865234
PCA fit: 1.9450891017913818
[[9.80392957e-01 2.27871914e-06]
 [2.27871914e-06 9.80464758e-01]]
PCA Transform: 0.052367448806762695
total iterations: 200
TLDA fit: 126.76297378540039
Whitened factor: 
[[-0.8500897   0.90167594]
 [ 0.52663803 -0.43241242]]
PCA Reverse Transform: 0.00021648406982421875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03980681642495772
Fit RMSE: 0.04010790183604941
 Test Against Ground Truth
[(' decentering', 0.002022266387939453), (' smoothing and normalization', 0.00028967857360839844)]
Smoothing and Normalization: 0.0005223751068115234
Fit RMSE: 0.03974989803653402
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[1.0, 1.0]
[1.0, 1.0]
Centering time: 0.003210783004760742
PCA fit: 0.11832165718078613
[[9.80392323e-01 2.82470157e-07]
 [2.82470157e-07 9.80400686e-01]]
PCA Transform: 0.00535893440246582
total iterations: 200
TLDA fit: 125.87599539756775
Whitened factor: 
[[ 0.90890205 -0.9462831 ]
 [-0.41700965  0.32333928]]
PCA Reverse Transform: 0.0002200603485107422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.11532009639892911
Fit RMSE: 0.11630974833547054
 Test Against Ground Truth
[(' decentering', 0.001989603042602539), (' smoothing and normalization', 0.00025725364685058594)]
Smoothing and Normalization: 0.00046372413635253906
Fit RMSE: 0.1149518248996147
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.003497600555419922
PCA fit: 0.6430749893188477
[[9.80392250e-01 3.42481201e-07]
 [3.42481201e-07 9.80490207e-01]]
PCA Transform: 0.026462554931640625
total iterations: 11
TLDA fit: 6.5781824588775635
Whitened factor: 
[[-0.9468167   0.96772945]
 [ 0.32177347 -0.25199145]]
PCA Reverse Transform: 0.0002269744873046875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056945298686042256
Fit RMSE: 0.0570791224875101
 Test Against Ground Truth
[(' decentering', 0.001968860626220703), (' smoothing and normalization', 0.0002892017364501953)]
Smoothing and Normalization: 0.0005385875701904297
Fit RMSE: 0.05684634649254227
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[1.0000000000000002, 0.9999999999999998]
Centering time: 0.01374506950378418
PCA fit: 1.9887263774871826
[[9.80393751e-01 7.47808596e-07]
 [7.47808596e-07 9.80480767e-01]]
PCA Transform: 0.05245184898376465
total iterations: 200
TLDA fit: 126.89267015457153
Whitened factor: 
[[ 0.8448645  -0.88931876]
 [-0.53498036  0.45728794]]
PCA Reverse Transform: 0.00022029876708984375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03955447096576366
Fit RMSE: 0.03984821392801818
 Test Against Ground Truth
[(' decentering', 0.0020248889923095703), (' smoothing and normalization', 0.00029969215393066406)]
Smoothing and Normalization: 0.0005338191986083984
Fit RMSE: 0.03948128212408019
sklearn Test Against Ground Truth
Done!
new version
Vocab: 100
num_tweets: 50000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.08120417594909668
PCA fit: 0.612144947052002
[[ 9.80392187e-01 -2.68459457e-08]
 [-2.68459458e-08  9.80393349e-01]]
PCA Transform: 0.005975246429443359
total iterations: 1000
TLDA fit: 629.9690926074982
Whitened factor: 
[[-0.28950474 -0.22519858]
 [-0.95717657  0.9743129 ]]
PCA Reverse Transform: 0.0002269744873046875
decenter with new strategy:
[0.06720131 0.08001016]
decenter with old strategy:
[0.19833921 0.20329523]
Fit RMSE new decenter: 0.12786786353764446
Fit RMSE: 0.1293002257218236
 Test Against Ground Truth
[(' decentering', 0.0037910938262939453), (' smoothing and normalization', 0.00029754638671875)]
Smoothing and Normalization: 0.0004661083221435547
Fit RMSE: 0.11932266149854966
sklearn Test Against Ground Truth
Centering time: 0.0031342506408691406
PCA fit: 0.11101746559143066
[[ 9.80392187e-01 -2.68459457e-08]
 [-2.68459458e-08  9.80393349e-01]]
PCA Transform: 0.005316972732543945
total iterations: 1000
TLDA fit: 644.3447265625
Whitened factor: 
[[-0.11047918 -0.0499295 ]
 [-0.9938784   0.9987528 ]]
PCA Reverse Transform: 0.00023031234741210938
decenter with new strategy:
[0.04194392 0.05190591]
decenter with old strategy:
[0.17080934 0.17680386]
Fit RMSE new decenter: 0.12835195107322422
Fit RMSE: 0.1293493828747026
 Test Against Ground Truth
[(' decentering', 0.0019576549530029297), (' smoothing and normalization', 0.00025844573974609375)]
Smoothing and Normalization: 0.00045490264892578125
Fit RMSE: 0.11932266149854966
sklearn Test Against Ground Truth
Centering time: 0.0031316280364990234
PCA fit: 0.11150050163269043
[[ 9.80392187e-01 -2.68459457e-08]
 [-2.68459458e-08  9.80393349e-01]]
PCA Transform: 0.0053212642669677734
total iterations: 1000
TLDA fit: 645.8250408172607
Whitened factor: 
[[-0.07330698 -0.01357522]
 [-0.99730945  0.9999078 ]]
PCA Reverse Transform: 0.00023031234741210938
decenter with new strategy:
[0.03719758 0.04559699]
decenter with old strategy:
[0.16512513 0.17127915]
Fit RMSE new decenter: 0.12870754144467517
Fit RMSE: 0.1293575999336153
 Test Against Ground Truth
[(' decentering', 0.0019233226776123047), (' smoothing and normalization', 0.00026154518127441406)]
Smoothing and Normalization: 0.0004506111145019531
Fit RMSE: 0.11932266149854966
sklearn Test Against Ground Truth
Centering time: 0.003152608871459961
PCA fit: 0.1115577220916748
[[ 9.80392187e-01 -2.68459457e-08]
 [-2.68459458e-08  9.80393349e-01]]
PCA Transform: 0.005311250686645508
total iterations: 32
TLDA fit: 20.176833391189575
Whitened factor: 
[[-0.7771648   0.9309459 ]
 [ 0.62929714 -0.36515725]]
PCA Reverse Transform: 0.00021219253540039062
decenter with new strategy:
[ 0.16252906 -0.11602513]
decenter with old strategy:
[0.284677   0.01709211]
Fit RMSE new decenter: 0.11948773913343846
Fit RMSE: 0.12071171061426013
 Test Against Ground Truth
[(' decentering', 0.0019059181213378906), (' smoothing and normalization', 0.0002453327178955078)]
Smoothing and Normalization: 0.00047278404235839844
Fit RMSE: 0.11932266149854966
sklearn Test Against Ground Truth
Centering time: 0.0031652450561523438
PCA fit: 0.11154866218566895
[[ 9.80392187e-01 -2.68459457e-08]
 [-2.68459458e-08  9.80393349e-01]]
PCA Transform: 0.00542140007019043
total iterations: 32
TLDA fit: 19.83918571472168
Whitened factor: 
[[-0.8614576   0.90854025]
 [ 0.50782967 -0.41779742]]
PCA Reverse Transform: 0.0005071163177490234
decenter with new strategy:
[ 0.1714104 -0.1084291]
decenter with old strategy:
[0.29658008 0.02010067]
Fit RMSE new decenter: 0.11952620073316134
Fit RMSE: 0.1204468011007446
 Test Against Ground Truth
[(' decentering', 0.0019085407257080078), (' smoothing and normalization', 0.0002474784851074219)]
Smoothing and Normalization: 0.00048041343688964844
Fit RMSE: 0.11932266149854966
sklearn Test Against Ground Truth
Centering time: 0.0031642913818359375
PCA fit: 0.11003398895263672
[[ 9.80392187e-01 -2.68459457e-08]
 [-2.68459458e-08  9.80393349e-01]]
PCA Transform: 0.005343914031982422
total iterations: 11
TLDA fit: 6.536360740661621
Whitened factor: 
[[-0.7852569  0.4336845]
 [ 0.6191702  0.9010648]]
PCA Reverse Transform: 0.00021529197692871094
decenter with new strategy:
[ 0.14073396 -0.01606385]
decenter with old strategy:
[0.28583141 0.10244667]
Fit RMSE new decenter: 0.12031377564343738
Fit RMSE: 0.12225439347087622
 Test Against Ground Truth
[(' decentering', 0.0019295215606689453), (' smoothing and normalization', 0.0002484321594238281)]
Smoothing and Normalization: 0.0004734992980957031
Fit RMSE: 0.11932266149854966
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.008023738861083984
PCA fit: 0.672478199005127
[[ 9.80393309e-01 -8.73140012e-07]
 [-8.73140012e-07  9.80469269e-01]]
PCA Transform: 0.02651357650756836
total iterations: 1000
TLDA fit: 640.4874577522278
Whitened factor: 
[[-0.98590475  0.9130045 ]
 [-0.16730784  0.40794945]]
PCA Reverse Transform: 0.00025343894958496094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05482121840863524
Fit RMSE: 0.05520028270227312
 Test Against Ground Truth
[(' decentering', 0.0020999908447265625), (' smoothing and normalization', 0.00035262107849121094)]
Smoothing and Normalization: 0.0005128383636474609
Fit RMSE: 0.05481850050186028
sklearn Test Against Ground Truth
Centering time: 0.007957220077514648
PCA fit: 0.6362264156341553
[[ 9.80393309e-01 -8.73140012e-07]
 [-8.73140012e-07  9.80469269e-01]]
PCA Transform: 0.02655816078186035
total iterations: 1000
TLDA fit: 637.7383165359497
Whitened factor: 
[[-0.9683364   0.9438743 ]
 [-0.24964911  0.33030498]]
PCA Reverse Transform: 0.00024199485778808594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0548135622380795
Fit RMSE: 0.05517922489258583
 Test Against Ground Truth
[(' decentering', 0.0020270347595214844), (' smoothing and normalization', 0.0002853870391845703)]
Smoothing and Normalization: 0.001165151596069336
Fit RMSE: 0.05481850050186028
sklearn Test Against Ground Truth
Centering time: 0.00801849365234375
PCA fit: 0.6431787014007568
[[ 9.80393309e-01 -8.73140012e-07]
 [-8.73140012e-07  9.80469269e-01]]
PCA Transform: 0.026485443115234375
total iterations: 1000
TLDA fit: 634.0156099796295
Whitened factor: 
[[-0.96365756  0.95029575]
 [-0.26714063  0.3113486 ]]
PCA Reverse Transform: 0.00023365020751953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05481155001654448
Fit RMSE: 0.055174775800717314
 Test Against Ground Truth
[(' decentering', 0.0019664764404296875), (' smoothing and normalization', 0.00028395652770996094)]
Smoothing and Normalization: 0.0005280971527099609
Fit RMSE: 0.05481850050186028
sklearn Test Against Ground Truth
Centering time: 0.007987022399902344
PCA fit: 0.649012565612793
[[ 9.80393309e-01 -8.73140012e-07]
 [-8.73140012e-07  9.80469269e-01]]
PCA Transform: 0.026499509811401367
total iterations: 1000
TLDA fit: 637.8470237255096
Whitened factor: 
[[-0.9857732   0.9126068 ]
 [-0.16808069  0.40883866]]
PCA Reverse Transform: 0.0002493858337402344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05482123055300534
Fit RMSE: 0.05520133723685198
 Test Against Ground Truth
[(' decentering', 0.0019948482513427734), (' smoothing and normalization', 0.0003185272216796875)]
Smoothing and Normalization: 0.0005261898040771484
Fit RMSE: 0.05481850050186028
sklearn Test Against Ground Truth
Centering time: 0.007886409759521484
PCA fit: 0.6598451137542725
[[ 9.80393309e-01 -8.73140012e-07]
 [-8.73140012e-07  9.80469269e-01]]
PCA Transform: 0.026469707489013672
total iterations: 1000
TLDA fit: 638.1731803417206
Whitened factor: 
[[-0.9678944   0.9436318 ]
 [-0.2513574   0.33099696]]
PCA Reverse Transform: 0.00023674964904785156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05481349583414042
Fit RMSE: 0.05518055446249046
 Test Against Ground Truth
[(' decentering', 0.001979351043701172), (' smoothing and normalization', 0.0002887248992919922)]
Smoothing and Normalization: 0.0005357265472412109
Fit RMSE: 0.05481850050186028
sklearn Test Against Ground Truth
Centering time: 0.007933855056762695
PCA fit: 0.6302716732025146
[[ 9.80393309e-01 -8.73140012e-07]
 [-8.73140012e-07  9.80469269e-01]]
PCA Transform: 0.026514053344726562
total iterations: 1000
TLDA fit: 639.8373522758484
Whitened factor: 
[[-0.96295255  0.94975454]
 [-0.26967067  0.3129957 ]]
PCA Reverse Transform: 0.0002510547637939453
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05481146871234478
Fit RMSE: 0.05517717645878281
 Test Against Ground Truth
[(' decentering', 0.0020046234130859375), (' smoothing and normalization', 0.0003018379211425781)]
Smoothing and Normalization: 0.0005190372467041016
Fit RMSE: 0.05481850050186028
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.014856338500976562
PCA fit: 1.9767165184020996
[[9.80394013e-01 4.71171541e-06]
 [4.71171541e-06 9.80760470e-01]]
PCA Transform: 0.052405357360839844
total iterations: 1000
TLDA fit: 636.5702760219574
Whitened factor: 
[[-0.7738579   0.94894177]
 [ 0.63335913 -0.31545117]]
PCA Reverse Transform: 0.0002560615539550781
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03921552407855953
Fit RMSE: 0.039621228975511485
 Test Against Ground Truth
[(' decentering', 0.002020120620727539), (' smoothing and normalization', 0.00029349327087402344)]
Smoothing and Normalization: 0.0005347728729248047
Fit RMSE: 0.039195195852899344
sklearn Test Against Ground Truth
Centering time: 0.015059232711791992
PCA fit: 1.940190315246582
[[9.80394013e-01 4.71171541e-06]
 [4.71171541e-06 9.80760470e-01]]
PCA Transform: 0.05225944519042969
total iterations: 1000
TLDA fit: 636.1554033756256
Whitened factor: 
[[-0.846055    0.9010636 ]
 [ 0.53309554 -0.43368688]]
PCA Reverse Transform: 0.00022673606872558594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.039235895597887024
Fit RMSE: 0.03959763746311341
 Test Against Ground Truth
[(' decentering', 0.0020248889923095703), (' smoothing and normalization', 0.0002911090850830078)]
Smoothing and Normalization: 0.0005190372467041016
Fit RMSE: 0.039195195852899344
sklearn Test Against Ground Truth
Centering time: 0.014978647232055664
PCA fit: 1.950019359588623
[[9.80394013e-01 4.71171541e-06]
 [4.71171541e-06 9.80760470e-01]]
PCA Transform: 0.05252265930175781
total iterations: 1000
TLDA fit: 638.5369279384613
Whitened factor: 
[[-0.86117727  0.8886343 ]
 [ 0.5083049  -0.45861652]]
PCA Reverse Transform: 0.0003032684326171875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.039239575491624
Fit RMSE: 0.03959294665776941
 Test Against Ground Truth
[(' decentering', 0.0021162033081054688), (' smoothing and normalization', 0.0002887248992919922)]
Smoothing and Normalization: 0.0005333423614501953
Fit RMSE: 0.039195195852899344
sklearn Test Against Ground Truth
Centering time: 0.014802694320678711
PCA fit: 1.9507791996002197
[[9.80394013e-01 4.71171541e-06]
 [4.71171541e-06 9.80760470e-01]]
PCA Transform: 0.052247047424316406
total iterations: 1000
TLDA fit: 642.927583694458
Whitened factor: 
[[-0.7750236   0.94706136]
 [ 0.6319324  -0.3210526 ]]
PCA Reverse Transform: 0.00023746490478515625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03921611210664207
Fit RMSE: 0.03962258947354208
 Test Against Ground Truth
[(' decentering', 0.0020360946655273438), (' smoothing and normalization', 0.0002963542938232422)]
Smoothing and Normalization: 0.0005257129669189453
Fit RMSE: 0.039195195852899344
sklearn Test Against Ground Truth
Centering time: 0.01503753662109375
PCA fit: 1.9453480243682861
[[9.80394013e-01 4.71171541e-06]
 [4.71171541e-06 9.80760470e-01]]
PCA Transform: 0.05226421356201172
total iterations: 1000
TLDA fit: 645.3849415779114
Whitened factor: 
[[-0.8433159   0.9019064 ]
 [ 0.5374182  -0.43193156]]
PCA Reverse Transform: 0.000225067138671875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03923537821348816
Fit RMSE: 0.039599997327342984
 Test Against Ground Truth
[(' decentering', 0.0019910335540771484), (' smoothing and normalization', 0.0002925395965576172)]
Smoothing and Normalization: 0.0005161762237548828
Fit RMSE: 0.039195195852899344
sklearn Test Against Ground Truth
Centering time: 0.015094757080078125
PCA fit: 1.9449777603149414
[[9.80394013e-01 4.71171541e-06]
 [4.71171541e-06 9.80760470e-01]]
PCA Transform: 0.05233907699584961
total iterations: 1000
TLDA fit: 639.9451439380646
Whitened factor: 
[[-0.8575544   0.8898473 ]
 [ 0.51439327 -0.4562586 ]]
PCA Reverse Transform: 0.00023794174194335938
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.039238941264692906
Fit RMSE: 0.03959609689884999
 Test Against Ground Truth
[(' decentering', 0.002038717269897461), (' smoothing and normalization', 0.0003159046173095703)]
Smoothing and Normalization: 0.0005123615264892578
Fit RMSE: 0.039195195852899344
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 50000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.02131962776184082
PCA fit: 3.971811294555664
[[9.80392472e-01 2.08008548e-07]
 [2.08008548e-07 9.80607596e-01]]
PCA Transform: 0.08312129974365234
total iterations: 1000
TLDA fit: 640.8497471809387
Whitened factor: 
[[-0.9735166   0.999607  ]
 [ 0.22861616 -0.02803358]]
PCA Reverse Transform: 0.0002760887145996094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03341278157097864
Fit RMSE: 0.03346316903090463
 Test Against Ground Truth
[(' decentering', 0.0020868778228759766), (' smoothing and normalization', 0.0002884864807128906)]
Smoothing and Normalization: 0.0005125999450683594
Fit RMSE: 0.033368422638117894
sklearn Test Against Ground Truth
Centering time: 0.02224135398864746
PCA fit: 4.001779794692993
[[9.80392472e-01 2.08008548e-07]
 [2.08008548e-07 9.80607596e-01]]
PCA Transform: 0.08102893829345703
total iterations: 1000
TLDA fit: 639.2785160541534
Whitened factor: 
[[-0.98690104  0.9953839 ]
 [ 0.16132641 -0.09597299]]
PCA Reverse Transform: 0.0002465248107910156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.033413098967533424
Fit RMSE: 0.03345970986542073
 Test Against Ground Truth
[(' decentering', 0.0021102428436279297), (' smoothing and normalization', 0.00029468536376953125)]
Smoothing and Normalization: 0.0005271434783935547
Fit RMSE: 0.033368422638117894
sklearn Test Against Ground Truth
Centering time: 0.022166967391967773
PCA fit: 4.000479698181152
[[9.80392472e-01 2.08008548e-07]
 [2.08008548e-07 9.80607596e-01]]
PCA Transform: 0.08097362518310547
total iterations: 1000
TLDA fit: 642.8117847442627
Whitened factor: 
[[-0.9892334   0.9939562 ]
 [ 0.1463467  -0.10977781]]
PCA Reverse Transform: 0.0002639293670654297
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03341315427849367
Fit RMSE: 0.033461076920583095
 Test Against Ground Truth
[(' decentering', 0.0021791458129882812), (' smoothing and normalization', 0.00030994415283203125)]
Smoothing and Normalization: 0.0005393028259277344
Fit RMSE: 0.033368422638117894
sklearn Test Against Ground Truth
Centering time: 0.022092580795288086
PCA fit: 3.985614776611328
[[9.80392472e-01 2.08008548e-07]
 [2.08008548e-07 9.80607596e-01]]
PCA Transform: 0.0809788703918457
total iterations: 1000
TLDA fit: 638.0677490234375
Whitened factor: 
[[ 0.2302197 -0.320619 ]
 [-0.9731387  0.9472082]]
PCA Reverse Transform: 0.000247955322265625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.033952045409174285
Fit RMSE: 0.03440815578852854
 Test Against Ground Truth
[(' decentering', 0.002133607864379883), (' smoothing and normalization', 0.0002930164337158203)]
Smoothing and Normalization: 0.0012505054473876953
Fit RMSE: 0.033368422638117894
sklearn Test Against Ground Truth
Centering time: 0.022039413452148438
PCA fit: 3.971195697784424
[[9.80392472e-01 2.08008548e-07]
 [2.08008548e-07 9.80607596e-01]]
PCA Transform: 0.07844948768615723
total iterations: 1000
TLDA fit: 640.7524290084839
Whitened factor: 
[[ 0.2602774  -0.29116267]
 [-0.96553385  0.95667356]]
PCA Reverse Transform: 0.0002415180206298828
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0338783783562448
Fit RMSE: 0.03440729007761929
 Test Against Ground Truth
[(' decentering', 0.002133607864379883), (' smoothing and normalization', 0.0002875328063964844)]
Smoothing and Normalization: 0.0005156993865966797
Fit RMSE: 0.033368422638117894
sklearn Test Against Ground Truth
Centering time: 0.0222775936126709
PCA fit: 4.007699966430664
[[9.80392472e-01 2.08008548e-07]
 [2.08008548e-07 9.80607596e-01]]
PCA Transform: 0.0809469223022461
total iterations: 1000
TLDA fit: 643.5158603191376
Whitened factor: 
[[ 0.26737192 -0.28458047]
 [-0.9635934   0.9586522 ]]
PCA Reverse Transform: 0.0002467632293701172
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.033857917444110854
Fit RMSE: 0.03440672207901824
 Test Against Ground Truth
[(' decentering', 0.002232074737548828), (' smoothing and normalization', 0.0003101825714111328)]
Smoothing and Normalization: 0.0005304813385009766
Fit RMSE: 0.033368422638117894
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 50000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.028870582580566406
PCA fit: 7.280190467834473
[[ 9.80393112e-01 -3.83652068e-06]
 [-3.83652068e-06  9.80540167e-01]]
PCA Transform: 0.09581947326660156
total iterations: 1000
TLDA fit: 640.9246757030487
Whitened factor: 
[[-0.99561274  0.9282186 ]
 [-0.0935697   0.37203544]]
PCA Reverse Transform: 0.00024199485778808594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028877834399560046
Fit RMSE: 0.029048638162239473
 Test Against Ground Truth
[(' decentering', 0.0022270679473876953), (' smoothing and normalization', 0.00030159950256347656)]
Smoothing and Normalization: 0.0006399154663085938
Fit RMSE: 0.02885643659658867
sklearn Test Against Ground Truth
Centering time: 0.029170751571655273
PCA fit: 7.358552694320679
[[ 9.80393112e-01 -3.83652068e-06]
 [-3.83652068e-06  9.80540167e-01]]
PCA Transform: 0.10426998138427734
total iterations: 1000
TLDA fit: 646.1573855876923
Whitened factor: 
[[-0.98170334  0.9592829 ]
 [-0.19041684  0.28244704]]
PCA Reverse Transform: 0.0005364418029785156
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02889039798690775
Fit RMSE: 0.02903580474143754
 Test Against Ground Truth
[(' decentering', 0.002218961715698242), (' smoothing and normalization', 0.0002987384796142578)]
Smoothing and Normalization: 0.0005280971527099609
Fit RMSE: 0.02885643659658867
sklearn Test Against Ground Truth
Centering time: 0.029302120208740234
PCA fit: 7.3361005783081055
[[ 9.80393112e-01 -3.83652068e-06]
 [-3.83652068e-06  9.80540167e-01]]
PCA Transform: 0.10456633567810059
total iterations: 1000
TLDA fit: 638.7692439556122
Whitened factor: 
[[-0.9774923   0.9652847 ]
 [-0.21097152  0.2611998 ]]
PCA Reverse Transform: 0.0003390312194824219
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0288929092608825
Fit RMSE: 0.029033114495970107
 Test Against Ground Truth
[(' decentering', 0.002260446548461914), (' smoothing and normalization', 0.00029778480529785156)]
Smoothing and Normalization: 0.0005292892456054688
Fit RMSE: 0.02885643659658867
sklearn Test Against Ground Truth
Centering time: 0.029730558395385742
PCA fit: 7.342272758483887
[[ 9.80393112e-01 -3.83652068e-06]
 [-3.83652068e-06  9.80540167e-01]]
PCA Transform: 0.10435628890991211
total iterations: 1000
TLDA fit: 644.6581523418427
Whitened factor: 
[[-0.9957187   0.92617077]
 [-0.09243513  0.37710452]]
PCA Reverse Transform: 0.000255584716796875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028877279939184656
Fit RMSE: 0.029050256189179113
 Test Against Ground Truth
[(' decentering', 0.002254962921142578), (' smoothing and normalization', 0.0003001689910888672)]
Smoothing and Normalization: 0.0005328655242919922
Fit RMSE: 0.02885643659658867
sklearn Test Against Ground Truth
Centering time: 0.029251813888549805
PCA fit: 7.337477922439575
[[ 9.80393112e-01 -3.83652068e-06]
 [-3.83652068e-06  9.80540167e-01]]
PCA Transform: 0.09922075271606445
total iterations: 1000
TLDA fit: 643.1963217258453
Whitened factor: 
[[-0.9816007   0.95838416]
 [-0.19094513  0.28548172]]
PCA Reverse Transform: 0.00024271011352539062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028890212490682514
Fit RMSE: 0.02903685052560734
 Test Against Ground Truth
[(' decentering', 0.002289295196533203), (' smoothing and normalization', 0.0003535747528076172)]
Smoothing and Normalization: 0.0005221366882324219
Fit RMSE: 0.02885643659658867
sklearn Test Against Ground Truth
Centering time: 0.028283357620239258
PCA fit: 7.322594881057739
[[ 9.80393112e-01 -3.83652068e-06]
 [-3.83652068e-06  9.80540167e-01]]
PCA Transform: 0.10347604751586914
total iterations: 1000
TLDA fit: 642.9411175251007
Whitened factor: 
[[-0.9770931   0.96449393]
 [-0.21281236  0.2641051 ]]
PCA Reverse Transform: 0.00024962425231933594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028892815062437294
Fit RMSE: 0.029034337221854167
 Test Against Ground Truth
[(' decentering', 0.002238035202026367), (' smoothing and normalization', 0.00030040740966796875)]
Smoothing and Normalization: 0.0005743503570556641
Fit RMSE: 0.02885643659658867
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[1.0, 1.0]
[1.0, 1.0]
Centering time: 0.0036139488220214844
PCA fit: 0.1251990795135498
[[9.80392155e-01 1.85834752e-08]
 [1.85834746e-08 9.80406359e-01]]
PCA Transform: 0.005353689193725586
total iterations: 1000
TLDA fit: 649.1211023330688
Whitened factor: 
[[-0.03799417 -0.3419677 ]
 [-0.99927795  0.93971175]]
PCA Reverse Transform: 0.00023365020751953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12746084656088208
Fit RMSE: 0.13086020995946007
 Test Against Ground Truth
[(' decentering', 0.0019462108612060547), (' smoothing and normalization', 0.00024962425231933594)]
Smoothing and Normalization: 0.0004951953887939453
Fit RMSE: 0.12323053272808786
sklearn Test Against Ground Truth
Centering time: 0.0031354427337646484
PCA fit: 0.1186373233795166
[[9.80392155e-01 1.85834752e-08]
 [1.85834746e-08 9.80406359e-01]]
PCA Transform: 0.005340099334716797
total iterations: 1000
TLDA fit: 646.7734663486481
Whitened factor: 
[[ 0.08762276 -0.22243024]
 [-0.9961537   0.9749486 ]]
PCA Reverse Transform: 0.00022673606872558594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12686543056864577
Fit RMSE: 0.13084045074773692
 Test Against Ground Truth
[(' decentering', 0.0019295215606689453), (' smoothing and normalization', 0.00025463104248046875)]
Smoothing and Normalization: 0.00048470497131347656
Fit RMSE: 0.12323053272808786
sklearn Test Against Ground Truth
Centering time: 0.0030815601348876953
PCA fit: 0.11823296546936035
[[9.80392155e-01 1.85834752e-08]
 [1.85834746e-08 9.80406359e-01]]
PCA Transform: 0.005353689193725586
total iterations: 1000
TLDA fit: 643.5604882240295
Whitened factor: 
[[ 0.11693574 -0.19479437]
 [-0.99313956  0.9808441 ]]
PCA Reverse Transform: 0.00023484230041503906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12820870827353562
Fit RMSE: 0.13083020396066639
 Test Against Ground Truth
[(' decentering', 0.0019822120666503906), (' smoothing and normalization', 0.00025343894958496094)]
Smoothing and Normalization: 0.0004646778106689453
Fit RMSE: 0.12323053272808786
sklearn Test Against Ground Truth
Centering time: 0.003074169158935547
PCA fit: 0.11865711212158203
[[9.80392155e-01 1.85834752e-08]
 [1.85834746e-08 9.80406359e-01]]
PCA Transform: 0.005338430404663086
total iterations: 1000
TLDA fit: 650.0535244941711
Whitened factor: 
[[-0.03262465 -0.33660758]
 [-0.99946773  0.941645  ]]
PCA Reverse Transform: 0.00024509429931640625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1274309666140304
Fit RMSE: 0.1308612558078488
 Test Against Ground Truth
[(' decentering', 0.001939535140991211), (' smoothing and normalization', 0.00026154518127441406)]
Smoothing and Normalization: 0.0004658699035644531
Fit RMSE: 0.12323053272808786
sklearn Test Against Ground Truth
Centering time: 0.0030934810638427734
PCA fit: 0.11808514595031738
[[9.80392155e-01 1.85834752e-08]
 [1.85834746e-08 9.80406359e-01]]
PCA Transform: 0.005340576171875
total iterations: 1000
TLDA fit: 639.5694394111633
Whitened factor: 
[[ 0.09440529 -0.21437086]
 [-0.9955338   0.97675234]]
PCA Reverse Transform: 0.00023484230041503906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12738675786771278
Fit RMSE: 0.13084626363527457
 Test Against Ground Truth
[(' decentering', 0.001901388168334961), (' smoothing and normalization', 0.00026488304138183594)]
Smoothing and Normalization: 0.0004954338073730469
Fit RMSE: 0.12323053272808786
sklearn Test Against Ground Truth
Centering time: 0.0030939579010009766
PCA fit: 0.11822700500488281
[[9.80392155e-01 1.85834752e-08]
 [1.85834746e-08 9.80406359e-01]]
PCA Transform: 0.005364179611206055
total iterations: 1000
TLDA fit: 651.049839258194
Whitened factor: 
[[ 0.12165002 -0.18765311]
 [-0.9925731   0.9822354 ]]
PCA Reverse Transform: 0.00042319297790527344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12837565916743915
Fit RMSE: 0.13084135887331985
 Test Against Ground Truth
[(' decentering', 0.0019769668579101562), (' smoothing and normalization', 0.0002486705780029297)]
Smoothing and Normalization: 0.00048613548278808594
Fit RMSE: 0.12323053272808786
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.003347158432006836
PCA fit: 0.5233664512634277
[[ 9.80392248e-01 -8.63554612e-08]
 [-8.63554612e-08  9.80480975e-01]]
PCA Transform: 0.0264434814453125
total iterations: 1000
TLDA fit: 645.1598949432373
Whitened factor: 
[[ 0.10352481 -0.21255234]
 [-0.9946269   0.9771497 ]]
PCA Reverse Transform: 0.00024271011352539062
decenter with new strategy:
[ 0.00353069 -0.0013334 ]
decenter with old strategy:
[0.00860068 0.00409056]
Fit RMSE new decenter: 0.058241023164640644
Fit RMSE: 0.058808688468968896
 Test Against Ground Truth
[(' decentering', 0.001981019973754883), (' smoothing and normalization', 0.0002853870391845703)]
Smoothing and Normalization: 0.0005249977111816406
Fit RMSE: 0.05614008613301175
sklearn Test Against Ground Truth
Centering time: 0.007308244705200195
PCA fit: 0.5146844387054443
[[ 9.80392248e-01 -8.63554612e-08]
 [-8.63554612e-08  9.80480975e-01]]
PCA Transform: 0.026541948318481445
total iterations: 1000
TLDA fit: 649.132116317749
Whitened factor: 
[[-0.957689    0.97155356]
 [ 0.2878052  -0.23682001]]
PCA Reverse Transform: 0.00023365020751953125
decenter with new strategy:
[-0.00465632  0.00754072]
decenter with old strategy:
[0.00055062 0.01278455]
Fit RMSE new decenter: 0.05624345802586052
Fit RMSE: 0.05637292470092342
 Test Against Ground Truth
[(' decentering', 0.0019750595092773438), (' smoothing and normalization', 0.0002913475036621094)]
Smoothing and Normalization: 0.0008556842803955078
Fit RMSE: 0.05614008613301175
sklearn Test Against Ground Truth
Centering time: 0.0074651241302490234
PCA fit: 0.5251655578613281
[[ 9.80392248e-01 -8.63554612e-08]
 [-8.63554612e-08  9.80480975e-01]]
PCA Transform: 0.026473522186279297
total iterations: 1000
TLDA fit: 649.4305231571198
Whitened factor: 
[[-0.9611071   0.9690108 ]
 [ 0.2761759  -0.24701853]]
PCA Reverse Transform: 0.00023627281188964844
decenter with new strategy:
[-0.00466908  0.00754715]
decenter with old strategy:
[0.00054563 0.01278289]
Fit RMSE new decenter: 0.05624345174301864
Fit RMSE: 0.056372499123175386
 Test Against Ground Truth
[(' decentering', 0.0019588470458984375), (' smoothing and normalization', 0.00031375885009765625)]
Smoothing and Normalization: 0.0005369186401367188
Fit RMSE: 0.05614008613301175
sklearn Test Against Ground Truth
Centering time: 0.0074574947357177734
PCA fit: 0.533339262008667
[[ 9.80392248e-01 -8.63554612e-08]
 [-8.63554612e-08  9.80480975e-01]]
PCA Transform: 0.026432275772094727
total iterations: 1000
TLDA fit: 642.698379278183
Whitened factor: 
[[ 0.09588353 -0.22016552]
 [-0.99539256  0.97546256]]
PCA Reverse Transform: 0.0002574920654296875
decenter with new strategy:
[ 0.00350269 -0.00140523]
decenter with old strategy:
[0.008556   0.00404728]
Fit RMSE new decenter: 0.05825683867432261
Fit RMSE: 0.05880900649111329
 Test Against Ground Truth
[(' decentering', 0.0019826889038085938), (' smoothing and normalization', 0.00029349327087402344)]
Smoothing and Normalization: 0.0005199909210205078
Fit RMSE: 0.05614008613301175
sklearn Test Against Ground Truth
Centering time: 0.007428646087646484
PCA fit: 0.5385336875915527
[[ 9.80392248e-01 -8.63554612e-08]
 [-8.63554612e-08  9.80480975e-01]]
PCA Transform: 0.026502132415771484
total iterations: 1000
TLDA fit: 644.1507813930511
Whitened factor: 
[[ 0.13837641 -0.17838532]
 [-0.99037975  0.9839607 ]]
PCA Reverse Transform: 0.00023698806762695312
decenter with new strategy:
[ 0.00364231 -0.00100961]
decenter with old strategy:
[0.00880345 0.00428583]
Fit RMSE new decenter: 0.05815577948197148
Fit RMSE: 0.05880555710445275
 Test Against Ground Truth
[(' decentering', 0.0020363330841064453), (' smoothing and normalization', 0.0003199577331542969)]
Smoothing and Normalization: 0.0005090236663818359
Fit RMSE: 0.05614008613301175
sklearn Test Against Ground Truth
Centering time: 0.007382392883300781
PCA fit: 0.5141870975494385
[[ 9.80392248e-01 -8.63554612e-08]
 [-8.63554612e-08  9.80480975e-01]]
PCA Transform: 0.02648472785949707
total iterations: 1000
TLDA fit: 646.4114961624146
Whitened factor: 
[[-0.96059185  0.9683906 ]
 [ 0.27796286 -0.24943864]]
PCA Reverse Transform: 0.00023794174194335938
decenter with new strategy:
[-0.00466851  0.0075468 ]
decenter with old strategy:
[0.00054634 0.0127824 ]
Fit RMSE new decenter: 0.05624337296181419
Fit RMSE: 0.056374020973822186
 Test Against Ground Truth
[(' decentering', 0.001970052719116211), (' smoothing and normalization', 0.0002872943878173828)]
Smoothing and Normalization: 0.0007839202880859375
Fit RMSE: 0.05614008613301175
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.014246225357055664
PCA fit: 1.9907209873199463
[[9.80392572e-01 3.21850188e-07]
 [3.21850188e-07 9.80417723e-01]]
PCA Transform: 0.05244016647338867
total iterations: 1000
TLDA fit: 645.7372806072235
Whitened factor: 
[[-0.8838489  0.7018302]
 [-0.4677725  0.7123443]]
PCA Reverse Transform: 0.00023698806762695312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0405289045287752
Fit RMSE: 0.04084145428516247
 Test Against Ground Truth
[(' decentering', 0.0021576881408691406), (' smoothing and normalization', 0.0003523826599121094)]
Smoothing and Normalization: 0.0010879039764404297
Fit RMSE: 0.04051443402894094
sklearn Test Against Ground Truth
Centering time: 0.01519322395324707
PCA fit: 1.9698154926300049
[[9.80392572e-01 3.21850188e-07]
 [3.21850188e-07 9.80417723e-01]]
PCA Transform: 0.05213594436645508
total iterations: 1000
TLDA fit: 651.0356106758118
Whitened factor: 
[[-0.8318301   0.7695325 ]
 [-0.5550304   0.63860774]]
PCA Reverse Transform: 0.00023245811462402344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.040545196556277266
Fit RMSE: 0.04082008894637338
 Test Against Ground Truth
[(' decentering', 0.0020532608032226562), (' smoothing and normalization', 0.00029349327087402344)]
Smoothing and Normalization: 0.0005371570587158203
Fit RMSE: 0.04051443402894094
sklearn Test Against Ground Truth
Centering time: 0.014421701431274414
PCA fit: 1.996805191040039
[[9.80392572e-01 3.21850188e-07]
 [3.21850188e-07 9.80417723e-01]]
PCA Transform: 0.05222320556640625
total iterations: 1000
TLDA fit: 643.733645439148
Whitened factor: 
[[-0.81883967  0.7848047 ]
 [-0.5740223   0.61974317]]
PCA Reverse Transform: 0.0002338886260986328
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.040548406722897404
Fit RMSE: 0.04081531790813185
 Test Against Ground Truth
[(' decentering', 0.0020575523376464844), (' smoothing and normalization', 0.0002944469451904297)]
Smoothing and Normalization: 0.0005009174346923828
Fit RMSE: 0.04051443402894094
sklearn Test Against Ground Truth
Centering time: 0.013797283172607422
PCA fit: 1.9859943389892578
[[9.80392572e-01 3.21850188e-07]
 [3.21850188e-07 9.80417723e-01]]
PCA Transform: 0.05218338966369629
total iterations: 1000
TLDA fit: 647.521577835083
Whitened factor: 
[[-0.88262355  0.7024634 ]
 [-0.47008044  0.71171993]]
PCA Reverse Transform: 0.00025844573974609375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04052917199075449
Fit RMSE: 0.04084179128622063
 Test Against Ground Truth
[(' decentering', 0.002056121826171875), (' smoothing and normalization', 0.0002989768981933594)]
Smoothing and Normalization: 0.0005321502685546875
Fit RMSE: 0.04051443402894094
sklearn Test Against Ground Truth
Centering time: 0.013794183731079102
PCA fit: 1.9700071811676025
[[9.80392572e-01 3.21850188e-07]
 [3.21850188e-07 9.80417723e-01]]
PCA Transform: 0.05226635932922363
total iterations: 1000
TLDA fit: 647.7331404685974
Whitened factor: 
[[-0.83072877  0.76937515]
 [-0.5566774   0.63879734]]
PCA Reverse Transform: 0.0002474784851074219
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04054530006377416
Fit RMSE: 0.04082089417483276
 Test Against Ground Truth
[(' decentering', 0.0020449161529541016), (' smoothing and normalization', 0.00029349327087402344)]
Smoothing and Normalization: 0.0005362033843994141
Fit RMSE: 0.04051443402894094
sklearn Test Against Ground Truth
Centering time: 0.013826131820678711
PCA fit: 1.9549775123596191
[[9.80392572e-01 3.21850188e-07]
 [3.21850188e-07 9.80417723e-01]]
PCA Transform: 0.052216291427612305
total iterations: 1000
TLDA fit: 645.741491317749
Whitened factor: 
[[-0.8176226   0.78395164]
 [-0.57575464  0.620822  ]]
PCA Reverse Transform: 0.0002300739288330078
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04054840731328723
Fit RMSE: 0.040816753609636824
 Test Against Ground Truth
[(' decentering', 0.0021872520446777344), (' smoothing and normalization', 0.00029158592224121094)]
Smoothing and Normalization: 0.0005266666412353516
Fit RMSE: 0.04051443402894094
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 50000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.020724058151245117
PCA fit: 3.980564594268799
[[9.80394626e-01 9.12841221e-06]
 [9.12841221e-06 9.80506150e-01]]
PCA Transform: 0.08090472221374512
total iterations: 1000
TLDA fit: 643.7454991340637
Whitened factor: 
[[ 0.29516625  0.2752202 ]
 [ 0.9554459  -0.96138126]]
PCA Reverse Transform: 0.0002505779266357422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.034440878223043536
Fit RMSE: 0.034447286734625215
 Test Against Ground Truth
[(' decentering', 0.002172231674194336), (' smoothing and normalization', 0.00030612945556640625)]
Smoothing and Normalization: 0.0005588531494140625
Fit RMSE: 0.032673691313131595
sklearn Test Against Ground Truth
Centering time: 0.021136999130249023
PCA fit: 4.037670850753784
[[9.80394626e-01 9.12841221e-06]
 [9.12841221e-06 9.80506150e-01]]
PCA Transform: 0.08098816871643066
total iterations: 1000
TLDA fit: 643.8856401443481
Whitened factor: 
[[ 0.09626326  0.08540577]
 [ 0.9953559  -0.9963463 ]]
PCA Reverse Transform: 0.00023412704467773438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.034387513677125156
Fit RMSE: 0.03443953873067035
 Test Against Ground Truth
[(' decentering', 0.0021140575408935547), (' smoothing and normalization', 0.0002918243408203125)]
Smoothing and Normalization: 0.0005362033843994141
Fit RMSE: 0.032673691313131595
sklearn Test Against Ground Truth
Centering time: 0.020775318145751953
PCA fit: 4.0079686641693115
[[9.80394626e-01 9.12841221e-06]
 [9.12841221e-06 9.80506150e-01]]
PCA Transform: 0.08098363876342773
total iterations: 1000
TLDA fit: 643.6387310028076
Whitened factor: 
[[ 0.05532375  0.04640307]
 [ 0.99846846 -0.99892277]]
PCA Reverse Transform: 0.00023865699768066406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03438537522212852
Fit RMSE: 0.0344380963732947
 Test Against Ground Truth
[(' decentering', 0.002218961715698242), (' smoothing and normalization', 0.0003075599670410156)]
Smoothing and Normalization: 0.001188039779663086
Fit RMSE: 0.032673691313131595
sklearn Test Against Ground Truth
Centering time: 0.021150588989257812
PCA fit: 4.0008368492126465
[[9.80394626e-01 9.12841221e-06]
 [9.12841221e-06 9.80506150e-01]]
PCA Transform: 0.0809628963470459
total iterations: 1000
TLDA fit: 647.1528496742249
Whitened factor: 
[[-0.9485832   0.8026873 ]
 [-0.3165282   0.59640014]]
PCA Reverse Transform: 0.00022935867309570312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0327163749135531
Fit RMSE: 0.032949190264386564
 Test Against Ground Truth
[(' decentering', 0.002141237258911133), (' smoothing and normalization', 0.00029778480529785156)]
Smoothing and Normalization: 0.0005323886871337891
Fit RMSE: 0.032673691313131595
sklearn Test Against Ground Truth
Centering time: 0.02087879180908203
PCA fit: 4.032064437866211
[[9.80394626e-01 9.12841221e-06]
 [9.12841221e-06 9.80506150e-01]]
PCA Transform: 0.0809333324432373
total iterations: 1000
TLDA fit: 646.4415512084961
Whitened factor: 
[[-0.9096841   0.860246  ]
 [-0.41530085  0.50987923]]
PCA Reverse Transform: 0.0002510547637939453
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.032722240409159935
Fit RMSE: 0.03293513002030215
 Test Against Ground Truth
[(' decentering', 0.002172708511352539), (' smoothing and normalization', 0.00029754638671875)]
Smoothing and Normalization: 0.0010426044464111328
Fit RMSE: 0.032673691313131595
sklearn Test Against Ground Truth
Centering time: 0.02125072479248047
PCA fit: 4.015808582305908
[[9.80394626e-01 9.12841221e-06]
 [9.12841221e-06 9.80506150e-01]]
PCA Transform: 0.08095622062683105
total iterations: 1000
TLDA fit: 646.3870108127594
Whitened factor: 
[[-0.89923435  0.8724447 ]
 [-0.43746737  0.48871285]]
PCA Reverse Transform: 0.00023508071899414062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03272328906812637
Fit RMSE: 0.03293256945704164
 Test Against Ground Truth
[(' decentering', 0.002158641815185547), (' smoothing and normalization', 0.0003407001495361328)]
Smoothing and Normalization: 0.0005269050598144531
Fit RMSE: 0.032673691313131595
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.02787017822265625
PCA fit: 7.2559192180633545
[[9.80395770e-01 1.55841624e-05]
 [1.55841624e-05 9.80714867e-01]]
PCA Transform: 0.10426592826843262
total iterations: 1000
TLDA fit: 648.3986866474152
Whitened factor: 
[[ 0.8150717  -0.93525785]
 [-0.5793602   0.35396725]]
PCA Reverse Transform: 0.0002346038818359375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028331340624649752
Fit RMSE: 0.028540368958543137
 Test Against Ground Truth
[(' decentering', 0.002210378646850586), (' smoothing and normalization', 0.00032973289489746094)]
Smoothing and Normalization: 0.0005280971527099609
Fit RMSE: 0.028290956242400692
sklearn Test Against Ground Truth
Centering time: 0.028080224990844727
PCA fit: 7.30803370475769
[[9.80395770e-01 1.55841624e-05]
 [1.55841624e-05 9.80714867e-01]]
PCA Transform: 0.10429763793945312
total iterations: 1000
TLDA fit: 645.0256056785583
Whitened factor: 
[[ 0.8612746  -0.9007318 ]
 [-0.5081398   0.43437555]]
PCA Reverse Transform: 0.00028133392333984375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028335597580156493
Fit RMSE: 0.02853265809450916
 Test Against Ground Truth
[(' decentering', 0.002318143844604492), (' smoothing and normalization', 0.000293731689453125)]
Smoothing and Normalization: 0.0005354881286621094
Fit RMSE: 0.028290956242400692
sklearn Test Against Ground Truth
Centering time: 0.028249025344848633
PCA fit: 7.29057240486145
[[9.80395770e-01 1.55841624e-05]
 [1.55841624e-05 9.80714867e-01]]
PCA Transform: 0.10440349578857422
total iterations: 1000
TLDA fit: 653.8047652244568
Whitened factor: 
[[ 0.8718373  -0.8916961 ]
 [-0.4897956   0.45263472]]
PCA Reverse Transform: 0.0002491474151611328
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02833631295283695
Fit RMSE: 0.02853133418923287
 Test Against Ground Truth
[(' decentering', 0.002238035202026367), (' smoothing and normalization', 0.0002982616424560547)]
Smoothing and Normalization: 0.0005335807800292969
Fit RMSE: 0.028290956242400692
sklearn Test Against Ground Truth
Centering time: 0.028118371963500977
PCA fit: 7.30937647819519
[[9.80395770e-01 1.55841624e-05]
 [1.55841624e-05 9.80714867e-01]]
PCA Transform: 0.10428786277770996
total iterations: 1000
TLDA fit: 654.3414657115936
Whitened factor: 
[[ 0.8124863 -0.9354232]
 [-0.5829803  0.35353  ]]
PCA Reverse Transform: 0.000247955322265625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028331139942743905
Fit RMSE: 0.028542010183308195
 Test Against Ground Truth
[(' decentering', 0.0023086071014404297), (' smoothing and normalization', 0.00030040740966796875)]
Smoothing and Normalization: 0.0011854171752929688
Fit RMSE: 0.028290956242400692
sklearn Test Against Ground Truth
Centering time: 0.02814459800720215
PCA fit: 7.302262783050537
[[9.80395770e-01 1.55841624e-05]
 [1.55841624e-05 9.80714867e-01]]
PCA Transform: 0.10429549217224121
total iterations: 1000
TLDA fit: 646.2915229797363
Whitened factor: 
[[ 0.8592716  -0.90132165]
 [-0.51151973  0.43315056]]
PCA Reverse Transform: 0.00024366378784179688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028335484169266453
Fit RMSE: 0.02853355298825153
 Test Against Ground Truth
[(' decentering', 0.0022764205932617188), (' smoothing and normalization', 0.0002970695495605469)]
Smoothing and Normalization: 0.0005269050598144531
Fit RMSE: 0.028290956242400692
sklearn Test Against Ground Truth
Centering time: 0.028241872787475586
PCA fit: 7.285835266113281
[[9.80395770e-01 1.55841624e-05]
 [1.55841624e-05 9.80714867e-01]]
PCA Transform: 0.10446405410766602
total iterations: 1000
TLDA fit: 647.9068503379822
Whitened factor: 
[[ 0.86948824 -0.8923324 ]
 [-0.49395373  0.4513791 ]]
PCA Reverse Transform: 0.00024247169494628906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028336194150409385
Fit RMSE: 0.02853244649660895
 Test Against Ground Truth
[(' decentering', 0.002251148223876953), (' smoothing and normalization', 0.0003409385681152344)]
Smoothing and Normalization: 0.0005238056182861328
Fit RMSE: 0.028290956242400692
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.0031075477600097656
PCA fit: 0.1193244457244873
[[ 9.80392221e-01 -1.17811667e-06]
 [-1.17811667e-06  9.80412017e-01]]
PCA Transform: 0.0053026676177978516
total iterations: 1000
TLDA fit: 651.3862690925598
Whitened factor: 
[[-0.40155596  0.24599943]
 [-0.9158345   0.96926993]]
PCA Reverse Transform: 0.00024509429931640625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12526727549901198
Fit RMSE: 0.1292368667988416
 Test Against Ground Truth
[(' decentering', 0.0019371509552001953), (' smoothing and normalization', 0.0002543926239013672)]
Smoothing and Normalization: 0.0004794597625732422
Fit RMSE: 0.12367797590881574
sklearn Test Against Ground Truth
Centering time: 0.003090381622314453
PCA fit: 0.11590337753295898
[[ 9.80392221e-01 -1.17811667e-06]
 [-1.17811667e-06  9.80412017e-01]]
PCA Transform: 0.0053021907806396484
total iterations: 1000
TLDA fit: 643.2561218738556
Whitened factor: 
[[-0.3537069   0.29855847]
 [-0.9353563   0.95439136]]
PCA Reverse Transform: 0.0002276897430419922
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1252562779918085
Fit RMSE: 0.12921801013715592
 Test Against Ground Truth
[(' decentering', 0.001983642578125), (' smoothing and normalization', 0.00025343894958496094)]
Smoothing and Normalization: 0.00048422813415527344
Fit RMSE: 0.12367797590881574
sklearn Test Against Ground Truth
Centering time: 0.0030825138092041016
PCA fit: 0.11580848693847656
[[ 9.80392221e-01 -1.17811667e-06]
 [-1.17811667e-06  9.80412017e-01]]
PCA Transform: 0.005346775054931641
total iterations: 1000
TLDA fit: 644.0148649215698
Whitened factor: 
[[-0.34250733  0.31162995]
 [-0.9395152   0.95020354]]
PCA Reverse Transform: 0.00022125244140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1252651965565611
Fit RMSE: 0.1292082824587097
 Test Against Ground Truth
[(' decentering', 0.0019745826721191406), (' smoothing and normalization', 0.0002503395080566406)]
Smoothing and Normalization: 0.0004818439483642578
Fit RMSE: 0.12367797590881574
sklearn Test Against Ground Truth
Centering time: 0.003108501434326172
PCA fit: 0.11620545387268066
[[ 9.80392221e-01 -1.17811667e-06]
 [-1.17811667e-06  9.80412017e-01]]
PCA Transform: 0.005316257476806641
total iterations: 1000
TLDA fit: 651.6212961673737
Whitened factor: 
[[-0.4037672   0.24400859]
 [-0.91486174  0.9697732 ]]
PCA Reverse Transform: 0.00023293495178222656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12526672651794185
Fit RMSE: 0.12923547305302718
 Test Against Ground Truth
[(' decentering', 0.00191497802734375), (' smoothing and normalization', 0.000255584716796875)]
Smoothing and Normalization: 0.0004863739013671875
Fit RMSE: 0.12367797590881574
sklearn Test Against Ground Truth
Centering time: 0.0031080245971679688
PCA fit: 0.11602354049682617
[[ 9.80392221e-01 -1.17811667e-06]
 [-1.17811667e-06  9.80412017e-01]]
PCA Transform: 0.005301713943481445
total iterations: 1000
TLDA fit: 644.5789532661438
Whitened factor: 
[[-0.35111344  0.29917535]
 [-0.936333    0.9541982 ]]
PCA Reverse Transform: 0.00023365020751953125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12526323232932035
Fit RMSE: 0.129226935920655
 Test Against Ground Truth
[(' decentering', 0.0019271373748779297), (' smoothing and normalization', 0.0002498626708984375)]
Smoothing and Normalization: 0.0004849433898925781
Fit RMSE: 0.12367797590881574
sklearn Test Against Ground Truth
Centering time: 0.0030961036682128906
PCA fit: 0.11595821380615234
[[ 9.80392221e-01 -1.17811667e-06]
 [-1.17811667e-06  9.80412017e-01]]
PCA Transform: 0.0053157806396484375
total iterations: 1000
TLDA fit: 649.7709698677063
Whitened factor: 
[[-0.3395203   0.31151456]
 [-0.9405987   0.9502414 ]]
PCA Reverse Transform: 0.00023937225341796875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12527619488771713
Fit RMSE: 0.12922228047844597
 Test Against Ground Truth
[(' decentering', 0.0019233226776123047), (' smoothing and normalization', 0.0002503395080566406)]
Smoothing and Normalization: 0.0004949569702148438
Fit RMSE: 0.12367797590881574
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.006665468215942383
PCA fit: 0.6591413021087646
[[ 9.80393300e-01 -1.62560328e-06]
 [-1.62560328e-06  9.80573421e-01]]
PCA Transform: 0.026537179946899414
total iterations: 1000
TLDA fit: 645.2905416488647
Whitened factor: 
[[-0.966743   0.8154403]
 [-0.2557498  0.5788412]]
PCA Reverse Transform: 0.00023245811462402344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05435339618126464
Fit RMSE: 0.054902629908291485
 Test Against Ground Truth
[(' decentering', 0.0020258426666259766), (' smoothing and normalization', 0.0003440380096435547)]
Smoothing and Normalization: 0.000591278076171875
Fit RMSE: 0.05427535194979046
sklearn Test Against Ground Truth
Centering time: 0.006930351257324219
PCA fit: 0.6530656814575195
[[ 9.80393300e-01 -1.62560328e-06]
 [-1.62560328e-06  9.80573421e-01]]
PCA Transform: 0.026507139205932617
total iterations: 1000
TLDA fit: 643.8286321163177
Whitened factor: 
[[ 0.11053772  0.05875975]
 [ 0.993872   -0.9982721 ]]
PCA Reverse Transform: 0.00023627281188964844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05714102293486744
Fit RMSE: 0.058350711063710536
 Test Against Ground Truth
[(' decentering', 0.0019805431365966797), (' smoothing and normalization', 0.0003342628479003906)]
Smoothing and Normalization: 0.0011005401611328125
Fit RMSE: 0.05427535194979046
sklearn Test Against Ground Truth
Centering time: 0.006873130798339844
PCA fit: 0.6275670528411865
[[ 9.80393300e-01 -1.62560328e-06]
 [-1.62560328e-06  9.80573421e-01]]
PCA Transform: 0.026539325714111328
total iterations: 1000
TLDA fit: 638.9356334209442
Whitened factor: 
[[-0.91947764  0.8900448 ]
 [-0.39314234  0.4558731 ]]
PCA Reverse Transform: 0.00023126602172851562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05437331171294102
Fit RMSE: 0.05486574030973386
 Test Against Ground Truth
[(' decentering', 0.0019631385803222656), (' smoothing and normalization', 0.0003018379211425781)]
Smoothing and Normalization: 0.0005342960357666016
Fit RMSE: 0.05427535194979046
sklearn Test Against Ground Truth
Centering time: 0.006908893585205078
PCA fit: 0.6478724479675293
[[ 9.80393300e-01 -1.62560328e-06]
 [-1.62560328e-06  9.80573421e-01]]
PCA Transform: 0.02649092674255371
total iterations: 1000
TLDA fit: 645.3768510818481
Whitened factor: 
[[-0.9657955   0.81614095]
 [-0.25930497  0.5778529 ]]
PCA Reverse Transform: 0.00023221969604492188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05435372697275387
Fit RMSE: 0.054904068847514
 Test Against Ground Truth
[(' decentering', 0.0019774436950683594), (' smoothing and normalization', 0.0003008842468261719)]
Smoothing and Normalization: 0.0005457401275634766
Fit RMSE: 0.05427535194979046
sklearn Test Against Ground Truth
Centering time: 0.006939888000488281
PCA fit: 0.6365022659301758
[[ 9.80393300e-01 -1.62560328e-06]
 [-1.62560328e-06  9.80573421e-01]]
PCA Transform: 0.026486873626708984
total iterations: 1000
TLDA fit: 643.6059076786041
Whitened factor: 
[[-0.92823356  0.8774887 ]
 [-0.37199774  0.47959742]]
PCA Reverse Transform: 0.0002651214599609375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05437062615035068
Fit RMSE: 0.05487425565234785
 Test Against Ground Truth
[(' decentering', 0.0019664764404296875), (' smoothing and normalization', 0.00029921531677246094)]
Smoothing and Normalization: 0.0005173683166503906
Fit RMSE: 0.05427535194979046
sklearn Test Against Ground Truth
Centering time: 0.006936311721801758
PCA fit: 0.6454331874847412
[[ 9.80393300e-01 -1.62560328e-06]
 [-1.62560328e-06  9.80573421e-01]]
PCA Transform: 0.02648782730102539
total iterations: 1000
TLDA fit: 646.0256156921387
Whitened factor: 
[[-0.91788733  0.890055  ]
 [-0.3968409   0.4558531 ]]
PCA Reverse Transform: 0.00023818016052246094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05437342501629404
Fit RMSE: 0.05486900294433631
 Test Against Ground Truth
[(' decentering', 0.001996755599975586), (' smoothing and normalization', 0.000301361083984375)]
Smoothing and Normalization: 0.0005345344543457031
Fit RMSE: 0.05427535194979046
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.013888120651245117
PCA fit: 1.9203369617462158
[[ 9.80392822e-01 -1.77692437e-06]
 [-1.77692437e-06  9.80528376e-01]]
PCA Transform: 0.05233430862426758
total iterations: 1000
TLDA fit: 643.0149683952332
Whitened factor: 
[[ 0.07877292  0.00650692]
 [-0.99689263  0.9999788 ]]
PCA Reverse Transform: 0.00023651123046875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04169313064706366
Fit RMSE: 0.041988122634961524
 Test Against Ground Truth
[(' decentering', 0.002028226852416992), (' smoothing and normalization', 0.000354766845703125)]
Smoothing and Normalization: 0.0005307197570800781
Fit RMSE: 0.03970329139157026
sklearn Test Against Ground Truth
Centering time: 0.013932943344116211
PCA fit: 1.9127814769744873
[[ 9.80392822e-01 -1.77692437e-06]
 [-1.77692437e-06  9.80528376e-01]]
PCA Transform: 0.05216526985168457
total iterations: 1000
TLDA fit: 644.2145338058472
Whitened factor: 
[[ 0.05512353 -0.02639276]
 [-0.99847955  0.9996516 ]]
PCA Reverse Transform: 0.0002460479736328125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04153478006544789
Fit RMSE: 0.04197694236129455
 Test Against Ground Truth
[(' decentering', 0.002078533172607422), (' smoothing and normalization', 0.00034308433532714844)]
Smoothing and Normalization: 0.000514984130859375
Fit RMSE: 0.03970329139157026
sklearn Test Against Ground Truth
Centering time: 0.013877630233764648
PCA fit: 1.9356443881988525
[[ 9.80392822e-01 -1.77692437e-06]
 [-1.77692437e-06  9.80528376e-01]]
PCA Transform: 0.052168846130371094
total iterations: 1000
TLDA fit: 636.2758722305298
Whitened factor: 
[[ 0.5269701  -0.5234439 ]
 [-0.84988385  0.8520602 ]]
PCA Reverse Transform: 0.0002257823944091797
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.039917089801396584
Fit RMSE: 0.04078341376401972
 Test Against Ground Truth
[(' decentering', 0.002008676528930664), (' smoothing and normalization', 0.00029754638671875)]
Smoothing and Normalization: 0.0005323886871337891
Fit RMSE: 0.03970329139157026
sklearn Test Against Ground Truth
Centering time: 0.013705968856811523
PCA fit: 1.9428737163543701
[[ 9.80392822e-01 -1.77692437e-06]
 [-1.77692437e-06  9.80528376e-01]]
PCA Transform: 0.052196502685546875
total iterations: 14
TLDA fit: 8.468845844268799
Whitened factor: 
[[-0.80353236  0.5762392 ]
 [ 0.5952611   0.8172811 ]]
PCA Reverse Transform: 0.0002181529998779297
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03977486251899394
Fit RMSE: 0.04037458174180921
 Test Against Ground Truth
[(' decentering', 0.0020356178283691406), (' smoothing and normalization', 0.0002970695495605469)]
Smoothing and Normalization: 0.0005729198455810547
Fit RMSE: 0.03970329139157026
sklearn Test Against Ground Truth
Centering time: 0.013732194900512695
PCA fit: 1.9292278289794922
[[ 9.80392822e-01 -1.77692437e-06]
 [-1.77692437e-06  9.80528376e-01]]
PCA Transform: 0.05223393440246582
total iterations: 14
TLDA fit: 8.433900356292725
Whitened factor: 
[[-0.803661    0.57640237]
 [ 0.5950874   0.81716603]]
PCA Reverse Transform: 0.0002486705780029297
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03977479730369085
Fit RMSE: 0.04037427513611885
 Test Against Ground Truth
[(' decentering', 0.0020294189453125), (' smoothing and normalization', 0.00029778480529785156)]
Smoothing and Normalization: 0.0009143352508544922
Fit RMSE: 0.03970329139157026
sklearn Test Against Ground Truth
Centering time: 0.013896703720092773
PCA fit: 1.9210610389709473
[[ 9.80392822e-01 -1.77692437e-06]
 [-1.77692437e-06  9.80528376e-01]]
PCA Transform: 0.052223920822143555
total iterations: 14
TLDA fit: 8.389251708984375
Whitened factor: 
[[-0.8038147   0.57659906]
 [ 0.59487975  0.81702733]]
PCA Reverse Transform: 0.00021958351135253906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03977471877048385
Fit RMSE: 0.04037390714629663
 Test Against Ground Truth
[(' decentering', 0.002077817916870117), (' smoothing and normalization', 0.00029397010803222656)]
Smoothing and Normalization: 0.0008244514465332031
Fit RMSE: 0.03970329139157026
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.0206453800201416
PCA fit: 4.0297582149505615
[[9.80392703e-01 9.71271479e-07]
 [9.71271479e-07 9.80434387e-01]]
PCA Transform: 0.08095788955688477
total iterations: 1000
TLDA fit: 639.7723777294159
Whitened factor: 
[[-0.90416807  0.7521317 ]
 [-0.42717692  0.65901285]]
PCA Reverse Transform: 0.0002422332763671875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.033040504490853606
Fit RMSE: 0.03328721974554273
 Test Against Ground Truth
[(' decentering', 0.0021266937255859375), (' smoothing and normalization', 0.00033020973205566406)]
Smoothing and Normalization: 0.0005147457122802734
Fit RMSE: 0.033003563530752074
sklearn Test Against Ground Truth
Centering time: 0.021114110946655273
PCA fit: 4.037712574005127
[[9.80392703e-01 9.71271479e-07]
 [9.71271479e-07 9.80434387e-01]]
PCA Transform: 0.08085465431213379
total iterations: 1000
TLDA fit: 638.761316537857
Whitened factor: 
[[-0.8601302  0.8111982]
 [-0.5100747  0.5847714]]
PCA Reverse Transform: 0.00022220611572265625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03304958517573463
Fit RMSE: 0.03327199960117943
 Test Against Ground Truth
[(' decentering', 0.002224445343017578), (' smoothing and normalization', 0.0003104209899902344)]
Smoothing and Normalization: 0.0011601448059082031
Fit RMSE: 0.033003563530752074
sklearn Test Against Ground Truth
Centering time: 0.02105116844177246
PCA fit: 4.0343828201293945
[[9.80392703e-01 9.71271479e-07]
 [9.71271479e-07 9.80434387e-01]]
PCA Transform: 0.08100676536560059
total iterations: 1000
TLDA fit: 637.8393337726593
Whitened factor: 
[[-0.84982896  0.8239236 ]
 [-0.5270586   0.5667009 ]]
PCA Reverse Transform: 0.00023746490478515625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03305129264009231
Fit RMSE: 0.03326857006346527
 Test Against Ground Truth
[(' decentering', 0.002093791961669922), (' smoothing and normalization', 0.0002930164337158203)]
Smoothing and Normalization: 0.000537872314453125
Fit RMSE: 0.033003563530752074
sklearn Test Against Ground Truth
Centering time: 0.021028757095336914
PCA fit: 4.014142274856567
[[9.80392703e-01 9.71271479e-07]
 [9.71271479e-07 9.80434387e-01]]
PCA Transform: 0.08099555969238281
total iterations: 1000
TLDA fit: 641.8722515106201
Whitened factor: 
[[-0.908233    0.74687505]
 [-0.41846502  0.6649645 ]]
PCA Reverse Transform: 0.00024175643920898438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.033039562865857595
Fit RMSE: 0.03328820161241406
 Test Against Ground Truth
[(' decentering', 0.0021560192108154297), (' smoothing and normalization', 0.0003185272216796875)]
Smoothing and Normalization: 0.0005612373352050781
Fit RMSE: 0.033003563530752074
sklearn Test Against Ground Truth
Centering time: 0.0209500789642334
PCA fit: 4.01692271232605
[[9.80392703e-01 9.71271479e-07]
 [9.71271479e-07 9.80434387e-01]]
PCA Transform: 0.08099579811096191
total iterations: 1000
TLDA fit: 642.0169823169708
Whitened factor: 
[[-0.862315   0.8081976]
 [-0.5063722  0.5889115]]
PCA Reverse Transform: 0.0002524852752685547
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.033049185431307235
Fit RMSE: 0.033272918936980055
 Test Against Ground Truth
[(' decentering', 0.0021309852600097656), (' smoothing and normalization', 0.0002930164337158203)]
Smoothing and Normalization: 0.0005044937133789062
Fit RMSE: 0.033003563530752074
sklearn Test Against Ground Truth
Centering time: 0.02112579345703125
PCA fit: 4.021764039993286
[[9.80392703e-01 9.71271479e-07]
 [9.71271479e-07 9.80434387e-01]]
PCA Transform: 0.07814168930053711
total iterations: 1000
TLDA fit: 636.4075884819031
Whitened factor: 
[[-0.850509    0.8214932 ]
 [-0.5259605   0.57021844]]
PCA Reverse Transform: 0.00025391578674316406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03305106122984963
Fit RMSE: 0.033269969899317096
 Test Against Ground Truth
[(' decentering', 0.0021309852600097656), (' smoothing and normalization', 0.0003001689910888672)]
Smoothing and Normalization: 0.00115966796875
Fit RMSE: 0.033003563530752074
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 50000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.02777838706970215
PCA fit: 7.227357625961304
[[ 9.80393401e-01 -2.05262765e-06]
 [-2.05262765e-06  9.80515849e-01]]
PCA Transform: 0.10435795783996582
total iterations: 1000
TLDA fit: 635.1600651741028
Whitened factor: 
[[-0.95047873  0.8133225 ]
 [-0.31078938  0.5818131 ]]
PCA Reverse Transform: 0.00023555755615234375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02856776768451794
Fit RMSE: 0.028773304628333492
 Test Against Ground Truth
[(' decentering', 0.0022840499877929688), (' smoothing and normalization', 0.00028634071350097656)]
Smoothing and Normalization: 0.0005567073822021484
Fit RMSE: 0.028532489997173574
sklearn Test Against Ground Truth
Centering time: 0.028364896774291992
PCA fit: 7.258135795593262
[[ 9.80393401e-01 -2.05262765e-06]
 [-2.05262765e-06  9.80515849e-01]]
PCA Transform: 0.10449886322021484
total iterations: 1000
TLDA fit: 633.1419036388397
Whitened factor: 
[[-0.91280466  0.86667407]
 [-0.40839648  0.49887493]]
PCA Reverse Transform: 0.00023436546325683594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028575106685760417
Fit RMSE: 0.028764332129771635
 Test Against Ground Truth
[(' decentering', 0.0022420883178710938), (' smoothing and normalization', 0.00029540061950683594)]
Smoothing and Normalization: 0.0005323886871337891
Fit RMSE: 0.028532489997173574
sklearn Test Against Ground Truth
Centering time: 0.02799248695373535
PCA fit: 7.274266242980957
[[ 9.80393401e-01 -2.05262765e-06]
 [-2.05262765e-06  9.80515849e-01]]
PCA Transform: 0.10435605049133301
total iterations: 1000
TLDA fit: 641.4143364429474
Whitened factor: 
[[-0.9030997   0.87809354]
 [-0.42943087  0.47848922]]
PCA Reverse Transform: 0.00023984909057617188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028576423377820685
Fit RMSE: 0.028762483631915088
 Test Against Ground Truth
[(' decentering', 0.002218008041381836), (' smoothing and normalization', 0.00034546852111816406)]
Smoothing and Normalization: 0.0005545616149902344
Fit RMSE: 0.028532489997173574
sklearn Test Against Ground Truth
Centering time: 0.028544187545776367
PCA fit: 7.268064022064209
[[ 9.80393401e-01 -2.05262765e-06]
 [-2.05262765e-06  9.80515849e-01]]
PCA Transform: 0.10330057144165039
total iterations: 1000
TLDA fit: 632.9073860645294
Whitened factor: 
[[-0.9521277   0.80592424]
 [-0.30570066  0.5920188 ]]
PCA Reverse Transform: 0.00022935867309570312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02856686161108108
Fit RMSE: 0.02877675591825046
 Test Against Ground Truth
[(' decentering', 0.0022208690643310547), (' smoothing and normalization', 0.00030922889709472656)]
Smoothing and Normalization: 0.0005214214324951172
Fit RMSE: 0.028532489997173574
sklearn Test Against Ground Truth
Centering time: 0.028699159622192383
PCA fit: 7.291935920715332
[[ 9.80393401e-01 -2.05262765e-06]
 [-2.05262765e-06  9.80515849e-01]]
PCA Transform: 0.10418081283569336
total iterations: 1000
TLDA fit: 635.8944656848907
Whitened factor: 
[[-0.9133719   0.8640801 ]
 [-0.40712625  0.5033544 ]]
PCA Reverse Transform: 0.0002288818359375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02857487504839685
Fit RMSE: 0.02876578285722307
 Test Against Ground Truth
[(' decentering', 0.00226593017578125), (' smoothing and normalization', 0.0002837181091308594)]
Smoothing and Normalization: 0.0005428791046142578
Fit RMSE: 0.028532489997173574
sklearn Test Against Ground Truth
Centering time: 0.028475046157836914
PCA fit: 7.263559579849243
[[ 9.80393401e-01 -2.05262765e-06]
 [-2.05262765e-06  9.80515849e-01]]
PCA Transform: 0.10425496101379395
total iterations: 28
TLDA fit: 17.394537687301636
Whitened factor: 
[[-0.9143997   0.88945526]
 [-0.4048125   0.45702225]]
PCA Reverse Transform: 0.00023651123046875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028576693757478927
Fit RMSE: 0.028747148823939465
 Test Against Ground Truth
[(' decentering', 0.002270936965942383), (' smoothing and normalization', 0.0002949237823486328)]
Smoothing and Normalization: 0.000545501708984375
Fit RMSE: 0.028532489997173574
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.0035429000854492188
PCA fit: 0.12291622161865234
[[9.80392169e-01 2.11673692e-08]
 [2.11673693e-08 9.80414389e-01]]
PCA Transform: 0.005318641662597656
total iterations: 1000
TLDA fit: 635.3715970516205
Whitened factor: 
[[-0.8336298   0.96257   ]
 [ 0.55232364 -0.2710332 ]]
PCA Reverse Transform: 0.00022864341735839844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12117097175857701
Fit RMSE: 0.12208647949037614
 Test Against Ground Truth
[(' decentering', 0.0019342899322509766), (' smoothing and normalization', 0.00024962425231933594)]
Smoothing and Normalization: 0.0004909038543701172
Fit RMSE: 0.12095670562711425
sklearn Test Against Ground Truth
Centering time: 0.0031251907348632812
PCA fit: 0.11960196495056152
[[9.80392169e-01 2.11673692e-08]
 [2.11673693e-08 9.80414389e-01]]
PCA Transform: 0.005281925201416016
total iterations: 1000
TLDA fit: 634.452586889267
Whitened factor: 
[[-0.88731235  0.92831796]
 [ 0.46116897 -0.37178737]]
PCA Reverse Transform: 0.00024271011352539062
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12115818542376577
Fit RMSE: 0.122014043653952
 Test Against Ground Truth
[(' decentering', 0.001916646957397461), (' smoothing and normalization', 0.0002551078796386719)]
Smoothing and Normalization: 0.0004839897155761719
Fit RMSE: 0.12095670562711425
sklearn Test Against Ground Truth
Centering time: 0.0030961036682128906
PCA fit: 0.1193993091583252
[[9.80392169e-01 2.11673692e-08]
 [2.11673693e-08 9.80414389e-01]]
PCA Transform: 0.0052793025970458984
total iterations: 1000
TLDA fit: 632.9133899211884
Whitened factor: 
[[-0.898036   0.9196519]
 [ 0.4399218 -0.3927345]]
PCA Reverse Transform: 0.0002346038818359375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12115374493547344
Fit RMSE: 0.12199821407452019
 Test Against Ground Truth
[(' decentering', 0.0019683837890625), (' smoothing and normalization', 0.00025653839111328125)]
Smoothing and Normalization: 0.0004851818084716797
Fit RMSE: 0.12095670562711425
sklearn Test Against Ground Truth
Centering time: 0.0030879974365234375
PCA fit: 0.11972641944885254
[[9.80392169e-01 2.11673692e-08]
 [2.11673693e-08 9.80414389e-01]]
PCA Transform: 0.005326271057128906
total iterations: 1000
TLDA fit: 636.9602899551392
Whitened factor: 
[[-0.835648    0.96145827]
 [ 0.54926544 -0.27495098]]
PCA Reverse Transform: 0.00023865699768066406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1211707162944591
Fit RMSE: 0.12208423941382106
 Test Against Ground Truth
[(' decentering', 0.0019383430480957031), (' smoothing and normalization', 0.00029158592224121094)]
Smoothing and Normalization: 0.0004825592041015625
Fit RMSE: 0.12095670562711425
sklearn Test Against Ground Truth
Centering time: 0.0031006336212158203
PCA fit: 0.11953091621398926
[[9.80392169e-01 2.11673692e-08]
 [2.11673693e-08 9.80414389e-01]]
PCA Transform: 0.005396366119384766
total iterations: 1000
TLDA fit: 654.9777257442474
Whitened factor: 
[[-0.8855956   0.92910427]
 [ 0.464457   -0.36981782]]
PCA Reverse Transform: 0.0002391338348388672
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.12115860495018448
Fit RMSE: 0.12201835141737274
 Test Against Ground Truth
[(' decentering', 0.001954317092895508), (' smoothing and normalization', 0.00025177001953125)]
Smoothing and Normalization: 0.00047779083251953125
Fit RMSE: 0.12095670562711425
sklearn Test Against Ground Truth
Centering time: 0.0030930042266845703
PCA fit: 0.11967849731445312
[[9.80392169e-01 2.11673692e-08]
 [2.11673693e-08 9.80414389e-01]]
PCA Transform: 0.005321502685546875
total iterations: 1000
TLDA fit: 643.933342218399
Whitened factor: 
[[-0.8961921   0.920192  ]
 [ 0.4436663  -0.39146727]]
PCA Reverse Transform: 0.0002262592315673828
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.1211541246415967
Fit RMSE: 0.12200416900576391
 Test Against Ground Truth
[(' decentering', 0.0019063949584960938), (' smoothing and normalization', 0.0002837181091308594)]
Smoothing and Normalization: 0.0005002021789550781
Fit RMSE: 0.12095670562711425
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[0.9999999999999999, 0.9999999999999999]
Centering time: 0.0032510757446289062
PCA fit: 0.6207082271575928
[[ 9.80392916e-01 -5.13972694e-06]
 [-5.13972694e-06  9.80457608e-01]]
PCA Transform: 0.026430368423461914
total iterations: 1000
TLDA fit: 635.5165967941284
Whitened factor: 
[[-0.9668643   0.8344151 ]
 [-0.25529107  0.55113655]]
PCA Reverse Transform: 0.0002448558807373047
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056285482964540084
Fit RMSE: 0.05668761545243195
 Test Against Ground Truth
[(' decentering', 0.001966714859008789), (' smoothing and normalization', 0.0003142356872558594)]
Smoothing and Normalization: 0.0011527538299560547
Fit RMSE: 0.056171093794766744
sklearn Test Against Ground Truth
Centering time: 0.006814002990722656
PCA fit: 0.6373534202575684
[[ 9.80392916e-01 -5.13972694e-06]
 [-5.13972694e-06  9.80457608e-01]]
PCA Transform: 0.02645707130432129
total iterations: 1000
TLDA fit: 634.2895784378052
Whitened factor: 
[[-0.9333327   0.88828886]
 [-0.35901275  0.45928526]]
PCA Reverse Transform: 0.00029087066650390625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05629027178444733
Fit RMSE: 0.056668319846621205
 Test Against Ground Truth
[(' decentering', 0.0019698143005371094), (' smoothing and normalization', 0.00029158592224121094)]
Smoothing and Normalization: 0.0011715888977050781
Fit RMSE: 0.056171093794766744
sklearn Test Against Ground Truth
Centering time: 0.006670236587524414
PCA fit: 0.6324334144592285
[[ 9.80392916e-01 -5.13972694e-06]
 [-5.13972694e-06  9.80457608e-01]]
PCA Transform: 0.02643108367919922
total iterations: 1000
TLDA fit: 635.5886173248291
Whitened factor: 
[[-0.9242073   0.89990824]
 [-0.38189104  0.43607944]]
PCA Reverse Transform: 0.00024366378784179688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05629058617011458
Fit RMSE: 0.05666383636905297
 Test Against Ground Truth
[(' decentering', 0.001972198486328125), (' smoothing and normalization', 0.0003418922424316406)]
Smoothing and Normalization: 0.0005581378936767578
Fit RMSE: 0.056171093794766744
sklearn Test Against Ground Truth
Centering time: 0.0067822933197021484
PCA fit: 0.6152491569519043
[[ 9.80392916e-01 -5.13972694e-06]
 [-5.13972694e-06  9.80457608e-01]]
PCA Transform: 0.026505708694458008
total iterations: 1000
TLDA fit: 636.6030592918396
Whitened factor: 
[[-0.9661341   0.8339535 ]
 [-0.25804088  0.5518348 ]]
PCA Reverse Transform: 0.0002391338348388672
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056285459279098886
Fit RMSE: 0.05668994486908721
 Test Against Ground Truth
[(' decentering', 0.002007722854614258), (' smoothing and normalization', 0.00030875205993652344)]
Smoothing and Normalization: 0.0008232593536376953
Fit RMSE: 0.056171093794766744
sklearn Test Against Ground Truth
Centering time: 0.006768465042114258
PCA fit: 0.6370270252227783
[[ 9.80392916e-01 -5.13972694e-06]
 [-5.13972694e-06  9.80457608e-01]]
PCA Transform: 0.026476383209228516
total iterations: 1000
TLDA fit: 635.1436111927032
Whitened factor: 
[[-0.9324451   0.88776207]
 [-0.36131182  0.4603027 ]]
PCA Reverse Transform: 0.00023245811462402344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05629022859390112
Fit RMSE: 0.05667067639938971
 Test Against Ground Truth
[(' decentering', 0.001951456069946289), (' smoothing and normalization', 0.0002892017364501953)]
Smoothing and Normalization: 0.0005240440368652344
Fit RMSE: 0.056171093794766744
sklearn Test Against Ground Truth
Centering time: 0.0070002079010009766
PCA fit: 0.6095969676971436
[[ 9.80392916e-01 -5.13972694e-06]
 [-5.13972694e-06  9.80457608e-01]]
PCA Transform: 0.026476383209228516
total iterations: 1000
TLDA fit: 638.1847913265228
Whitened factor: 
[[-0.9232687   0.89881253]
 [-0.384155    0.43833318]]
PCA Reverse Transform: 0.00022983551025390625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056290515250449265
Fit RMSE: 0.05666718180016316
 Test Against Ground Truth
[(' decentering', 0.0019488334655761719), (' smoothing and normalization', 0.00029158592224121094)]
Smoothing and Normalization: 0.0005779266357421875
Fit RMSE: 0.056171093794766744
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.013898611068725586
PCA fit: 1.991713285446167
[[ 9.80393299e-01 -8.04905182e-06]
 [-8.04905182e-06  9.80774149e-01]]
PCA Transform: 0.05241823196411133
total iterations: 1000
TLDA fit: 646.3923346996307
Whitened factor: 
[[-0.8846385   0.95659745]
 [ 0.46627766 -0.29141268]]
PCA Reverse Transform: 0.0002415180206298828
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.040071833098853
Fit RMSE: 0.040276791885634246
 Test Against Ground Truth
[(' decentering', 0.0021047592163085938), (' smoothing and normalization', 0.00032782554626464844)]
Smoothing and Normalization: 0.0005290508270263672
Fit RMSE: 0.04003489431304296
sklearn Test Against Ground Truth
Centering time: 0.014111995697021484
PCA fit: 1.9709444046020508
[[ 9.80393299e-01 -8.04905182e-06]
 [-8.04905182e-06  9.80774149e-01]]
PCA Transform: 0.05218100547790527
total iterations: 1000
TLDA fit: 642.7333862781525
Whitened factor: 
[[-0.91220886  0.9380836 ]
 [ 0.40972573 -0.346409  ]]
PCA Reverse Transform: 0.0002353191375732422
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04007169713152286
Fit RMSE: 0.040271590720698865
 Test Against Ground Truth
[(' decentering', 0.002028226852416992), (' smoothing and normalization', 0.00032806396484375)]
Smoothing and Normalization: 0.0005283355712890625
Fit RMSE: 0.04003489431304296
sklearn Test Against Ground Truth
Centering time: 0.01409149169921875
PCA fit: 2.0095677375793457
[[ 9.80393299e-01 -8.04905182e-06]
 [-8.04905182e-06  9.80774149e-01]]
PCA Transform: 0.05218791961669922
total iterations: 1000
TLDA fit: 639.5399894714355
Whitened factor: 
[[-0.919315    0.93408316]
 [ 0.39352253 -0.3570556 ]]
PCA Reverse Transform: 0.0002779960632324219
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0400715596427012
Fit RMSE: 0.040269132154647176
 Test Against Ground Truth
[(' decentering', 0.002111196517944336), (' smoothing and normalization', 0.0002892017364501953)]
Smoothing and Normalization: 0.0010929107666015625
Fit RMSE: 0.04003489431304296
sklearn Test Against Ground Truth
Centering time: 0.014371156692504883
PCA fit: 1.9776949882507324
[[ 9.80393299e-01 -8.04905182e-06]
 [-8.04905182e-06  9.80774149e-01]]
PCA Transform: 0.05217170715332031
total iterations: 18
TLDA fit: 11.050272703170776
Whitened factor: 
[[-0.9303311  -0.03235317]
 [ 0.36672065  0.9994765 ]]
PCA Reverse Transform: 0.00025272369384765625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0405751740380263
Fit RMSE: 0.04088291602757953
 Test Against Ground Truth
[(' decentering', 0.002070903778076172), (' smoothing and normalization', 0.0003185272216796875)]
Smoothing and Normalization: 0.0005369186401367188
Fit RMSE: 0.04003489431304296
sklearn Test Against Ground Truth
Centering time: 0.014219999313354492
PCA fit: 1.9743711948394775
[[ 9.80393299e-01 -8.04905182e-06]
 [-8.04905182e-06  9.80774149e-01]]
PCA Transform: 0.05223560333251953
total iterations: 13
TLDA fit: 7.793844938278198
Whitened factor: 
[[-0.8788062   0.9052861 ]
 [ 0.477179   -0.42480254]]
PCA Reverse Transform: 0.000217437744140625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04007031442457256
Fit RMSE: 0.04033065376246044
 Test Against Ground Truth
[(' decentering', 0.0020461082458496094), (' smoothing and normalization', 0.0003437995910644531)]
Smoothing and Normalization: 0.0011074542999267578
Fit RMSE: 0.04003489431304296
sklearn Test Against Ground Truth
Centering time: 0.014081716537475586
PCA fit: 1.9698545932769775
[[ 9.80393299e-01 -8.04905182e-06]
 [-8.04905182e-06  9.80774149e-01]]
PCA Transform: 0.052132606506347656
total iterations: 11
TLDA fit: 6.4789934158325195
Whitened factor: 
[[-0.86632884  0.88233393]
 [ 0.49947414 -0.47062376]]
PCA Reverse Transform: 0.0002148151397705078
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0400691421046823
Fit RMSE: 0.040362754274400295
 Test Against Ground Truth
[(' decentering', 0.0020608901977539062), (' smoothing and normalization', 0.00029969215393066406)]
Smoothing and Normalization: 0.0005218982696533203
Fit RMSE: 0.04003489431304296
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.014171361923217773
PCA fit: 4.027852296829224
[[ 9.80394027e-01 -1.77985370e-06]
 [-1.77985370e-06  9.80415914e-01]]
PCA Transform: 0.08119511604309082
total iterations: 1000
TLDA fit: 642.1218333244324
Whitened factor: 
[[-0.9683396   0.84190345]
 [-0.24963637  0.539628  ]]
PCA Reverse Transform: 0.00023293495178222656
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03253064524731325
Fit RMSE: 0.032813658378928474
 Test Against Ground Truth
[(' decentering', 0.002119302749633789), (' smoothing and normalization', 0.0003097057342529297)]
Smoothing and Normalization: 0.0005235671997070312
Fit RMSE: 0.032482228168295234
sklearn Test Against Ground Truth
Centering time: 0.02154397964477539
PCA fit: 4.027300596237183
[[ 9.80394027e-01 -1.77985370e-06]
 [-1.77985370e-06  9.80415914e-01]]
PCA Transform: 0.08102560043334961
total iterations: 1000
TLDA fit: 649.3642072677612
Whitened factor: 
[[-0.93549764  0.8951384 ]
 [-0.353333    0.44578838]]
PCA Reverse Transform: 0.0003409385681152344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03253973964073642
Fit RMSE: 0.032799930827670784
 Test Against Ground Truth
[(' decentering', 0.0021810531616210938), (' smoothing and normalization', 0.0002999305725097656)]
Smoothing and Normalization: 0.0005290508270263672
Fit RMSE: 0.032482228168295234
sklearn Test Against Ground Truth
Centering time: 0.021461009979248047
PCA fit: 4.037795305252075
[[ 9.80394027e-01 -1.77985370e-06]
 [-1.77985370e-06  9.80415914e-01]]
PCA Transform: 0.08089089393615723
total iterations: 1000
TLDA fit: 644.6594896316528
Whitened factor: 
[[-0.92729133  0.90579593]
 [-0.37434053  0.4237142 ]]
PCA Reverse Transform: 0.00023603439331054688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.032541178186533494
Fit RMSE: 0.03279698288868827
 Test Against Ground Truth
[(' decentering', 0.0020875930786132812), (' smoothing and normalization', 0.00029754638671875)]
Smoothing and Normalization: 0.0005300045013427734
Fit RMSE: 0.032482228168295234
sklearn Test Against Ground Truth
Centering time: 0.02120375633239746
PCA fit: 4.0445170402526855
[[ 9.80394027e-01 -1.77985370e-06]
 [-1.77985370e-06  9.80415914e-01]]
PCA Transform: 0.08091616630554199
total iterations: 1000
TLDA fit: 644.9556474685669
Whitened factor: 
[[-0.96991825  0.83822113]
 [-0.24343091  0.5453305 ]]
PCA Reverse Transform: 0.0002346038818359375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.032529941211850544
Fit RMSE: 0.032814891386952934
 Test Against Ground Truth
[(' decentering', 0.002123117446899414), (' smoothing and normalization', 0.00029158592224121094)]
Smoothing and Normalization: 0.0005881786346435547
Fit RMSE: 0.032482228168295234
sklearn Test Against Ground Truth
Centering time: 0.021141767501831055
PCA fit: 4.041646957397461
[[ 9.80394027e-01 -1.77985370e-06]
 [-1.77985370e-06  9.80415914e-01]]
PCA Transform: 0.08095908164978027
total iterations: 1000
TLDA fit: 636.6334867477417
Whitened factor: 
[[-0.9368313   0.8925422 ]
 [-0.34978154  0.45096397]]
PCA Reverse Transform: 0.0002663135528564453
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03253941078331455
Fit RMSE: 0.03280123390861
 Test Against Ground Truth
[(' decentering', 0.0021147727966308594), (' smoothing and normalization', 0.00030422210693359375)]
Smoothing and Normalization: 0.0011768341064453125
Fit RMSE: 0.032482228168295234
sklearn Test Against Ground Truth
Centering time: 0.0214388370513916
PCA fit: 4.059354305267334
[[ 9.80394027e-01 -1.77985370e-06]
 [-1.77985370e-06  9.80415914e-01]]
PCA Transform: 0.08098387718200684
total iterations: 1000
TLDA fit: 648.2828230857849
Whitened factor: 
[[-0.9276044   0.90374786]
 [-0.37356418  0.4280653 ]]
PCA Reverse Transform: 0.00023746490478515625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03254099426133851
Fit RMSE: 0.032798847176943854
 Test Against Ground Truth
[(' decentering', 0.0021734237670898438), (' smoothing and normalization', 0.000308990478515625)]
Smoothing and Normalization: 0.0005273818969726562
Fit RMSE: 0.032482228168295234
sklearn Test Against Ground Truth
Vocab: 2000
num_tweets: 50000
density: 15
[1.0000000000000002, 0.9999999999999997]
Centering time: 0.028185606002807617
PCA fit: 7.337262868881226
[[ 9.80394322e-01 -1.40725810e-06]
 [-1.40725810e-06  9.80479678e-01]]
PCA Transform: 0.10257363319396973
total iterations: 1000
TLDA fit: 632.440443277359
Whitened factor: 
[[-0.9635495   0.8484193 ]
 [-0.26753038  0.52932477]]
PCA Reverse Transform: 0.000255584716796875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0281763056306149
Fit RMSE: 0.028385031825410793
 Test Against Ground Truth
[(' decentering', 0.0022611618041992188), (' smoothing and normalization', 0.00028228759765625)]
Smoothing and Normalization: 0.0005333423614501953
Fit RMSE: 0.02813571039095597
sklearn Test Against Ground Truth
Centering time: 0.02854752540588379
PCA fit: 7.359099626541138
[[ 9.80394322e-01 -1.40725810e-06]
 [-1.40725810e-06  9.80479678e-01]]
PCA Transform: 0.1043405532836914
total iterations: 1000
TLDA fit: 636.9841237068176
Whitened factor: 
[[-0.9320068   0.89616805]
 [-0.36244088  0.44371498]]
PCA Reverse Transform: 0.0002307891845703125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02817719585426833
Fit RMSE: 0.028380992949486813
 Test Against Ground Truth
[(' decentering', 0.002238750457763672), (' smoothing and normalization', 0.0002987384796142578)]
Smoothing and Normalization: 0.0005142688751220703
Fit RMSE: 0.02813571039095597
sklearn Test Against Ground Truth
Centering time: 0.028347492218017578
PCA fit: 7.378195524215698
[[ 9.80394322e-01 -1.40725810e-06]
 [-1.40725810e-06  9.80479678e-01]]
PCA Transform: 0.10025906562805176
total iterations: 1000
TLDA fit: 634.5413007736206
Whitened factor: 
[[-0.924775    0.9059794 ]
 [-0.38051438  0.42332166]]
PCA Reverse Transform: 0.00023484230041503906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02817705275612694
Fit RMSE: 0.028379649478843062
 Test Against Ground Truth
[(' decentering', 0.0022399425506591797), (' smoothing and normalization', 0.0002789497375488281)]
Smoothing and Normalization: 0.0005209445953369141
Fit RMSE: 0.02813571039095597
sklearn Test Against Ground Truth
Centering time: 0.02843189239501953
PCA fit: 7.369430780410767
[[ 9.80394322e-01 -1.40725810e-06]
 [-1.40725810e-06  9.80479678e-01]]
PCA Transform: 0.103271484375
total iterations: 1000
TLDA fit: 636.1070237159729
Whitened factor: 
[[-0.9647846   0.8434829 ]
 [-0.26304108  0.5371561 ]]
PCA Reverse Transform: 0.00022840499877929688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02817611452897071
Fit RMSE: 0.028387176003754736
 Test Against Ground Truth
[(' decentering', 0.0022461414337158203), (' smoothing and normalization', 0.0002913475036621094)]
Smoothing and Normalization: 0.0005192756652832031
Fit RMSE: 0.02813571039095597
sklearn Test Against Ground Truth
Centering time: 0.028395891189575195
PCA fit: 7.376539945602417
[[ 9.80394322e-01 -1.40725810e-06]
 [-1.40725810e-06  9.80479678e-01]]
PCA Transform: 0.10660219192504883
total iterations: 1000
TLDA fit: 637.5744457244873
Whitened factor: 
[[-0.9324416   0.8931971 ]
 [-0.36132076  0.44966558]]
PCA Reverse Transform: 0.00025272369384765625
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.02817719039658107
Fit RMSE: 0.028382964854503097
 Test Against Ground Truth
[(' decentering', 0.0022902488708496094), (' smoothing and normalization', 0.0002830028533935547)]
Smoothing and Normalization: 0.0005364418029785156
Fit RMSE: 0.02813571039095597
sklearn Test Against Ground Truth
Centering time: 0.028423547744750977
PCA fit: 7.391848802566528
[[ 9.80394322e-01 -1.40725810e-06]
 [-1.40725810e-06  9.80479678e-01]]
PCA Transform: 0.10434627532958984
total iterations: 1000
TLDA fit: 639.9057047367096
Whitened factor: 
[[-0.9241282  0.903085 ]
 [-0.3820827  0.429462 ]]
PCA Reverse Transform: 0.0002372264862060547
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.028177044227951052
Fit RMSE: 0.028382613821941186
 Test Against Ground Truth
[(' decentering', 0.002236604690551758), (' smoothing and normalization', 0.0002956390380859375)]
Smoothing and Normalization: 0.0005276203155517578
Fit RMSE: 0.02813571039095597
sklearn Test Against Ground Truth
Vocab: 100
num_tweets: 50000
density: 15
[0.9999999999999999, 1.0000000000000002]
Centering time: 0.003157377243041992
PCA fit: 0.1138768196105957
[[ 9.80392171e-01 -5.94460002e-08]
 [-5.94460003e-08  9.80400719e-01]]
PCA Transform: 0.005300760269165039
total iterations: 1000
TLDA fit: 655.090824842453
Whitened factor: 
[[-0.89829075  0.75447047]
 [-0.43940157  0.656334  ]]
PCA Reverse Transform: 0.0002522468566894531
decenter with new strategy:
[ 4.79332664 -2.56298215]
decenter with old strategy:
[8.19918301 0.56977695]
Fit RMSE new decenter: 0.12068945632014651
Fit RMSE: 0.12192954352753795
 Test Against Ground Truth
[(' decentering', 0.001971721649169922), (' smoothing and normalization', 0.0002803802490234375)]
Smoothing and Normalization: 0.0004799365997314453
Fit RMSE: 0.12040304176923583
sklearn Test Against Ground Truth
Centering time: 0.003082275390625
PCA fit: 0.11187887191772461
[[ 9.80392171e-01 -5.94460002e-08]
 [-5.94460003e-08  9.80400719e-01]]
PCA Transform: 0.0052831172943115234
total iterations: 1000
TLDA fit: 647.3828480243683
Whitened factor: 
[[-0.857677   0.8070951]
 [-0.5141888  0.5904214]]
PCA Reverse Transform: 0.00024127960205078125
decenter with new strategy:
[ 4.76883123 -2.81846608]
decenter with old strategy:
[8.0689688  0.38360532]
Fit RMSE new decenter: 0.12067113905277455
Fit RMSE: 0.12188574169421454
 Test Against Ground Truth
[(' decentering', 0.0019233226776123047), (' smoothing and normalization', 0.0002474784851074219)]
Smoothing and Normalization: 0.0004885196685791016
Fit RMSE: 0.12040304176923583
sklearn Test Against Ground Truth
Centering time: 0.0031070709228515625
PCA fit: 0.11084580421447754
[[ 9.80392171e-01 -5.94460002e-08]
 [-5.94460003e-08  9.80400719e-01]]
PCA Transform: 0.005290985107421875
total iterations: 1000
TLDA fit: 648.2825853824615
Whitened factor: 
[[-0.84816885  0.8184977 ]
 [-0.52972597  0.5745098 ]]
PCA Reverse Transform: 0.00024080276489257812
decenter with new strategy:
[ 4.7592258  -2.87634736]
decenter with old strategy:
[8.0373743  0.34418358]
Fit RMSE new decenter: 0.120666217172813
Fit RMSE: 0.12187443408239117
 Test Against Ground Truth
[(' decentering', 0.0019307136535644531), (' smoothing and normalization', 0.00026106834411621094)]
Smoothing and Normalization: 0.0005064010620117188
Fit RMSE: 0.12040304176923583
sklearn Test Against Ground Truth
Centering time: 0.003080606460571289
PCA fit: 0.11203479766845703
[[ 9.80392171e-01 -5.94460002e-08]
 [-5.94460003e-08  9.80400719e-01]]
PCA Transform: 0.005338191986083984
total iterations: 1000
TLDA fit: 652.0911729335785
Whitened factor: 
[[-0.89800596  0.75362283]
 [-0.43998322  0.657307  ]]
PCA Reverse Transform: 0.00023245811462402344
decenter with new strategy:
[ 4.79169492 -2.5595351 ]
decenter with old strategy:
[8.19830214 0.57282555]
Fit RMSE new decenter: 0.12068945934133049
Fit RMSE: 0.12193421503467701
 Test Against Ground Truth
[(' decentering', 0.0019295215606689453), (' smoothing and normalization', 0.0002491474151611328)]
Smoothing and Normalization: 0.00048661231994628906
Fit RMSE: 0.12040304176923583
sklearn Test Against Ground Truth
Centering time: 0.003081083297729492
PCA fit: 0.11207747459411621
[[ 9.80392171e-01 -5.94460002e-08]
 [-5.94460003e-08  9.80400719e-01]]
PCA Transform: 0.005318403244018555
total iterations: 1000
TLDA fit: 651.1242802143097
Whitened factor: 
[[-0.85609233  0.8074914 ]
 [-0.51682293  0.58987933]]
PCA Reverse Transform: 0.00023293495178222656
decenter with new strategy:
[ 4.76567248 -2.82151897]
decenter with old strategy:
[8.06372832 0.38222899]
Fit RMSE new decenter: 0.12067057551517214
Fit RMSE: 0.1218901829529535
 Test Against Ground Truth
[(' decentering', 0.001980304718017578), (' smoothing and normalization', 0.0002467632293701172)]
Smoothing and Normalization: 0.0004730224609375
Fit RMSE: 0.12040304176923583
sklearn Test Against Ground Truth
Centering time: 0.003092527389526367
PCA fit: 0.11199116706848145
[[ 9.80392171e-01 -5.94460002e-08]
 [-5.94460003e-08  9.80400719e-01]]
PCA Transform: 0.0053577423095703125
total iterations: 1000
TLDA fit: 643.4270389080048
Whitened factor: 
[[-0.8456249  0.8191348]
 [-0.5337776  0.5736011]]
PCA Reverse Transform: 0.00023293495178222656
decenter with new strategy:
[ 4.75395795 -2.88140926]
decenter with old strategy:
[8.02886178 0.34199221]
Fit RMSE new decenter: 0.12066526911952091
Fit RMSE: 0.12188119874524471
 Test Against Ground Truth
[(' decentering', 0.0019161701202392578), (' smoothing and normalization', 0.0002491474151611328)]
Smoothing and Normalization: 0.0005173683166503906
Fit RMSE: 0.12040304176923583
sklearn Test Against Ground Truth
Vocab: 500
num_tweets: 50000
density: 15
[1.0, 1.0]
Centering time: 0.0033850669860839844
PCA fit: 0.6613595485687256
[[ 9.80392388e-01 -1.65192093e-07]
 [-1.65192093e-07  9.80409345e-01]]
PCA Transform: 0.026577234268188477
total iterations: 1000
TLDA fit: 639.3713531494141
Whitened factor: 
[[-0.85550785  0.69192153]
 [-0.51778984  0.72197276]]
PCA Reverse Transform: 0.00023055076599121094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05608931371705033
Fit RMSE: 0.05659048048858459
 Test Against Ground Truth
[(' decentering', 0.001962900161743164), (' smoothing and normalization', 0.0002815723419189453)]
Smoothing and Normalization: 0.000514984130859375
Fit RMSE: 0.05600447269249625
sklearn Test Against Ground Truth
Centering time: 0.006948709487915039
PCA fit: 0.6419851779937744
[[ 9.80392388e-01 -1.65192093e-07]
 [-1.65192093e-07  9.80409345e-01]]
PCA Transform: 0.0265505313873291
total iterations: 1000
TLDA fit: 633.4720129966736
Whitened factor: 
[[-0.80843633  0.7511198 ]
 [-0.58858365  0.66016597]]
PCA Reverse Transform: 0.00024247169494628906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05610936920175848
Fit RMSE: 0.05655681204867759
 Test Against Ground Truth
[(' decentering', 0.002036571502685547), (' smoothing and normalization', 0.00029778480529785156)]
Smoothing and Normalization: 0.0010938644409179688
Fit RMSE: 0.05600447269249625
sklearn Test Against Ground Truth
Centering time: 0.00698542594909668
PCA fit: 0.6118805408477783
[[ 9.80392388e-01 -1.65192093e-07]
 [-1.65192093e-07  9.80409345e-01]]
PCA Transform: 0.026470422744750977
total iterations: 1000
TLDA fit: 635.2134082317352
Whitened factor: 
[[-0.79758555  0.7641556 ]
 [-0.6032058   0.6450318 ]]
PCA Reverse Transform: 0.00022482872009277344
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05611337825366809
Fit RMSE: 0.05654904701619613
 Test Against Ground Truth
[(' decentering', 0.0019638538360595703), (' smoothing and normalization', 0.0002894401550292969)]
Smoothing and Normalization: 0.0005273818969726562
Fit RMSE: 0.05600447269249625
sklearn Test Against Ground Truth
Centering time: 0.00707554817199707
PCA fit: 0.6491780281066895
[[ 9.80392388e-01 -1.65192093e-07]
 [-1.65192093e-07  9.80409345e-01]]
PCA Transform: 0.026406049728393555
total iterations: 1000
TLDA fit: 643.801769733429
Whitened factor: 
[[ 0.90231794 -0.9862124 ]
 [-0.4310714   0.16548444]]
PCA Reverse Transform: 0.00022792816162109375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05607904041107503
Fit RMSE: 0.056475588879198374
 Test Against Ground Truth
[(' decentering', 0.0020105838775634766), (' smoothing and normalization', 0.0003027915954589844)]
Smoothing and Normalization: 0.001081228256225586
Fit RMSE: 0.05600447269249625
sklearn Test Against Ground Truth
Centering time: 0.007023334503173828
PCA fit: 0.6524074077606201
[[ 9.80392388e-01 -1.65192093e-07]
 [-1.65192093e-07  9.80409345e-01]]
PCA Transform: 0.026485681533813477
total iterations: 1000
TLDA fit: 642.4377520084381
Whitened factor: 
[[ 0.93798435 -0.96653205]
 [-0.34667766  0.25654596]]
PCA Reverse Transform: 0.00022935867309570312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.056106098971851356
Fit RMSE: 0.056436248309761114
 Test Against Ground Truth
[(' decentering', 0.0019383430480957031), (' smoothing and normalization', 0.0002834796905517578)]
Smoothing and Normalization: 0.0011639595031738281
Fit RMSE: 0.05600447269249625
sklearn Test Against Ground Truth
Centering time: 0.006969928741455078
PCA fit: 0.6309378147125244
[[ 9.80392388e-01 -1.65192093e-07]
 [-1.65192093e-07  9.80409345e-01]]
PCA Transform: 0.026448965072631836
total iterations: 1000
TLDA fit: 647.3576538562775
Whitened factor: 
[[ 0.9452024  -0.9608078 ]
 [-0.32648504  0.2772154 ]]
PCA Reverse Transform: 0.00024366378784179688
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.05611188246464459
Fit RMSE: 0.056428083267327414
 Test Against Ground Truth
[(' decentering', 0.0019578933715820312), (' smoothing and normalization', 0.00030350685119628906)]
Smoothing and Normalization: 0.0005877017974853516
Fit RMSE: 0.05600447269249625
sklearn Test Against Ground Truth
Vocab: 1000
num_tweets: 50000
density: 15
[1.0, 0.9999999999999998]
Centering time: 0.013957977294921875
PCA fit: 1.9926531314849854
[[9.80392912e-01 3.23952435e-06]
 [3.23952435e-06 9.80461150e-01]]
PCA Transform: 0.0523533821105957
total iterations: 1000
TLDA fit: 630.7948868274689
Whitened factor: 
[[-0.9813188   0.8813656 ]
 [-0.1923888   0.47243482]]
PCA Reverse Transform: 0.00024127960205078125
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.039729922360305224
Fit RMSE: 0.040001346813974346
 Test Against Ground Truth
[(' decentering', 0.002089977264404297), (' smoothing and normalization', 0.00029397010803222656)]
Smoothing and Normalization: 0.0005114078521728516
Fit RMSE: 0.03967178229989315
sklearn Test Against Ground Truth
Centering time: 0.01406407356262207
PCA fit: 1.9737114906311035
[[9.80392912e-01 3.23952435e-06]
 [3.23952435e-06 9.80461150e-01]]
PCA Transform: 0.05214262008666992
total iterations: 1000
TLDA fit: 632.9639973640442
Whitened factor: 
[[-0.9567826   0.92455393]
 [-0.2908043   0.3810512 ]]
PCA Reverse Transform: 0.00023055076599121094
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03973662992950255
Fit RMSE: 0.03998731669743021
 Test Against Ground Truth
[(' decentering', 0.002149820327758789), (' smoothing and normalization', 0.0002980232238769531)]
Smoothing and Normalization: 0.0005338191986083984
Fit RMSE: 0.03967178229989315
sklearn Test Against Ground Truth
Centering time: 0.014313220977783203
PCA fit: 1.9779167175292969
[[9.80392912e-01 3.23952435e-06]
 [3.23952435e-06 9.80461150e-01]]
PCA Transform: 0.052093505859375
total iterations: 1000
TLDA fit: 642.4928076267242
Whitened factor: 
[[-0.95059294  0.93300533]
 [-0.3104401   0.35986272]]
PCA Reverse Transform: 0.0002448558807373047
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.039737542270066636
Fit RMSE: 0.03998438263843921
 Test Against Ground Truth
[(' decentering', 0.002033710479736328), (' smoothing and normalization', 0.00029850006103515625)]
Smoothing and Normalization: 0.0005309581756591797
Fit RMSE: 0.03967178229989315
sklearn Test Against Ground Truth
Centering time: 0.014233827590942383
PCA fit: 1.9592812061309814
[[9.80392912e-01 3.23952435e-06]
 [3.23952435e-06 9.80461150e-01]]
PCA Transform: 0.05339789390563965
total iterations: 1000
TLDA fit: 631.6472325325012
Whitened factor: 
[[ 0.7492586  -0.8413744 ]
 [-0.66227764  0.5404528 ]]
PCA Reverse Transform: 0.0002486705780029297
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.039739110341417944
Fit RMSE: 0.040088815655278366
 Test Against Ground Truth
[(' decentering', 0.0020532608032226562), (' smoothing and normalization', 0.0002932548522949219)]
Smoothing and Normalization: 0.0005395412445068359
Fit RMSE: 0.03967178229989315
sklearn Test Against Ground Truth
Centering time: 0.014278411865234375
PCA fit: 1.9729294776916504
[[9.80392912e-01 3.23952435e-06]
 [3.23952435e-06 9.80461150e-01]]
PCA Transform: 0.05218100547790527
total iterations: 73
TLDA fit: 45.79285407066345
Whitened factor: 
[[ 0.74073124 -0.7735659 ]
 [-0.67180145  0.6337158 ]]
PCA Reverse Transform: 0.00021409988403320312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03974048050064711
Fit RMSE: 0.040154911982859295
 Test Against Ground Truth
[(' decentering', 0.002034902572631836), (' smoothing and normalization', 0.0002999305725097656)]
Smoothing and Normalization: 0.0005116462707519531
Fit RMSE: 0.03967178229989315
sklearn Test Against Ground Truth
Centering time: 0.014299631118774414
PCA fit: 1.993192434310913
[[9.80392912e-01 3.23952435e-06]
 [3.23952435e-06 9.80461150e-01]]
PCA Transform: 0.05223870277404785
total iterations: 166
TLDA fit: 105.24288749694824
Whitened factor: 
[[ 0.74028456 -0.75836456]
 [-0.6722937   0.6518306 ]]
PCA Reverse Transform: 0.0002205371856689453
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03974076350701992
Fit RMSE: 0.04016858148798802
 Test Against Ground Truth
[(' decentering', 0.0020699501037597656), (' smoothing and normalization', 0.0002989768981933594)]
Smoothing and Normalization: 0.0007584095001220703
Fit RMSE: 0.03967178229989315
sklearn Test Against Ground Truth
Vocab: 1500
num_tweets: 50000
density: 15
[1.0, 0.9999999999999998]
Centering time: 0.008835792541503906
PCA fit: 3.9951095581054688
[[9.80394100e-01 1.54902784e-05]
 [1.54902784e-05 9.80594708e-01]]
PCA Transform: 0.08125662803649902
total iterations: 1000
TLDA fit: 642.612119436264
Whitened factor: 
[[-0.17475417 -0.20093827]
 [-0.9846121   0.97960395]]
PCA Reverse Transform: 0.00023221969604492188
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03421791515254187
Fit RMSE: 0.03470215957657022
 Test Against Ground Truth
[(' decentering', 0.002117156982421875), (' smoothing and normalization', 0.0002989768981933594)]
Smoothing and Normalization: 0.0005211830139160156
Fit RMSE: 0.03311123330110081
sklearn Test Against Ground Truth
Centering time: 0.021557092666625977
PCA fit: 4.017092704772949
[[9.80394100e-01 1.54902784e-05]
 [1.54902784e-05 9.80594708e-01]]
PCA Transform: 0.08091950416564941
total iterations: 1000
TLDA fit: 637.3152892589569
Whitened factor: 
[[-0.04874568 -0.07633777]
 [-0.9988112   0.99708205]]
PCA Reverse Transform: 0.00023126602172851562
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.034650896612010845
Fit RMSE: 0.03469897898661393
 Test Against Ground Truth
[(' decentering', 0.002119302749633789), (' smoothing and normalization', 0.0002918243408203125)]
Smoothing and Normalization: 0.0005431175231933594
Fit RMSE: 0.03311123330110081
sklearn Test Against Ground Truth
Centering time: 0.021546363830566406
PCA fit: 4.031141042709351
[[9.80394100e-01 1.54902784e-05]
 [1.54902784e-05 9.80594708e-01]]
PCA Transform: 0.08098554611206055
total iterations: 1000
TLDA fit: 637.5229277610779
Whitened factor: 
[[-0.02196999 -0.04804093]
 [-0.99975866  0.99884546]]
PCA Reverse Transform: 0.000225067138671875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03471645893080411
Fit RMSE: 0.034699811570859315
 Test Against Ground Truth
[(' decentering', 0.0020949840545654297), (' smoothing and normalization', 0.00029659271240234375)]
Smoothing and Normalization: 0.0005180835723876953
Fit RMSE: 0.03311123330110081
sklearn Test Against Ground Truth
Centering time: 0.022031068801879883
PCA fit: 4.051210880279541
[[9.80394100e-01 1.54902784e-05]
 [1.54902784e-05 9.80594708e-01]]
PCA Transform: 0.0827333927154541
total iterations: 13
TLDA fit: 7.871164321899414
Whitened factor: 
[[-0.82380074  0.96490216]
 [ 0.5668794  -0.2626097 ]]
PCA Reverse Transform: 0.00022339820861816406
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03314443620033356
Fit RMSE: 0.033337462248469354
 Test Against Ground Truth
[(' decentering', 0.0021724700927734375), (' smoothing and normalization', 0.00029778480529785156)]
Smoothing and Normalization: 0.0005478858947753906
Fit RMSE: 0.03311123330110081
sklearn Test Against Ground Truth
Centering time: 0.02165675163269043
PCA fit: 4.031833171844482
[[9.80394100e-01 1.54902784e-05]
 [1.54902784e-05 9.80594708e-01]]
PCA Transform: 0.08104276657104492
total iterations: 1000
TLDA fit: 639.6249377727509
Whitened factor: 
[[-0.04627854 -0.07659081]
 [-0.99892855  0.9970627 ]]
PCA Reverse Transform: 0.0002300739288330078
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0346436358706213
Fit RMSE: 0.03469661714470739
 Test Against Ground Truth
[(' decentering', 0.0021080970764160156), (' smoothing and normalization', 0.00033402442932128906)]
Smoothing and Normalization: 0.0005259513854980469
Fit RMSE: 0.03311123330110081
sklearn Test Against Ground Truth
Centering time: 0.02149677276611328
PCA fit: 4.032543182373047
[[9.80394100e-01 1.54902784e-05]
 [1.54902784e-05 9.80594708e-01]]
PCA Transform: 0.08104085922241211
total iterations: 1000
TLDA fit: 637.2913782596588
Whitened factor: 
[[-0.01793042 -0.04828083]
 [-0.9998393   0.99883384]]
PCA Reverse Transform: 0.00023174285888671875
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.034706236158776285
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cupy/__init__.py:778: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  return _numpy.asarray(a, order=order)
starting
text length: 
11314
2098
53845
got data
(11314, 2098)
('sklearn LDA', 175.79474115371704)
('gensim LDA', 270.7281310558319)
Centering time: 0.07977294921875
PCA fit: 3.5823886394500732
PCA Transform: 0.31182408332824707
total iterations: 11
TLDA fit: 6.24299430847168
PCA Reverse Transform: 0.0014755725860595703
weights shape:
(2,)
fac2 shape: 
(2098, 2)
decenter with new strategy:
[ 0.02678672 -0.00088579]
Decentering: 0.0008149147033691406
decenter with old strategy:
[0.04586426 0.01838031]
Smoothing and Normalization: 0.0033385753631591797
[('centering', 0.07977294921875), ('PCA fit', 3.5823886394500732), ('PCA transform', 0.31182408332824707), ('TLDA fit', 6.24299430847168), ('unwhiten factors', 0.0014755725860595703), (' decentering', 0.0008149147033691406), (' smoothing and normalization', 0.0033385753631591797)]
(2, 2098)
(2098, 2098)
(2, 2098)
(2, 2097)
(2, 2098)
sklearn
[-0.2724446463108879, -0.2670881019222048] [0.4411738131854339, 0.3365636606437144] -0.26976637411654636 0.38886873691457413
gensim
[-0.06957752152813192, -0.021593771313574955] [0.018556588806976602, -0.00027865489219301194] -0.045585646420853436 0.009138966957391794
tlda
[-0.38284449604581927, -0.23346564279871565] [0.24851466507614695, 0.4306841790240866] -0.30815506942226745 0.3395994220501168
-1.966827103056814
[array(['belong', 'babi', 'differ', 'eye', 'announc', 'consid', 'dual',
       'catch', 'author', 'accus', 'analog', 'biggest', 'binari', 'data',
       'attend', 'countri', 'andi', 'award', 'adopt', 'circuit', 'aid',
       'attempt', 'constitut', 'applic', 'away', 'break', 'apart', 'bill',
       'advic', 'allow', 'hope', 'depth', 'cover', 'figur', 'australia',
       'decrypt', 'annual', 'around', 'blood', 'sinc', 'comput',
       'counter', 'dan', 'medicin', 'easi', 'mirror', 'explan',
       'acknowledg', 'common', 'approach', 'angl', 'due', 'appreci',
       'note', 'accord', 'argument', 'chip', 'cheer', 'eight', 'attack',
       'destruct', 'core', 'bound', 'estim', 'comfort', 'brief', 'adjust',
       'agre', 'border', 'armenian', 'concern', 'cop', 'excus', 'came',
       'alter', 'bush', 'conclud', 'faith', 'eric', 'boot', 'along',
       'august', 'achiev', 'awar', 'scientist', 'attent', 'identifi',
       'faq', 'build', 'ask', 'extern', 'access', 'age', 'chang', 'dave',
       'contain', 'bomb', 'buy', 'dark', 'stream'], dtype='<U13'), array(['larg', 'found', 'bought', 'chain', 'etc', 'clearli', 'elimin',
       'huge', 'abus', 'japanes', 'code', 'charg', 'armenian',
       'california', 'eastern', 'among', 'clean', 'across', 'best', 'huh',
       'cpu', 'deal', 'blame', 'life', 'accur', 'cheer', 'brain', 'ask',
       'assembl', 'black', 'closer', 'chang', 'civil', 'argic',
       'constant', 'east', 'democrat', 'cultur', 'electr', 'bear',
       'beyond', 'around', 'back', 'easi', 'investig', 'dollar', 'border',
       'approxim', 'band', 'eat', 'chip', 'appli', 'amount', 'basi',
       'deep', 'director', 'cap', 'brand', 'histor', 'anyth', 'dead',
       'absolut', 'decrypt', 'aid', 'ought', 'cool', 'mostli', 'church',
       'becam', 'gave', 'classifi', 'comment', 'cop', 'basebal', 'arm',
       'hot', 'consequ', 'center', 'despit', 'awar', 'acknowledg', 'burn',
       'insist', 'although', 'factor', 'fit', 'clear', 'pixel', 'decemb',
       'american', 'divis', 'bias', 'face', 'duti', 'chri', 'batteri',
       'eight', 'confus', 'greater', 'branch'], dtype='<U13')]
-4.752713731642503
[array(['file', 'edu', 'program', 'imag', 'system', 'avail', 'includ',
       'inform', 'entri', 'window', 'output', 'com', 'ftp', 'version',
       'list', 'pub', 'mail', 'line', 'data', 'set', 'run', 'anonym',
       'name', 'server', 'number', 'support', 'user', 'format', 'sourc',
       'bit', 'gener', 'base', 'softwar', 'widget', 'graphic', 'read',
       'send', 'info', 'state', 'code', 'provid', 'subject', 'display',
       'color', 'comput', 'applic', 'control', 'part', 'build', 'max',
       'note', 'address', 'check', 'space', 'current', 'requir',
       'section', 'follow', 'must', 'email', 'open', 'packag', 'site',
       'archiv', 'case', 'find', 'differ', 'sun', 'chang', 'internet',
       'gun', 'group', 'type', 'start', 'public', 'help', 'author',
       'directori', 'return', 'mit', 'machin', 'convert', 'question',
       'etc', 'standard', 'key', 'rule', 'said', 'write', 'mean',
       'messag', 'drive', 'law', 'gif', 'exampl', 'function', 'free',
       'faq', 'resourc', 'size'], dtype='<U13'), array(['max', 'file', 'system', 'program', 'edu', 'god', 'part', 'window',
       'includ', 'number', 'run', 'question', 'read', 'said', 'believ',
       'line', 'state', 'drive', 'output', 'mean', 'inform', 'key',
       'differ', 'someth', 'bit', 'back', 'name', 'help', 'find', 'sinc',
       'gener', 'case', 'com', 'day', 'start', 'follow', 'end', 'realli',
       'game', 'set', 'law', 'go', 'still', 'must', 'person', 'give',
       'last', 'govern', 'mail', 'reason', 'ask', 'sure', 'control',
       'support', 'space', 'power', 'avail', 'let', 'interest', 'list',
       'anoth', 'might', 'group', 'made', 'possibl', 'long', 'data',
       'without', 'etc', 'got', 'christian', 'better', 'chang', 'tell',
       'never', 'exist', 'base', 'fact', 'armenian', 'live', 'put', 'lot',
       'place', 'car', 'card', 'check', 'probabl', 'howev', 'entri',
       'provid', 'comput', 'world', 'actual', 'play', 'sourc', 'second',
       'around', 'air', 'gun', 'open'], dtype='<U13')]
-1.9619329737399704
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
text length: 
11314
Traceback (most recent call last):
  File "coherence_comparison.py", line 378, in <module>
    main()
  File "coherence_comparison.py", line 277, in main
    texts, vocab, texts_lemmatized = get_uci_data()
  File "coherence_comparison.py", line 107, in get_uci_data
    texts = [stem(removeStopwords(tokenize(regexchars(cleanLine(line))))) for line in texts]
  File "coherence_comparison.py", line 107, in <listcomp>
    texts = [stem(removeStopwords(tokenize(regexchars(cleanLine(line))))) for line in texts]
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/preprocess_efficient.py", line 95, in removeStopwords
    return list(filtered2)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/preprocess_efficient.py", line 93, in <lambda>
    filtered = filter(lambda word: word not in stop_words, words)
KeyboardInterrupt
starting
text length: 
11314
2098
53845
got data
(11314, 2098)
('sklearn LDA', 61.524845361709595)
('gensim LDA', 66.69794964790344)
Centering time: 0.1230778694152832
PCA fit: 3.134061574935913
PCA Transform: 0.01362752914428711
total iterations: 500
TLDA fit: 18.087917804718018
PCA Reverse Transform: 0.00040841102600097656
Decentering: 0.05163431167602539
decenter with old strategy:
[0.03849629 0.01917369 0.0188611  0.02251584 0.03703446 0.02244144
 0.02175353 0.0223966  0.02978688 0.03340731 0.02181191 0.03277017
 0.02284825 0.03932109 0.01881159 0.03417136 0.02539059 0.02569819
 0.02106477 0.04830651]
Smoothing and Normalization: 0.00032806396484375
[('centering', 0.1230778694152832), ('PCA fit', 3.134061574935913), ('PCA transform', 0.01362752914428711), ('TLDA fit', 18.087917804718018), ('unwhiten factors', 0.00040841102600097656), (' decentering', 0.05163431167602539), (' smoothing and normalization', 0.00032806396484375)]
(20, 2098)
(2098, 2098)
(20, 2098)
(20, 2097)
(20, 2098)
-2.0644126250295116
[array(['huh', 'etc', 'huge', 'elimin', 'pixel', 'behind', 'innoc',
       'particip', 'bill', 'ought', 'inde', 'van', 'target', 'forget',
       'insist', 'homosexu', 'loss', 'bar', 'algorithm', 'indic'],
      dtype='<U13'), array(['eye', 'angl', 'depth', 'award', 'differ', 'catch', 'belong',
       'sinc', 'roman', 'note', 'propos', 'contain', 'fear', 'auto',
       'tabl', 'north', 'age', 'backup', 'brief', 'australia'],
      dtype='<U13'), array(['presenc', 'cop', 'gather', 'compet', 'exampl', 'comput',
       'scientif', 'commun', 'field', 'rememb', 'expand', 'dark',
       'concern', 'enhanc', 'fax', 'court', 'annoy', 'applic', 'show',
       'elsewher'], dtype='<U13'), array(['eastern', 'amount', 'across', 'gave', 'california', 'beyond',
       'apr', 'arm', 'approxim', 'east', 'constant', 'although', 'chain',
       'brain', 'best', 'doctrin', 'detroit', 'engag', 'clearli', 'dead'],
      dtype='<U13'), array(['absolut', 'capit', 'access', 'send', 'season', 'liber', 'promot',
       'health', 'enabl', 'rumor', 'primarili', 'car', 'larri', 'announc',
       'along', 'grant', 'career', 'declar', 'accid', 'effort'],
      dtype='<U13'), array(['life', 'demand', 'maintain', 'civilian', 'occasion', 'electr',
       'huge', 'foot', 'pre', 'machin', 'mar', 'pray', 'pictur', 'man',
       'etc', 'hard', 'democrat', 'defend', 'confus', 'own'], dtype='<U13'), array(['japanes', 'cpu', 'argic', 'david', 'broken', 'known', 'typic',
       'significantli', 'keep', 'inher', 'suppli', 'best', 'howev',
       'public', 'comment', 'dog', 'crime', 'predict', 'simm', 'jxp'],
      dtype='<U13'), array(['attitud', 'chang', 'dollar', 'batteri', 'around', 'bought',
       'honda', 'identifi', 'beauti', 'april', 'facil', 'closer',
       'anywher', 'ibm', 'gotten', 'acknowledg', 'eight', 'elimin', 'cap',
       'author'], dtype='<U13'), array(['dual', 'advic', 'bound', 'bill', 'build', 'report', 'babi',
       'blue', 'afraid', 'debat', 'product', 'binari', 'complet',
       'respect', 'measur', 'privaci', 'adopt', 'creat', 'award',
       'strang'], dtype='<U13'), array(['found', 'charg', 'chain', 'civil', 'challeng', 'happi', 'black',
       'limit', 'child', 'brand', 'director', 'combin', 'safeti', 'appli',
       'charact', 'clearli', 'okay', 'futur', 'chip', 'batteri'],
      dtype='<U13'), array(['armenian', 'eat', 'adequ', 'oil', 'entir', 'decemb', 'deal',
       'anoth', 'presid', 'cultur', 'easi', 'code', 'california',
       'around', 'hook', 'shut', 'ohio', 'black', 'cheer', 'henc'],
      dtype='<U13'), array(['clipper', 'church', 'babi', 'motif', 'jesu', 'clearli', 'code',
       'mother', 'basic', 'record', 'trust', 'spot', 'backup', 'fed',
       'cellular', 'spring', 'food', 'substanti', 'els', 'free'],
      dtype='<U13'), array(['consid', 'hope', 'attempt', 'eric', 'home', 'huge', 'babi',
       'identifi', 'analog', 'idea', 'war', 'map', 'mari', 'attent',
       'agre', 'came', 'binari', 'aid', 'elimin', 'men'], dtype='<U13'), array(['adopt', 'due', 'appreci', 'analog', 'medicin', 'away', 'babi',
       'eye', 'pull', 'stream', 'buy', 'catch', 'approach', 'bomb',
       'along', 'bigger', 'attempt', 'provid', 'fan', 'resid'],
      dtype='<U13'), array(['data', 'everywher', 'otherwis', 'dave', 'attend', 'argument',
       'function', 'corpor', 'mostli', 'overal', 'divis', 'gone', 'aid',
       'go', 'eight', 'vendor', 'dod', 'bottom', 'accus', 'fourth'],
      dtype='<U13'), array(['countri', 'convert', 'nazi', 'aw', 'morn', 'binari', 'babi',
       'format', 'nasa', 'continu', 'launch', 'noth', 'near', 'moral',
       'network', 'conclud', 'achiev', 'correspond', 'concern', 'corpor'],
      dtype='<U13'), array(['bought', 'abus', 'among', 'atheist', 'assembl', 'anyth', 'deep',
       'eastern', 'american', 'committe', 'clean', 'california', 'brain',
       'german', 'conveni', 'electr', 'closer', 'chri', 'kick', 'fix'],
      dtype='<U13'), array(['larg', 'border', 'fit', 'fals', 'month', 'involv', 'present',
       'student', 'hate', 'built', 'constant', 'provid', 'trip', 'requir',
       'mechan', 'compet', 'holi', 'violent', 'hold', 'rocket'],
      dtype='<U13'), array(['announc', 'author', 'accus', 'biggest', 'circuit', 'blood',
       'belong', 'constitut', 'break', 'data', 'allow', 'attend', 'dan',
       'away', 'babi', 'corrupt', 'collect', 'annual', 'destruct',
       'differ'], dtype='<U13'), array(['belong', 'explan', 'made', 'differ', 'excus', 'faith', 'applic',
       'accord', 'ill', 'complex', 'aid', 'lift', 'around', 'drink',
       'bear', 'wall', 'aspect', 'acknowledg', 'technic', 'turn'],
      dtype='<U13')]
-3.9466284134298943
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[array(['presid', 'go', 'said', 'someth', 'program', 'question', 'packag',
       'believ', 'administr', 'made', 'group', 'state', 'talk', 'max',
       'job', 'mean', 'consid', 'govern', 'support', 'discuss'],
      dtype='<U13'), array(['entri', 'file', 'program', 'section', 'rule', 'must', 'build',
       'info', 'number', 'sourc', 'follow', 'max', 'remark', 'list',
       'compil', 'line', 'includ', 'author', 'name', 'send'], dtype='<U13'), array(['output', 'file', 'program', 'entri', 'max', 'line', 'check',
       'stream', 'open', 'build', 'return', 'read', 'name', 'remark',
       'write', 'rule', 'includ', 'info', 'author', 'size'], dtype='<U13'), array(['window', 'avail', 'edu', 'program', 'includ', 'file', 'version',
       'system', 'com', 'server', 'subject', 'motif', 'inform', 'sourc',
       'run', 'base', 'mit', 'sun', 'support', 'ftp'], dtype='<U13'), array(['bit', 'mac', 'scsi', 'program', 'system', 'edu', 'ibm', 'window',
       'max', 'number', 'chip', 'state', 'com', 'key', 'do', 'run',
       'control', 'card', 'data', 'sinc'], dtype='<U13'), array(['imag', 'bit', 'file', 'color', 'mac', 'program', 'gif', 'window',
       'scsi', 'version', 'system', 'format', 'max', 'run', 'softwar',
       'do', 'display', 'ibm', 'hardwar', 'drive'], dtype='<U13'), array(['file', 'gun', 'imag', 'state', 'firearm', 'control', 'congress',
       'law', 'bill', 'gif', 'amend', 'program', 'max', 'format', 'unit',
       'color', 'ftp', 'avail', 'system', 'inform'], dtype='<U13'), array(['said', 'armenian', 'someth', 'go', 'went', 'start', 'happen',
       'apart', 'kill', 'came', 'live', 'day', 'tell', 'ask', 'back',
       'told', 'max', 'name', 'everyth', 'build'], dtype='<U13'), array(['god', 'believ', 'jesu', 'atheist', 'christian', 'exist', 'mean',
       'question', 'religion', 'max', 'must', 'atheism', 'system',
       'belief', 'true', 'state', 'person', 'gener', 'bibl', 'file'],
      dtype='<U13'), array(['anonym', 'internet', 'privaci', 'file', 'system', 'inform',
       'mail', 'user', 'email', 'comput', 'servic', 'edu', 'address',
       'network', 'secur', 'messag', 'gener', 'site', 'pub', 'server'],
      dtype='<U13'), array(['drive', 'disk', 'system', 'control', 'hard', 'support', 'bio',
       'rom', 'card', 'featur', 'includ', 'max', 'floppi', 'data', 'must',
       'head', 'interfac', 'run', 'power', 'speed'], dtype='<U13'), array(['war', 'turkish', 'jew', 'state', 'govern', 'armenian', 'russian',
       'south', 'program', 'file', 'max', 'secret', 'island', 'world',
       'militari', 'unit', 'nuclear', 'american', 'turkey', 'power'],
      dtype='<U13'), array(['edu', 'com', 'graphic', 'pub', 'avail', 'file', 'mail', 'ftp',
       'system', 'includ', 'server', 'imag', 'send', 'program', 'list',
       'data', 'support', 'version', 'run', 'base'], dtype='<U13'), array(['imag', 'program', 'system', 'data', 'state', 'number', 'health',
       'center', 'report', 'includ', 'research', 'avail', 'univers',
       'inform', 'nation', 'max', 'space', 'gener', 'turkish', 'case'],
      dtype='<U13'), array(['max', 'system', 'file', 'edu', 'program', 'god', 'part', 'window',
       'includ', 'run', 'question', 'state', 'said', 'number', 'believ',
       'mean', 'drive', 'inform', 'key', 'differ'], dtype='<U13'), array(['widget', 'window', 'file', 'applic', 'set', 'avail', 'edu',
       'system', 'resourc', 'program', 'includ', 'convert', 'valu',
       'inform', 'server', 'data', 'run', 'list', 'type', 'com'],
      dtype='<U13'), array(['space', 'launch', 'satellit', 'system', 'data', 'program',
       'orbit', 'max', 'nasa', 'market', 'edu', 'commerci', 'includ',
       'servic', 'gener', 'mission', 'technolog', 'report', 'project',
       'inform'], dtype='<U13'), array(['imag', 'file', 'gif', 'format', 'program', 'color', 'avail',
       'data', 'version', 'system', 'display', 'softwar', 'graphic',
       'set', 'ftp', 'includ', 'edu', 'max', 'packag', 'free'],
      dtype='<U13'), array(['hockey', 'team', 'game', 'edu', 'leagu', 'season', 'nhl', 'play',
       'list', 'player', 'max', 'draft', 'divis', 'mail', 'com', 'final',
       'avail', 'said', 'score', 'win'], dtype='<U13'), array(['key', 'encrypt', 'system', 'inform', 'bit', 'secur', 'law',
       'comput', 'privaci', 'file', 'chip', 'pub', 'data', 'mail',
       'program', 'public', 'avail', 'max', 'govern', 'part'],
      dtype='<U13')]
-2.153973598154459
starting
text length: 
11314
2098
53845
got data
(11314, 2098)
('sklearn LDA', 61.12182664871216)
('gensim LDA', 67.41542291641235)
Centering time: 0.12792253494262695
PCA fit: 3.1883084774017334
PCA Transform: 0.013394594192504883
total iterations: 200
TLDA fit: 7.510876893997192
PCA Reverse Transform: 0.00042247772216796875
Decentering: 0.051262855529785156
decenter with old strategy:
[0.02101314 0.04039386 0.03697993 0.02609831 0.02109776 0.03242912
 0.03701494 0.04824449 0.0218663  0.01881162 0.03477226 0.02979298
 0.02363587 0.02540895 0.03903754 0.01388883 0.02234778 0.01917702
 0.0218545  0.02492667]
Smoothing and Normalization: 0.0003390312194824219
[('centering', 0.12792253494262695), ('PCA fit', 3.1883084774017334), ('PCA transform', 0.013394594192504883), ('TLDA fit', 7.510876893997192), ('unwhiten factors', 0.00042247772216796875), (' decentering', 0.051262855529785156), (' smoothing and normalization', 0.0003390312194824219)]
(20, 2098)
(2098, 2098)
(20, 2098)
(20, 2097)
(20, 2098)
-2.222849283498694
[array(['bottom', 'fourth', 'ought', 'realiti', 'algorithm', 'declar',
       'hook', 'cours', 'liber', 'overal', 'equival', 'aid', 'compon',
       'mostli', 'sub', 'etc', 'everywher', 'critic', 'attend', 'discov'],
      dtype='<U13'), array(['belong', 'explan', 'made', 'excus', 'differ', 'applic', 'ill',
       'complex', 'lift', 'aid', 'accord', 'technic', 'acknowledg',
       'bear', 'around', 'august', 'chip', 'drink', 'field', 'turn'],
      dtype='<U13'), array(['huh', 'etc', 'huge', 'elimin', 'pixel', 'particip', 'behind',
       'innoc', 'insist', 'bill', 'identifi', 'implement', 'inde', 'code',
       'detroit', 'ought', 'van', 'ibm', 'cover', 'intellig'],
      dtype='<U13'), array(['attempt', 'hope', 'home', 'consid', 'idea', 'attent', 'scientist',
       'came', 'war', 'eric', 'huge', 'elimin', 'binari', 'men', 'hospit',
       'geb', 'compress', 'attend', 'aid', 'goal'], dtype='<U13'), array(['presenc', 'gather', 'fix', 'draft', 'commun', 'current',
       'america', 'elsewher', 'faith', 'rais', 'coverag', 'whenev',
       'flaw', 'mass', 'given', 'treatment', 'bunch', 'flame', 'agenc',
       'annoy'], dtype='<U13'), array(['circuit', 'sinc', 'dan', 'counter', 'break', 'angl', 'away',
       'extern', 'biggest', 'cover', 'religion', 'applic', 'analog',
       'third', 'previou', 'legitim', 'propos', 'traffic', 'data',
       'rememb'], dtype='<U13'), array(['japanes', 'cpu', 'argic', 'david', 'broken', 'known', 'typic',
       'keep', 'significantli', 'best', 'catch', 'inher', 'howev',
       'suppli', 'public', 'comment', 'apolog', 'driven', 'introduc',
       'area'], dtype='<U13'), array(['black', 'chain', 'armenian', 'clearli', 'deal', 'dollar',
       'batteri', 'cheer', 'bank', 'brand', 'code', 'church', 'oil',
       'clipper', 'bear', 'california', 'blame', 'chief', 'chang', 'awar'],
      dtype='<U13'), array(['found', 'charg', 'civil', 'challeng', 'limit', 'chain', 'charact',
       'child', 'consult', 'appli', 'weight', 'director', 'elimin',
       'lord', 'clear', 'economi', 'clean', 'chip', 'peter', 'graphic'],
      dtype='<U13'), array(['absolut', 'argument', 'compromis', 'brought', 'access', 'capit',
       'go', 'cop', 'door', 'dark', 'otherwis', 'concern', 'apr', 'engag',
       'death', 'gave', 'divid', 'border', 'exampl', 'cheer'],
      dtype='<U13'), array(['dual', 'adopt', 'advic', 'babi', 'bill', 'bound', 'due',
       'appreci', 'stream', 'analog', 'pull', 'build', 'bomb', 'bush',
       'complet', 'brother', 'report', 'scientif', 'along', 'cooper'],
      dtype='<U13'), array(['consid', 'identifi', 'eric', 'mari', 'hope', 'agre', 'map',
       'analog', 'profession', 'profit', 'babi', 'border', 'support',
       'mask', 'foot', 'decrypt', 'offer', 'honest', 'pick', 'aid'],
      dtype='<U13'), array(['life', 'happi', 'civilian', 'demand', 'safeti', 'maintain', 'gun',
       'occasion', 'etc', 'foot', 'huge', 'pre', 'electr', 'mar',
       'pictur', 'pray', 'claim', 'democrat', 'hard', 'man'], dtype='<U13'), array(['across', 'eastern', 'among', 'california', 'beyond', 'gave',
       'approxim', 'factor', 'back', 'amount', 'classifi', 'abus',
       'apart', 'ask', 'dead', 'center', 'duti', 'assembl', 'approv',
       'around'], dtype='<U13'), array(['belong', 'blood', 'differ', 'medicin', 'babi', 'biggest', 'figur',
       'eye', 'announc', 'collect', 'destruct', 'break', 'corrupt',
       'catch', 'warrant', 'fear', 'refus', 'away', 'binari', 'data'],
      dtype='<U13'), array(['announc', 'author', 'accus', 'constitut', 'allow', 'annual',
       'attack', 'dear', 'eight', 'cultur', 'attitud', 'attend', 'divis',
       'dave', 'data', 'increas', 'accord', 'fals', 'attent',
       'acknowledg'], dtype='<U13'), array(['larg', 'compet', 'primarili', 'statement', 'station', 'start',
       'statist', 'squar', 'stat', 'seek', 'announc', 'state', 'standard',
       'star', 'border', 'fit', 'page', 'england', 'fals', 'roger'],
      dtype='<U13'), array(['countri', 'morn', 'convert', 'babi', 'differ', 'aw', 'corpor',
       'conclud', 'binari', 'format', 'everywher', 'dod', 'nasa',
       'launch', 'nazi', 'continu', 'came', 'noth', 'concern', 'moral'],
      dtype='<U13'), array(['eye', 'depth', 'award', 'catch', 'angl', 'differ', 'auto',
       'roman', 'contain', 'note', 'tabl', 'backup', 'age', 'none',
       'north', 'apart', 'folk', 'propos', 'australia', 'protest'],
      dtype='<U13'), array(['bought', 'atheist', 'electr', 'committe', 'deep', 'abus',
       'german', 'johnson', 'action', 'chapter', 'assembl', 'kick',
       'manipul', 'clean', 'life', 'eastern', 'crap', 'featur', 'complex',
       'particularli'], dtype='<U13')]
-3.658155880230885
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[array(['hockey', 'team', 'game', 'edu', 'leagu', 'season', 'nhl', 'play',
       'list', 'player', 'max', 'draft', 'divis', 'mail', 'com', 'final',
       'avail', 'said', 'score', 'win'], dtype='<U13'), array(['file', 'program', 'output', 'max', 'presid', 'entri', 'line',
       'state', 'number', 'inform', 'system', 'includ', 'health', 'go',
       'said', 'check', 'case', 'open', 'report', 'question'],
      dtype='<U13'), array(['bit', 'mac', 'scsi', 'program', 'system', 'window', 'ibm', 'max',
       'number', 'state', 'chip', 'edu', 'do', 'control', 'key', 'data',
       'run', 'card', 'health', 'com'], dtype='<U13'), array(['imag', 'gif', 'program', 'format', 'file', 'color', 'avail',
       'data', 'version', 'system', 'display', 'softwar', 'graphic',
       'set', 'includ', 'ftp', 'edu', 'max', 'packag', 'free'],
      dtype='<U13'), array(['file', 'gun', 'imag', 'state', 'firearm', 'control', 'congress',
       'gif', 'bill', 'law', 'program', 'format', 'amend', 'max', 'color',
       'unit', 'ftp', 'avail', 'system', 'version'], dtype='<U13'), array(['war', 'turkish', 'jew', 'state', 'govern', 'armenian', 'russian',
       'south', 'program', 'file', 'max', 'secret', 'island', 'world',
       'militari', 'nuclear', 'unit', 'american', 'turkey', 'power'],
      dtype='<U13'), array(['presid', 'go', 'said', 'someth', 'believ', 'packag', 'question',
       'administr', 'made', 'talk', 'group', 'job', 'mean', 'state',
       'consid', 'program', 'govern', 'max', 'discuss', 'support'],
      dtype='<U13'), array(['key', 'encrypt', 'system', 'bit', 'inform', 'secur', 'law',
       'comput', 'privaci', 'chip', 'file', 'pub', 'data', 'mail',
       'program', 'public', 'avail', 'max', 'govern', 'part'],
      dtype='<U13'), array(['imag', 'bit', 'file', 'color', 'mac', 'program', 'gif', 'window',
       'scsi', 'version', 'system', 'format', 'max', 'run', 'softwar',
       'do', 'ibm', 'display', 'hardwar', 'machin'], dtype='<U13'), array(['max', 'system', 'file', 'edu', 'program', 'god', 'part', 'window',
       'includ', 'run', 'question', 'state', 'said', 'number', 'believ',
       'mean', 'drive', 'inform', 'key', 'differ'], dtype='<U13'), array(['anonym', 'internet', 'privaci', 'system', 'inform', 'mail',
       'user', 'file', 'email', 'comput', 'edu', 'servic', 'address',
       'network', 'secur', 'gener', 'messag', 'server', 'site', 'pub'],
      dtype='<U13'), array(['god', 'believ', 'jesu', 'atheist', 'christian', 'exist', 'mean',
       'question', 'religion', 'max', 'must', 'atheism', 'system',
       'belief', 'true', 'state', 'person', 'gener', 'bibl', 'file'],
      dtype='<U13'), array(['edu', 'com', 'graphic', 'file', 'pub', 'avail', 'mail', 'ftp',
       'system', 'includ', 'server', 'program', 'imag', 'send', 'list',
       'data', 'support', 'version', 'run', 'base'], dtype='<U13'), array(['space', 'launch', 'satellit', 'system', 'data', 'program',
       'orbit', 'max', 'nasa', 'market', 'edu', 'commerci', 'includ',
       'servic', 'gener', 'mission', 'technolog', 'report', 'project',
       'inform'], dtype='<U13'), array(['imag', 'data', 'system', 'state', 'program', 'number', 'health',
       'center', 'report', 'research', 'avail', 'includ', 'univers',
       'nation', 'inform', 'space', 'gener', 'turkish', 'medic',
       'develop'], dtype='<U13'), array(['output', 'file', 'entri', 'program', 'max', 'line', 'stream',
       'check', 'open', 'build', 'return', 'read', 'remark', 'write',
       'rule', 'name', 'info', 'author', 'size', 'section'], dtype='<U13'), array(['said', 'armenian', 'someth', 'go', 'went', 'start', 'happen',
       'apart', 'kill', 'came', 'live', 'day', 'tell', 'ask', 'back',
       'told', 'max', 'name', 'everyth', 'took'], dtype='<U13'), array(['entri', 'file', 'program', 'section', 'rule', 'must', 'build',
       'info', 'number', 'sourc', 'follow', 'max', 'remark', 'list',
       'compil', 'line', 'includ', 'author', 'name', 'send'], dtype='<U13'), array(['drive', 'disk', 'system', 'control', 'hard', 'support', 'bio',
       'rom', 'card', 'featur', 'includ', 'max', 'floppi', 'data', 'must',
       'head', 'interfac', 'run', 'power', 'speed'], dtype='<U13'), array(['window', 'avail', 'edu', 'program', 'file', 'includ', 'version',
       'system', 'server', 'com', 'widget', 'subject', 'motif', 'inform',
       'run', 'sourc', 'mit', 'base', 'set', 'sun'], dtype='<U13')]
-2.175767006971178
starting
text length: 
11314
2098
53845
got data
(11314, 2098)
('sklearn LDA', 63.52117991447449)
('gensim LDA', 63.832810401916504)
Centering time: 0.1260066032409668
PCA fit: 3.1882152557373047
PCA Transform: 0.012918233871459961
iterations for initialization: 21
min ortho loss: 14.999999999999995
total iterations: 200
TLDA fit: 7.581705808639526
PCA Reverse Transform: 0.00043082237243652344
Decentering: 0.05155539512634277
decenter with old strategy:
[0.02590778 0.0241426  0.0386758  0.0290487  0.01898061 0.04264991
 0.02314243 0.04264991 0.01879181 0.01898061 0.0275781  0.0275781
 0.04976807 0.04976807 0.03038095 0.04411034 0.02590778 0.03030676
 0.02590778 0.04264991]
Smoothing and Normalization: 0.00032639503479003906
[('centering', 0.1260066032409668), ('PCA fit', 3.1882152557373047), ('PCA transform', 0.012918233871459961), ('TLDA fit', 7.581705808639526), ('unwhiten factors', 0.00043082237243652344), (' decentering', 0.05155539512634277), (' smoothing and normalization', 0.00032639503479003906)]
(20, 2098)
(2098, 2098)
(20, 2098)
(20, 2097)
(20, 2098)
-2.14389151041329
[array(['found', 'charg', 'civil', 'chain', 'challeng', 'limit', 'child',
       'charact', 'appli', 'clear', 'weight', 'consult', 'okay', 'elimin',
       'futur', 'chip', 'lord', 'combin', 'america', 'leg'], dtype='<U13'), array(['japanes', 'cpu', 'argic', 'david', 'broken', 'known', 'typic',
       'keep', 'significantli', 'best', 'inher', 'catch', 'howev',
       'suppli', 'apolog', 'driven', 'introduc', 'jxp', 'area', 'isra'],
      dtype='<U13'), array(['life', 'electr', 'maintain', 'demand', 'occasion', 'foot',
       'bought', 'pre', 'huge', 'pray', 'civilian', 'man', 'machin',
       'poor', 'own', 'manipul', 'defend', 'democrat', 'pictur', 'vehicl'],
      dtype='<U13'), array(['amount', 'gave', 'cheer', 'arm', 'approxim', 'compromis',
       'across', 'els', 'although', 'algorithm', 'intens', 'mirror',
       'death', 'dark', 'babi', 'classifi', 'basi', 'hot', 'beyond',
       'apr'], dtype='<U13'), array(['dual', 'advic', 'bill', 'bound', 'argument', 'build', 'otherwis',
       'dave', 'vendor', 'babi', 'data', 'debat', 'blue', 'afraid', 'go',
       'medicin', 'product', 'faith', 'privaci', 'express'], dtype='<U13'), array(['happi', 'church', 'safeti', 'clipper', 'etc', 'code', 'combin',
       'gun', 'food', 'clean', 'civilian', 'blame', 'brand', 'chain',
       'eat', 'qualiti', 'motif', 'armenian', 'option', 'entir'],
      dtype='<U13'), array(['announc', 'author', 'accus', 'constitut', 'allow', 'annual',
       'attack', 'san', 'attitud', 'eat', 'increas', 'lack', 'dear',
       'april', 'fals', 'acknowledg', 'andi', 'book', 'attent', 'eight'],
      dtype='<U13'), array(['absolut', 'cop', 'capit', 'access', 'gather', 'elsewher', 'send',
       'among', 'commun', 'enabl', 'along', 'health', 'everyth', 'heart',
       'promot', 'accid', 'flaw', 'catch', 'american', 'bunch'],
      dtype='<U13'), array(['huh', 'etc', 'elimin', 'huge', 'behind', 'particip', 'identifi',
       'innoc', 'bill', 'insist', 'inde', 'target', 'intellect', 'van',
       'ibm', 'processor', 'forget', 'honda', 'implement', 'bar'],
      dtype='<U13'), array(['compet', 'distinct', 'belief', 'disagre', 'occupi', 'simpli',
       'invest', 'around', 'specul', 'atheism', 'colleg', 'phil', 'south',
       'dsl', 'clearli', 'educ', 'base', 'internet', 'real', 'violenc'],
      dtype='<U13'), array(['larg', 'border', 'fit', 'fals', 'month', 'involv', 'present',
       'hate', 'student', 'built', 'constant', 'provid', 'trip', 'requir',
       'mechan', 'eye', 'becom', 'societi', 'medicin', 'pub'],
      dtype='<U13'), array(['realiti', 'declar', 'shut', 'liber', 'fourth', 'decemb', 'hook',
       'henc', 'season', 'rememb', 'grant', 'cours', 'primarili', 'rumor',
       'discov', 'apart', 'bibl', 'chang', 'prohibit', 'seek'],
      dtype='<U13'), array(['eye', 'belong', 'circuit', 'differ', 'biggest', 'note', 'blood',
       'break', 'angl', 'dan', 'catch', 'away', 'depth', 'award',
       'australia', 'figur', 'sinc', 'data', 'roman', 'destruct'],
      dtype='<U13'), array(['overal', 'ought', 'pixel', 'everywher', 'equival', 'offic',
       'compon', 'public', 'mostli', 'rate', 'sub', 'critic', 'bottom',
       'movi', 'attend', 'content', 'fool', 'dog', 'doctor', 'terribl'],
      dtype='<U13'), array(['adopt', 'due', 'appreci', 'eye', 'analog', 'catch', 'stream',
       'pull', 'buy', 'babi', 'bomb', 'approach', 'report', 'cooper',
       'bigger', 'attempt', 'brother', 'provid', 'resid', 'along'],
      dtype='<U13'), array(['countri', 'differ', 'morn', 'convert', 'aw', 'babi', 'nazi',
       'binari', 'conclud', 'corpor', 'format', 'dod', 'launch', 'nasa',
       'came', 'continu', 'everywher', 'moral', 'concern', 'correspond'],
      dtype='<U13'), array(['consid', 'hope', 'attempt', 'eric', 'home', 'babi', 'identifi',
       'analog', 'idea', 'map', 'huge', 'mari', 'war', 'attent', 'came',
       'agre', 'aid', 'binari', 'men', 'profession'], dtype='<U13'), array(['divis', 'black', 'around', 'armenian', 'clearli', 'presenc',
       'dollar', 'batteri', 'deal', 'cheer', 'cach', 'director', 'easi',
       'chain', 'defens', 'bank', 'adequ', 'assist', 'awar', 'california'],
      dtype='<U13'), array(['belong', 'explan', 'differ', 'made', 'excus', 'applic', 'attend',
       'lift', 'chip', 'ill', 'acknowledg', 'aid', 'complex', 'accord',
       'warrant', 'around', 'bear', 'august', 'correctli', 'age'],
      dtype='<U13'), array(['eastern', 'bought', 'california', 'abus', 'assembl', 'among',
       'deep', 'anyth', 'across', 'back', 'constant', 'investig',
       'closer', 'beyond', 'cultur', 'brain', 'dead', 'factor', 'accur',
       'ensur'], dtype='<U13')]
-3.9985573338457194
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[array(['edu', 'window', 'avail', 'file', 'program', 'includ', 'version',
       'com', 'server', 'system', 'widget', 'subject', 'inform', 'motif',
       'run', 'sourc', 'base', 'mit', 'set', 'ftp'], dtype='<U13'), array(['file', 'imag', 'gun', 'gif', 'color', 'program', 'format',
       'state', 'control', 'firearm', 'version', 'bit', 'system', 'avail',
       'congress', 'ftp', 'bill', 'edu', 'max', 'display'], dtype='<U13'), array(['program', 'system', 'state', 'data', 'number', 'imag', 'health',
       'center', 'report', 'research', 'includ', 'avail', 'inform',
       'univers', 'max', 'nation', 'medic', 'case', 'space', 'gener'],
      dtype='<U13'), array(['imag', 'file', 'bit', 'color', 'program', 'gif', 'format', 'mac',
       'version', 'system', 'softwar', 'window', 'display', 'edu',
       'avail', 'data', 'run', 'set', 'max', 'support'], dtype='<U13'), array(['output', 'file', 'program', 'entri', 'max', 'line', 'check',
       'stream', 'open', 'build', 'return', 'read', 'name', 'remark',
       'write', 'rule', 'includ', 'info', 'author', 'size'], dtype='<U13'), array(['anonym', 'internet', 'privaci', 'file', 'system', 'inform',
       'mail', 'comput', 'user', 'email', 'edu', 'pub', 'key', 'secur',
       'network', 'address', 'messag', 'servic', 'gener', 'ftp'],
      dtype='<U13'), array(['said', 'armenian', 'someth', 'go', 'went', 'start', 'happen',
       'apart', 'kill', 'came', 'live', 'day', 'tell', 'ask', 'back',
       'told', 'name', 'everyth', 'build', 'took'], dtype='<U13'), array(['anonym', 'internet', 'privaci', 'file', 'system', 'inform',
       'mail', 'comput', 'user', 'email', 'edu', 'pub', 'key', 'secur',
       'network', 'address', 'messag', 'servic', 'gener', 'ftp'],
      dtype='<U13'), array(['max', 'system', 'file', 'edu', 'program', 'part', 'god', 'window',
       'includ', 'run', 'question', 'state', 'said', 'number', 'believ',
       'mean', 'drive', 'inform', 'key', 'differ'], dtype='<U13'), array(['output', 'file', 'program', 'entri', 'max', 'line', 'check',
       'stream', 'open', 'build', 'return', 'read', 'name', 'remark',
       'write', 'rule', 'includ', 'info', 'author', 'size'], dtype='<U13'), array(['space', 'launch', 'satellit', 'system', 'data', 'program', 'edu',
       'orbit', 'nasa', 'max', 'includ', 'market', 'imag', 'commerci',
       'servic', 'gener', 'report', 'mission', 'avail', 'technolog'],
      dtype='<U13'), array(['space', 'launch', 'satellit', 'system', 'data', 'program', 'edu',
       'orbit', 'nasa', 'max', 'includ', 'market', 'imag', 'commerci',
       'servic', 'gener', 'report', 'mission', 'avail', 'technolog'],
      dtype='<U13'), array(['key', 'encrypt', 'bit', 'system', 'inform', 'secur', 'law',
       'comput', 'privaci', 'file', 'chip', 'pub', 'data', 'program',
       'mail', 'avail', 'public', 'govern', 'max', 'part'], dtype='<U13'), array(['key', 'encrypt', 'bit', 'system', 'inform', 'secur', 'law',
       'comput', 'privaci', 'file', 'chip', 'pub', 'data', 'program',
       'mail', 'avail', 'public', 'govern', 'max', 'part'], dtype='<U13'), array(['god', 'believ', 'jesu', 'atheist', 'exist', 'christian', 'mean',
       'question', 'religion', 'must', 'max', 'atheism', 'system',
       'belief', 'true', 'state', 'bibl', 'file', 'person', 'gener'],
      dtype='<U13'), array(['imag', 'program', 'system', 'data', 'state', 'file', 'edu',
       'number', 'avail', 'includ', 'center', 'space', 'report', 'health',
       'turkish', 'inform', 'research', 'gener', 'univers', 'nation'],
      dtype='<U13'), array(['edu', 'window', 'avail', 'file', 'program', 'includ', 'version',
       'com', 'server', 'system', 'widget', 'subject', 'inform', 'motif',
       'run', 'sourc', 'base', 'mit', 'set', 'ftp'], dtype='<U13'), array(['imag', 'file', 'color', 'program', 'gif', 'bit', 'format',
       'system', 'version', 'edu', 'avail', 'softwar', 'data', 'display',
       'mac', 'graphic', 'set', 'window', 'includ', 'run'], dtype='<U13'), array(['edu', 'window', 'avail', 'file', 'program', 'includ', 'version',
       'com', 'server', 'system', 'widget', 'subject', 'inform', 'motif',
       'run', 'sourc', 'base', 'mit', 'set', 'ftp'], dtype='<U13'), array(['anonym', 'internet', 'privaci', 'file', 'system', 'inform',
       'mail', 'comput', 'user', 'email', 'edu', 'pub', 'key', 'secur',
       'network', 'address', 'messag', 'servic', 'gener', 'ftp'],
      dtype='<U13')]
-2.053479550979737
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
new version
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000007, 0.9999999999999996]
Centering time: 0.08884954452514648
PCA fit: 0.747647762298584
[[9.80392157e-01 2.89599707e-09]
 [2.89599691e-09 9.80392164e-01]]
PCA Transform: 0.006468772888183594
total iterations: 28
TLDA fit: 14.400110960006714
Whitened factor: 
[[-0.9661365   0.7934174 ]
 [-0.25803143  0.6086779 ]]
PCA Reverse Transform: 0.0010309219360351562
decenter with new strategy:
[ 0.00292215 -0.00122361]
decenter with old strategy:
[0.00516818 0.00086605]
Fit RMSE new decenter: 0.04604691485406333
Fit RMSE: 0.04638203715711953
 Test Against Ground Truth
[(' decentering', 0.0034418106079101562), (' smoothing and normalization', 0.00028896331787109375)]
Smoothing and Normalization: 0.0005540847778320312
Fit RMSE: 0.045579960234809896
sklearn Test Against Ground Truth
M1: 0.00017142295837402344
M2: 0.17397522926330566
[[1.0000000e+00 2.0489097e-08]
 [2.0023435e-08 9.9999970e-01]]
W: 0.004446268081665039
Whiten X: 0.00014543533325195312
Whiten M1: 9.083747863769531e-05
Parafac M3: 1.019200325012207
Parafac Decomposition: 78.78827214241028
Unwhitening parafac factors: 4.649162292480469e-05
Initialization
[[-0.4493208 -0.8933704]
 [-0.8933704  0.4493209]]
SGD Calc: 497.68416452407837
Unwhitening factors: 7.343292236328125e-05
Smoothing and Normalization: 0.0021066665649414062
Fit RMSE: 0.04610358154010212
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005481243133544922
Fit RMSE: 0.04734624164078954
 Test Against Ground Truth
[('M1 calc', 0.00017142295837402344), ('M2 calc', 0.17397522926330566), ('W calc', 0.004446268081665039), ('whiten X', 0.00014543533325195312), ('whiten M1', 9.083747863769531e-05), ('construct M3', 1.019200325012207), ('decompose parafac', 78.78827214241028), ('unwhiten factors parafac', 4.649162292480469e-05), ('parafac smoothing and normalization', 0.0021066665649414062)] [('M1 calc', 0.00017142295837402344), ('M2 calc', 0.17397522926330566), ('W calc', 0.004446268081665039), ('whiten X', 0.00014543533325195312), ('SGD calc', 497.68416452407837), ('unwhiten factors SGD', 7.343292236328125e-05), (' smoothing and normalization', 0.0005481243133544922)] [array([0.99374439]), array([0.99474394])] [array([0.97548814]), array([nan])]
Vocab: 1000
num_tweets: 20000
density: 15
[1.0000000000000022, 1.0000000000000018]
Centering time: 0.0052454471588134766
PCA fit: 0.6909794807434082
[[ 9.80392158e-01 -2.13954326e-09]
 [-2.13954314e-09  9.80392190e-01]]
PCA Transform: 0.011297225952148438
total iterations: 36
TLDA fit: 18.610021114349365
Whitened factor: 
[[ 0.31087786  0.2943844 ]
 [ 0.9504498  -0.95568705]]
PCA Reverse Transform: 0.0002193450927734375
decenter with new strategy:
[-6.35032542e-04  3.84679925e-05]
decenter with old strategy:
[0.00617649 0.00680044]
Fit RMSE new decenter: 0.037900068626314024
Fit RMSE: 0.03760586286637645
 Test Against Ground Truth
[(' decentering', 0.001295328140258789), (' smoothing and normalization', 0.00032520294189453125)]
Smoothing and Normalization: 0.0005083084106445312
Fit RMSE: 0.033264214114206866
sklearn Test Against Ground Truth
M1: 0.00020956993103027344
Traceback (most recent call last):
  File "generate_tables.py", line 585, in <module>
    main()
  File "generate_tables.py", line 529, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15(vocab=vocab, seed=seed_arr[j], theta=theta, learning_rate=10*lr)
  File "generate_tables.py", line 302, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
new version
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0000000000000002]
Centering time: 0.08360600471496582
PCA fit: 0.7617266178131104
[[9.80392158e-01 4.77821870e-09]
 [4.77821797e-09 9.80392165e-01]]
PCA Transform: 0.006526947021484375
total iterations: 170
TLDA fit: 92.79904246330261
Whitened factor: 
[[ 0.61688983 -0.6608668 ]
 [-0.7870495   0.75050324]]
PCA Reverse Transform: 0.0010576248168945312
decenter with new strategy:
[ 0.00948801 -0.00508263]
decenter with old strategy:
[0.01782539 0.00345677]
Fit RMSE new decenter: 0.048749285554347155
Fit RMSE: 0.04883733841692679
 Test Against Ground Truth
[(' decentering', 0.0038292407989501953), (' smoothing and normalization', 0.00029015541076660156)]
Smoothing and Normalization: 0.0005071163177490234
Fit RMSE: 0.048212456368244366
sklearn Test Against Ground Truth
M1: 0.00017762184143066406
M2: 0.17540955543518066
[[1.0000005e+00 1.5561091e-07]
 [1.3706085e-07 1.0000004e+00]]
W: 0.01255035400390625
Whiten X: 0.00014448165893554688
Whiten M1: 9.942054748535156e-05
Parafac M3: 0.9847173690795898
Parafac Decomposition: 80.582022190094
Unwhitening parafac factors: 4.4345855712890625e-05
Initialization
[[-0.05978179 -0.9982115 ]
 [-0.9982115   0.05978173]]
SGD Calc: 503.35912466049194
Unwhitening factors: 7.843971252441406e-05
Smoothing and Normalization: 0.0021347999572753906
Fit RMSE: 0.050596795352634574
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005495548248291016
Fit RMSE: 0.048682155174471
 Test Against Ground Truth
[('M1 calc', 0.00017762184143066406), ('M2 calc', 0.17540955543518066), ('W calc', 0.01255035400390625), ('whiten X', 0.00014448165893554688), ('whiten M1', 9.942054748535156e-05), ('construct M3', 0.9847173690795898), ('decompose parafac', 80.582022190094), ('unwhiten factors parafac', 4.4345855712890625e-05), ('parafac smoothing and normalization', 0.0021347999572753906)] [('M1 calc', 0.00017762184143066406), ('M2 calc', 0.17540955543518066), ('W calc', 0.01255035400390625), ('whiten X', 0.00014448165893554688), ('SGD calc', 503.35912466049194), ('unwhiten factors SGD', 7.843971252441406e-05), (' smoothing and normalization', 0.0005495548248291016)] [array([0.02231406]), array([0.9826409])] [array([0.99987875]), array([0.99975307])]
Vocab: 1000
num_tweets: 20000
density: 15
[1.0000000000000002, 1.0000000000000009]
Centering time: 0.0058994293212890625
PCA fit: 0.7128312587738037
[[ 9.80392157e-01 -2.59872839e-09]
 [-2.59872848e-09  9.80392165e-01]]
PCA Transform: 0.011285781860351562
total iterations: 18
TLDA fit: 9.216908693313599
Whitened factor: 
[[-0.88903093  0.8200607 ]
 [-0.4578472   0.5722767 ]]
PCA Reverse Transform: 0.00020051002502441406
decenter with new strategy:
[ 1.44177023e-04 -5.58891598e-05]
decenter with old strategy:
[2.63579286e-04 6.02411888e-05]
Fit RMSE new decenter: 0.03371358184088564
Fit RMSE: 0.033955608866667444
 Test Against Ground Truth
[(' decentering', 0.001306772232055664), (' smoothing and normalization', 0.00028705596923828125)]
Smoothing and Normalization: 0.0005202293395996094
Fit RMSE: 0.03336702104550513
sklearn Test Against Ground Truth
M1: 0.00018286705017089844
Traceback (most recent call last):
  File "generate_tables.py", line 589, in <module>
    main()
  File "generate_tables.py", line 529, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15(vocab=vocab, seed=seed_arr[j], theta=theta, learning_rate=10*lr)
  File "generate_tables.py", line 302, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
new version
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999993, 1.0000000000000004]
Centering time: 0.08016824722290039
PCA fit: 0.7829914093017578
[[ 9.80392156e-01 -2.63646192e-09]
 [-2.63646165e-09  9.80392140e-01]]
PCA Transform: 0.006478071212768555
total iterations: 67
TLDA fit: 35.64508128166199
Whitened factor: 
[[ 0.8477813 -0.9013678]
 [-0.5303459  0.4330545]]
PCA Reverse Transform: 0.001039266586303711
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.046040342107348045
Fit RMSE: 0.046106264673728393
 Test Against Ground Truth
[(' decentering', 0.003328084945678711), (' smoothing and normalization', 0.0002880096435546875)]
Smoothing and Normalization: 0.0004932880401611328
Fit RMSE: 0.045473288815320305
sklearn Test Against Ground Truth
M1: 0.00020122528076171875
M2: 0.1746976375579834
[[1.0000002e+00 1.2933742e-07]
 [1.3623503e-07 1.0000000e+00]]
W: 0.0044231414794921875
Whiten X: 0.00014925003051757812
Whiten M1: 9.107589721679688e-05
Parafac M3: 1.013448715209961
Parafac Decomposition: 79.6641674041748
Unwhitening parafac factors: 4.673004150390625e-05
Initialization
[[-0.24343431 -0.9699174 ]
 [-0.9699174   0.24343425]]
SGD Calc: 499.7227852344513
Unwhitening factors: 7.367134094238281e-05
Smoothing and Normalization: 0.0021157264709472656
Fit RMSE: 0.04602953205734465
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0006043910980224609
Fit RMSE: 0.048210725243609365
 Test Against Ground Truth
[('M1 calc', 0.00020122528076171875), ('M2 calc', 0.1746976375579834), ('W calc', 0.0044231414794921875), ('whiten X', 0.00014925003051757812), ('whiten M1', 9.107589721679688e-05), ('construct M3', 1.013448715209961), ('decompose parafac', 79.6641674041748), ('unwhiten factors parafac', 4.673004150390625e-05), ('parafac smoothing and normalization', 0.0021157264709472656)] [('M1 calc', 0.00020122528076171875), ('M2 calc', 0.1746976375579834), ('W calc', 0.0044231414794921875), ('whiten X', 0.00014925003051757812), ('SGD calc', 499.7227852344513), ('unwhiten factors SGD', 7.367134094238281e-05), (' smoothing and normalization', 0.0006043910980224609)] [array([0.99784317]), array([0.99376967])] [array([0.92845195]), array([nan])]
Vocab: 1000
num_tweets: 20000
density: 15
[1.0000000000000013, 1.0000000000000009]
Centering time: 0.0023064613342285156
PCA fit: 0.7219667434692383
[[9.80392157e-01 1.94684803e-10]
 [1.94685124e-10 9.80392146e-01]]
PCA Transform: 0.011239051818847656
total iterations: 60
TLDA fit: 31.643046617507935
Whitened factor: 
[[-0.9768827  0.9357752]
 [-0.2137762  0.3525971]]
PCA Reverse Transform: 0.00022101402282714844
decenter with new strategy:
[ 0.10616311 -0.05818404]
decenter with old strategy:
[0.18389941 0.01828307]
Fit RMSE new decenter: 0.03466013274586073
Fit RMSE: 0.03473996733485723
 Test Against Ground Truth
[(' decentering', 0.0013082027435302734), (' smoothing and normalization', 0.00028252601623535156)]
Smoothing and Normalization: 0.0005135536193847656
Fit RMSE: 0.03425687246224501
sklearn Test Against Ground Truth
M1: 0.0002186298370361328
Traceback (most recent call last):
  File "generate_tables.py", line 593, in <module>
    main()
  File "generate_tables.py", line 533, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15(vocab=vocab, seed=seed_arr[j], theta=theta, learning_rate=10*lr)
  File "generate_tables.py", line 306, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
new version
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999997, 0.9999999999999994]
Centering time: 0.08128213882446289
PCA fit: 0.6803977489471436
PCA Transform: 0.322751522064209
total iterations: 50
TLDA fit: 25.287445068359375
Whitened factor: 
[[ 0.11060241  0.06749013]
 [ 0.9938647  -0.99771994]]
PCA Reverse Transform: 0.001055002212524414
decenter with new strategy:
[2.14153255e-04 8.38492361e-05]
decenter with old strategy:
[0.00108024 0.00094387]
Fit RMSE new decenter: 0.04772317483702181
Fit RMSE: 0.05143240495743545
 Test Against Ground Truth
M1: 0.0002014636993408203
M2: 0.0866239070892334
[[1.0000002e+00 0.0000000e+00]
 [1.7229468e-08 9.9999982e-01]]
W: 0.004332542419433594
Whiten X: 0.00013494491577148438
Whiten M1: 0.00010132789611816406
Parafac M3: 0.5273828506469727
Parafac Decomposition: 79.29589080810547
Unwhitening parafac factors: 4.00543212890625e-05
Initialization
[[-0.45242453 -0.8918026 ]
 [-0.8918026   0.45242465]]
SGD Calc: 255.49380326271057
Unwhitening factors: 7.176399230957031e-05
Smoothing and Normalization: 0.0021076202392578125
Fit RMSE: 0.04669839547022071
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0006682872772216797
Fit RMSE: 0.04639756901570579
 Test Against Ground Truth
[('M1 calc', 0.0002014636993408203), ('M2 calc', 0.0866239070892334), ('W calc', 0.004332542419433594), ('whiten X', 0.00013494491577148438), ('whiten M1', 0.00010132789611816406), ('construct M3', 0.5273828506469727), ('decompose parafac', 79.29589080810547), ('unwhiten factors parafac', 4.00543212890625e-05), ('parafac smoothing and normalization', 0.0021076202392578125)] [('M1 calc', 0.0002014636993408203), ('M2 calc', 0.0866239070892334), ('W calc', 0.004332542419433594), ('whiten X', 0.00013494491577148438), ('SGD calc', 255.49380326271057), ('unwhiten factors SGD', 7.176399230957031e-05), (' smoothing and normalization', 0.0006682872772216797)] [array([0.98949979]), array([0.99825861])] [array([0.99958497]), array([0.99968562])]
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999989, 0.9999999999999997]
Centering time: 0.002564668655395508
PCA fit: 0.6054360866546631
PCA Transform: 0.004187107086181641
total iterations: 31
TLDA fit: 15.90500545501709
Whitened factor: 
[[-0.93137443  0.8713703 ]
 [-0.3640628   0.49062604]]
PCA Reverse Transform: 0.0006041526794433594
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.034202670265197486
Fit RMSE: 0.0343767065720833
 Test Against Ground Truth
M1: 0.000186920166015625
Traceback (most recent call last):
  File "generate_tables.py", line 596, in <module>
    main()
  File "generate_tables.py", line 536, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15(vocab=vocab, seed=seed_arr[j], theta=theta, learning_rate=10*lr)
  File "generate_tables.py", line 306, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 63, in get_M2
    sum_ = batched_tensor_dot(x, x) #(tensor outer product : produce (n_docs, n_words,n_words))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/tenalg/core_tenalg/tensor_product.py", line 32, in batched_tensor_dot
    return tl.reshape(tensor1, shape_1) * tl.reshape(tensor2, shape_2)
  File "cupy/_core/core.pyx", line 1086, in cupy._core.core.ndarray.__mul__
  File "cupy/_core/_kernel.pyx", line 1083, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 578, in cupy._core._kernel._get_out_args
  File "cupy/_core/core.pyx", line 2495, in cupy._core.core._ndarray_init
  File "cupy/_core/core.pyx", line 177, in cupy._core.core.ndarray._init_fast
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
new version
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999999, 0.9999999999999997]
Centering time: 0.08413457870483398
PCA fit: 0.6885900497436523
PCA Transform: 0.33217430114746094
total iterations: 50
TLDA fit: 24.828282117843628
Whitened factor: 
[[-0.92179316  0.8498513 ]
 [-0.387682    0.52702254]]
PCA Reverse Transform: 0.0010645389556884766
decenter with new strategy:
[-0.00082064  0.00130856]
decenter with old strategy:
[0.00037593 0.00246781]
Fit RMSE new decenter: 0.04536443009611166
Fit RMSE: 0.0456210862505271
 Test Against Ground Truth
M1: 0.00016355514526367188
Traceback (most recent call last):
  File "generate_tables.py", line 596, in <module>
    main()
  File "generate_tables.py", line 536, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15(vocab=vocab, seed=seed_arr[j], theta=theta, learning_rate=10*lr)
  File "generate_tables.py", line 306, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 65, in get_M2
    sum_ = tl.zeros(1, x.shape[1]**2)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/__init__.py", line 161, in inner
    return _get_backend_method(name)(*args, **kwargs)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cupy/_creation/basic.py", line 209, in zeros
    a = cupy.ndarray(shape, dtype, order=order)
  File "cupy/_core/core.pyx", line 148, in cupy._core.core.ndarray.__init__
  File "cupy/_core/_dtype.pyx", line 64, in cupy._core._dtype.get_dtype_with_itemsize
TypeError: Cannot interpret '250000' as a data type
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999999]
Centering time: 0.08204174041748047
PCA fit: 0.69862961769104
PCA Transform: 0.31894803047180176
total iterations: 83
TLDA fit: 41.36880302429199
Whitened factor: 
[[-0.98595166  0.9608054 ]
 [-0.16703127  0.2772238 ]]
PCA Reverse Transform: 0.0010650157928466797
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04915618624873018
Fit RMSE: 0.049241896237667476
 Test Against Ground Truth
M1: 0.00016307830810546875
Traceback (most recent call last):
  File "generate_tables.py", line 596, in <module>
    main()
  File "generate_tables.py", line 536, in main
    res_parafac, res_uncentered, accuracy_parafac, accuracy_uncentered = gen_fit_0_15(vocab=vocab, seed=seed_arr[j], theta=theta, learning_rate=10*lr)
  File "generate_tables.py", line 306, in gen_fit_0_15
    M2_img = tlda_mid.get_M2(x, M1, alpha_0)
  File "/home/danny/Dropbox/TLDA-Dev/version0_15/tensor_lda_clean.py", line 70, in get_M2
    sum_ = sum_.reshape(sum_, (x.shape[1], x.shape[1]))
  File "cupy/_core/core.pyx", line 601, in cupy._core.core.ndarray.reshape
  File "cupy/_core/_routines_manipulation.pyx", line 79, in cupy._core._routines_manipulation._ndarray_reshape
  File "stringsource", line 48, in vector.from_py.__pyx_convert_vector_from_py_Py_ssize_t
TypeError: 'cupy._core.core.ndarray' object cannot be interpreted as an integer
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999991]
Centering time: 0.08304595947265625
PCA fit: 0.69942307472229
PCA Transform: 0.3166320323944092
total iterations: 82
TLDA fit: 42.10019493103027
Whitened factor: 
[[-0.74181503  0.5632347 ]
 [ 0.67060465  0.826297  ]]
PCA Reverse Transform: 0.0010409355163574219
decenter with new strategy:
[-0.03074981  0.04331935]
decenter with old strategy:
[0.01642833 0.08665063]
Fit RMSE new decenter: 0.045975343736469346
Fit RMSE: 0.047002203194295517
 Test Against Ground Truth
M1: 0.00016045570373535156
M2: 9.239773273468018
[[1.00000000e+00 1.73472348e-16]
 [6.93889390e-17 1.00000000e+00]]
W: 0.007513999938964844
Whiten X: 0.0001537799835205078
Whiten M1: 8.654594421386719e-05
Parafac M3: 0.5265500545501709
Parafac Decomposition: 80.18525171279907
Unwhitening parafac factors: 3.886222839355469e-05
Initialization
[[-0.7732141 -0.6341451]
 [-0.6341451  0.7732141]]
SGD Calc: 252.62406373023987
Unwhitening factors: 7.104873657226562e-05
Smoothing and Normalization: 0.0021255016326904297
Fit RMSE: 0.04604592966530049
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005853176116943359
Fit RMSE: 0.04868114574845039
 Test Against Ground Truth
[('M1 calc', 0.00016045570373535156), ('M2 calc', 9.239773273468018), ('W calc', 0.007513999938964844), ('whiten X', 0.0001537799835205078), ('whiten M1', 8.654594421386719e-05), ('construct M3', 0.5265500545501709), ('decompose parafac', 80.18525171279907), ('unwhiten factors parafac', 3.886222839355469e-05), ('parafac smoothing and normalization', 0.0021255016326904297)] [('M1 calc', 0.00016045570373535156), ('M2 calc', 9.239773273468018), ('W calc', 0.007513999938964844), ('whiten X', 0.0001537799835205078), ('SGD calc', 252.62406373023987), ('unwhiten factors SGD', 7.104873657226562e-05), (' smoothing and normalization', 0.0005853176116943359)] [array([0.99142336]), array([0.99543457])] [array([0.93744436]), array([nan])]
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999986]
Centering time: 0.0032491683959960938
PCA fit: 0.49941325187683105
PCA Transform: 0.0041887760162353516
total iterations: 41
TLDA fit: 21.08409547805786
Whitened factor: 
[[ 0.0623556   0.13492307]
 [ 0.99805397 -0.9908561 ]]
PCA Reverse Transform: 0.0002288818359375
decenter with new strategy:
[4.45439983e-05 5.06168016e-06]
decenter with old strategy:
[0.00020056 0.00016291]
Fit RMSE new decenter: 0.035416685785818476
Fit RMSE: 0.03733066876496097
 Test Against Ground Truth
M1: 0.00018858909606933594
M2: 17.557471752166748
[[ 1.00000000e+00 -1.21430643e-17]
 [-1.56125113e-17  1.00000000e+00]]
W: 0.014394521713256836
Whiten X: 0.00010967254638671875
Whiten M1: 8.678436279296875e-05
Parafac M3: 0.5235869884490967
Parafac Decomposition: 80.46617865562439
Unwhitening parafac factors: 3.6716461181640625e-05
Initialization
[[-0.68647707 -0.72715145]
 [-0.72715145  0.68647707]]
SGD Calc: 263.5021526813507
Unwhitening factors: 6.747245788574219e-05
Smoothing and Normalization: 0.00032067298889160156
Fit RMSE: 0.03559642818202003
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005726814270019531
Fit RMSE: 0.03404071793678791
 Test Against Ground Truth
[('M1 calc', 0.00018858909606933594), ('M2 calc', 17.557471752166748), ('W calc', 0.014394521713256836), ('whiten X', 0.00010967254638671875), ('whiten M1', 8.678436279296875e-05), ('construct M3', 0.5235869884490967), ('decompose parafac', 80.46617865562439), ('unwhiten factors parafac', 3.6716461181640625e-05), ('parafac smoothing and normalization', 0.00032067298889160156)] [('M1 calc', 0.00018858909606933594), ('M2 calc', 17.557471752166748), ('W calc', 0.014394521713256836), ('whiten X', 0.00010967254638671875), ('SGD calc', 263.5021526813507), ('unwhiten factors SGD', 6.747245788574219e-05), (' smoothing and normalization', 0.0005726814270019531)] [array([0.17684484]), array([0.98856498])] [array([0.99934894]), array([0.99940421])]
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999996, 1.0]
Centering time: 0.0022814273834228516
PCA fit: 0.21952176094055176
PCA Transform: 0.0023374557495117188
total iterations: 128
TLDA fit: 66.80087208747864
Whitened factor: 
[[-0.73944443  0.68052226]
 [-0.67321765  0.73272747]]
PCA Reverse Transform: 0.00022721290588378906
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.046845321716297766
Fit RMSE: 0.04759431338622781
 Test Against Ground Truth
M1: 0.00016307830810546875
M2: 8.970313549041748
[[ 1.00000000e+00 -2.77555756e-17]
 [ 5.55111512e-17  1.00000000e+00]]
W: 0.005156993865966797
Whiten X: 0.00010800361633300781
Whiten M1: 8.463859558105469e-05
Parafac M3: 0.5120022296905518
Parafac Decomposition: 79.62571620941162
Unwhitening parafac factors: 3.5762786865234375e-05
Initialization
[[-0.973703   -0.22782046]
 [-0.22782046  0.97370315]]
SGD Calc: 256.02186274528503
Unwhitening factors: 7.367134094238281e-05
Smoothing and Normalization: 0.0003337860107421875
Fit RMSE: 0.04853465274965295
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005533695220947266
Fit RMSE: 0.04684859800612143
 Test Against Ground Truth
[('M1 calc', 0.00016307830810546875), ('M2 calc', 8.970313549041748), ('W calc', 0.005156993865966797), ('whiten X', 0.00010800361633300781), ('whiten M1', 8.463859558105469e-05), ('construct M3', 0.5120022296905518), ('decompose parafac', 79.62571620941162), ('unwhiten factors parafac', 3.5762786865234375e-05), ('parafac smoothing and normalization', 0.0003337860107421875)] [('M1 calc', 0.00016307830810546875), ('M2 calc', 8.970313549041748), ('W calc', 0.005156993865966797), ('whiten X', 0.00010800361633300781), ('SGD calc', 256.02186274528503), ('unwhiten factors SGD', 7.367134094238281e-05), (' smoothing and normalization', 0.0005533695220947266)] [array([0.45020715]), array([0.96980114])] [array([0.99967839]), array([0.99956524])]
Traceback (most recent call last):
  File "generate_tables.py", line 596, in <module>
    main()
  File "generate_tables.py", line 550, in main
    tot_parafac[vocab] = [(x, y + res_parafac[i][1]) for i, (x, y) in enumerate(tot_parafac)]
  File "generate_tables.py", line 550, in <listcomp>
    tot_parafac[vocab] = [(x, y + res_parafac[i][1]) for i, (x, y) in enumerate(tot_parafac)]
TypeError: cannot unpack non-iterable int object
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999989]
Centering time: 0.08250546455383301
PCA fit: 0.6856715679168701
PCA Transform: 0.3179023265838623
total iterations: 90
TLDA fit: 45.06070160865784
Whitened factor: 
[[ 0.7898301  -0.84041864]
 [-0.6133258   0.54193777]]
PCA Reverse Transform: 0.0010662078857421875
decenter with new strategy:
[ 0.03839515 -0.0172581 ]
decenter with old strategy:
[0.07873444 0.02403729]
Fit RMSE new decenter: 0.04909814327650046
Fit RMSE: 0.04922894532048561
 Test Against Ground Truth
M1: 0.00016260147094726562
M2: 9.266671180725098
[[1.00000000e+00 1.38777878e-16]
 [1.52655666e-16 1.00000000e+00]]
W: 0.007328987121582031
Whiten X: 0.00014066696166992188
Whiten M1: 8.654594421386719e-05
Parafac M3: 0.5307345390319824
Parafac Decomposition: 77.51308608055115
Unwhitening parafac factors: 4.220008850097656e-05
Initialization
[[-0.8326334 -0.5538246]
 [-0.5538246  0.8326334]]
SGD Calc: 242.75108695030212
Unwhitening factors: 7.939338684082031e-05
Smoothing and Normalization: 0.0021796226501464844
Fit RMSE: 0.051092041960210735
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005838871002197266
Fit RMSE: 0.04910397238849656
 Test Against Ground Truth
[('M1 calc', 0.00016260147094726562), ('M2 calc', 9.266671180725098), ('W calc', 0.007328987121582031), ('whiten X', 0.00014066696166992188), ('whiten M1', 8.654594421386719e-05), ('construct M3', 0.5307345390319824), ('decompose parafac', 77.51308608055115), ('unwhiten factors parafac', 4.220008850097656e-05), ('parafac smoothing and normalization', 0.0021796226501464844)] [('M1 calc', 0.00016260147094726562), ('M2 calc', 9.266671180725098), ('W calc', 0.007328987121582031), ('whiten X', 0.00014066696166992188), ('SGD calc', 242.75108695030212), ('unwhiten factors SGD', 7.939338684082031e-05), (' smoothing and normalization', 0.0005838871002197266)] [array([0.08176138]), array([0.97440845])] [array([0.9997909]), array([0.99967095])]
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 1.0000000000000004]
Centering time: 0.0017948150634765625
PCA fit: 0.49840545654296875
PCA Transform: 0.004664897918701172
total iterations: 59
TLDA fit: 29.139567136764526
Whitened factor: 
[[-0.9983781   0.9799813 ]
 [-0.0569318   0.19908944]]
PCA Reverse Transform: 0.0002269744873046875
decenter with new strategy:
[-0.04979962  0.07777704]
decenter with old strategy:
[0.01170569 0.13882567]
Fit RMSE new decenter: 0.035162438063011926
Fit RMSE: 0.035253323230846796
 Test Against Ground Truth
M1: 0.0001823902130126953
M2: 17.099822998046875
[[1.00000000e+00 8.50014503e-17]
 [3.55618313e-17 1.00000000e+00]]
W: 0.014897346496582031
Whiten X: 0.00011205673217773438
Whiten M1: 8.678436279296875e-05
Parafac M3: 0.5197458267211914
Parafac Decomposition: 78.71790647506714
Unwhitening parafac factors: 3.9577484130859375e-05
Initialization
[[-0.7388631  0.6738555]
 [ 0.6738555  0.7388631]]
SGD Calc: 253.63883781433105
Unwhitening factors: 7.033348083496094e-05
Smoothing and Normalization: 0.0003306865692138672
Fit RMSE: 0.0352968754191237
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005414485931396484
Fit RMSE: 0.037182452524072714
 Test Against Ground Truth
[('M1 calc', 0.0001823902130126953), ('M2 calc', 17.099822998046875), ('W calc', 0.014897346496582031), ('whiten X', 0.00011205673217773438), ('whiten M1', 8.678436279296875e-05), ('construct M3', 0.5197458267211914), ('decompose parafac', 78.71790647506714), ('unwhiten factors parafac', 3.9577484130859375e-05), ('parafac smoothing and normalization', 0.0003306865692138672)] [('M1 calc', 0.0001823902130126953), ('M2 calc', 17.099822998046875), ('W calc', 0.014897346496582031), ('whiten X', 0.00011205673217773438), ('SGD calc', 253.63883781433105), ('unwhiten factors SGD', 7.033348083496094e-05), (' smoothing and normalization', 0.0005414485931396484)] [array([0.99376882]), array([0.99887025])] [array([0.96082343]), array([nan])]
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999997, 1.0000000000000002]
Centering time: 0.002237081527709961
PCA fit: 0.2284071445465088
PCA Transform: 0.0032701492309570312
total iterations: 76
TLDA fit: 38.643146276474
Whitened factor: 
[[-0.96674544  0.93250877]
 [-0.25574052  0.36114743]]
PCA Reverse Transform: 0.0006008148193359375
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.04937217032655823
Fit RMSE: 0.049586000392391846
 Test Against Ground Truth
M1: 0.0001723766326904297
M2: 8.922904968261719
[[ 1.00000000e+00 -3.05311332e-16]
 [-2.08166817e-16  1.00000000e+00]]
W: 0.005090236663818359
Whiten X: 0.00010943412780761719
Whiten M1: 8.606910705566406e-05
Parafac M3: 0.5096738338470459
Parafac Decomposition: 78.37937355041504
Unwhitening parafac factors: 3.910064697265625e-05
Initialization
[[-0.99535465 -0.09627669]
 [-0.09627669  0.9953546 ]]
SGD Calc: 247.92700505256653
Unwhitening factors: 7.104873657226562e-05
Smoothing and Normalization: 0.0003173351287841797
Fit RMSE: 0.05107693769626998
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005700588226318359
Fit RMSE: 0.049392980865869
 Test Against Ground Truth
[('M1 calc', 0.0001723766326904297), ('M2 calc', 8.922904968261719), ('W calc', 0.005090236663818359), ('whiten X', 0.00010943412780761719), ('whiten M1', 8.606910705566406e-05), ('construct M3', 0.5096738338470459), ('decompose parafac', 78.37937355041504), ('unwhiten factors parafac', 3.910064697265625e-05), ('parafac smoothing and normalization', 0.0003173351287841797)] [('M1 calc', 0.0001723766326904297), ('M2 calc', 8.922904968261719), ('W calc', 0.005090236663818359), ('whiten X', 0.00010943412780761719), ('SGD calc', 247.92700505256653), ('unwhiten factors SGD', 7.104873657226562e-05), (' smoothing and normalization', 0.0005700588226318359)] [array([0.25813226]), array([0.98648079])] [array([0.99978777]), array([0.99977124])]
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999984, 1.0000000000000002]
Centering time: 0.004142045974731445
PCA fit: 0.5225858688354492
PCA Transform: 0.004669666290283203
total iterations: 50
TLDA fit: 24.87885093688965
Whitened factor: 
[[-0.8980309   0.81596243]
 [-0.4399326   0.5781049 ]]
PCA Reverse Transform: 0.00022029876708984375
decenter with new strategy:
[0.00022698 0.00527657]
decenter with old strategy:
[0.01111329 0.01575816]
Fit RMSE new decenter: 0.033646656548465395
Fit RMSE: 0.0337897076384833
 Test Against Ground Truth
M1: 0.0001823902130126953
M2: 17.320119380950928
[[ 1.00000000e+00 -7.11236625e-17]
 [-7.11236625e-17  1.00000000e+00]]
W: 0.014117956161499023
Whiten X: 0.00011467933654785156
Whiten M1: 8.273124694824219e-05
Parafac M3: 0.5099551677703857
Parafac Decomposition: 78.48617029190063
Unwhitening parafac factors: 3.7670135498046875e-05
Initialization
[[-0.9974613   0.07120964]
 [ 0.07120964  0.9974614 ]]
SGD Calc: 243.6590929031372
Unwhitening factors: 7.605552673339844e-05
Smoothing and Normalization: 0.00032138824462890625
Fit RMSE: 0.03404236498700984
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005605220794677734
Fit RMSE: 0.033570523611770806
 Test Against Ground Truth
[('M1 calc', 0.0001823902130126953), ('M2 calc', 17.320119380950928), ('W calc', 0.014117956161499023), ('whiten X', 0.00011467933654785156), ('whiten M1', 8.273124694824219e-05), ('construct M3', 0.5099551677703857), ('decompose parafac', 78.48617029190063), ('unwhiten factors parafac', 3.7670135498046875e-05), ('parafac smoothing and normalization', 0.00032138824462890625)] [('M1 calc', 0.0001823902130126953), ('M2 calc', 17.320119380950928), ('W calc', 0.014117956161499023), ('whiten X', 0.00011467933654785156), ('SGD calc', 243.6590929031372), ('unwhiten factors SGD', 7.605552673339844e-05), (' smoothing and normalization', 0.0005605220794677734)] [array([0.95830668]), array([0.99693635])] [array([0.99903205]), array([0.99944308])]
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999989, 1.0]
Centering time: 0.0022492408752441406
PCA fit: 0.1978461742401123
PCA Transform: 0.0032815933227539062
total iterations: 127
TLDA fit: 65.80506753921509
Whitened factor: 
[[-0.9433738   0.90394324]
 [-0.33173168  0.42765266]]
PCA Reverse Transform: 0.0002295970916748047
decenter with new strategy:
[-0.00185522  0.00305891]
decenter with old strategy:
[0.00044074 0.00531467]
Fit RMSE new decenter: 0.04807242664984814
Fit RMSE: 0.04803680085275064
 Test Against Ground Truth
M1: 0.00016236305236816406
M2: 9.471436738967896
[[ 1.00000000e+00 -5.55111512e-17]
 [-1.52655666e-16  1.00000000e+00]]
W: 0.00470733642578125
Whiten X: 9.179115295410156e-05
Whiten M1: 6.866455078125e-05
Parafac M3: 0.5006170272827148
Parafac Decomposition: 79.39422225952148
Unwhitening parafac factors: 3.743171691894531e-05
Initialization
[[-0.758456  -0.6517243]
 [-0.6517243  0.758456 ]]
SGD Calc: 249.17309474945068
Unwhitening factors: 7.2479248046875e-05
Smoothing and Normalization: 0.0003426074981689453
Fit RMSE: 0.04804024546975993
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0006468296051025391
Fit RMSE: 0.05021238090785876
 Test Against Ground Truth
[('M1 calc', 0.00016236305236816406), ('M2 calc', 9.471436738967896), ('W calc', 0.00470733642578125), ('whiten X', 9.179115295410156e-05), ('whiten M1', 6.866455078125e-05), ('construct M3', 0.5006170272827148), ('decompose parafac', 79.39422225952148), ('unwhiten factors parafac', 3.743171691894531e-05), ('parafac smoothing and normalization', 0.0003426074981689453)] [('M1 calc', 0.00016236305236816406), ('M2 calc', 9.471436738967896), ('W calc', 0.00470733642578125), ('whiten X', 9.179115295410156e-05), ('SGD calc', 249.17309474945068), ('unwhiten factors SGD', 7.2479248046875e-05), (' smoothing and normalization', 0.0006468296051025391)] [array([0.98231125]), array([0.9992127])] [array([0.95022756]), array([nan])]
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 1.0]
Centering time: 0.004070281982421875
PCA fit: 0.6065554618835449
PCA Transform: 0.0046749114990234375
total iterations: 111
TLDA fit: 56.69042181968689
Whitened factor: 
[[ 0.876997   -0.90859795]
 [-0.48049608  0.41767186]]
PCA Reverse Transform: 0.00023412704467773438
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.03311629929078709
Fit RMSE: 0.03313915729221989
 Test Against Ground Truth
M1: 0.0001819133758544922
M2: 17.865214347839355
[[ 1.00000000e+00 -1.73472348e-17]
 [-2.60208521e-17  1.00000000e+00]]
W: 0.013850927352905273
Whiten X: 0.00010943412780761719
Whiten M1: 8.58306884765625e-05
Parafac M3: 0.5141727924346924
Parafac Decomposition: 78.473060131073
Unwhitening parafac factors: 3.7670135498046875e-05
Initialization
[[-0.81662095 -0.5771743 ]
 [-0.5771743   0.81662095]]
SGD Calc: 247.09433364868164
Unwhitening factors: 6.461143493652344e-05
Smoothing and Normalization: 0.0003333091735839844
Fit RMSE: 0.033484767197430355
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0006093978881835938
Fit RMSE: 0.03314377611218727
 Test Against Ground Truth
[('M1 calc', 0.0001819133758544922), ('M2 calc', 17.865214347839355), ('W calc', 0.013850927352905273), ('whiten X', 0.00010943412780761719), ('whiten M1', 8.58306884765625e-05), ('construct M3', 0.5141727924346924), ('decompose parafac', 78.473060131073), ('unwhiten factors parafac', 3.7670135498046875e-05), ('parafac smoothing and normalization', 0.0003333091735839844)] [('M1 calc', 0.0001819133758544922), ('M2 calc', 17.865214347839355), ('W calc', 0.013850927352905273), ('whiten X', 0.00010943412780761719), ('SGD calc', 247.09433364868164), ('unwhiten factors SGD', 6.461143493652344e-05), (' smoothing and normalization', 0.0006093978881835938)] [array([0.97405226]), array([0.99778191])] [array([0.99913047]), array([0.9993889])]
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 1.0000000000000009]
Centering time: 0.0026144981384277344
PCA fit: 0.21039223670959473
PCA Transform: 0.003277301788330078
total iterations: 59
TLDA fit: 29.687586307525635
Whitened factor: 
[[-0.9372634   0.8837929 ]
 [-0.34862193  0.46787837]]
PCA Reverse Transform: 0.00022220611572265625
decenter with new strategy:
[ 0.17214126 -0.0904024 ]
decenter with old strategy:
[0.30193705 0.03632831]
Fit RMSE new decenter: 0.04753833996237342
Fit RMSE: 0.04763841633807235
 Test Against Ground Truth
M1: 0.00016999244689941406
M2: 8.92169713973999
[[ 1.00000000e+00 -2.77555756e-17]
 [-5.55111512e-17  1.00000000e+00]]
W: 0.005135774612426758
Whiten X: 0.00010991096496582031
Whiten M1: 8.559226989746094e-05
Parafac M3: 0.5158290863037109
Parafac Decomposition: 79.07559204101562
Unwhitening parafac factors: 3.910064697265625e-05
Initialization
[[-0.06755316 -0.9977158 ]
 [-0.9977158   0.06755298]]
SGD Calc: 252.79515171051025
Unwhitening factors: 6.985664367675781e-05
Smoothing and Normalization: 0.0003142356872558594
Fit RMSE: 0.048431307078022665
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005555152893066406
Fit RMSE: 0.04753553334796632
 Test Against Ground Truth
[('M1 calc', 0.00016999244689941406), ('M2 calc', 8.92169713973999), ('W calc', 0.005135774612426758), ('whiten X', 0.00010991096496582031), ('whiten M1', 8.559226989746094e-05), ('construct M3', 0.5158290863037109), ('decompose parafac', 79.07559204101562), ('unwhiten factors parafac', 3.910064697265625e-05), ('parafac smoothing and normalization', 0.0003142356872558594)] [('M1 calc', 0.00016999244689941406), ('M2 calc', 8.92169713973999), ('W calc', 0.005135774612426758), ('whiten X', 0.00010991096496582031), ('SGD calc', 252.79515171051025), ('unwhiten factors SGD', 6.985664367675781e-05), (' smoothing and normalization', 0.0005555152893066406)] [array([0.85433264]), array([0.99538463])] [array([0.99963506]), array([0.99972582])]
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000018]
Centering time: 0.004057407379150391
PCA fit: 0.6121222972869873
PCA Transform: 0.004678010940551758
total iterations: 140
TLDA fit: 71.55337953567505
Whitened factor: 
[[ 0.10588588  0.00266954]
 [-0.9943783   0.9999964 ]]
PCA Reverse Transform: 0.00022935867309570312
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.035384473720116265
Fit RMSE: 0.03692249459059362
 Test Against Ground Truth
M1: 0.00017571449279785156
M2: 17.370743989944458
[[ 1.00000000e+00 -3.19189120e-16]
 [-2.81025203e-16  1.00000000e+00]]
W: 0.013921976089477539
Whiten X: 0.00011014938354492188
Whiten M1: 8.702278137207031e-05
Parafac M3: 0.49551868438720703
Parafac Decomposition: 79.18909621238708
Unwhitening parafac factors: 3.981590270996094e-05
Initialization
[[-0.7141335 -0.7000097]
 [-0.7000097  0.7141334]]
SGD Calc: 246.81363582611084
Unwhitening factors: 7.605552673339844e-05
Smoothing and Normalization: 0.00032782554626464844
Fit RMSE: 0.03366797712429248
parafac Test Against Ground Truth
Smoothing and Normalization: 0.000560760498046875
Fit RMSE: 0.03508827099472959
 Test Against Ground Truth
[('M1 calc', 0.00017571449279785156), ('M2 calc', 17.370743989944458), ('W calc', 0.013921976089477539), ('whiten X', 0.00011014938354492188), ('whiten M1', 8.702278137207031e-05), ('construct M3', 0.49551868438720703), ('decompose parafac', 79.18909621238708), ('unwhiten factors parafac', 3.981590270996094e-05), ('parafac smoothing and normalization', 0.00032782554626464844)] [('M1 calc', 0.00017571449279785156), ('M2 calc', 17.370743989944458), ('W calc', 0.013921976089477539), ('whiten X', 0.00011014938354492188), ('SGD calc', 246.81363582611084), ('unwhiten factors SGD', 7.605552673339844e-05), (' smoothing and normalization', 0.000560760498046875)] [array([0.99446567]), array([0.99616937])] [array([0.94375247]), array([nan])]
Vocab: 500
num_tweets: 10000
density: 15
[1.000000000000001, 1.0000000000000004]
Centering time: 0.0025985240936279297
PCA fit: 0.2033076286315918
PCA Transform: 0.003278017044067383
total iterations: 48
TLDA fit: 24.216017484664917
Whitened factor: 
[[ 0.02898538  0.14386564]
 [ 0.99957985 -0.98959726]]
PCA Reverse Transform: 0.0002181529998779297
decenter with new strategy:
[0.0024128  0.00044185]
decenter with old strategy:
[0.00994443 0.00813701]
Fit RMSE new decenter: 0.04772334410776642
Fit RMSE: 0.05156019133042779
 Test Against Ground Truth
M1: 0.00017309188842773438
M2: 9.166979551315308
[[1.00000000e+00 6.24500451e-17]
 [3.46944695e-17 1.00000000e+00]]
W: 0.005239009857177734
Whiten X: 0.00010347366333007812
Whiten M1: 8.0108642578125e-05
Parafac M3: 0.5317518711090088
Parafac Decomposition: 79.8291847705841
Unwhitening parafac factors: 3.910064697265625e-05
Initialization
[[-0.94747484  0.31983072]
 [ 0.31983072  0.9474747 ]]
SGD Calc: 253.59874486923218
Unwhitening factors: 7.319450378417969e-05
Smoothing and Normalization: 0.00031304359436035156
Fit RMSE: 0.04894935495001537
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005476474761962891
Fit RMSE: 0.046511866096324614
 Test Against Ground Truth
[('M1 calc', 0.00017309188842773438), ('M2 calc', 9.166979551315308), ('W calc', 0.005239009857177734), ('whiten X', 0.00010347366333007812), ('whiten M1', 8.0108642578125e-05), ('construct M3', 0.5317518711090088), ('decompose parafac', 79.8291847705841), ('unwhiten factors parafac', 3.910064697265625e-05), ('parafac smoothing and normalization', 0.00031304359436035156)] [('M1 calc', 0.00017309188842773438), ('M2 calc', 9.166979551315308), ('W calc', 0.005239009857177734), ('whiten X', 0.00010347366333007812), ('SGD calc', 253.59874486923218), ('unwhiten factors SGD', 7.319450378417969e-05), (' smoothing and normalization', 0.0005476474761962891)] [array([0.2506305]), array([0.9945023])] [array([0.99969888]), array([0.99952251])]
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000007]
Centering time: 0.004090070724487305
PCA fit: 0.6201856136322021
PCA Transform: 0.004688262939453125
total iterations: 48
TLDA fit: 23.912860870361328
Whitened factor: 
[[ 0.09421603  0.11182855]
 [ 0.99555176 -0.99372756]]
PCA Reverse Transform: 0.00022101402282714844
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.0347193173594752
Fit RMSE: 0.036651962733380596
 Test Against Ground Truth
M1: 0.00017452239990234375
M2: 17.39959979057312
[[ 1.00000000e+00 -2.24646690e-16]
 [-3.21357524e-16  1.00000000e+00]]
W: 0.013982295989990234
Whiten X: 0.00010919570922851562
Whiten M1: 8.177757263183594e-05
Parafac M3: 0.5104973316192627
Parafac Decomposition: 78.60155606269836
Unwhitening parafac factors: 3.910064697265625e-05
Initialization
[[-0.31671727  0.94852006]
 [ 0.94852006  0.31671715]]
SGD Calc: 248.07700943946838
Unwhitening factors: 7.367134094238281e-05
Smoothing and Normalization: 0.0003292560577392578
Fit RMSE: 0.03447670162896636
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005517005920410156
Fit RMSE: 0.035118057469258335
 Test Against Ground Truth
[('M1 calc', 0.00017452239990234375), ('M2 calc', 17.39959979057312), ('W calc', 0.013982295989990234), ('whiten X', 0.00010919570922851562), ('whiten M1', 8.177757263183594e-05), ('construct M3', 0.5104973316192627), ('decompose parafac', 78.60155606269836), ('unwhiten factors parafac', 3.910064697265625e-05), ('parafac smoothing and normalization', 0.0003292560577392578)] [('M1 calc', 0.00017452239990234375), ('M2 calc', 17.39959979057312), ('W calc', 0.013982295989990234), ('whiten X', 0.00010919570922851562), ('SGD calc', 248.07700943946838), ('unwhiten factors SGD', 7.367134094238281e-05), (' smoothing and normalization', 0.0005517005920410156)] [array([0.65612377]), array([0.98579286])] [array([0.94758477]), array([nan])]
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0]
Centering time: 0.002255678176879883
PCA fit: 0.19669604301452637
PCA Transform: 0.0032262802124023438
total iterations: 99
TLDA fit: 50.83603620529175
Whitened factor: 
[[-0.12354117  0.0203615 ]
 [-0.9923395   0.99979264]]
PCA Reverse Transform: 0.00020766258239746094
decenter with new strategy:
[0.05425512 0.00881276]
decenter with old strategy:
[0.14840517 0.10203167]
Fit RMSE new decenter: 0.04822566830107841
Fit RMSE: 0.05144558882841928
 Test Against Ground Truth
M1: 0.00016260147094726562
M2: 9.205693483352661
[[1.00000000e+00 2.91433544e-16]
 [2.84494650e-16 1.00000000e+00]]
W: 0.00510716438293457
Whiten X: 0.00011014938354492188
Whiten M1: 8.511543273925781e-05
Parafac M3: 0.5027894973754883
Parafac Decomposition: 79.14399790763855
Unwhitening parafac factors: 3.814697265625e-05
Initialization
[[-0.4111061  -0.91158754]
 [-0.91158754  0.41110605]]
SGD Calc: 250.0590958595276
Unwhitening factors: 6.914138793945312e-05
Smoothing and Normalization: 0.00031447410583496094
Fit RMSE: 0.04781578072775831
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0006220340728759766
Fit RMSE: 0.049456263744907934
 Test Against Ground Truth
[('M1 calc', 0.00016260147094726562), ('M2 calc', 9.205693483352661), ('W calc', 0.00510716438293457), ('whiten X', 0.00011014938354492188), ('whiten M1', 8.511543273925781e-05), ('construct M3', 0.5027894973754883), ('decompose parafac', 79.14399790763855), ('unwhiten factors parafac', 3.814697265625e-05), ('parafac smoothing and normalization', 0.00031447410583496094)] [('M1 calc', 0.00016260147094726562), ('M2 calc', 9.205693483352661), ('W calc', 0.00510716438293457), ('whiten X', 0.00011014938354492188), ('SGD calc', 250.0590958595276), ('unwhiten factors SGD', 6.914138793945312e-05), (' smoothing and normalization', 0.0006220340728759766)] [array([0.99346232]), array([0.98981755])] [array([0.932758]), array([nan])]
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000002, 1.0000000000000013]
Centering time: 0.0040705204010009766
PCA fit: 0.588219404220581
PCA Transform: 0.00466609001159668
total iterations: 41
TLDA fit: 20.850572109222412
Whitened factor: 
[[-0.1397811  -0.03532873]
 [-0.99018246  0.99937576]]
PCA Reverse Transform: 0.0002396106719970703
decenter with new strategy:
[0. 0.]
decenter with old strategy:
[0. 0.]
Fit RMSE new decenter: 0.035501006841975
Fit RMSE: 0.03675788720783639
 Test Against Ground Truth
M1: 0.0001773834228515625
M2: 19.272066354751587
[[1.00000000e+00 2.37657116e-16]
 [2.44596010e-16 1.00000000e+00]]
W: 0.013953685760498047
Whiten X: 0.00011134147644042969
Whiten M1: 8.559226989746094e-05
Parafac M3: 0.5011575222015381
Parafac Decomposition: 79.65413451194763
Unwhitening parafac factors: 4.172325134277344e-05
Initialization
[[-0.1437968  -0.98960733]
 [-0.98960733  0.14379662]]
SGD Calc: 254.35932159423828
Unwhitening factors: 8.559226989746094e-05
Smoothing and Normalization: 0.00034689903259277344
Fit RMSE: 0.03301242350332453
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0006868839263916016
Fit RMSE: 0.03460877725494963
 Test Against Ground Truth
[('M1 calc', 0.0001773834228515625), ('M2 calc', 19.272066354751587), ('W calc', 0.013953685760498047), ('whiten X', 0.00011134147644042969), ('whiten M1', 8.559226989746094e-05), ('construct M3', 0.5011575222015381), ('decompose parafac', 79.65413451194763), ('unwhiten factors parafac', 4.172325134277344e-05), ('parafac smoothing and normalization', 0.00034689903259277344)] [('M1 calc', 0.0001773834228515625), ('M2 calc', 19.272066354751587), ('W calc', 0.013953685760498047), ('whiten X', 0.00011134147644042969), ('SGD calc', 254.35932159423828), ('unwhiten factors SGD', 8.559226989746094e-05), (' smoothing and normalization', 0.0006868839263916016)] [array([0.99766706]), array([0.99436341])] [array([0.93057101]), array([nan])]
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0]
Centering time: 0.0026369094848632812
PCA fit: 0.20078158378601074
PCA Transform: 0.002447366714477539
total iterations: 200
TLDA fit: 101.41940188407898
Whitened factor: 
[[-0.656246    0.6943612 ]
 [ 0.75454706 -0.71962667]]
PCA Reverse Transform: 0.00023126602172851562
decenter with new strategy:
[ 0.00034578 -0.00026146]
decenter with old strategy:
[ 0.00050126 -0.00010301]
Fit RMSE new decenter: 0.04844559825569156
Fit RMSE: 0.049536649045483175
 Test Against Ground Truth
M1: 0.00016045570373535156
M2: 8.9688138961792
[[ 1.00000000e+00 -1.31838984e-16]
 [-1.38777878e-16  1.00000000e+00]]
W: 0.005142927169799805
Whiten X: 0.00010752677917480469
Whiten M1: 8.177757263183594e-05
Parafac M3: 0.516516923904419
Parafac Decomposition: 78.11781740188599
Unwhitening parafac factors: 3.886222839355469e-05
Initialization
[[-0.9987477  -0.05003107]
 [-0.05003107  0.99874765]]
SGD Calc: 254.50105166435242
Unwhitening factors: 7.677078247070312e-05
Smoothing and Normalization: 0.000335693359375
Fit RMSE: 0.0501470595514681
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0006077289581298828
Fit RMSE: 0.04856814421755291
 Test Against Ground Truth
[('M1 calc', 0.00016045570373535156), ('M2 calc', 8.9688138961792), ('W calc', 0.005142927169799805), ('whiten X', 0.00010752677917480469), ('whiten M1', 8.177757263183594e-05), ('construct M3', 0.516516923904419), ('decompose parafac', 78.11781740188599), ('unwhiten factors parafac', 3.886222839355469e-05), ('parafac smoothing and normalization', 0.000335693359375)] [('M1 calc', 0.00016045570373535156), ('M2 calc', 8.9688138961792), ('W calc', 0.005142927169799805), ('whiten X', 0.00010752677917480469), ('SGD calc', 254.50105166435242), ('unwhiten factors SGD', 7.677078247070312e-05), (' smoothing and normalization', 0.0006077289581298828)] [array([0.49479926]), array([0.98609026])] [array([0.99950278]), array([0.99966051])]
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999997, 1.0000000000000004]
Centering time: 0.004080772399902344
PCA fit: 0.5224611759185791
PCA Transform: 0.004667520523071289
total iterations: 52
TLDA fit: 26.029481887817383
Whitened factor: 
[[-0.9117997   0.84686553]
 [-0.4106353   0.53180707]]
PCA Reverse Transform: 0.00024700164794921875
decenter with new strategy:
[-0.00217033  0.00356269]
decenter with old strategy:
[0.00077489 0.00642727]
Fit RMSE new decenter: 0.03339601986782254
Fit RMSE: 0.03353651922761707
 Test Against Ground Truth
M1: 0.00017142295837402344
M2: 17.43807077407837
[[1.00000000e+00 1.21430643e-16]
 [1.76941795e-16 1.00000000e+00]]
W: 0.013886213302612305
Whiten X: 0.00011181831359863281
Whiten M1: 8.845329284667969e-05
Parafac M3: 0.49547624588012695
Parafac Decomposition: 78.04231595993042
Unwhitening parafac factors: 3.981590270996094e-05
Initialization
[[-0.759603 -0.650387]
 [-0.650387  0.759603]]
SGD Calc: 252.46088457107544
Unwhitening factors: 7.700920104980469e-05
Smoothing and Normalization: 0.00032210350036621094
Fit RMSE: 0.03473980103270228
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005652904510498047
Fit RMSE: 0.033365505314195375
 Test Against Ground Truth
[('M1 calc', 0.00017142295837402344), ('M2 calc', 17.43807077407837), ('W calc', 0.013886213302612305), ('whiten X', 0.00011181831359863281), ('whiten M1', 8.845329284667969e-05), ('construct M3', 0.49547624588012695), ('decompose parafac', 78.04231595993042), ('unwhiten factors parafac', 3.981590270996094e-05), ('parafac smoothing and normalization', 0.00032210350036621094)] [('M1 calc', 0.00017142295837402344), ('M2 calc', 17.43807077407837), ('W calc', 0.013886213302612305), ('whiten X', 0.00011181831359863281), ('SGD calc', 252.46088457107544), ('unwhiten factors SGD', 7.700920104980469e-05), (' smoothing and normalization', 0.0005652904510498047)] [array([0.40124696]), array([0.99444314])] [array([0.99924622]), array([0.99938259])]
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.0022611618041992188
PCA fit: 0.20037436485290527
PCA Transform: 0.0034017562866210938
total iterations: 65
TLDA fit: 32.610461473464966
Whitened factor: 
[[-0.9135067   0.85044456]
 [-0.4068238   0.52606463]]
PCA Reverse Transform: 0.0002300739288330078
decenter with new strategy:
[ 0.06321863 -0.0331702 ]
decenter with old strategy:
[0.11310588 0.01524895]
Fit RMSE new decenter: 0.04968147596442137
Fit RMSE: 0.04973148468942699
 Test Against Ground Truth
M1: 0.00016045570373535156
M2: 8.96156120300293
[[ 1.00000000e+00 -1.76941795e-16]
 [-9.02056208e-17  1.00000000e+00]]
W: 0.0051119327545166016
Whiten X: 0.00010609626770019531
Whiten M1: 8.296966552734375e-05
Parafac M3: 0.5125522613525391
Parafac Decomposition: 78.72745609283447
Unwhitening parafac factors: 3.8623809814453125e-05
Initialization
[[-0.02532911 -0.99967915]
 [-0.99967915  0.02532911]]
SGD Calc: 251.9646782875061
Unwhitening factors: 6.937980651855469e-05
Smoothing and Normalization: 0.00031638145446777344
Fit RMSE: 0.05142222022143769
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005474090576171875
Fit RMSE: 0.049590282945270694
 Test Against Ground Truth
[('M1 calc', 0.00016045570373535156), ('M2 calc', 8.96156120300293), ('W calc', 0.0051119327545166016), ('whiten X', 0.00010609626770019531), ('whiten M1', 8.296966552734375e-05), ('construct M3', 0.5125522613525391), ('decompose parafac', 78.72745609283447), ('unwhiten factors parafac', 3.8623809814453125e-05), ('parafac smoothing and normalization', 0.00031638145446777344)] [('M1 calc', 0.00016045570373535156), ('M2 calc', 8.96156120300293), ('W calc', 0.0051119327545166016), ('whiten X', 0.00010609626770019531), ('SGD calc', 251.9646782875061), ('unwhiten factors SGD', 6.937980651855469e-05), (' smoothing and normalization', 0.0005474090576171875)] [array([0.19697224]), array([0.98420649])] [array([0.99973267]), array([0.99971655])]
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000016, 1.000000000000001]
Centering time: 0.0040721893310546875
PCA fit: 0.49498438835144043
PCA Transform: 0.004670143127441406
total iterations: 42
TLDA fit: 21.12967324256897
Whitened factor: 
[[-0.9098056   0.8233441 ]
 [-0.41503483  0.5675425 ]]
PCA Reverse Transform: 0.0002181529998779297
decenter with new strategy:
[-0.10836645  0.15640742]
decenter with old strategy:
[0.0186921  0.27869938]
Fit RMSE new decenter: 0.0336177313815697
Fit RMSE: 0.033786007801330356
 Test Against Ground Truth
M1: 0.0001747608184814453
M2: 17.413783073425293
[[1.00000000e+00 4.99600361e-16]
 [5.03069808e-16 1.00000000e+00]]
W: 0.014742612838745117
Whiten X: 0.00010895729064941406
Whiten M1: 8.559226989746094e-05
Parafac M3: 0.5014207363128662
Parafac Decomposition: 78.36029267311096
Unwhitening parafac factors: 3.8623809814453125e-05
Initialization
[[-0.7854898 -0.6188744]
 [-0.6188744  0.78549  ]]
SGD Calc: 251.39530873298645
Unwhitening factors: 7.700920104980469e-05
Smoothing and Normalization: 0.0003268718719482422
Fit RMSE: 0.03459389903453825
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005693435668945312
Fit RMSE: 0.033554054122400916
 Test Against Ground Truth
[('M1 calc', 0.0001747608184814453), ('M2 calc', 17.413783073425293), ('W calc', 0.014742612838745117), ('whiten X', 0.00010895729064941406), ('whiten M1', 8.559226989746094e-05), ('construct M3', 0.5014207363128662), ('decompose parafac', 78.36029267311096), ('unwhiten factors parafac', 3.8623809814453125e-05), ('parafac smoothing and normalization', 0.0003268718719482422)] [('M1 calc', 0.0001747608184814453), ('M2 calc', 17.413783073425293), ('W calc', 0.014742612838745117), ('whiten X', 0.00010895729064941406), ('SGD calc', 251.39530873298645), ('unwhiten factors SGD', 7.700920104980469e-05), (' smoothing and normalization', 0.0005693435668945312)] [array([0.62773565]), array([0.98948823])] [array([0.99927857]), array([0.9993039])]
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000002, 1.0000000000000009]
Centering time: 0.0026540756225585938
PCA fit: 0.20456409454345703
PCA Transform: 0.003176450729370117
total iterations: 65
TLDA fit: 33.06482720375061
Whitened factor: 
[[-0.9685124   0.9302277 ]
 [-0.24896541  0.3669829 ]]
PCA Reverse Transform: 0.00021648406982421875
decenter with new strategy:
[-0.00850415  0.01500365]
decenter with old strategy:
[0.00645144 0.02971181]
Fit RMSE new decenter: 0.04730239337403824
Fit RMSE: 0.047267631025409186
 Test Against Ground Truth
M1: 0.0001647472381591797
M2: 9.16208791732788
[[ 1.00000000e+00 -2.61943245e-16]
 [-1.90819582e-16  1.00000000e+00]]
W: 0.0050432682037353516
Whiten X: 0.00011110305786132812
Whiten M1: 8.749961853027344e-05
Parafac M3: 0.5038886070251465
Parafac Decomposition: 77.90325140953064
Unwhitening parafac factors: 3.886222839355469e-05
Initialization
[[-0.92727566 -0.37437922]
 [-0.37437922  0.92727566]]
SGD Calc: 251.27292680740356
Unwhitening factors: 7.009506225585938e-05
Smoothing and Normalization: 0.00030994415283203125
Fit RMSE: 0.04721720457418375
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/core.py:885: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.
  warnings.warn('In partial_svd: converting to NumPy.'
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005421638488769531
Fit RMSE: 0.04911434746997954
 Test Against Ground Truth
[('M1 calc', 0.0001647472381591797), ('M2 calc', 9.16208791732788), ('W calc', 0.0050432682037353516), ('whiten X', 0.00011110305786132812), ('whiten M1', 8.749961853027344e-05), ('construct M3', 0.5038886070251465), ('decompose parafac', 77.90325140953064), ('unwhiten factors parafac', 3.886222839355469e-05), ('parafac smoothing and normalization', 0.00030994415283203125)] [('M1 calc', 0.0001647472381591797), ('M2 calc', 9.16208791732788), ('W calc', 0.0050432682037353516), ('whiten X', 0.00011110305786132812), ('SGD calc', 251.27292680740356), ('unwhiten factors SGD', 7.009506225585938e-05), (' smoothing and normalization', 0.0005421638488769531)] [array([0.99941677]), array([0.99563851])] [array([0.93881474]), array([nan])]
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999987, 0.9999999999999994]
Centering time: 0.004500150680541992
PCA fit: 0.4901261329650879
PCA Transform: 0.004683494567871094
total iterations: 81
TLDA fit: 40.956329107284546
Whitened factor: 
[[-0.9260849   0.8737976 ]
 [-0.37731522  0.48628974]]
PCA Reverse Transform: 0.00022530555725097656
decenter with new strategy:
[-4.47354804e-05  1.18050262e-04]
decenter with old strategy:
[0.00011375 0.0002735 ]
Fit RMSE new decenter: 0.03392800204543267
Fit RMSE: 0.034202062821552005
 Test Against Ground Truth
M1: 0.00017118453979492188
M2: 17.3017737865448
[[ 1.00000000e+00 -6.93889390e-18]
 [-8.32667268e-17  1.00000000e+00]]
W: 0.014733552932739258
Whiten X: 0.00011324882507324219
Whiten M1: 8.034706115722656e-05
Parafac M3: 0.5101444721221924
Parafac Decomposition: 78.95576858520508
Unwhitening parafac factors: 3.838539123535156e-05
Initialization
[[-0.34719515 -0.93779296]
 [-0.93779296  0.3471951 ]]
SGD Calc: 251.22528433799744
Unwhitening factors: 7.462501525878906e-05
Smoothing and Normalization: 0.0003228187561035156
Fit RMSE: 0.034744376770825335
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005679130554199219
Fit RMSE: 0.03401621991379448
 Test Against Ground Truth
[('M1 calc', 0.00017118453979492188), ('M2 calc', 17.3017737865448), ('W calc', 0.014733552932739258), ('whiten X', 0.00011324882507324219), ('whiten M1', 8.034706115722656e-05), ('construct M3', 0.5101444721221924), ('decompose parafac', 78.95576858520508), ('unwhiten factors parafac', 3.838539123535156e-05), ('parafac smoothing and normalization', 0.0003228187561035156)] [('M1 calc', 0.00017118453979492188), ('M2 calc', 17.3017737865448), ('W calc', 0.014733552932739258), ('whiten X', 0.00011324882507324219), ('SGD calc', 251.22528433799744), ('unwhiten factors SGD', 7.462501525878906e-05), (' smoothing and normalization', 0.0005679130554199219)] [array([0.80317062]), array([0.98872039])] [array([0.99913793]), array([0.99945907])]
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999992, 1.0000000000000007]
Centering time: 0.0025300979614257812
PCA fit: 0.19014358520507812
PCA Transform: 0.002330780029296875
total iterations: 110
TLDA fit: 56.21190857887268
Whitened factor: 
[[ 0.81716245 -0.85653746]
 [-0.5764075   0.51608485]]
PCA Reverse Transform: 0.00022864341735839844
decenter with new strategy:
[ 0.00015744 -0.0001099 ]
decenter with old strategy:
[ 2.35170264e-04 -3.06645658e-05]
Fit RMSE new decenter: 0.04637324035362022
Fit RMSE: 0.046388544229751215
 Test Against Ground Truth
M1: 0.00016164779663085938
M2: 9.295818090438843
[[1.00000000e+00 1.49186219e-16]
 [1.80411242e-16 1.00000000e+00]]
W: 0.005553007125854492
Whiten X: 0.0001163482666015625
Whiten M1: 9.107589721679688e-05
Parafac M3: 0.5092625617980957
Parafac Decomposition: 78.55978846549988
Unwhitening parafac factors: 4.029273986816406e-05
Initialization
[[-0.396783   -0.9179125 ]
 [-0.9179125   0.39678293]]
SGD Calc: 249.06572365760803
Unwhitening factors: 7.176399230957031e-05
Smoothing and Normalization: 0.0003135204315185547
Fit RMSE: 0.04631741065117366
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005776882171630859
Fit RMSE: 0.048460372969075495
 Test Against Ground Truth
[('M1 calc', 0.00016164779663085938), ('M2 calc', 9.295818090438843), ('W calc', 0.005553007125854492), ('whiten X', 0.0001163482666015625), ('whiten M1', 9.107589721679688e-05), ('construct M3', 0.5092625617980957), ('decompose parafac', 78.55978846549988), ('unwhiten factors parafac', 4.029273986816406e-05), ('parafac smoothing and normalization', 0.0003135204315185547)] [('M1 calc', 0.00016164779663085938), ('M2 calc', 9.295818090438843), ('W calc', 0.005553007125854492), ('whiten X', 0.0001163482666015625), ('SGD calc', 249.06572365760803), ('unwhiten factors SGD', 7.176399230957031e-05), (' smoothing and normalization', 0.0005776882171630859)] [array([0.98503246]), array([0.99918912])] [array([0.95160186]), array([nan])]
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000018, 1.0]
Centering time: 0.004564762115478516
PCA fit: 0.5271830558776855
PCA Transform: 0.004672050476074219
total iterations: 91
TLDA fit: 46.42684078216553
Whitened factor: 
[[-0.7778524   0.70408475]
 [-0.628447    0.710116  ]]
PCA Reverse Transform: 0.0002453327178955078
decenter with new strategy:
[ 4.01893654e-04 -5.17049075e-05]
decenter with old strategy:
[0.00095879 0.0004905 ]
Fit RMSE new decenter: 0.03355191787720809
Fit RMSE: 0.034119968704701326
 Test Against Ground Truth
M1: 0.00017571449279785156
M2: 17.44372320175171
[[1.00000000e+00 5.89805982e-17]
 [9.71445147e-17 1.00000000e+00]]
W: 0.02382183074951172
Whiten X: 0.00010466575622558594
Whiten M1: 7.891654968261719e-05
Parafac M3: 0.500244140625
Parafac Decomposition: 78.19952630996704
Unwhitening parafac factors: 3.933906555175781e-05
Initialization
[[-0.23029554 -0.9731208 ]
 [-0.9731208   0.23029542]]
SGD Calc: 247.52201056480408
Unwhitening factors: 7.963180541992188e-05
Smoothing and Normalization: 0.0003266334533691406
Fit RMSE: 0.03479058200362467
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0005800724029541016
Fit RMSE: 0.03364545750928274
 Test Against Ground Truth
[('M1 calc', 0.00017571449279785156), ('M2 calc', 17.44372320175171), ('W calc', 0.02382183074951172), ('whiten X', 0.00010466575622558594), ('whiten M1', 7.891654968261719e-05), ('construct M3', 0.500244140625), ('decompose parafac', 78.19952630996704), ('unwhiten factors parafac', 3.933906555175781e-05), ('parafac smoothing and normalization', 0.0003266334533691406)] [('M1 calc', 0.00017571449279785156), ('M2 calc', 17.44372320175171), ('W calc', 0.02382183074951172), ('whiten X', 0.00010466575622558594), ('SGD calc', 247.52201056480408), ('unwhiten factors SGD', 7.963180541992188e-05), (' smoothing and normalization', 0.0005800724029541016)] [array([0.49827402]), array([0.9802521])] [array([0.99934566]), array([0.99918363])]
Done!
new version
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999996, 0.9999999999999996]
Centering time: 0.07947897911071777
PCA fit: 0.7232553958892822
PCA Transform: 0.30972790718078613
Traceback (most recent call last):
  File "generate_tables.py", line 599, in <module>
    main()
  File "generate_tables.py", line 530, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 10, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 452, in gen_fit_0_20
    tlda.fit(x_whit,verbose=False)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final.py", line 142, in fit
    prev_fac = tl.copy(self.factors_)
AttributeError: 'TLDA' object has no attribute 'factors_'
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000007, 1.0000000000000004]
Centering time: 0.08218050003051758
PCA fit: 0.685340404510498
PCA Transform: 0.3216254711151123
total iterations: 65
TLDA fit: 32.74590444564819
Whitened factor: 
[[-0.03600623  0.18746115]
 [ 0.99935156 -0.982272  ]]
PCA Reverse Transform: 0.001065969467163086
decenter with new strategy:
[ 0.00029706 -0.00016112]
decenter with old strategy:
[0.0006069  0.00015709]
Fit RMSE new decenter: 0.049664087811294576
Fit RMSE: 0.05272108770122544
 Test Against Ground Truth
Traceback (most recent call last):
  File "generate_tables.py", line 599, in <module>
    main()
  File "generate_tables.py", line 544, in main
    acc_uncentered.append(accuracy_uncentered)
NameError: name 'accuracy_uncentered' is not defined
starting
text length: 
11314
2098
53845
got data
(11314, 2098)
('sklearn LDA', 63.568071126937866)
('gensim LDA', 62.70705461502075)
Centering time: 0.1226646900177002
PCA fit: 3.144240140914917
PCA Transform: 0.013947486877441406
total iterations: 200
TLDA fit: 7.137620687484741
PCA Reverse Transform: 0.0004162788391113281
Decentering: 0.05189251899719238
decenter with old strategy:
[0.02491998 0.0227829  0.01892622 0.02699272 0.02692784 0.01879176
 0.0226575  0.02559743 0.02637984 0.04948105 0.02512329 0.04099606
 0.01892622 0.03724608 0.01917273 0.01879176 0.040511   0.03331773
 0.03947564 0.03030721]
Smoothing and Normalization: 0.0003364086151123047
[('centering', 0.1226646900177002), ('PCA fit', 3.144240140914917), ('PCA transform', 0.013947486877441406), ('TLDA fit', 7.137620687484741), ('unwhiten factors', 0.0004162788391113281), (' decentering', 0.05189251899719238), (' smoothing and normalization', 0.0003364086151123047)]
(20, 2098)
M1: 0.03660106658935547
M2: 249.4968032836914
[[ 1.00000000e+00  1.75846750e-15  9.56185001e-16  5.13213875e-16
  -8.55251200e-15 -6.07375478e-15 -2.85681851e-15  1.99780513e-15
  -1.22432853e-15 -1.07769696e-15 -1.46757606e-15 -1.00782013e-15
  -1.67997127e-16  2.21873301e-14  2.69380872e-15  8.89121676e-15
  -1.62678844e-14 -2.31870729e-14  1.22809748e-14  3.85019707e-14]
 [ 1.75331754e-15  1.00000000e+00  4.85722573e-16  8.29197822e-16
  -5.82867088e-16 -4.44089210e-16  5.81132364e-16 -3.08780779e-15
   3.36536354e-16  4.02455846e-15  1.91513472e-15  1.80758186e-15
  -1.34614542e-15 -1.09634524e-15  7.14706072e-16  1.29063427e-15
   4.75314232e-16  1.62370117e-15 -4.69069228e-15 -4.43395320e-15]
 [ 9.44751750e-16  3.05311332e-16  1.00000000e+00  9.02056208e-17
  -1.02001740e-15  1.50573998e-15  1.39645240e-15 -6.59194921e-17
   5.48172618e-16 -9.02056208e-17 -8.23993651e-17  1.02001740e-15
  -1.00613962e-15 -5.75928194e-16 -1.60288449e-15  8.56953397e-16
  -1.13797860e-15 -7.77156117e-16  1.27675648e-15  1.06728862e-15]
 [ 5.16910326e-16  9.69710423e-16  1.27502175e-16  1.00000000e+00
  -1.09634524e-15  7.07767178e-16  4.63171168e-16  1.38777878e-16
   7.97972799e-17  3.74700271e-16  6.80011603e-16  5.55111512e-16
  -9.50628465e-16 -2.41126563e-16 -9.81853487e-16  1.59247615e-15
  -9.61036806e-16 -2.11636264e-16  1.34614542e-15  1.71390679e-15]
 [-8.55243068e-15 -7.77156117e-16 -1.01307851e-15 -1.12410081e-15
   1.00000000e+00  4.99600361e-16  7.63278329e-17  7.49400542e-16
   2.08166817e-17 -3.88578059e-16  1.66533454e-16 -4.82253126e-16
   3.22658567e-16 -7.65880415e-16  3.72965547e-17  5.20417043e-16
  -6.62664368e-16  9.99200722e-16 -1.69309011e-15  3.12250226e-16]
 [-6.06376149e-15 -5.79397641e-16  1.63757896e-15  7.49400542e-16
   5.55111512e-16  1.00000000e+00 -1.24900090e-16 -9.71445147e-16
  -1.28369537e-16 -6.93889390e-17 -1.13797860e-15  1.04083409e-15
  -1.38777878e-16  1.56125113e-16 -2.01227923e-16  5.55111512e-16
  -1.71173839e-15 -1.30104261e-16  9.02056208e-16 -6.45750814e-16]
 [-2.87037866e-15  5.00467723e-16  1.31014991e-15  5.27355937e-16
   5.55111512e-17 -1.38777878e-17  1.00000000e+00  6.34908792e-16
   1.81278603e-16 -2.77555756e-16 -1.14665222e-15  1.48318857e-16
  -1.69655956e-15 -2.25514052e-17 -1.08246745e-15  7.14706072e-16
  -1.08642479e-15 -6.50521303e-16  1.28716482e-15  7.04297731e-16]
 [ 1.99809990e-15 -3.12250226e-15  4.33680869e-19  2.08166817e-16
   7.49400542e-16 -1.01307851e-15  5.44703171e-16  1.00000000e+00
  -1.28369537e-15  5.62050406e-16 -5.20417043e-18  7.51135265e-16
   9.02056208e-16  9.05525654e-16 -1.61676228e-15 -1.21430643e-16
  -7.28583860e-17 -1.52655666e-16 -1.72431514e-15 -4.44089210e-16]
 [-1.22403567e-15  2.71484224e-16  6.00214323e-16  6.33174069e-17
   9.10729825e-17 -9.71445147e-17 -2.51534904e-17 -1.31145095e-15
   1.00000000e+00  4.09394740e-16 -7.77156117e-16 -2.13370988e-16
  -4.59701721e-16 -3.35668993e-16  6.93889390e-18 -1.38777878e-16
  -4.30211422e-16 -1.08420217e-16  1.15879528e-15 -8.32667268e-16]
 [-1.07675506e-15  4.08006962e-15 -2.35922393e-16  3.88578059e-16
  -3.81639165e-16 -6.59194921e-17 -3.33066907e-16  5.42968448e-16
   4.23272528e-16  1.00000000e+00  4.99600361e-16  1.73472348e-17
   9.02056208e-16  9.02056208e-17  3.26128013e-16 -4.23272528e-16
   5.26488575e-16  1.58727198e-16 -1.55084279e-15 -8.51749227e-16]
 [-1.46958946e-15  1.96023753e-15 -9.80118764e-17  5.96744876e-16
   9.02056208e-17 -1.15532584e-15 -1.28022593e-15  6.76542156e-17
  -7.37257477e-16  7.91033905e-16  1.00000000e+00 -9.02056208e-17
  -1.12410081e-15 -9.19403442e-17  2.07299455e-16 -4.85722573e-17
  -4.33680869e-16  1.15359111e-16  1.04083409e-16 -7.28583860e-17]
 [-1.00380689e-15  1.76117801e-15  9.92261828e-16  5.44703171e-16
  -4.45823933e-16  9.81853487e-16  1.65232411e-16  7.38992201e-16
  -2.09901541e-16  2.42861287e-17  6.59194921e-17  1.00000000e+00
  -1.23859256e-15 -9.62771529e-16  3.26995375e-16  4.12864187e-16
   1.32879818e-15  1.73472348e-15 -7.99707522e-16  1.65123991e-15]
 [-1.67854825e-16 -1.34527806e-15 -1.02348685e-15 -7.77156117e-16
   2.32452946e-16 -1.11022302e-16 -1.70263109e-15  8.88178420e-16
  -3.79037080e-16  8.46545056e-16 -1.03736464e-15 -1.24206201e-15
   1.00000000e+00  7.42461648e-16  9.10729825e-17 -2.77555756e-16
  -1.02695630e-15 -1.05124243e-15  1.86309301e-15  4.88162028e-16]
 [ 2.21833728e-14 -9.68843061e-16 -5.96744876e-16 -3.16153353e-16
  -7.74554032e-16  2.48065457e-16 -6.93889390e-18  9.87925020e-16
  -3.74700271e-16  1.04083409e-16 -2.89698820e-16 -1.00440489e-15
   6.73072709e-16  1.00000000e+00  1.38040621e-15 -1.42594270e-15
   1.70002901e-15  5.84601811e-16  4.85722573e-17  9.21138166e-16]
 [ 2.67951080e-15  6.92154667e-16 -1.24769986e-15 -1.09200843e-15
   2.08166817e-16 -3.17454396e-16 -9.58434720e-16 -1.51441359e-15
  -8.89045781e-17  4.26741975e-16  3.00974523e-16  2.91433544e-16
   9.88792381e-17  1.28586378e-15  1.00000000e+00 -4.66640615e-16
  -4.00070602e-17  3.95516953e-16 -1.58900670e-15  6.35342473e-16]
 [ 8.88912458e-15  1.25593980e-15  8.08381140e-16  1.54650598e-15
   5.37764278e-16  5.51642065e-16  9.71445147e-16 -2.59341160e-16
  -1.00613962e-16 -3.95516953e-16 -3.12250226e-17  4.30211422e-16
  -2.08166817e-16 -1.31665512e-15 -5.44703171e-16  1.00000000e+00
  -5.34294831e-16  9.71445147e-17 -1.31145095e-15 -3.22658567e-16]
 [-1.62668027e-14  4.04624251e-16 -1.17787724e-15 -8.59555482e-16
  -5.87203897e-16 -1.73363927e-15 -1.07051412e-15 -1.09287579e-16
  -4.36282954e-16  5.57713598e-16 -5.65519853e-16  1.25767452e-15
  -1.00613962e-15  1.74166237e-15 -3.33934269e-17 -4.19803081e-16
   1.00000000e+00  3.19189120e-16  8.32667268e-17  1.18828558e-15]
 [-2.31875981e-14  1.50920942e-15 -7.29451222e-16 -1.53523028e-16
   8.60422844e-16 -1.38018937e-16 -6.39245601e-16 -1.06034972e-16
  -1.43114687e-17  9.23740251e-17  1.31838984e-16  1.71846044e-15
  -9.69710423e-16  6.31439345e-16  3.71230824e-16  6.24500451e-17
   3.05311332e-16  1.00000000e+00  6.93889390e-17 -2.84494650e-16]
 [ 1.22993622e-14 -4.86589935e-15  1.38083989e-15  1.40512602e-15
  -1.71217207e-15  8.50014503e-16  1.23425575e-15 -1.73559084e-15
   1.23685784e-15 -1.68441650e-15  5.55111512e-17 -6.34908792e-16
   1.73472348e-15  2.77555756e-17 -1.58553726e-15 -1.23512311e-15
   6.93889390e-17  1.52655666e-16  1.00000000e+00 -1.93161459e-15]
 [ 3.85045135e-14 -4.31078784e-15  1.11607772e-15  1.74686654e-15
   3.75567633e-16 -6.51497085e-16  6.24500451e-16 -4.77048956e-16
  -7.19042881e-16 -9.55832635e-16 -9.23740251e-17  1.63215795e-15
   4.57912788e-16  9.19403442e-16  6.76975836e-16 -3.29597460e-16
   1.17961196e-15 -1.24900090e-16 -1.81365339e-15  1.00000000e+00]]
W: 0.07670712471008301
Whiten X: 0.06051039695739746
Whiten M1: 3.8623809814453125e-05
Parafac M3: 0.88582444190979
Parafac Decomposition: 149.65329694747925
Unwhitening parafac factors: 0.0003554821014404297
Initialization
[[-0.32893701  0.27226772  0.2682052   0.20251464  0.29220283  0.08913934
  -0.11293399 -0.10538356  0.31302949  0.14978651 -0.23603465  0.0727089
  -0.29146449  0.08050677 -0.12379546  0.01639744  0.21030776  0.2479909
  -0.28093332  0.33828178]
 [-0.20978515 -0.02375041  0.15639384  0.08398449 -0.30654427  0.14439799
  -0.09168752 -0.34745219 -0.30056182  0.0620724  -0.26208031  0.16892727
  -0.06501519  0.11581494 -0.21642347  0.26964835  0.24414639 -0.13724372
   0.52131612 -0.03387867]
 [-0.3043233   0.03406805 -0.28058803 -0.1822342  -0.12505678 -0.23077644
  -0.07072779 -0.13721057  0.07340418 -0.19203498 -0.17989867  0.11952921
  -0.17317947  0.07802614  0.42667869 -0.36558971 -0.16751689  0.3490507
   0.31901042  0.0958622 ]
 [-0.09956783 -0.15404961 -0.10082275 -0.43542499  0.12756473  0.17949768
   0.11535989 -0.10934512  0.01830631 -0.34506729  0.23418763  0.3447402
  -0.03333377 -0.02930619 -0.39525928  0.11445026 -0.2215577  -0.04014674
  -0.01669154  0.42126623]
 [-0.32522759 -0.11170242 -0.35115448  0.04930614 -0.26235234  0.3892483
   0.43869596 -0.09350182 -0.07090745  0.0499045  -0.0814806  -0.25805583
   0.28865985  0.06322188  0.07988023 -0.06065664  0.22408847  0.0661884
  -0.28122461  0.15051127]
 [-0.29955325 -0.03498361  0.3426828   0.12615966  0.10048623 -0.25972782
   0.48997189  0.40628134 -0.17743254 -0.15849643  0.09336284  0.22292434
  -0.02332519  0.16156146  0.27455836  0.08667433  0.09763517 -0.20897508
   0.0702025   0.10156488]
 [-0.4576594   0.14614924 -0.19855158  0.00744658 -0.22267345 -0.05974119
  -0.02935832  0.14839455  0.1495687   0.19440525  0.11636671  0.35852752
  -0.0265991  -0.26209136 -0.10766686  0.27436401 -0.20114106  0.04607
  -0.18999583 -0.46527586]
 [ 0.00569227 -0.3012567   0.26669399  0.29393494  0.1010925  -0.01401305
   0.1800927  -0.21034849  0.00967573  0.0859451  -0.18982904 -0.03816176
   0.2914962  -0.25439417  0.07466615  0.22127098 -0.54350506  0.32093553
   0.08325416  0.1003501 ]
 [-0.07919912  0.10546725  0.36178256 -0.29252072 -0.08956654 -0.12364748
   0.28795876  0.02105078 -0.18422199 -0.00692608 -0.04355372 -0.21739626
  -0.02441113  0.05713095 -0.46546881 -0.3618345  -0.04457117  0.35299274
  -0.02736608 -0.31232436]
 [-0.08255581  0.18290111  0.01254846 -0.17822923  0.35029112  0.31935872
  -0.02204047  0.14299691 -0.20041689 -0.36310329 -0.22236    -0.16332613
  -0.04899462 -0.54423698  0.17321744  0.15371971  0.17372092  0.07688256
   0.11207442 -0.18213062]
 [-0.31911139 -0.11153261  0.0710003  -0.15052258  0.03686436 -0.45705244
  -0.23950459 -0.21375823 -0.19551274  0.09861395 -0.09358067 -0.15027392
   0.20976198 -0.3713478  -0.04108848 -0.1897828   0.06078692 -0.39579783
  -0.22007296  0.19246217]
 [-0.32726418  0.06879209  0.0380824   0.14470997  0.15543297 -0.05645478
  -0.13301581 -0.06967079  0.40520341 -0.27468968  0.39166312 -0.34711341
   0.3648143   0.13342769 -0.12602091  0.05548107  0.08318534 -0.00971752
   0.33555738 -0.11350782]
 [-0.22930895 -0.41400393 -0.01265091  0.27191169 -0.06173234  0.11727245
  -0.44383278  0.31378465 -0.30814199 -0.30585781 -0.03527793 -0.15084393
  -0.18495095  0.21438232 -0.12560073 -0.03253693 -0.15882567  0.09606596
  -0.20796874 -0.04129268]
 [-0.10281163 -0.25027488  0.20897068 -0.17966388  0.23860032  0.28085745
  -0.22522389 -0.0572895  -0.19478563  0.35385192  0.37034206  0.29982165
   0.2339754   0.04166159  0.24987956 -0.20442448  0.20894597  0.23459472
   0.01925529 -0.10513516]
 [-0.01729876 -0.36655378 -0.06009013 -0.03419702  0.30845555  0.09190522
   0.09781807 -0.00678718  0.34650789 -0.06288212 -0.48890641  0.23546751
   0.12159055  0.15336385 -0.10686205 -0.25150083  0.03296369 -0.29361696
   0.00443241 -0.35855628]
 [-0.09582345  0.01827051  0.41247277 -0.18848681 -0.39375724  0.41338273
  -0.09996622  0.21377951  0.3151725   0.08836688 -0.0154886  -0.13849264
  -0.08416134 -0.13102261  0.1631984  -0.20095957 -0.26090623 -0.30428678
   0.09649056  0.12407276]
 [ 0.06198969 -0.22763823  0.09515204 -0.46431873 -0.18301755 -0.22164658
  -0.15504577  0.27168642  0.21500881  0.02745338 -0.25307913 -0.08876293
   0.21812439  0.13700035  0.06874889  0.45379259  0.2499396   0.26417601
  -0.06070339  0.085799  ]
 [-0.18660408 -0.13045975 -0.2574554  -0.18625242  0.35298768  0.0071595
   0.0998093   0.19298321 -0.07202772  0.52570504  0.00638758 -0.36323416
  -0.29492765  0.10293146 -0.08475371  0.13640307 -0.19100735 -0.09830143
   0.29706791  0.0635122 ]
 [-0.03272307 -0.29396429  0.17034912 -0.14902157 -0.01279827 -0.0361818
   0.13929736 -0.49212529  0.11358658 -0.13539074  0.20890833 -0.20259523
  -0.46417173  0.0566388   0.26771111  0.22581377  0.04359808 -0.07569116
  -0.23691404 -0.27788711]
 [-0.07670624  0.43458441  0.06291154 -0.21908109  0.15675533  0.10569823
  -0.12476224 -0.15856171 -0.23204418 -0.04388996 -0.15672994 -0.04020259
   0.26315864  0.48325611  0.19114969  0.17081467 -0.37647343 -0.16592713
  -0.2118008  -0.11075466]]
SGD Calc: 6365.936964035034
Unwhitening factors: 0.00029778480529785156
Smoothing and Normalization: 0.0006971359252929688
Smoothing and Normalization: 0.0005891323089599609
(2098, 2098)
(20, 2098)
(20, 2097)
(20, 2098)
-2.29800075490362
[array(['rememb', 'season', 'america', 'draft', 'primarili', 'grant',
       'faith', 'liber', 'seek', 'effort', 'announc', 'decemb', 'page',
       'station', 'start', 'bibl', 'uucp', 'statement', 'third',
       'statist'], dtype='<U13'), array(['black', 'clearli', 'deal', 'chain', 'dollar', 'armenian', 'cheer',
       'bank', 'batteri', 'brand', 'california', 'blame', 'awar',
       'church', 'code', 'chief', 'bear', 'clipper', 'motif', 'decemb'],
      dtype='<U13'), array(['huh', 'elimin', 'etc', 'huge', 'happi', 'safeti', 'behind',
       'particip', 'innoc', 'bill', 'inde', 'intellect', 'ibm',
       'identifi', 'processor', 'van', 'forget', 'chang', 'code', 'honda'],
      dtype='<U13'), array(['andi', 'corrupt', 'blood', 'warrant', 'function', 'san', 'alter',
       'sub', 'ethnic', 'licens', 'offens', 'biggest', 'evid', 'cycl',
       'acknowledg', 'everywher', 'car', 'finish', 'approach', 'collect'],
      dtype='<U13'), array(['larg', 'compet', 'followup', 'border', 'fit', 'fals', 'month',
       'involv', 'present', 'hate', 'built', 'student', 'constant',
       'provid', 'requir', 'mechan', 'truck', 'found', 'black', 'dual'],
      dtype='<U13'), array(['announc', 'author', 'accus', 'constitut', 'allow', 'attack',
       'annual', 'dear', 'attitud', 'attend', 'increas', 'agre', 'accord',
       'normal', 'acknowledg', 'aliv', 'eight', 'lack', 'cheaper', 'book'],
      dtype='<U13'), array(['corpor', 'cours', 'algorithm', 'bottom', 'everywher', 'aid',
       'dod', 'fourth', 'hook', 'radio', 'virginia', 'declar', 'etc',
       'realiti', 'board', 'race', 'dictionari', 'output', 'overal',
       'discov'], dtype='<U13'), array(['adopt', 'appreci', 'bound', 'analog', 'pull', 'stream', 'bill',
       'cooper', 'away', 'bigger', 'brother', 'complet', 'provid',
       'strongli', 'attempt', 'sinc', 'along', 'babi', 'bomb', 'legitim'],
      dtype='<U13'), array(['presenc', 'gather', 'concern', 'cop', 'applic', 'comput',
       'scientif', 'dark', 'exampl', 'binari', 'field', 'money', 'expand',
       'comfort', 'noth', 'rest', 'consum', 'bill', 'scientist', 'achiev'],
      dtype='<U13'), array(['bought', 'abus', 'assembl', 'eastern', 'among', 'atheist', 'deep',
       'committe', 'fix', 'electr', 'kick', 'clean', 'german', 'action',
       'california', 'johnson', 'anyth', 'featur', 'chri', 'american'],
      dtype='<U13'), array(['belong', 'differ', 'circuit', 'eye', 'biggest', 'note', 'angl',
       'break', 'australia', 'depth', 'dan', 'destruct', 'data', 'award',
       'away', 'figur', 'sinc', 'catch', 'counter', 'roman'], dtype='<U13'), array(['japanes', 'cpu', 'argic', 'broken', 'david', 'known', 'typic',
       'significantli', 'keep', 'best', 'catch', 'inher', 'howev',
       'suppli', 'public', 'dog', 'apolog', 'comment', 'driven',
       'introduc'], dtype='<U13'), array(['dual', 'eye', 'advic', 'medicin', 'babi', 'catch', 'due', 'bush',
       'award', 'build', 'pub', 'traffic', 'buy', 'differ', 'product',
       'respect', 'afraid', 'auto', 'bill', 'belong'], dtype='<U13'), array(['absolut', 'argument', 'capit', 'go', 'otherwis', 'access',
       'brought', 'send', 'dave', 'dual', 'vendor', 'grab', 'cop',
       'conveni', 'data', 'enabl', 'health', 'divid', 'commun',
       'distinct'], dtype='<U13'), array(['consid', 'attempt', 'explan', 'hope', 'home', 'aid', 'analog',
       'belong', 'map', 'eric', 'differ', 'mari', 'excus', 'agre',
       'attent', 'war', 'applic', 'made', 'independ', 'ill'], dtype='<U13'), array(['life', 'civilian', 'demand', 'maintain', 'occasion', 'pre',
       'democrat', 'pray', 'pictur', 'huge', 'machin', 'claim', 'man',
       'defend', 'accomplish', 'mar', 'own', 'foot', 'vehicl', 'electr'],
      dtype='<U13'), array(['across', 'eastern', 'apart', 'gave', 'beyond', 'around',
       'california', 'approxim', 'armenian', 'factor', 'east', 'easi',
       'back', 'amount', 'ask', 'center', 'laugh', 'constant', 'anyth',
       'arm'], dtype='<U13'), array(['found', 'charg', 'civil', 'challeng', 'chain', 'limit', 'child',
       'combin', 'charact', 'appli', 'director', 'divis', 'consult',
       'weight', 'elimin', 'clear', 'okay', 'chip', 'lord', 'clean'],
      dtype='<U13'), array(['pixel', 'huge', 'came', 'eric', 'attend', 'occur', 'compress',
       'idea', 'identifi', 'core', 'ought', 'babi', 'men', 'mask',
       'mostli', 'movi', 'stupid', 'conclud', 'content', 'detroit'],
      dtype='<U13'), array(['countri', 'morn', 'convert', 'aw', 'format', 'nasa', 'continu',
       'babi', 'moral', 'compromis', 'launch', 'differ', 'correspond',
       'conclud', 'minim', 'binari', 'hear', 'coupl', 'conflict',
       'network'], dtype='<U13')]
-3.8252250278558626
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[array(['imag', 'bit', 'file', 'color', 'mac', 'program', 'window', 'gif',
       'scsi', 'system', 'version', 'run', 'softwar', 'format', 'do',
       'max', 'ibm', 'edu', 'display', 'hardwar'], dtype='<U13'), array(['said', 'armenian', 'someth', 'go', 'went', 'start', 'happen',
       'apart', 'kill', 'came', 'live', 'day', 'tell', 'told', 'ask',
       'back', 'name', 'everyth', 'build', 'took'], dtype='<U13'), array(['output', 'file', 'program', 'entri', 'max', 'line', 'check',
       'stream', 'open', 'build', 'return', 'read', 'name', 'remark',
       'write', 'rule', 'info', 'includ', 'author', 'size'], dtype='<U13'), array(['edu', 'com', 'imag', 'graphic', 'pub', 'avail', 'file', 'ftp',
       'mail', 'system', 'includ', 'data', 'program', 'server', 'send',
       'list', 'support', 'version', 'packag', 'format'], dtype='<U13'), array(['imag', 'file', 'gif', 'color', 'format', 'program', 'version',
       'avail', 'bit', 'data', 'system', 'display', 'softwar', 'edu',
       'graphic', 'set', 'includ', 'ftp', 'free', 'convert'], dtype='<U13'), array(['max', 'system', 'file', 'edu', 'program', 'part', 'god', 'window',
       'includ', 'run', 'question', 'state', 'said', 'number', 'believ',
       'mean', 'drive', 'inform', 'key', 'differ'], dtype='<U13'), array(['file', 'imag', 'gun', 'state', 'firearm', 'control', 'gif',
       'program', 'color', 'congress', 'format', 'bill', 'law', 'amend',
       'avail', 'version', 'max', 'system', 'ftp', 'unit'], dtype='<U13'), array(['window', 'avail', 'edu', 'program', 'file', 'includ', 'version',
       'com', 'server', 'system', 'widget', 'subject', 'motif', 'inform',
       'run', 'sourc', 'mit', 'base', 'set', 'sun'], dtype='<U13'), array(['space', 'launch', 'satellit', 'system', 'data', 'program',
       'orbit', 'max', 'nasa', 'edu', 'market', 'includ', 'commerci',
       'servic', 'gener', 'imag', 'mission', 'report', 'technolog',
       'project'], dtype='<U13'), array(['key', 'encrypt', 'system', 'inform', 'bit', 'secur', 'law',
       'comput', 'privaci', 'file', 'pub', 'chip', 'data', 'mail',
       'program', 'avail', 'public', 'govern', 'max', 'part'],
      dtype='<U13'), array(['window', 'edu', 'avail', 'file', 'program', 'includ', 'version',
       'server', 'com', 'system', 'subject', 'motif', 'inform', 'widget',
       'run', 'sourc', 'mit', 'base', 'set', 'sun'], dtype='<U13'), array(['anonym', 'file', 'internet', 'privaci', 'system', 'inform',
       'mail', 'user', 'comput', 'email', 'edu', 'secur', 'network',
       'pub', 'address', 'key', 'servic', 'gener', 'messag', 'program'],
      dtype='<U13'), array(['output', 'file', 'program', 'entri', 'max', 'line', 'check',
       'stream', 'open', 'build', 'return', 'read', 'name', 'remark',
       'write', 'rule', 'info', 'includ', 'author', 'size'], dtype='<U13'), array(['bit', 'mac', 'scsi', 'program', 'system', 'window', 'edu', 'ibm',
       'key', 'chip', 'max', 'do', 'com', 'run', 'number', 'card',
       'standard', 'data', 'drive', 'support'], dtype='<U13'), array(['entri', 'file', 'program', 'section', 'rule', 'must', 'build',
       'info', 'number', 'sourc', 'follow', 'max', 'remark', 'list',
       'compil', 'line', 'includ', 'author', 'name', 'send'], dtype='<U13'), array(['max', 'system', 'file', 'edu', 'program', 'part', 'god', 'window',
       'includ', 'run', 'question', 'state', 'said', 'number', 'believ',
       'mean', 'drive', 'inform', 'key', 'differ'], dtype='<U13'), array(['program', 'imag', 'state', 'system', 'data', 'number', 'health',
       'center', 'report', 'research', 'includ', 'univers', 'avail',
       'nation', 'inform', 'max', 'space', 'gener', 'turkish', 'medic'],
      dtype='<U13'), array(['war', 'turkish', 'jew', 'state', 'govern', 'armenian', 'russian',
       'south', 'program', 'file', 'max', 'secret', 'island', 'world',
       'militari', 'nuclear', 'unit', 'american', 'turkey', 'system'],
      dtype='<U13'), array(['presid', 'go', 'said', 'someth', 'program', 'packag', 'question',
       'believ', 'administr', 'made', 'group', 'talk', 'state', 'job',
       'max', 'mean', 'consid', 'govern', 'support', 'discuss'],
      dtype='<U13'), array(['god', 'believ', 'jesu', 'atheist', 'christian', 'exist', 'mean',
       'question', 'religion', 'must', 'max', 'atheism', 'system',
       'belief', 'true', 'bibl', 'state', 'person', 'gener', 'religi'],
      dtype='<U13')]
-2.1399348759287995
-2.1879825044840944
-2.3280366835247346
starting
text length: 
11314
2098
53845
got data
(11314, 2098)
Centering time: 0.07812857627868652
PCA fit: 3.56081485748291
cupy
PCA Transform: 0.31589770317077637
total iterations: 200
cupy
TLDA fit: 117.52026534080505
PCA Reverse Transform: 0.0008780956268310547
Decentering: 0.0006866455078125
decenter with old strategy:
[0.0262798  0.01879176 0.03356846 0.03029466 0.01917357 0.04028031
 0.02280109 0.03518547 0.0294959  0.03098613 0.02324095 0.01892825
 0.02507956 0.03194258 0.02280114 0.04882819 0.01892818 0.04670797
 0.02539584 0.0394867 ]
Smoothing and Normalization: 0.003770112991333008
[('centering', 0.07812857627868652), ('PCA fit', 3.56081485748291), ('PCA transform', 0.31589770317077637), ('TLDA fit', 117.52026534080505), ('unwhiten factors', 0.0008780956268310547), (' decentering', 0.0006866455078125), (' smoothing and normalization', 0.003770112991333008)]
(20, 2098)
M1: 0.0001685619354248047
M2: 52.81109952926636
[[ 1.00000000e+00 -7.07794283e-16  2.83003872e-16  4.13182672e-16
  -5.63297239e-16  2.15696601e-15  4.48466676e-16  1.95386784e-16
   2.30582697e-15  3.52257286e-16 -2.89011707e-15 -1.86687417e-15
  -2.91330545e-15 -1.37091944e-15 -1.67205659e-15  2.28131045e-15
  -2.88348989e-15  2.75571666e-15  2.19131627e-15  1.02390698e-14]
 [-6.88583576e-16  1.00000000e+00  5.89805982e-17  1.50834206e-15
  -8.60422844e-16  9.33281230e-16  4.93528829e-16  3.26128013e-16
   6.51388665e-16 -1.05471187e-15 -3.35148576e-15 -3.27862737e-16
  -2.47024623e-15 -3.33760797e-15  2.43468440e-15  1.96804378e-15
  -1.97238059e-15 -3.46597751e-15  6.61276589e-15  3.58578185e-15]
 [ 2.79619128e-16  1.02348685e-16  1.00000000e+00 -3.76434994e-16
  -1.94289029e-16 -1.63064007e-15 -2.64545330e-16 -1.10328413e-15
   5.74003735e-16  4.78783679e-16 -1.71303943e-17 -4.20670443e-16
   2.25514052e-16 -1.22731686e-15 -2.40172465e-15 -4.13731549e-16
   1.75467280e-15  1.43982049e-15  5.16947596e-16  2.11072479e-15]
 [ 4.11149369e-16  1.56667214e-15 -3.74700271e-16  1.00000000e+00
   3.60822483e-16 -3.46944695e-16 -4.16333634e-16 -8.32667268e-17
   1.43114687e-16  1.80411242e-16  4.92661467e-16 -4.09394740e-16
  -2.35922393e-16 -1.27068495e-16  4.40619763e-16 -4.19803081e-16
  -4.32813507e-16  7.09501902e-16  7.84962373e-17  7.52869989e-16]
 [-5.52346797e-16 -8.60422844e-16 -3.29597460e-16  3.53883589e-16
   1.00000000e+00  5.27355937e-16 -4.85722573e-17  8.60422844e-16
  -5.46437895e-16 -5.62050406e-16  3.01841885e-16  6.14092110e-16
   5.23886490e-16 -3.59087760e-16  2.68188249e-15  4.51028104e-16
  -2.84494650e-16 -1.33573708e-15 -1.89084859e-16 -5.10767643e-16]
 [ 2.14552260e-15  8.73433270e-16 -1.53002611e-15 -3.33066907e-16
   2.49800181e-16  1.00000000e+00  2.35922393e-16 -1.26287869e-15
  -2.01227923e-16 -5.75928194e-16 -1.06165077e-15  6.38378239e-16
  -1.74166237e-15  6.59194921e-16  1.52655666e-16  1.08940634e-15
   2.08166817e-16  1.09255053e-15 -7.17741838e-16  1.06251813e-16]
 [ 4.50175142e-16  6.12140547e-16 -1.63931368e-16 -3.98986399e-16
   3.46944695e-17  2.77555756e-17  1.00000000e+00 -6.24500451e-17
   2.75821033e-16 -1.04083409e-17 -6.88685220e-16  4.23272528e-16
  -1.43982049e-15  3.69496100e-16  6.24500451e-16  3.85108612e-16
  -7.06899816e-16  2.29742440e-16  1.24900090e-15 -4.36933476e-16]
 [ 1.96508891e-16  4.06792655e-16 -1.12583554e-15 -8.67361738e-17
   9.15933995e-16 -1.30451205e-15 -1.00613962e-16  1.00000000e+00
   1.11802928e-15  1.00960906e-15 -1.24900090e-16  4.97865638e-16
  -9.10729825e-16  2.22044605e-16 -2.18575158e-16 -2.84494650e-16
   5.65519853e-16 -6.40980324e-16  2.65412692e-16  1.27025127e-15]
 [ 2.30589134e-15  5.68555619e-16  6.31181847e-16  1.42681006e-16
  -6.08887940e-16 -1.59594560e-16  2.68882139e-16  1.03909936e-15
   1.00000000e+00 -8.15320034e-17  1.29887420e-16 -1.39645240e-16
   2.10335221e-16  9.22872889e-16 -2.71484224e-16  2.16840434e-16
  -1.21604116e-15 -9.78384040e-16  1.60461922e-16 -1.33964020e-15]
 [ 3.53002675e-16 -1.19348975e-15  4.02455846e-16  1.59594560e-16
  -5.16947596e-16 -5.82867088e-16 -3.12250226e-17  1.02869102e-15
  -7.97972799e-17  1.00000000e+00  2.01227923e-16 -7.14706072e-16
  -9.71445147e-16  2.22044605e-16 -1.11369247e-15 -3.08780779e-16
   1.57859836e-15 -1.85094995e-15  1.50920942e-16 -4.55364912e-16]
 [-2.88472380e-15 -3.36449618e-15 -6.60279123e-17  4.99600361e-16
   2.98372438e-16 -1.03389519e-15 -7.19910243e-16 -1.63064007e-16
   1.40837862e-16  2.35922393e-16  1.00000000e+00 -4.89192020e-16
   8.67361738e-17 -7.12971349e-16  3.64291930e-16  2.04697370e-16
   6.93455710e-16 -2.28116137e-16  2.49800181e-16  1.29713948e-15]
 [-1.87454956e-15 -3.57786717e-16 -4.43438689e-16 -4.40619763e-16
   6.05418493e-16  6.55725474e-16  3.85975973e-16  4.87457297e-16
  -1.02782366e-16 -6.90419943e-16 -4.40619763e-16  1.00000000e+00
  -2.29850861e-16  9.81419807e-16  3.81422324e-16  3.76001313e-16
  -1.33920652e-15  4.25007252e-16  2.86229374e-16 -6.99960923e-16]
 [-2.91495039e-15 -2.31585584e-15  2.94902991e-16 -3.01841885e-16
   5.03069808e-16 -1.73472348e-15 -1.36175793e-15 -8.94249952e-16
   1.92120625e-16 -8.60422844e-16  1.38777878e-16 -2.54136989e-16
   1.00000000e+00 -1.04083409e-16 -1.24553146e-15  1.11672824e-17
   1.36869682e-15 -1.14491749e-16 -1.01654796e-15 -4.25007252e-17]
 [-1.36850709e-15 -3.34281214e-15 -1.21604116e-15 -9.79034562e-17
  -3.75567633e-16  6.00214323e-16  3.71230824e-16  1.62196645e-16
   9.37618039e-16  2.84494650e-16 -8.28330460e-16  9.29811783e-16
  -1.00613962e-16  1.00000000e+00  5.27464357e-17  6.34908792e-16
  -5.37764278e-17  7.85829735e-16 -2.77555756e-16  2.23345648e-17]
 [-1.67109775e-15  2.31666899e-15 -2.07776504e-15  4.96130914e-16
   2.53703308e-15  2.77555756e-16  5.96744876e-16 -3.07262896e-16
  -2.14672030e-16 -1.18134669e-15  4.47124976e-16  4.75097392e-16
  -1.19175503e-15  5.23669649e-17  1.00000000e+00  1.13277443e-15
   1.70349845e-15 -1.73906028e-15  7.49400542e-16  1.42464165e-16]
 [ 2.28232435e-15  1.94646816e-15 -4.38234518e-16 -3.91180144e-16
   5.32560107e-16  1.04430353e-15  4.51895465e-16 -2.32452946e-16
   2.71701064e-16 -2.70616862e-16  1.66533454e-16  3.92481186e-16
   2.51534904e-17  6.28837260e-16  1.09200843e-15  1.00000000e+00
   5.82867088e-16 -3.15719673e-16 -1.71390679e-15  3.63207728e-17]
 [-2.88070124e-15 -2.20244829e-15  1.92294097e-15 -3.75133952e-16
  -2.62919027e-16  3.56919355e-16 -7.24247051e-16  5.46871576e-16
  -1.26136081e-15  1.50920942e-15  7.34221711e-16 -1.37476835e-15
   1.23338839e-15 -1.64798730e-16  1.87176663e-15  6.80011603e-16
   1.00000000e+00 -4.68375339e-16  1.71737624e-15 -4.51678625e-16]
 [ 2.75760216e-15 -3.50262354e-15  1.30212681e-15  6.80011603e-16
  -1.26071029e-15  1.02202318e-15  1.81170183e-16 -6.90419943e-16
  -8.15753715e-16 -1.76724954e-15 -1.80411242e-16  4.01588485e-16
  -8.76035355e-17  7.65013053e-16 -1.62478538e-15 -2.51534904e-16
  -4.40619763e-16  1.00000000e+00 -2.81025203e-15  5.87637577e-16]
 [ 2.19298212e-15  6.58392611e-15  3.51281504e-16 -3.06829215e-17
  -1.59160879e-16 -7.92118107e-16  1.06858966e-15  1.50920942e-16
   1.62521906e-16  7.89299182e-17  3.12250226e-16  2.99239800e-16
  -9.90527105e-16 -3.40005801e-16  8.89045781e-16 -1.64798730e-15
   1.73819292e-15 -2.90739655e-15  1.00000000e+00  1.78329573e-15]
 [ 1.02372667e-14  3.46532698e-15  2.13029464e-15  7.55472074e-16
  -5.54975987e-16  2.44704430e-16 -3.79199710e-16  1.26396289e-15
  -1.35362641e-15 -4.71194264e-16  1.18785190e-15 -7.09935583e-16
  -3.36102673e-17 -8.10983225e-17  9.71445147e-17  1.14925430e-17
  -4.02889527e-16  5.41016884e-16  1.71824360e-15  1.00000000e+00]]
W: 0.0814664363861084
Whiten X: 0.00013780593872070312
cupy
Whiten M1: 9.703636169433594e-05
Parafac M3: 0.9158885478973389
cupy
Parafac Decomposition: 797.4350030422211
cupy
Unwhitening parafac factors: 4.887580871582031e-05
Initialization
starting
text length: 
11314
2098
53845
got data
(11314, 2098)
Centering time: 0.12410926818847656
PCA fit: 3.1161720752716064
numpy
{'dtype': dtype('float64')}
PCA Transform: 0.013889074325561523
{'dtype': dtype('float64')}
total iterations: 43
numpy
TLDA fit: 1.525460958480835
PCA Reverse Transform: 0.00039839744567871094
Decentering: 0.05164217948913574
decenter with old strategy:
[0.01917567 0.02302523 0.02593955 0.02862408 0.0499407  0.0499407
 0.02593955 0.0318136  0.01896768 0.03982104 0.01896768 0.02833815
 0.03266814 0.02389053 0.02302523 0.0187918  0.03266814 0.03963763
 0.0187918  0.03982104]
Smoothing and Normalization: 0.0003330707550048828
numpy
[[2.13119946e-04 1.81910369e-04 2.00628006e-04 ... 3.80015851e-04
  2.54269204e-04 2.96299430e-04]
 [7.37305608e-04 1.12858390e-03 9.26165010e-04 ... 1.18238116e-03
  8.70780016e-04 8.28145435e-04]
 [4.91481257e-04 3.81961989e-04 2.91381107e-04 ... 6.82743873e-04
  3.76518606e-04 2.54454606e-04]
 ...
 [8.91512120e-05 5.86888336e-05 9.11616667e-05 ... 7.70838792e-05
  1.04817903e-04 6.59393898e-05]
 [2.26436808e-04 1.34278907e-04 2.72287144e-04 ... 1.58708826e-04
  2.20858112e-04 1.91473219e-04]
 [1.14871364e-04 2.06381705e-04 6.14624153e-05 ... 1.15567001e-04
  1.58743266e-04 9.02908267e-05]]
[('centering', 0.12410926818847656), ('PCA fit', 3.1161720752716064), ('PCA transform', 0.013889074325561523), ('TLDA fit', 1.525460958480835), ('unwhiten factors', 0.00039839744567871094), (' decentering', 0.05164217948913574), (' smoothing and normalization', 0.0003330707550048828)]
numpy
M1: 0.03661513328552246
M2: 259.93323707580566
[[ 1.00000000e+00 -9.07477218e-17 -5.64515611e-15 -4.37319045e-15
  -1.38008095e-15 -4.78675259e-17 -1.04661424e-15  1.79110199e-16
  -5.77303776e-16  7.15573434e-17 -3.31115343e-16  2.03884219e-15
   3.45535232e-16  2.66854681e-15 -1.25984292e-15  5.03189070e-15
   4.80898145e-14  3.00634084e-14 -7.44077109e-15  8.81907394e-14]
 [-9.42984840e-17  1.00000000e+00 -5.96744876e-16 -9.10729825e-16
   2.22044605e-16  2.35922393e-16  1.30451205e-15  1.48492330e-15
   1.36696210e-15  1.91513472e-15  4.99600361e-16 -2.23605856e-15
   1.72084569e-15 -9.15933995e-16 -2.62290190e-15 -2.77555756e-17
   2.79637424e-15  3.98292510e-15  2.16493490e-15 -7.84095011e-16]
 [-5.64335362e-15 -6.17561557e-16  1.00000000e+00 -3.65159292e-16
   1.52655666e-16  2.49800181e-16 -2.75821033e-16 -1.20563282e-16
   1.02001740e-15  7.42461648e-16 -9.88792381e-17  4.57966998e-16
  -8.32667268e-17  8.98586761e-16  2.07299455e-15 -7.97972799e-17
   3.79904441e-16 -2.22738494e-15 -6.93889390e-16  7.64579372e-16]
 [-4.37603944e-15 -1.05991604e-15 -3.92914867e-16  1.00000000e+00
   4.09394740e-16  2.63677968e-16 -3.98986399e-16  3.05311332e-16
   2.30718222e-16  6.24500451e-16 -7.21644966e-16 -1.30451205e-15
   6.93889390e-17 -1.20129601e-16 -9.67975700e-16 -4.71844785e-16
   1.70002901e-16  4.89192020e-16 -8.60422844e-16  3.46944695e-17]
 [-1.37897641e-15  8.67361738e-17  1.52655666e-16  4.64905892e-16
   1.00000000e+00 -3.05311332e-16  1.17961196e-16 -2.49800181e-16
   2.21177243e-17 -4.16333634e-17 -1.17961196e-16 -4.61436445e-16
  -5.55111512e-17 -4.27609337e-16 -8.12717948e-16  4.92661467e-16
  -2.18575158e-16 -6.93889390e-18  9.29811783e-16 -7.28583860e-17]
 [-5.80725789e-17  3.98119038e-16  1.63064007e-16  2.49800181e-16
  -1.94289029e-16  1.00000000e+00 -6.66133815e-16  9.71445147e-17
   3.72965547e-16  5.89805982e-17 -3.67761377e-16  3.88578059e-16
  -9.85322934e-16 -3.95516953e-16  2.42861287e-16 -3.88578059e-16
   2.29200339e-16  8.69855403e-16 -3.64291930e-16  9.23740251e-17]
 [-1.03705759e-15  1.41293227e-15 -7.50267903e-17 -3.19189120e-16
   2.42861287e-16 -5.27355937e-16  1.00000000e+00  2.04697370e-16
  -3.63424568e-16 -1.04083409e-15  9.10729825e-16 -7.14706072e-16
  -5.18682319e-16 -7.16440796e-16 -3.57353036e-16  3.08780779e-16
   1.85008259e-15 -3.64291930e-17 -8.32667268e-16 -6.76542156e-16]
 [ 1.80126638e-16  1.46063717e-15 -2.84494650e-16  4.02455846e-16
  -1.24900090e-16  4.16333634e-17 -7.28583860e-17  1.00000000e+00
   1.17961196e-16  6.71337985e-16  3.59087760e-16  2.46330734e-16
   4.85722573e-17  7.09501902e-16  4.64905892e-16  2.72351586e-16
   1.31838984e-16 -7.29884903e-16  2.26207941e-15 -3.12250226e-17]
 [-5.75315154e-16  1.36175793e-15  1.03302783e-15  2.05564732e-16
   6.59194921e-17  3.46944695e-16 -2.23779328e-16  7.97972799e-17
   1.00000000e+00  1.12410081e-15  3.46944695e-16 -6.83481050e-16
   1.29236899e-15  4.05057932e-16 -8.21391566e-16  7.61543606e-16
   3.65159292e-16 -7.77156117e-16  1.14491749e-15 -5.36029554e-16]
 [ 7.04257074e-17  1.73819292e-15  9.33281230e-16  7.28583860e-16
  -1.38777878e-17  6.59194921e-17 -1.06165077e-15  5.37764278e-16
   1.14491749e-15  1.00000000e+00 -4.30211422e-16  1.01307851e-15
  -1.16573418e-15 -3.33066907e-16  1.93595140e-15 -1.08246745e-15
  -1.53089347e-15  1.36782946e-15 -1.56472058e-15  1.92944619e-15]
 [-3.32332530e-16  5.10008702e-16 -2.15539392e-16 -6.86950496e-16
  -1.52655666e-16 -3.71230824e-16  8.88178420e-16  2.79290480e-16
   2.63677968e-16 -5.68989300e-16  1.00000000e+00  5.06539255e-16
  -4.78783679e-16  3.66026653e-16  1.00700698e-15 -1.12757026e-15
  -6.24500451e-16  3.43475248e-16  1.59594560e-16 -2.09034179e-16]
 [ 2.04438347e-15 -2.25752576e-15  4.12864187e-16 -1.25940924e-15
  -4.45823933e-16  5.13478149e-16 -7.78023479e-16  2.70616862e-16
  -6.78276879e-16  1.14318277e-15  7.21644966e-16  1.00000000e+00
  -3.57353036e-16 -1.47451495e-16 -4.96130914e-16  2.13370988e-16
   7.77156117e-16 -6.10188983e-16  7.19910243e-16 -7.88214979e-16]
 [ 3.46307726e-16  1.56645530e-15 -7.80625564e-17 -3.46944695e-17
  -1.33573708e-16 -1.00613962e-15 -6.00214323e-16 -9.02056208e-17
   1.38777878e-15 -1.19348975e-15 -4.37150316e-16 -3.15719673e-16
   1.00000000e+00 -9.71445147e-17  6.24500451e-16 -1.15879528e-15
  -4.68375339e-16  4.26741975e-16 -4.99600361e-16  6.56538626e-16]
 [ 2.66660710e-15 -9.08995101e-16  8.76902717e-16 -1.11239143e-16
  -3.85542293e-16 -4.23272528e-16 -6.71337985e-16  6.79577922e-16
   2.91433544e-16 -3.85108612e-16  2.49800181e-16 -2.41126563e-16
  -1.87350135e-16  1.00000000e+00 -2.17881269e-15  1.02695630e-15
   1.06858966e-15  2.08166817e-16  1.24206201e-15 -8.43075609e-16]
 [-1.23590747e-15 -2.83106871e-15  1.97281427e-15 -1.07552856e-15
  -1.11846296e-15  3.17454396e-16 -2.86229374e-16  4.81385765e-16
  -7.45931095e-16  1.99840144e-15  8.81673207e-16 -3.68628739e-16
   6.33174069e-16 -2.08773970e-15  1.00000000e+00 -1.99666672e-15
  -1.73363927e-16  3.88578059e-16 -4.09394740e-16  1.69135539e-17]
 [ 5.03331202e-15  9.02056208e-17 -8.67361738e-18 -5.23019128e-16
   5.10008702e-16 -3.92047506e-16  5.58580959e-16  4.36282954e-16
   7.95370714e-16 -8.91647867e-16 -1.14838694e-15  1.52655666e-16
  -1.25247035e-15  9.22872889e-16 -2.04697370e-15  1.00000000e+00
  -1.45716772e-16 -1.24900090e-16  1.13797860e-15  1.14838694e-15]
 [ 4.80892618e-14  2.46287366e-15  3.28296418e-16 -1.12757026e-17
  -2.02962647e-16  1.97704266e-16  1.70413542e-15  2.46330734e-16
   3.36970035e-16 -1.57426155e-15 -5.75928194e-16  8.51749227e-16
  -3.36536354e-16  1.03736464e-15 -1.44632570e-16 -1.49186219e-16
   1.00000000e+00 -5.82867088e-16 -3.45556916e-15 -3.03576608e-16]
 [ 3.00621327e-14  3.92307714e-15 -2.17621060e-15  5.10876064e-16
  -4.42354486e-17  8.52616588e-16  3.98986399e-17 -7.43979531e-16
  -7.61109925e-16  1.37737044e-15  2.74953671e-16 -6.38378239e-16
   3.08780779e-16  2.65412692e-16  3.34801631e-16 -4.16333634e-17
  -4.44089210e-16  1.00000000e+00  6.80011603e-16 -7.42461648e-16]
 [-7.44697815e-15  2.22391550e-15 -6.66133815e-16 -8.43075609e-16
   9.55832635e-16 -4.16333634e-16 -9.00321484e-16  2.35748920e-15
   1.15879528e-15 -1.59768032e-15  1.04083409e-16  5.93275429e-16
  -4.64905892e-16  1.34614542e-15 -4.12864187e-16  1.17961196e-15
  -3.40005801e-15  7.38992201e-16  1.00000000e+00  8.69096461e-16]
 [ 8.81878422e-14 -7.89732862e-16  8.01550666e-16  4.25007252e-17
  -1.14491749e-16  1.31513724e-16 -7.08634540e-16 -7.71951947e-17
  -6.60929644e-16  1.92922935e-15 -2.26815094e-16 -7.55797334e-16
   6.59249131e-16 -8.18789481e-16 -4.47775497e-17  1.03389519e-15
  -2.86229374e-16 -8.25728375e-16  9.13331910e-16  1.00000000e+00]]
W: 0.08156681060791016
{'dtype': dtype('float64')}
{'dtype': dtype('float64')}
Whiten X: 0.061602115631103516
numpy
{'dtype': dtype('float64')}
Whiten M1: 2.574920654296875e-05
Parafac M3: 0.8947632312774658
numpy
Parafac Decomposition: 3.2651546001434326
numpy
Unwhitening parafac factors: 0.00034332275390625
SGD Calc: 2.384185791015625e-07
Unwhitening factors: 2.384185791015625e-07
Smoothing and Normalization: 0.0006797313690185547
(2098, 2098)
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[array(['entri', 'file', 'program', 'section', 'rule', 'must', 'build',
       'info', 'number', 'sourc', 'follow', 'max', 'remark', 'list',
       'compil', 'line', 'includ', 'author', 'name', 'send'], dtype='<U13'), array(['said', 'armenian', 'someth', 'go', 'went', 'start', 'happen',
       'apart', 'kill', 'came', 'live', 'day', 'tell', 'ask', 'told',
       'back', 'name', 'everyth', 'build', 'took'], dtype='<U13'), array(['edu', 'window', 'avail', 'file', 'program', 'includ', 'version',
       'com', 'server', 'system', 'widget', 'subject', 'inform', 'motif',
       'run', 'sourc', 'base', 'mit', 'set', 'ftp'], dtype='<U13'), array(['imag', 'file', 'bit', 'color', 'program', 'mac', 'gif', 'format',
       'system', 'version', 'window', 'softwar', 'scsi', 'display', 'run',
       'edu', 'avail', 'data', 'max', 'do'], dtype='<U13'), array(['key', 'encrypt', 'system', 'bit', 'inform', 'secur', 'comput',
       'law', 'privaci', 'file', 'pub', 'chip', 'data', 'mail', 'program',
       'avail', 'public', 'govern', 'max', 'ftp'], dtype='<U13'), array(['key', 'encrypt', 'system', 'bit', 'inform', 'secur', 'comput',
       'law', 'privaci', 'file', 'pub', 'chip', 'data', 'mail', 'program',
       'avail', 'public', 'govern', 'max', 'ftp'], dtype='<U13'), array(['edu', 'window', 'avail', 'file', 'program', 'includ', 'version',
       'com', 'server', 'system', 'widget', 'subject', 'inform', 'motif',
       'run', 'sourc', 'base', 'mit', 'set', 'ftp'], dtype='<U13'), array(['imag', 'file', 'program', 'color', 'gif', 'format', 'bit', 'edu',
       'system', 'avail', 'version', 'data', 'softwar', 'display',
       'graphic', 'set', 'ftp', 'includ', 'window', 'free'], dtype='<U13'), array(['output', 'file', 'program', 'entri', 'max', 'line', 'check',
       'stream', 'open', 'build', 'return', 'read', 'name', 'remark',
       'write', 'rule', 'includ', 'info', 'author', 'size'], dtype='<U13'), array(['anonym', 'internet', 'file', 'privaci', 'system', 'inform',
       'mail', 'user', 'comput', 'edu', 'email', 'pub', 'secur',
       'network', 'address', 'servic', 'key', 'messag', 'gener', 'ftp'],
      dtype='<U13'), array(['output', 'file', 'program', 'entri', 'max', 'line', 'check',
       'stream', 'open', 'build', 'return', 'read', 'name', 'remark',
       'write', 'rule', 'includ', 'info', 'author', 'size'], dtype='<U13'), array(['space', 'launch', 'satellit', 'system', 'data', 'program', 'edu',
       'imag', 'orbit', 'includ', 'nasa', 'max', 'market', 'commerci',
       'servic', 'gener', 'report', 'avail', 'inform', 'technolog'],
      dtype='<U13'), array(['war', 'turkish', 'jew', 'file', 'govern', 'imag', 'state',
       'program', 'armenian', 'russian', 'south', 'secret', 'island',
       'max', 'world', 'system', 'militari', 'nuclear', 'edu', 'ship'],
      dtype='<U13'), array(['file', 'imag', 'gun', 'gif', 'color', 'program', 'format',
       'state', 'control', 'firearm', 'version', 'system', 'avail', 'bit',
       'congress', 'ftp', 'bill', 'edu', 'max', 'law'], dtype='<U13'), array(['said', 'armenian', 'someth', 'go', 'went', 'start', 'happen',
       'apart', 'kill', 'came', 'live', 'day', 'tell', 'ask', 'told',
       'back', 'name', 'everyth', 'build', 'took'], dtype='<U13'), array(['max', 'system', 'file', 'edu', 'program', 'part', 'god', 'window',
       'includ', 'run', 'question', 'state', 'said', 'number', 'believ',
       'mean', 'drive', 'inform', 'key', 'differ'], dtype='<U13'), array(['war', 'turkish', 'jew', 'file', 'govern', 'imag', 'state',
       'program', 'armenian', 'russian', 'south', 'secret', 'island',
       'max', 'world', 'system', 'militari', 'nuclear', 'edu', 'ship'],
      dtype='<U13'), array(['presid', 'go', 'said', 'someth', 'program', 'packag', 'question',
       'believ', 'administr', 'made', 'group', 'state', 'talk', 'job',
       'max', 'mean', 'govern', 'consid', 'support', 'discuss'],
      dtype='<U13'), array(['max', 'system', 'file', 'edu', 'program', 'part', 'god', 'window',
       'includ', 'run', 'question', 'state', 'said', 'number', 'believ',
       'mean', 'drive', 'inform', 'key', 'differ'], dtype='<U13'), array(['anonym', 'internet', 'file', 'privaci', 'system', 'inform',
       'mail', 'user', 'comput', 'edu', 'email', 'pub', 'secur',
       'network', 'address', 'servic', 'key', 'messag', 'gener', 'ftp'],
      dtype='<U13')]
-2.133513774233232
-2.931267457587155
new version
Centering time: 0.4510498046875
PCA fit: 12.06215524673462
PCA Transform: 0.4422428607940674
total iterations: 200
TLDA fit: 15.749561786651611
Whitened factor: 
[[-0.50259279  0.07696343  0.30376284  0.01724117 -0.13220375  0.53617613
   0.33380426 -0.49114203 -0.36606016 -0.43191307  0.52457111  0.50831902
  -0.17137154  0.4512745  -0.43723641  0.22282783 -0.27187089 -0.48243574
  -0.09723687  0.03215042]
 [-0.0185765   0.01503819  0.06243207  0.08187849 -0.1005991   0.17579015
  -0.201802    0.10469118 -0.21864781 -0.14065691 -0.10837454  0.49797614
   0.00536003  0.06596923  0.05115677  0.13241197 -0.12459168 -0.43031845
  -0.07365532  0.24998762]
 [-0.14721335  0.21202985  0.11530152 -0.03910191 -0.26375061  0.17270219
  -0.43038931  0.02103649 -0.29890231 -0.14126861 -0.00649122  0.06411739
  -0.15401682  0.21640436  0.15455868  0.13680354 -0.12823917 -0.03652007
   0.04571671  0.42799264]
 [ 0.1018628  -0.02980121  0.11669356 -0.08583214 -0.00979888 -0.07375443
   0.45281014  0.1087606   0.22206525  0.04865146 -0.14293112  0.10908056
   0.02155357 -0.08458319 -0.27267087  0.003555   -0.00181536 -0.09920267
   0.09212726 -0.39262978]
 [-0.47090731  0.04821018 -0.01566992 -0.06663058 -0.07942473  0.11124422
  -0.18014764 -0.25709356 -0.16569698 -0.07648482  0.30741624 -0.04156231
  -0.04766702  0.38473551  0.0914764   0.01347614 -0.02227552  0.03003114
   0.05751406  0.19549585]
 [ 0.47602892  0.11195896  0.06553189 -0.0787031   0.0176286   0.0287209
  -0.06128163  0.13999088 -0.12515367  0.00641809 -0.18463922  0.19247398
  -0.13791911 -0.36820302 -0.04984062 -0.11359403  0.08676819 -0.23005531
   0.04661649  0.13030255]
 [ 0.02290288 -0.00248695  0.03454141 -0.56723584 -0.02699915 -0.01474708
  -0.02650776  0.15206822 -0.08966919  0.08920305 -0.1616679  -0.40682244
  -0.00624238 -0.00595301 -0.0200749  -0.23598076  0.20114663  0.46706701
   0.56710694  0.0747436 ]
 [ 0.14184151 -0.0849486  -0.12557015  0.39994448  0.14249129 -0.05157178
   0.06188399  0.08294686  0.17501656 -0.05073782 -0.11964287  0.03932869
   0.07317682 -0.16188235  0.04574747  0.2496483  -0.21008157 -0.04903121
  -0.37740237 -0.14948386]
 [ 0.08805008 -0.48415876 -0.27016024 -0.2125335   0.29423644 -0.2398823
  -0.01418457  0.05684492  0.08765972  0.30670449 -0.08543314  0.31366548
   0.51344993 -0.14286337  0.14632387 -0.45644362  0.41929851 -0.36679025
   0.1876524  -0.05410896]
 [-0.09400917 -0.49237126 -0.13581468 -0.20347348  0.13780384  0.32571718
  -0.11877258 -0.31479586 -0.21917205 -0.32774849  0.35595432 -0.12789251
   0.55932355  0.03859558  0.08262333  0.34828309 -0.32319731  0.1286406
   0.21958987  0.18995818]
 [-0.06768561  0.18604976 -0.04928172  0.28484886 -0.01996477 -0.33453383
   0.05439757 -0.11711695  0.22261205  0.41935898  0.16556449 -0.15592092
  -0.21680841  0.02069179  0.04753563 -0.48204508  0.4636133   0.12850853
  -0.29338526 -0.16718233]
 [-0.00858081 -0.16448826 -0.01668021  0.31978752  0.05316687  0.22609332
  -0.01793204 -0.07133057 -0.12498363 -0.32908112  0.06803529 -0.09222292
   0.15060826 -0.00871029 -0.01256282 -0.20706584  0.3755034   0.10056996
  -0.30228312  0.08507864]
 [-0.18592369  0.39277171  0.1353734  -0.23481142 -0.31425219  0.40145963
  -0.25905916  0.39951632 -0.32699817 -0.45134178 -0.30111092  0.07680043
  -0.34686948  0.2678783   0.10986713  0.36802793 -0.35308942 -0.08948044
   0.22709901  0.34852438]
 [ 0.38217108 -0.3683441  -0.57706425 -0.00856855  0.679656    0.06064605
  -0.17824643  0.31147507 -0.0487729  -0.03981218 -0.30123636  0.26827555
   0.26388919 -0.50034984  0.3247859   0.01186997 -0.01110101 -0.25586289
   0.00748511  0.09889126]
 [ 0.07581853  0.04862481 -0.16311943 -0.17770282  0.1605218  -0.12482853
   0.09826514 -0.44196385  0.27417551  0.01180561  0.37545037  0.08525676
  -0.0743488  -0.1610017   0.09134496  0.03732425 -0.03685139 -0.09694805
   0.17623138 -0.23389875]
 [-0.00871963 -0.13548685  0.2427835   0.14155913 -0.10624358 -0.13329883
   0.24977128  0.14738534  0.0358441   0.17813871 -0.09905375  0.06788835
   0.1333848   0.01646478 -0.31664909 -0.16997153  0.15788082 -0.0554889
  -0.14658483 -0.14105567]
 [ 0.05680914  0.03628274  0.02135098 -0.08239027 -0.01286918 -0.11513864
   0.04233423 -0.06762798  0.12284213  0.06321001  0.04777953  0.03374587
  -0.03881073 -0.07244978  0.01874579 -0.03844183  0.03566747 -0.03762272
   0.08449957 -0.09417357]
 [-0.09455432  0.1412515   0.07058288 -0.19710462 -0.08199164 -0.07197854
   0.03532553 -0.08173505  0.13506526  0.02092995  0.10028471 -0.09617605
  -0.12740914  0.10489267 -0.00728869 -0.0105336   0.01663764  0.08922373
   0.20940035 -0.08404862]
 [ 0.01058389 -0.16453951 -0.44871035  0.24003773  0.28800534  0.14479585
  -0.46786846 -0.01686285 -0.37931843 -0.03278531  0.01362175 -0.0966938
   0.13550176 -0.03322381  0.57727198 -0.06155552  0.05033229  0.10203535
  -0.26800555  0.43502792]
 [-0.16135261  0.16853616  0.34295601  0.10815804 -0.28057538  0.25690753
   0.07989279 -0.11616011 -0.33744732 -0.16310006  0.11316994  0.12145851
  -0.15553783  0.20029426 -0.32272625  0.08281471 -0.10062549 -0.11433993
  -0.13093989  0.17541638]]
PCA Reverse Transform: 0.0006802082061767578
Traceback (most recent call last):
  File "generate_tables.py", line 604, in <module>
    main()
  File "generate_tables.py", line 535, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 4000, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 478, in gen_fit_0_20
    res2, accuracy = postprocess(factors_unwhitened, mu, x, vocab, num_tops, smoothing, True, alpha_0 = alpha_0)
  File "generate_tables.py", line 135, in postprocess
    factors_unwhitened += wc
  File "cupy/_core/core.pyx", line 1466, in cupy._core.core.ndarray.__array_ufunc__
  File "cupy/_core/_kernel.pyx", line 1061, in cupy._core._kernel.ufunc.__call__
  File "cupy/_core/_kernel.pyx", line 109, in cupy._core._kernel._preprocess_args
TypeError: Unsupported type <class 'numpy.ndarray'>
new version
Centering time: 0.4610280990600586
PCA fit: 11.606519937515259
PCA Transform: 0.4482595920562744
total iterations: 200
TLDA fit: 15.717660665512085
Whitened factor: 
[[-0.50259279  0.07696343  0.30376284  0.01724117 -0.13220375  0.53617613
   0.33380426 -0.49114203 -0.36606016 -0.43191307  0.52457111  0.50831902
  -0.17137154  0.4512745  -0.43723641  0.22282783 -0.27187089 -0.48243574
  -0.09723687  0.03215042]
 [-0.0185765   0.01503819  0.06243207  0.08187849 -0.1005991   0.17579015
  -0.201802    0.10469118 -0.21864781 -0.14065691 -0.10837454  0.49797614
   0.00536003  0.06596923  0.05115677  0.13241197 -0.12459168 -0.43031845
  -0.07365532  0.24998762]
 [-0.14721335  0.21202985  0.11530152 -0.03910191 -0.26375061  0.17270219
  -0.43038931  0.02103649 -0.29890231 -0.14126861 -0.00649122  0.06411739
  -0.15401682  0.21640436  0.15455868  0.13680354 -0.12823917 -0.03652007
   0.04571671  0.42799264]
 [ 0.1018628  -0.02980121  0.11669356 -0.08583214 -0.00979888 -0.07375443
   0.45281014  0.1087606   0.22206525  0.04865146 -0.14293112  0.10908056
   0.02155357 -0.08458319 -0.27267087  0.003555   -0.00181536 -0.09920267
   0.09212726 -0.39262978]
 [-0.47090731  0.04821018 -0.01566992 -0.06663058 -0.07942473  0.11124422
  -0.18014764 -0.25709356 -0.16569698 -0.07648482  0.30741624 -0.04156231
  -0.04766702  0.38473551  0.0914764   0.01347614 -0.02227552  0.03003114
   0.05751406  0.19549585]
 [ 0.47602892  0.11195896  0.06553189 -0.0787031   0.0176286   0.0287209
  -0.06128163  0.13999088 -0.12515367  0.00641809 -0.18463922  0.19247398
  -0.13791911 -0.36820302 -0.04984062 -0.11359403  0.08676819 -0.23005531
   0.04661649  0.13030255]
 [ 0.02290288 -0.00248695  0.03454141 -0.56723584 -0.02699915 -0.01474708
  -0.02650776  0.15206822 -0.08966919  0.08920305 -0.1616679  -0.40682244
  -0.00624238 -0.00595301 -0.0200749  -0.23598076  0.20114663  0.46706701
   0.56710694  0.0747436 ]
 [ 0.14184151 -0.0849486  -0.12557015  0.39994448  0.14249129 -0.05157178
   0.06188399  0.08294686  0.17501656 -0.05073782 -0.11964287  0.03932869
   0.07317682 -0.16188235  0.04574747  0.2496483  -0.21008157 -0.04903121
  -0.37740237 -0.14948386]
 [ 0.08805008 -0.48415876 -0.27016024 -0.2125335   0.29423644 -0.2398823
  -0.01418457  0.05684492  0.08765972  0.30670449 -0.08543314  0.31366548
   0.51344993 -0.14286337  0.14632387 -0.45644362  0.41929851 -0.36679025
   0.1876524  -0.05410896]
 [-0.09400917 -0.49237126 -0.13581468 -0.20347348  0.13780384  0.32571718
  -0.11877258 -0.31479586 -0.21917205 -0.32774849  0.35595432 -0.12789251
   0.55932355  0.03859558  0.08262333  0.34828309 -0.32319731  0.1286406
   0.21958987  0.18995818]
 [-0.06768561  0.18604976 -0.04928172  0.28484886 -0.01996477 -0.33453383
   0.05439757 -0.11711695  0.22261205  0.41935898  0.16556449 -0.15592092
  -0.21680841  0.02069179  0.04753563 -0.48204508  0.4636133   0.12850853
  -0.29338526 -0.16718233]
 [-0.00858081 -0.16448826 -0.01668021  0.31978752  0.05316687  0.22609332
  -0.01793204 -0.07133057 -0.12498363 -0.32908112  0.06803529 -0.09222292
   0.15060826 -0.00871029 -0.01256282 -0.20706584  0.3755034   0.10056996
  -0.30228312  0.08507864]
 [-0.18592369  0.39277171  0.1353734  -0.23481142 -0.31425219  0.40145963
  -0.25905916  0.39951632 -0.32699817 -0.45134178 -0.30111092  0.07680043
  -0.34686948  0.2678783   0.10986713  0.36802793 -0.35308942 -0.08948044
   0.22709901  0.34852438]
 [ 0.38217108 -0.3683441  -0.57706425 -0.00856855  0.679656    0.06064605
  -0.17824643  0.31147507 -0.0487729  -0.03981218 -0.30123636  0.26827555
   0.26388919 -0.50034984  0.3247859   0.01186997 -0.01110101 -0.25586289
   0.00748511  0.09889126]
 [ 0.07581853  0.04862481 -0.16311943 -0.17770282  0.1605218  -0.12482853
   0.09826514 -0.44196385  0.27417551  0.01180561  0.37545037  0.08525676
  -0.0743488  -0.1610017   0.09134496  0.03732425 -0.03685139 -0.09694805
   0.17623138 -0.23389875]
 [-0.00871963 -0.13548685  0.2427835   0.14155913 -0.10624358 -0.13329883
   0.24977128  0.14738534  0.0358441   0.17813871 -0.09905375  0.06788835
   0.1333848   0.01646478 -0.31664909 -0.16997153  0.15788082 -0.0554889
  -0.14658483 -0.14105567]
 [ 0.05680914  0.03628274  0.02135098 -0.08239027 -0.01286918 -0.11513864
   0.04233423 -0.06762798  0.12284213  0.06321001  0.04777953  0.03374587
  -0.03881073 -0.07244978  0.01874579 -0.03844183  0.03566747 -0.03762272
   0.08449957 -0.09417357]
 [-0.09455432  0.1412515   0.07058288 -0.19710462 -0.08199164 -0.07197854
   0.03532553 -0.08173505  0.13506526  0.02092995  0.10028471 -0.09617605
  -0.12740914  0.10489267 -0.00728869 -0.0105336   0.01663764  0.08922373
   0.20940035 -0.08404862]
 [ 0.01058389 -0.16453951 -0.44871035  0.24003773  0.28800534  0.14479585
  -0.46786846 -0.01686285 -0.37931843 -0.03278531  0.01362175 -0.0966938
   0.13550176 -0.03322381  0.57727198 -0.06155552  0.05033229  0.10203535
  -0.26800555  0.43502792]
 [-0.16135261  0.16853616  0.34295601  0.10815804 -0.28057538  0.25690753
   0.07989279 -0.11616011 -0.33744732 -0.16310006  0.11316994  0.12145851
  -0.15553783  0.20029426 -0.32272625  0.08281471 -0.10062549 -0.11433993
  -0.13093989  0.17541638]]
PCA Reverse Transform: 0.0006895065307617188
decenter with old strategy:
[1.93708716e-04 1.28627620e-04 1.11156513e-04 8.51982314e-05
 1.48109554e-04 7.01124199e-05 9.92796193e-05 1.87937518e-04
 1.70538773e-04 2.10625476e-04 8.55114450e-05 7.15690735e-05
 1.64246615e-04 9.26098887e-05 1.87254134e-04 8.15931171e-05
 1.96294877e-04 2.06927538e-04 2.01575742e-04 1.45048344e-04]
Smoothing and Normalization: 9.632110595703125e-05
Traceback (most recent call last):
  File "generate_tables.py", line 605, in <module>
    main()
  File "generate_tables.py", line 536, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 4000, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 479, in gen_fit_0_20
    res2, accuracy = postprocess(factors_unwhitened, mu, x, vocab, num_tops, smoothing, True, alpha_0 = alpha_0)
  File "generate_tables.py", line 210, in postprocess
    permutation,RMSE = test_util.validate_gammad(factors_unwhitened.T, mu, num_tops = num_tops)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/test_util.py", line 106, in validate_gammad
    return permutation, tl.metrics.regression.RMSE(tl.tensor(np.array([sample[:, permutation[1][i]] for i in range(num_tops)])), factor.T)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/test_util.py", line 106, in <listcomp>
    return permutation, tl.metrics.regression.RMSE(tl.tensor(np.array([sample[:, permutation[1][i]] for i in range(num_tops)])), factor.T)
IndexError: index 2 is out of bounds for axis 0 with size 2
new version
Centering time: 0.4458332061767578
PCA fit: 11.46791386604309
PCA Transform: 0.31044578552246094
total iterations: 200
TLDA fit: 1.3931074142456055
Whitened factor: 
[[ 0.73235588 -0.75160043]
 [-0.68092206  0.65961867]]
PCA Reverse Transform: 4.482269287109375e-05
decenter with old strategy:
[4.31671232e-05 2.39017750e-04]
Smoothing and Normalization: 5.9604644775390625e-05
Fit RMSE: 0.0480112256184671
 Test Against Ground Truth
Centering time: 0.44490885734558105
PCA fit: 11.440134763717651
PCA Transform: 0.29958248138427734
total iterations: 200
TLDA fit: 1.3980679512023926
Whitened factor: 
[[-0.88129411  0.84628954]
 [-0.47256818  0.5327232 ]]
PCA Reverse Transform: 4.363059997558594e-05
Traceback (most recent call last):
  File "generate_tables.py", line 605, in <module>
    main()
  File "generate_tables.py", line 536, in main
    res_centered, accuracy_centered, res_sklearn, accuracy_sklearn = gen_fit_0_20(n_iter_train = 40001, batch_size_grad = 4000, vocab=vocab, seed=seed_arr[j], theta = theta, learning_rate = lr) 
  File "generate_tables.py", line 479, in gen_fit_0_20
    res2, accuracy = postprocess(factors_unwhitened, mu, x, vocab, num_tops, smoothing, True, alpha_0 = alpha_0)
  File "generate_tables.py", line 132, in postprocess
    wc   =  tl.reshape(wc,(vocab,1))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/__init__.py", line 161, in inner
    return _get_backend_method(name)(*args, **kwargs)
  File "<__array_function__ internals>", line 5, in reshape
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 298, in reshape
    return _wrapfunc(a, 'reshape', newshape, order=order)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 57, in _wrapfunc
    return bound(*args, **kwds)
ValueError: cannot reshape array of size 500 into shape (1000,1)
new version
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000013, 1.0000000000000007]
Centering time: 0.023443937301635742
PCA fit: 0.44002270698547363
PCA Transform: 0.0017018318176269531
total iterations: 142
TLDA fit: 3.894402265548706
Whitened factor: 
[[ 0.87873118 -0.90250043]
 [-0.477317    0.43068896]]
PCA Reverse Transform: 5.555152893066406e-05
decenter with old strategy:
[0.1451027  0.02995223]
Smoothing and Normalization: 5.7220458984375e-05
Fit RMSE: 0.046585888956651965
 Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
[1.0000000000000004, 1.0000000000000009]
Centering time: 0.0450136661529541
PCA fit: 1.0892128944396973
PCA Transform: 0.002938508987426758
total iterations: 93
TLDA fit: 2.559399127960205
Whitened factor: 
[[-0.89933259  0.85820138]
 [-0.43726524  0.51331316]]
PCA Reverse Transform: 7.700920104980469e-05
decenter with old strategy:
[8.54008622e-04 7.01930548e-05]
Smoothing and Normalization: 6.794929504394531e-05
Fit RMSE: 0.03328773317002328
 Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999996, 1.0000000000000004]
Centering time: 0.02133631706237793
PCA fit: 0.47609448432922363
PCA Transform: 0.001466989517211914
total iterations: 115
TLDA fit: 3.1344733238220215
Whitened factor: 
[[-0.23865235  0.16633364]
 [-0.97110507  0.98606953]]
PCA Reverse Transform: 5.078315734863281e-05
decenter with old strategy:
[0.03333544 0.01980083]
Smoothing and Normalization: 5.8650970458984375e-05
Fit RMSE: 0.049560548086332465
 Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
[0.9999999999999991, 0.9999999999999989]
Centering time: 0.04585528373718262
PCA fit: 1.112811803817749
PCA Transform: 0.0026476383209228516
total iterations: 61
TLDA fit: 1.7407724857330322
Whitened factor: 
[[-0.88968488  0.84610982]
 [-0.4565751   0.5330086 ]]
PCA Reverse Transform: 6.723403930664062e-05
decenter with old strategy:
[0.00036448 0.00051853]
Smoothing and Normalization: 7.677078247070312e-05
Fit RMSE: 0.03286294302400204
 Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999998, 1.0000000000000004]
Centering time: 0.021285295486450195
PCA fit: 0.530531644821167
PCA Transform: 0.0014691352844238281
total iterations: 72
TLDA fit: 1.96683669090271
Whitened factor: 
[[-0.05651598  0.15826708]
 [ 0.99840169 -0.98739634]]
PCA Reverse Transform: 5.269050598144531e-05
decenter with old strategy:
[ 4.73506424e-11 -9.87726274e-11]
Smoothing and Normalization: 5.91278076171875e-05
Fit RMSE: 0.05148082356274279
 Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
[0.9999999999999994, 0.9999999999999996]
Centering time: 0.04563021659851074
PCA fit: 1.1057400703430176
PCA Transform: 0.002651214599609375
total iterations: 200
TLDA fit: 5.468206167221069
Whitened factor: 
[[ 0.91752394 -0.92809697]
 [-0.39768055  0.37233858]]
PCA Reverse Transform: 6.818771362304688e-05
decenter with old strategy:
[0.00779426 0.04752679]
Smoothing and Normalization: 6.842613220214844e-05
Fit RMSE: 0.03324027783642153
 Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[0.9999999999999998, 0.9999999999999998]
Centering time: 0.02099609375
PCA fit: 0.4575819969177246
PCA Transform: 0.001489877700805664
total iterations: 200
TLDA fit: 5.583651781082153
Whitened factor: 
[[ 0.78291333 -0.80648756]
 [-0.62213079  0.59125106]]
PCA Reverse Transform: 5.1021575927734375e-05
decenter with old strategy:
[0.00346086 0.0340428 ]
Smoothing and Normalization: 5.9604644775390625e-05
Fit RMSE: 0.04655910587608174
 Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
[1.0, 0.9999999999999996]
Centering time: 0.04552006721496582
PCA fit: 1.11971116065979
PCA Transform: 0.0028057098388671875
total iterations: 144
TLDA fit: 4.019294738769531
Whitened factor: 
[[ 0.87899585 -0.89988532]
 [-0.47682942  0.43612661]]
PCA Reverse Transform: 7.367134094238281e-05
decenter with old strategy:
[0.00306579 0.00059053]
Smoothing and Normalization: 7.033348083496094e-05
Fit RMSE: 0.03253500658133647
 Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000009, 1.0000000000000018]
Centering time: 0.021253585815429688
PCA fit: 0.45395660400390625
PCA Transform: 0.001451253890991211
total iterations: 149
TLDA fit: 4.126335382461548
Whitened factor: 
[[-0.26433216  0.20243626]
 [-0.9644317   0.97929544]]
PCA Reverse Transform: 5.245208740234375e-05
decenter with old strategy:
[0.0151508  0.01053931]
Smoothing and Normalization: 5.91278076171875e-05
Fit RMSE: 0.052103628531177316
 Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
[1.000000000000001, 0.9999999999999989]
Centering time: 0.04576826095581055
PCA fit: 1.1074283123016357
PCA Transform: 0.002702951431274414
total iterations: 149
TLDA fit: 4.066052675247192
Whitened factor: 
[[ 0.84865861 -0.87259786]
 [-0.52894099  0.48843933]]
PCA Reverse Transform: 8.535385131835938e-05
decenter with old strategy:
[0.04993639 0.00461608]
Smoothing and Normalization: 7.009506225585938e-05
Fit RMSE: 0.033130025434633864
 Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000004, 1.000000000000001]
Centering time: 0.021047115325927734
PCA fit: 0.4627232551574707
PCA Transform: 0.0014438629150390625
total iterations: 200
TLDA fit: 5.482179641723633
Whitened factor: 
[[ 0.8981146  -0.91367317]
 [-0.43976148  0.40644967]]
PCA Reverse Transform: 5.125999450683594e-05
decenter with old strategy:
[0.00424724 0.00617531]
Smoothing and Normalization: 5.888938903808594e-05
Fit RMSE: 0.045391191806180016
 Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
[1.0000000000000016, 0.9999999999999989]
Centering time: 0.045432090759277344
PCA fit: 1.0938050746917725
PCA Transform: 0.003639698028564453
total iterations: 200
TLDA fit: 5.4933435916900635
Whitened factor: 
[[ 0.84913869 -0.8657666 ]
 [-0.52816994  0.50044799]]
PCA Reverse Transform: 7.2479248046875e-05
decenter with old strategy:
[0.07076475 0.00443831]
Smoothing and Normalization: 7.081031799316406e-05
Fit RMSE: 0.033873207030075646
 Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000004, 0.9999999999999997]
[1.0000000000000002, 1.0000000000000009]
Centering time: 0.021070241928100586
PCA fit: 0.4984111785888672
PCA Transform: 0.0014965534210205078
total iterations: 75
TLDA fit: 2.126903533935547
Whitened factor: 
[[ 0.0451524   0.04684159]
 [ 0.99898011 -0.99890233]]
PCA Reverse Transform: 5.4836273193359375e-05
decenter with old strategy:
[0.00045194 0.0002645 ]
Smoothing and Normalization: 5.841255187988281e-05
Fit RMSE: 0.052204408547199106
 Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
[0.9999999999999997, 1.0000000000000004]
Centering time: 0.04548001289367676
PCA fit: 1.0865402221679688
PCA Transform: 0.0026674270629882812
total iterations: 106
TLDA fit: 2.9528846740722656
Whitened factor: 
[[ 0.17315065 -0.06245051]
 [-0.98489535  0.99804806]]
PCA Reverse Transform: 8.606910705566406e-05
decenter with old strategy:
[0.02327794 0.02928127]
Smoothing and Normalization: 7.033348083496094e-05
Fit RMSE: 0.035811832246783575
 Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.000000000000001, 1.0000000000000009]
Centering time: 0.02091503143310547
PCA fit: 0.47811102867126465
PCA Transform: 0.0014390945434570312
total iterations: 56
TLDA fit: 1.6207916736602783
Whitened factor: 
[[-0.96384146  0.94008148]
 [-0.26647632  0.34094986]]
PCA Reverse Transform: 5.269050598144531e-05
decenter with old strategy:
[0.08500484 0.00932082]
Smoothing and Normalization: 7.295608520507812e-05
Fit RMSE: 0.048357633750603576
 Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
[1.0, 0.9999999999999996]
Centering time: 0.04557490348815918
PCA fit: 1.0896716117858887
PCA Transform: 0.0026788711547851562
total iterations: 105
TLDA fit: 2.8905959129333496
Whitened factor: 
[[ 0.20904668 -0.1041077 ]
 [-0.97790566  0.99456603]]
PCA Reverse Transform: 8.511543273925781e-05
decenter with old strategy:
[0.0718525 0.1076234]
Smoothing and Normalization: 6.866455078125e-05
Fit RMSE: 0.03605571905023393
 Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0000000000000007, 1.000000000000001]
Centering time: 0.02105569839477539
PCA fit: 0.4613058567047119
PCA Transform: 0.0014755725860595703
total iterations: 79
TLDA fit: 2.239745616912842
Whitened factor: 
[[-0.73678073  0.60115957]
 [ 0.67613176  0.79912901]]
PCA Reverse Transform: 7.009506225585938e-05
decenter with old strategy:
[0.07156597 0.01700547]
Smoothing and Normalization: 5.9604644775390625e-05
Fit RMSE: 0.048342824082291694
 Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
[1.0000000000000004, 1.0000000000000004]
Centering time: 0.045679569244384766
PCA fit: 1.0996487140655518
PCA Transform: 0.002836942672729492
total iterations: 85
TLDA fit: 2.324427843093872
Whitened factor: 
[[ 0.00623817  0.08221398]
 [ 0.99998054 -0.9966147 ]]
PCA Reverse Transform: 7.05718994140625e-05
decenter with old strategy:
[0.00021238 0.00027065]
Smoothing and Normalization: 6.890296936035156e-05
Fit RMSE: 0.03572081879365257
 Test Against Ground Truth
Vocab: 500
num_tweets: 20000
density: 15
[1.0, 0.9999999999999991]
Centering time: 0.02097487449645996
PCA fit: 0.4635176658630371
PCA Transform: 0.0014607906341552734
total iterations: 114
TLDA fit: 3.1461637020111084
Whitened factor: 
[[-0.98513889  0.97180672]
 [-0.17175959  0.23577894]]
PCA Reverse Transform: 5.459785461425781e-05
decenter with old strategy:
[ 5.35149560e-10 -5.01847441e-10]
Smoothing and Normalization: 5.841255187988281e-05
Fit RMSE: 0.04832217685280847
 Test Against Ground Truth
Vocab: 1000
num_tweets: 20000
density: 15
[1.0, 1.0]
Centering time: 0.045549869537353516
PCA fit: 1.0950219631195068
PCA Transform: 0.0026819705963134766
total iterations: 81
TLDA fit: 2.3285059928894043
Whitened factor: 
[[-0.2547811   0.16804282]
 [-0.96699876  0.9857797 ]]
PCA Reverse Transform: 8.726119995117188e-05
decenter with old strategy:
[0.06432838 0.03854514]
Smoothing and Normalization: 6.699562072753906e-05
Fit RMSE: 0.035529138408238826
 Test Against Ground Truth
Traceback (most recent call last):
  File "generate_tables.py", line 607, in <module>
    main()
  File "generate_tables.py", line 598, in main
    csvwriter.writerow([str(i), str(vocab), str(lr), str(theta), str(acc_centered[j][0]), str(acc_centered[j][1])])
IndexError: list index out of range
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999996]
Centering time: 0.010423421859741211
PCA fit: 0.2770407199859619
PCA Transform: 0.0009186267852783203
total iterations: 200
TLDA fit: 2.8423452377319336
Whitened factor: 
[[ 0.83122688 -0.85146254]
 [-0.55593334  0.52441543]]
PCA Reverse Transform: 5.1975250244140625e-05
decenter with old strategy:
[3.81463324e-04 1.62419861e-05]
Smoothing and Normalization: 5.555152893066406e-05
Fit RMSE: 0.04754696560873581
 Test Against Ground Truth
M1: 0.001687765121459961
M2: 6.656595945358276
[[1.00000000e+00 1.11022302e-16]
 [1.38777878e-16 1.00000000e+00]]
W: 0.004404306411743164
Whiten X: 0.006596088409423828
Whiten M1: 2.1696090698242188e-05
Parafac M3: 0.05666065216064453
Parafac Decomposition: 12.680349349975586
Unwhitening parafac factors: 1.6450881958007812e-05
Initialization
[[-0.96448589 -0.26413439]
 [-0.26413439  0.96448589]]
SGD Calc: 22.492056608200073
Unwhitening factors: 1.621246337890625e-05
Smoothing and Normalization: 4.315376281738281e-05
Fit RMSE: 0.047562570874537996
parafac Test Against Ground Truth
Smoothing and Normalization: 6.842613220214844e-05
Fit RMSE: 0.047655673430330954
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999989, 1.0]
Centering time: 0.0225675106048584
PCA fit: 0.6625349521636963
PCA Transform: 0.0013377666473388672
total iterations: 200
TLDA fit: 3.2594985961914062
Whitened factor: 
[[-0.75372137  0.70739811]
 [-0.65719411  0.70681533]]
PCA Reverse Transform: 0.00011205673217773438
decenter with old strategy:
[0.08777543 0.04296353]
Smoothing and Normalization: 7.653236389160156e-05
Fit RMSE: 0.03383038115267633
 Test Against Ground Truth
M1: 0.004785776138305664
M2: 19.98136568069458
[[ 1.00000000e+00 -3.60822483e-16]
 [-2.49800181e-16  1.00000000e+00]]
W: 0.013637304306030273
Whiten X: 0.015889406204223633
Whiten M1: 3.695487976074219e-05
Parafac M3: 0.056758880615234375
Parafac Decomposition: 13.291620016098022
Unwhitening parafac factors: 2.0503997802734375e-05
Initialization
[[-0.81895226 -0.57386165]
 [-0.57386165  0.81895226]]
SGD Calc: 21.886311054229736
Unwhitening factors: 2.2172927856445312e-05
Smoothing and Normalization: 7.033348083496094e-05
Fit RMSE: 0.03424715650523227
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010061264038085938
Fit RMSE: 0.03324198439153975
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999998]
Centering time: 0.008516311645507812
PCA fit: 0.2693188190460205
PCA Transform: 0.0007100105285644531
total iterations: 200
TLDA fit: 2.869987964630127
Whitened factor: 
[[ 0.80233117 -0.82272775]
 [-0.59687913  0.56843561]]
PCA Reverse Transform: 5.1975250244140625e-05
decenter with old strategy:
[0.04644966 0.0031164 ]
Smoothing and Normalization: 5.841255187988281e-05
Fit RMSE: 0.047927044363294355
 Test Against Ground Truth
M1: 0.0016810894012451172
M2: 6.661275625228882
[[ 1.00000000e+00 -2.77555756e-17]
 [-1.80411242e-16  1.00000000e+00]]
W: 0.004155397415161133
Whiten X: 0.0058803558349609375
Whiten M1: 2.0742416381835938e-05
Parafac M3: 0.05617713928222656
Parafac Decomposition: 12.699223279953003
Unwhitening parafac factors: 1.5735626220703125e-05
Initialization
[[-0.82772018 -0.56114107]
 [-0.56114107  0.82772018]]
SGD Calc: 22.988067388534546
Unwhitening factors: 1.4781951904296875e-05
Smoothing and Normalization: 4.100799560546875e-05
Fit RMSE: 0.04984766841505655
parafac Test Against Ground Truth
Smoothing and Normalization: 7.605552673339844e-05
Fit RMSE: 0.04794495215041808
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000013, 0.9999999999999999]
Centering time: 0.023045778274536133
PCA fit: 0.6503889560699463
PCA Transform: 0.0013282299041748047
total iterations: 200
TLDA fit: 2.85864520072937
Whitened factor: 
[[-0.36385229  0.30632252]
 [-0.93145666  0.95192779]]
PCA Reverse Transform: 0.00010514259338378906
decenter with old strategy:
[-1.10132960e-11 -1.77193526e-11]
Smoothing and Normalization: 6.771087646484375e-05
Fit RMSE: 0.03467869589571522
 Test Against Ground Truth
M1: 0.00487828254699707
M2: 19.875986099243164
[[ 1.00000000e+00 -1.45716772e-16]
 [-2.28983499e-16  1.00000000e+00]]
W: 0.014824867248535156
Whiten X: 0.01630854606628418
Whiten M1: 3.647804260253906e-05
Parafac M3: 0.05637645721435547
Parafac Decomposition: 13.256506443023682
Unwhitening parafac factors: 2.002716064453125e-05
Initialization
[[-0.08155244  0.99666905]
 [ 0.99666905  0.08155244]]
SGD Calc: 22.381199598312378
Unwhitening factors: 1.8358230590820312e-05
Smoothing and Normalization: 5.8650970458984375e-05
Fit RMSE: 0.0336799136209693
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010251998901367188
Fit RMSE: 0.03479488683639745
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 0.9999999999999996]
Centering time: 0.008226871490478516
PCA fit: 0.29914355278015137
PCA Transform: 0.0007371902465820312
total iterations: 157
TLDA fit: 2.263341188430786
Whitened factor: 
[[ 0.05989584  0.02828245]
 [ 0.99820463 -0.99959997]]
PCA Reverse Transform: 5.5789947509765625e-05
decenter with old strategy:
[-6.07442984e-10  6.75220747e-10]
Smoothing and Normalization: 5.793571472167969e-05
Fit RMSE: 0.05232481305536097
 Test Against Ground Truth
M1: 0.0016677379608154297
M2: 6.659996747970581
[[1.00000000e+00 5.55111512e-17]
 [2.77555756e-17 1.00000000e+00]]
W: 0.004719734191894531
Whiten X: 0.0059435367584228516
Whiten M1: 2.2649765014648438e-05
Parafac M3: 0.05629396438598633
Parafac Decomposition: 13.408349990844727
Unwhitening parafac factors: 1.71661376953125e-05
Initialization
[[-0.99701108 -0.07725874]
 [-0.07725874  0.99701108]]
SGD Calc: 22.174954652786255
Unwhitening factors: 1.8835067749023438e-05
Smoothing and Normalization: 5.173683166503906e-05
Fit RMSE: 0.04443260420301337
parafac Test Against Ground Truth
Smoothing and Normalization: 8.249282836914062e-05
Fit RMSE: 0.04952509976289115
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000018, 1.0000000000000013]
Centering time: 0.0232393741607666
PCA fit: 0.6558067798614502
PCA Transform: 0.0013203620910644531
total iterations: 200
TLDA fit: 3.05502986907959
Whitened factor: 
[[-3.28877959e-02  6.65212021e-04]
 [-9.99459050e-01  9.99999779e-01]]
PCA Reverse Transform: 7.796287536621094e-05
decenter with old strategy:
[0.00035971 0.00043122]
Smoothing and Normalization: 6.961822509765625e-05
Fit RMSE: 0.037491191220901145
 Test Against Ground Truth
M1: 0.004912137985229492
M2: 19.90482211112976
[[1.00000000e+00 6.07153217e-17]
 [7.54604712e-17 1.00000000e+00]]
W: 0.013646364212036133
Whiten X: 0.016041278839111328
Whiten M1: 3.314018249511719e-05
Parafac M3: 0.05706453323364258
Parafac Decomposition: 13.290992259979248
Unwhitening parafac factors: 2.002716064453125e-05
Initialization
[[-0.10316249 -0.99466452]
 [-0.99466452  0.10316249]]
SGD Calc: 22.32136368751526
Unwhitening factors: 2.09808349609375e-05
Smoothing and Normalization: 7.009506225585938e-05
Fit RMSE: 0.034823598795856205
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00011181831359863281
Fit RMSE: 0.03374077098052321
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.008127212524414062
PCA fit: 0.2688477039337158
PCA Transform: 0.0009348392486572266
total iterations: 200
TLDA fit: 2.90431547164917
Whitened factor: 
[[-0.60467087  0.57667846]
 [-0.79647545  0.81697121]]
PCA Reverse Transform: 5.507469177246094e-05
decenter with old strategy:
[0.00040807 0.00040439]
Smoothing and Normalization: 4.792213439941406e-05
Fit RMSE: 0.048421380377280644
 Test Against Ground Truth
M1: 0.0016291141510009766
M2: 6.64914608001709
[[ 1.00000000e+00 -4.44089210e-16]
 [-1.66533454e-16  1.00000000e+00]]
W: 0.004556179046630859
Whiten X: 0.00822591781616211
Whiten M1: 3.2901763916015625e-05
Parafac M3: 0.055390119552612305
Parafac Decomposition: 12.941198825836182
Unwhitening parafac factors: 1.6689300537109375e-05
Initialization
[[-0.94438779 -0.32883386]
 [-0.32883386  0.94438779]]
SGD Calc: 21.733936309814453
Unwhitening factors: 1.71661376953125e-05
Smoothing and Normalization: 5.14984130859375e-05
Fit RMSE: 0.04740772458525355
parafac Test Against Ground Truth
Smoothing and Normalization: 6.723403930664062e-05
Fit RMSE: 0.04755705647613469
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000013, 1.0000000000000016]
Centering time: 0.021129369735717773
PCA fit: 0.664398193359375
PCA Transform: 0.0013325214385986328
total iterations: 200
TLDA fit: 2.8115148544311523
Whitened factor: 
[[ 0.86673245 -0.89456977]
 [-0.49877335  0.44692833]]
PCA Reverse Transform: 9.679794311523438e-05
decenter with old strategy:
[0.00759435 0.05488004]
Smoothing and Normalization: 6.818771362304688e-05
Fit RMSE: 0.033653630026519804
 Test Against Ground Truth
M1: 0.0052301883697509766
M2: 20.13828468322754
[[1.00000000e+00 2.77555756e-17]
 [1.04083409e-17 1.00000000e+00]]
W: 0.012171745300292969
Whiten X: 0.01667928695678711
Whiten M1: 3.170967102050781e-05
Parafac M3: 0.05672264099121094
Parafac Decomposition: 12.673549175262451
Unwhitening parafac factors: 2.0742416381835938e-05
Initialization
[[-0.7501759   0.66123832]
 [ 0.66123832  0.7501759 ]]
SGD Calc: 21.61603879928589
Unwhitening factors: 1.811981201171875e-05
Smoothing and Normalization: 5.698204040527344e-05
Fit RMSE: 0.03412793424439653
parafac Test Against Ground Truth
Smoothing and Normalization: 8.606910705566406e-05
Fit RMSE: 0.03523119062659554
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999993]
Centering time: 0.00807046890258789
PCA fit: 0.30571842193603516
PCA Transform: 0.0007157325744628906
total iterations: 200
TLDA fit: 2.85241961479187
Whitened factor: 
[[ 0.87052565 -0.89566782]
 [-0.49212304  0.44472368]]
PCA Reverse Transform: 5.626678466796875e-05
decenter with old strategy:
[0.00337413 0.00038317]
Smoothing and Normalization: 4.6253204345703125e-05
Fit RMSE: 0.047464144132236874
 Test Against Ground Truth
M1: 0.0016055107116699219
M2: 6.963466644287109
[[ 1.00000000e+00 -1.80411242e-16]
 [-1.94289029e-16  1.00000000e+00]]
W: 0.004643440246582031
Whiten X: 0.008349418640136719
Whiten M1: 3.314018249511719e-05
Parafac M3: 0.057137250900268555
Parafac Decomposition: 13.020777225494385
Unwhitening parafac factors: 1.6450881958007812e-05
Initialization
[[-0.99752133 -0.0703648 ]
 [-0.0703648   0.99752133]]
SGD Calc: 22.131160736083984
Unwhitening factors: 1.6689300537109375e-05
Smoothing and Normalization: 4.124641418457031e-05
Fit RMSE: 0.049579243681563
parafac Test Against Ground Truth
Smoothing and Normalization: 8.225440979003906e-05
Fit RMSE: 0.047392590066302746
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999993]
Centering time: 0.02139735221862793
PCA fit: 0.6613214015960693
PCA Transform: 0.0013279914855957031
total iterations: 200
TLDA fit: 2.8774540424346924
Whitened factor: 
[[ 0.88184868 -0.89824109]
 [-0.47153251  0.43950307]]
PCA Reverse Transform: 9.322166442871094e-05
decenter with old strategy:
[0.00469884 0.036997  ]
Smoothing and Normalization: 6.794929504394531e-05
Fit RMSE: 0.03335398224046294
 Test Against Ground Truth
M1: 0.0048046112060546875
M2: 20.1507351398468
[[1.00000000e+00 5.06539255e-16]
 [4.44089210e-16 1.00000000e+00]]
W: 0.012202262878417969
Whiten X: 0.016282320022583008
Whiten M1: 3.4332275390625e-05
Parafac M3: 0.05694127082824707
Parafac Decomposition: 13.046729803085327
Unwhitening parafac factors: 2.193450927734375e-05
Initialization
[[-0.69330836  0.72064105]
 [ 0.72064105  0.69330836]]
SGD Calc: 21.916738033294678
Unwhitening factors: 1.7642974853515625e-05
Smoothing and Normalization: 5.817413330078125e-05
Fit RMSE: 0.03347827799078747
parafac Test Against Ground Truth
Smoothing and Normalization: 8.845329284667969e-05
Fit RMSE: 0.03331387112964138
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000007, 0.9999999999999993]
Centering time: 0.008127450942993164
PCA fit: 0.269528865814209
PCA Transform: 0.0006973743438720703
total iterations: 200
TLDA fit: 2.9130382537841797
Whitened factor: 
[[ 0.94697824 -0.95969969]
 [-0.32129771  0.28102757]]
PCA Reverse Transform: 5.459785461425781e-05
decenter with old strategy:
[0.02173581 0.00214047]
Smoothing and Normalization: 5.8650970458984375e-05
Fit RMSE: 0.04673317359600424
 Test Against Ground Truth
M1: 0.001567840576171875
M2: 6.795833587646484
[[1.00000000e+00 7.21644966e-16]
 [3.60822483e-16 1.00000000e+00]]
W: 0.0043985843658447266
Whiten X: 0.008265972137451172
Whiten M1: 3.1948089599609375e-05
Parafac M3: 0.05682039260864258
Parafac Decomposition: 12.733441591262817
Unwhitening parafac factors: 1.5735626220703125e-05
Initialization
[[-0.98952244 -0.14437916]
 [-0.14437916  0.98952244]]
SGD Calc: 22.020726680755615
Unwhitening factors: 1.2874603271484375e-05
Smoothing and Normalization: 4.100799560546875e-05
Fit RMSE: 0.04677274865842702
parafac Test Against Ground Truth
Smoothing and Normalization: 6.842613220214844e-05
Fit RMSE: 0.046823741523188495
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000022]
Centering time: 0.02116680145263672
PCA fit: 0.7026517391204834
PCA Transform: 0.0012919902801513672
total iterations: 200
TLDA fit: 2.8213183879852295
Whitened factor: 
[[ 0.80189333 -0.8228139 ]
 [-0.59746723  0.56831091]]
PCA Reverse Transform: 8.225440979003906e-05
decenter with old strategy:
[-3.88348249e-10  3.95861790e-10]
Smoothing and Normalization: 7.176399230957031e-05
Fit RMSE: 0.03370965002494189
 Test Against Ground Truth
M1: 0.004942655563354492
M2: 19.923070907592773
[[1.00000000e+00 5.20417043e-18]
 [2.08166817e-17 1.00000000e+00]]
W: 0.012948036193847656
Whiten X: 0.016218185424804688
Whiten M1: 3.7670135498046875e-05
Parafac M3: 0.056427001953125
Parafac Decomposition: 12.807466745376587
Unwhitening parafac factors: 2.1457672119140625e-05
Initialization
[[-0.21259246 -0.97714095]
 [-0.97714095  0.21259246]]
SGD Calc: 21.642832040786743
Unwhitening factors: 1.9073486328125e-05
Smoothing and Normalization: 6.246566772460938e-05
Fit RMSE: 0.03373614319683265
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010132789611816406
Fit RMSE: 0.03509951579162751
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000002, 1.0000000000000004]
Centering time: 0.008096456527709961
PCA fit: 0.2858879566192627
PCA Transform: 0.0006849765777587891
total iterations: 138
TLDA fit: 2.020059823989868
Whitened factor: 
[[ 0.03269834  0.05998469]
 [ 0.99946527 -0.9981993 ]]
PCA Reverse Transform: 5.5789947509765625e-05
decenter with old strategy:
[0.00212508 0.00099335]
Smoothing and Normalization: 5.7697296142578125e-05
Fit RMSE: 0.05327792800179511
 Test Against Ground Truth
M1: 0.0015664100646972656
M2: 6.769554138183594
[[ 1.00000000e+00 -3.12250226e-16]
 [-3.40005801e-16  1.00000000e+00]]
W: 0.0042188167572021484
Whiten X: 0.008553743362426758
Whiten M1: 2.8133392333984375e-05
Parafac M3: 0.05683326721191406
Parafac Decomposition: 12.655090808868408
Unwhitening parafac factors: 1.6927719116210938e-05
Initialization
[[-0.92095497 -0.38966901]
 [-0.38966901  0.92095497]]
SGD Calc: 22.13716149330139
Unwhitening factors: 1.52587890625e-05
Smoothing and Normalization: 5.53131103515625e-05
Fit RMSE: 0.05077492646594946
parafac Test Against Ground Truth
Smoothing and Normalization: 6.651878356933594e-05
Fit RMSE: 0.04864681241117847
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 0.9999999999999998]
Centering time: 0.021133899688720703
PCA fit: 0.6870365142822266
PCA Transform: 0.001341104507446289
total iterations: 200
TLDA fit: 2.7887041568756104
Whitened factor: 
[[ 0.87306918 -0.89378825]
 [-0.48759636  0.4484892 ]]
PCA Reverse Transform: 9.322166442871094e-05
decenter with old strategy:
[0.00011885 0.00047378]
Smoothing and Normalization: 5.7220458984375e-05
Fit RMSE: 0.033530491897549135
 Test Against Ground Truth
M1: 0.004860877990722656
M2: 19.72513246536255
[[ 1.00000000e+00 -3.05311332e-16]
 [-2.22044605e-16  1.00000000e+00]]
W: 0.012235164642333984
Whiten X: 0.016376018524169922
Whiten M1: 4.00543212890625e-05
Parafac M3: 0.05646800994873047
Parafac Decomposition: 13.339299201965332
Unwhitening parafac factors: 2.0503997802734375e-05
Initialization
[[-0.26673979 -0.96376858]
 [-0.96376858  0.26673979]]
SGD Calc: 22.121581315994263
Unwhitening factors: 2.0265579223632812e-05
Smoothing and Normalization: 6.103515625e-05
Fit RMSE: 0.03414667804177983
parafac Test Against Ground Truth
Smoothing and Normalization: 8.487701416015625e-05
Fit RMSE: 0.03351034351608951
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999999, 1.0]
Centering time: 0.008302450180053711
PCA fit: 0.2695918083190918
PCA Transform: 0.0007269382476806641
total iterations: 151
TLDA fit: 2.1468613147735596
Whitened factor: 
[[-0.91649408  0.88482187]
 [-0.40004825  0.46592946]]
PCA Reverse Transform: 5.7697296142578125e-05
decenter with old strategy:
[-4.66907844e-10  3.80248885e-10]
Smoothing and Normalization: 5.841255187988281e-05
Fit RMSE: 0.049168086709727575
 Test Against Ground Truth
M1: 0.0016019344329833984
M2: 6.772556304931641
[[1.00000000e+00 3.85975973e-16]
 [2.32452946e-16 1.00000000e+00]]
W: 0.0041735172271728516
Whiten X: 0.008276224136352539
Whiten M1: 3.075599670410156e-05
Parafac M3: 0.05666637420654297
Parafac Decomposition: 12.947314023971558
Unwhitening parafac factors: 1.5497207641601562e-05
Initialization
[[-0.02265144  0.99974342]
 [ 0.99974342  0.02265144]]
SGD Calc: 22.060776233673096
Unwhitening factors: 1.6450881958007812e-05
Smoothing and Normalization: 4.38690185546875e-05
Fit RMSE: 0.04909639689728035
parafac Test Against Ground Truth
Smoothing and Normalization: 8.58306884765625e-05
Fit RMSE: 0.04902962192557588
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 1.0000000000000004]
Centering time: 0.021275758743286133
PCA fit: 0.7060337066650391
PCA Transform: 0.0013248920440673828
total iterations: 200
TLDA fit: 3.070819616317749
Whitened factor: 
[[ 0.84325687 -0.86251335]
 [-0.5375108   0.5060343 ]]
PCA Reverse Transform: 9.34600830078125e-05
decenter with old strategy:
[-2.45797746e-10  2.54296647e-10]
Smoothing and Normalization: 6.961822509765625e-05
Fit RMSE: 0.03326696605307909
 Test Against Ground Truth
M1: 0.0049974918365478516
M2: 20.048126697540283
[[1.00000000e+00 3.81639165e-17]
 [3.46944695e-17 1.00000000e+00]]
W: 0.012842893600463867
Whiten X: 0.016359567642211914
Whiten M1: 3.504753112792969e-05
Parafac M3: 0.05708599090576172
Parafac Decomposition: 12.807015657424927
Unwhitening parafac factors: 1.9788742065429688e-05
Initialization
[[-0.9896831  0.1432737]
 [ 0.1432737  0.9896831]]
SGD Calc: 22.167938947677612
Unwhitening factors: 2.3603439331054688e-05
Smoothing and Normalization: 8.559226989746094e-05
Fit RMSE: 0.03480235525831918
parafac Test Against Ground Truth
Smoothing and Normalization: 8.726119995117188e-05
Fit RMSE: 0.03327232070643865
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.000000000000001, 1.0000000000000007]
Centering time: 0.008282661437988281
PCA fit: 0.3205714225769043
PCA Transform: 0.0007131099700927734
total iterations: 200
TLDA fit: 2.793063163757324
Whitened factor: 
[[-0.38868491  0.43628337]
 [ 0.92137074 -0.89980933]]
PCA Reverse Transform: 5.507469177246094e-05
decenter with old strategy:
[-2.06131698e-10  2.38515620e-10]
Smoothing and Normalization: 5.984306335449219e-05
Fit RMSE: 0.048640968422931065
 Test Against Ground Truth
M1: 0.0018398761749267578
M2: 6.8100903034210205
[[1.00000000e+00 4.51028104e-16]
 [4.70110062e-16 1.00000000e+00]]
W: 0.004122495651245117
Whiten X: 0.008241415023803711
Whiten M1: 3.0279159545898438e-05
Parafac M3: 0.05620622634887695
Parafac Decomposition: 12.607307434082031
Unwhitening parafac factors: 1.52587890625e-05
Initialization
[[-0.96599519 -0.25856005]
 [-0.25856005  0.96599519]]
SGD Calc: 22.347142696380615
Unwhitening factors: 1.6450881958007812e-05
Smoothing and Normalization: 6.008148193359375e-05
Fit RMSE: 0.04641043020667613
parafac Test Against Ground Truth
Smoothing and Normalization: 8.0108642578125e-05
Fit RMSE: 0.04849770921014318
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000013, 1.0000000000000002]
Centering time: 0.021125078201293945
PCA fit: 0.6662189960479736
PCA Transform: 0.0013148784637451172
total iterations: 158
TLDA fit: 2.2752151489257812
Whitened factor: 
[[ 0.04969021  0.03540159]
 [ 0.99876468 -0.99937317]]
PCA Reverse Transform: 9.298324584960938e-05
decenter with old strategy:
[ 4.21420422e-04 -4.71986388e-06]
Smoothing and Normalization: 6.794929504394531e-05
Fit RMSE: 0.037323847283201775
 Test Against Ground Truth
M1: 0.00486445426940918
M2: 20.00068497657776
[[1.00000000e+00 3.05311332e-16]
 [2.08166817e-16 1.00000000e+00]]
W: 0.012163877487182617
Whiten X: 0.016062021255493164
Whiten M1: 3.504753112792969e-05
Parafac M3: 0.0565638542175293
Parafac Decomposition: 12.74493670463562
Unwhitening parafac factors: 2.5033950805664062e-05
Initialization
[[-0.61834575 -0.78590619]
 [-0.78590619  0.61834575]]
SGD Calc: 22.085384368896484
Unwhitening factors: 1.71661376953125e-05
Smoothing and Normalization: 5.9604644775390625e-05
Fit RMSE: 0.03346051251250226
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010204315185546875
Fit RMSE: 0.033412430025120475
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 0.9999999999999993]
Centering time: 0.008186578750610352
PCA fit: 0.3224062919616699
PCA Transform: 0.000720977783203125
total iterations: 156
TLDA fit: 2.268568515777588
Whitened factor: 
[[-0.91167107  0.87376752]
 [-0.41092074  0.48634383]]
PCA Reverse Transform: 5.698204040527344e-05
decenter with old strategy:
[0.00195736 0.00027992]
Smoothing and Normalization: 5.817413330078125e-05
Fit RMSE: 0.04607882572649455
 Test Against Ground Truth
M1: 0.0016515254974365234
M2: 6.6568238735198975
[[ 1.00000000e+00 -3.46944695e-16]
 [-4.85722573e-17  1.00000000e+00]]
W: 0.004045248031616211
Whiten X: 0.008377313613891602
Whiten M1: 3.314018249511719e-05
Parafac M3: 0.05683588981628418
Parafac Decomposition: 12.844277143478394
Unwhitening parafac factors: 1.52587890625e-05
Initialization
[[-0.20174227 -0.97943864]
 [-0.97943864  0.20174227]]
SGD Calc: 22.008193492889404
Unwhitening factors: 1.5497207641601562e-05
Smoothing and Normalization: 4.124641418457031e-05
Fit RMSE: 0.04608667895393369
parafac Test Against Ground Truth
Smoothing and Normalization: 6.818771362304688e-05
Fit RMSE: 0.04600596788606346
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0]
Centering time: 0.021208524703979492
PCA fit: 0.6423521041870117
PCA Transform: 0.0013210773468017578
total iterations: 200
TLDA fit: 2.845534086227417
Whitened factor: 
[[ 0.85791168 -0.87636801]
 [-0.51379718  0.48164209]]
PCA Reverse Transform: 7.677078247070312e-05
decenter with old strategy:
[0.01802872 0.01512163]
Smoothing and Normalization: 7.081031799316406e-05
Fit RMSE: 0.03377845323146572
 Test Against Ground Truth
M1: 0.004918336868286133
M2: 20.145474195480347
[[ 1.00000000e+00 -1.11022302e-16]
 [-5.55111512e-17  1.00000000e+00]]
W: 0.013196706771850586
Whiten X: 0.016040325164794922
Whiten M1: 3.6716461181640625e-05
Parafac M3: 0.0563807487487793
Parafac Decomposition: 12.889724254608154
Unwhitening parafac factors: 2.09808349609375e-05
Initialization
[[-0.95033489  0.31122917]
 [ 0.31122917  0.95033489]]
SGD Calc: 21.926328897476196
Unwhitening factors: 1.6927719116210938e-05
Smoothing and Normalization: 6.29425048828125e-05
Fit RMSE: 0.03376986302983955
parafac Test Against Ground Truth
Smoothing and Normalization: 8.416175842285156e-05
Fit RMSE: 0.03561616414326957
 Test Against Ground Truth
Traceback (most recent call last):
  File "generate_tables.py", line 607, in <module>
    main()
  File "generate_tables.py", line 597, in main
    csvwriter.writerow([str(i), str(vocab), str(lr), str(theta), str(acc_parafac[j][0]), str(acc_parafac[j][1]), str(acc_uncentered[j][0]), str(acc_uncentered[j][1]), str(acc_centered[j][0]), str(acc_centered[j][1])])
IndexError: list index out of range
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000002]
Centering time: 0.009700536727905273
PCA fit: 0.2976207733154297
PCA Transform: 0.0008709430694580078
total iterations: 200
TLDA fit: 2.9233155250549316
Whitened factor: 
[[ 0.85333035 -0.87369465]
 [-0.52137062  0.48647472]]
PCA Reverse Transform: 5.125999450683594e-05
decenter with old strategy:
[0.00874688 0.08239267]
Smoothing and Normalization: 5.841255187988281e-05
Fit RMSE: 0.046731974253886394
 Test Against Ground Truth
M1: 0.0016798973083496094
M2: 6.771667957305908
[[1.00000000e+00 3.88578059e-16]
 [3.74700271e-16 1.00000000e+00]]
W: 0.004385709762573242
Whiten X: 0.00878453254699707
Whiten M1: 2.9087066650390625e-05
Parafac M3: 0.057584524154663086
Parafac Decomposition: 13.109324216842651
Unwhitening parafac factors: 1.811981201171875e-05
Initialization
[[-0.96448589 -0.26413439]
 [-0.26413439  0.96448589]]
SGD Calc: 21.71783971786499
Unwhitening factors: 1.4781951904296875e-05
Smoothing and Normalization: 4.267692565917969e-05
Fit RMSE: 0.0466825016465698
parafac Test Against Ground Truth
Smoothing and Normalization: 8.511543273925781e-05
Fit RMSE: 0.046758927688542444
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 1.0000000000000009]
Centering time: 0.022542238235473633
PCA fit: 0.6399426460266113
PCA Transform: 0.0013463497161865234
total iterations: 200
TLDA fit: 2.8926687240600586
Whitened factor: 
[[-0.82825657  0.792461  ]
 [-0.56034905  0.60992259]]
PCA Reverse Transform: 7.534027099609375e-05
decenter with old strategy:
[0.01172409 0.14368267]
Smoothing and Normalization: 5.745887756347656e-05
Fit RMSE: 0.03324136229887804
 Test Against Ground Truth
M1: 0.004470109939575195
M2: 19.716623306274414
[[1.00000000e+00 2.28983499e-16]
 [1.63931368e-16 1.00000000e+00]]
W: 0.013669013977050781
Whiten X: 0.015649795532226562
Whiten M1: 3.528594970703125e-05
Parafac M3: 0.05697751045227051
Parafac Decomposition: 12.952517032623291
Unwhitening parafac factors: 2.09808349609375e-05
Initialization
[[-0.81895226 -0.57386165]
 [-0.57386165  0.81895226]]
SGD Calc: 21.66216278076172
Unwhitening factors: 2.002716064453125e-05
Smoothing and Normalization: 6.127357482910156e-05
Fit RMSE: 0.03397618395859171
parafac Test Against Ground Truth
Smoothing and Normalization: 7.462501525878906e-05
Fit RMSE: 0.033015078109637906
 Test Against Ground Truth
[[array([0.99086467]), array([0.99521697])], [array([0.68873641]), array([0.98435147])]]
[[array([0.96397675]), array([0.97418774])], [array([0.99921469]), array([0.99910967])]]
[[], []]
Traceback (most recent call last):
  File "generate_tables.py", line 609, in <module>
    main()
  File "generate_tables.py", line 599, in main
    csvwriter.writerow([str(i), str(vocab), str(lr), str(theta), str(acc_parafac[j][0]), str(acc_parafac[j][1]), str(acc_uncentered[j][0]), str(acc_uncentered[j][1]), str(acc_centered[j][0]), str(acc_centered[j][1])])
IndexError: list index out of range
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0]
Centering time: 0.009999513626098633
PCA fit: 0.26824378967285156
PCA Transform: 0.0016717910766601562
total iterations: 200
TLDA fit: 2.7671515941619873
Whitened factor: 
[[-0.47652128  0.43093721]
 [-0.87916294  0.90238192]]
PCA Reverse Transform: 5.435943603515625e-05
decenter with old strategy:
[0.00122895 0.00019999]
Smoothing and Normalization: 5.841255187988281e-05
Fit RMSE: 0.04995770713785701
 Test Against Ground Truth
M1: 0.0016317367553710938
M2: 6.717114686965942
[[ 1.00000000e+00 -2.22044605e-16]
 [ 6.93889390e-17  1.00000000e+00]]
W: 0.004313230514526367
Whiten X: 0.00859689712524414
Whiten M1: 2.86102294921875e-05
Parafac M3: 0.05669593811035156
Parafac Decomposition: 12.790117263793945
Unwhitening parafac factors: 1.621246337890625e-05
Initialization
[[-0.96448589 -0.26413439]
 [-0.26413439  0.96448589]]
SGD Calc: 21.14412021636963
Unwhitening factors: 1.3828277587890625e-05
Smoothing and Normalization: 4.220008850097656e-05
Fit RMSE: 0.04955490975740166
parafac Test Against Ground Truth
Smoothing and Normalization: 3.9577484130859375e-05
Fit RMSE: 0.04845200119160094
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 1.0000000000000013]
Centering time: 0.02223682403564453
PCA fit: 0.641728401184082
PCA Transform: 0.0013353824615478516
total iterations: 163
TLDA fit: 2.2618367671966553
Whitened factor: 
[[-0.89955144  0.86145304]
 [-0.43681485  0.50783724]]
PCA Reverse Transform: 8.034706115722656e-05
decenter with old strategy:
[0.00284937 0.01003846]
Smoothing and Normalization: 7.05718994140625e-05
Fit RMSE: 0.032436599207753165
 Test Against Ground Truth
M1: 0.00447845458984375
M2: 19.615910530090332
[[ 1.00000000e+00 -7.28583860e-17]
 [-3.46944695e-17  1.00000000e+00]]
W: 0.014225482940673828
Whiten X: 0.015772342681884766
Whiten M1: 3.361701965332031e-05
Parafac M3: 0.056368112564086914
Parafac Decomposition: 12.671268701553345
Unwhitening parafac factors: 1.9311904907226562e-05
Initialization
[[-0.81895226 -0.57386165]
 [-0.57386165  0.81895226]]
SGD Calc: 21.580766439437866
Unwhitening factors: 1.7881393432617188e-05
Smoothing and Normalization: 6.175041198730469e-05
Fit RMSE: 0.0342123095050419
parafac Test Against Ground Truth
Smoothing and Normalization: 0.000102996826171875
Fit RMSE: 0.03229734030564129
 Test Against Ground Truth
[[array([0.70017857]), array([0.96764628])], [array([0.03402187]), array([0.97968291])]]
[[array([0.99956926]), array([0.99962984])], [array([0.99922792]), array([0.99900628])]]
[[array([0.89362878]), array([0.86867654])], [array([0.99015706]), array([0.97389732])]]
Done!
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.000000000000001, 0.9999999999999997]
Centering time: 0.009638786315917969
PCA fit: 0.26882243156433105
PCA Transform: 0.0008985996246337891
total iterations: 200
TLDA fit: 2.825254201889038
Whitened factor: 
[[ 0.83488299 -0.85352432]
 [-0.55042746  0.521053  ]]
PCA Reverse Transform: 5.4836273193359375e-05
decenter with old strategy:
[0.00164961 0.00014047]
Smoothing and Normalization: 6.079673767089844e-05
Fit RMSE: 0.0473762200387715
 Test Against Ground Truth
M1: 0.001634836196899414
M2: 6.987150192260742
[[1.00000000e+00 1.95156391e-16]
 [2.17707796e-16 1.00000000e+00]]
W: 0.004629373550415039
Whiten X: 0.006514072418212891
Whiten M1: 2.4318695068359375e-05
Parafac M3: 0.05765533447265625
Parafac Decomposition: 12.901614427566528
Unwhitening parafac factors: 2.1696090698242188e-05
Initialization
[[-0.96448589 -0.26413439]
 [-0.26413439  0.96448589]]
SGD Calc: 21.61806344985962
Unwhitening factors: 1.3113021850585938e-05
Smoothing and Normalization: 4.220008850097656e-05
Fit RMSE: 0.04735405655713299
parafac Test Against Ground Truth
Smoothing and Normalization: 6.151199340820312e-05
Fit RMSE: 0.0486167515655408
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000007, 1.0]
Centering time: 0.02297210693359375
PCA fit: 0.6495494842529297
PCA Transform: 0.0016341209411621094
total iterations: 187
TLDA fit: 2.586202621459961
Whitened factor: 
[[-0.15035014  0.07080785]
 [-0.98863281  0.99748997]]
PCA Reverse Transform: 8.7738037109375e-05
decenter with old strategy:
[0.00016425 0.00021646]
Smoothing and Normalization: 6.842613220214844e-05
Fit RMSE: 0.036297646217876164
 Test Against Ground Truth
M1: 0.0044214725494384766
M2: 19.702019214630127
[[ 1.00000000e+00 -9.02056208e-17]
 [ 6.24500451e-17  1.00000000e+00]]
W: 0.014281272888183594
Whiten X: 0.01688241958618164
Whiten M1: 3.266334533691406e-05
Parafac M3: 0.05682063102722168
Parafac Decomposition: 12.675343990325928
Unwhitening parafac factors: 2.0265579223632812e-05
Initialization
[[-0.81895226 -0.57386165]
 [-0.57386165  0.81895226]]
SGD Calc: 21.697380542755127
Unwhitening factors: 1.8596649169921875e-05
Smoothing and Normalization: 5.8650970458984375e-05
Fit RMSE: 0.03431433901865128
parafac Test Against Ground Truth
Smoothing and Normalization: 8.296966552734375e-05
Fit RMSE: 0.03385144218880068
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000009]
Centering time: 0.008005619049072266
PCA fit: 0.2888474464416504
PCA Transform: 0.0007147789001464844
total iterations: 200
TLDA fit: 2.7648839950561523
Whitened factor: 
[[ 0.86094345 -0.8896016 ]
 [-0.50870067  0.45673733]]
PCA Reverse Transform: 5.412101745605469e-05
decenter with old strategy:
[4.33929331e-05 1.59162997e-04]
Smoothing and Normalization: 6.127357482910156e-05
Fit RMSE: 0.04816692928878458
 Test Against Ground Truth
M1: 0.0015106201171875
M2: 6.680347919464111
[[ 1.00000000e+00 -3.33066907e-16]
 [-3.05311332e-16  1.00000000e+00]]
W: 0.00428009033203125
Whiten X: 0.005882740020751953
Whiten M1: 2.0742416381835938e-05
Parafac M3: 0.05668377876281738
Parafac Decomposition: 12.60384750366211
Unwhitening parafac factors: 1.5735626220703125e-05
Initialization
[[-0.82772018 -0.56114107]
 [-0.56114107  0.82772018]]
SGD Calc: 21.71202254295349
Unwhitening factors: 1.2636184692382812e-05
Smoothing and Normalization: 4.267692565917969e-05
Fit RMSE: 0.04416401521452788
parafac Test Against Ground Truth
Smoothing and Normalization: 5.269050598144531e-05
Fit RMSE: 0.049109953942584925
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 1.0000000000000004]
Centering time: 0.022970914840698242
PCA fit: 0.6877949237823486
PCA Transform: 0.001322031021118164
total iterations: 126
TLDA fit: 1.8288462162017822
Whitened factor: 
[[-0.89733699  0.85894586]
 [-0.44134603  0.5120664 ]]
PCA Reverse Transform: 8.797645568847656e-05
decenter with old strategy:
[9.99549567e-04 1.98968782e-05]
Smoothing and Normalization: 6.866455078125e-05
Fit RMSE: 0.03403235775339917
 Test Against Ground Truth
M1: 0.004476785659790039
M2: 19.799246788024902
[[1.00000000e+00 6.29704622e-16]
 [6.57460197e-16 1.00000000e+00]]
W: 0.014932394027709961
Whiten X: 0.01675248146057129
Whiten M1: 3.647804260253906e-05
Parafac M3: 0.05783224105834961
Parafac Decomposition: 12.652203798294067
Unwhitening parafac factors: 1.9550323486328125e-05
Initialization
[[-0.08155244  0.99666905]
 [ 0.99666905  0.08155244]]
SGD Calc: 21.84742307662964
Unwhitening factors: 1.8358230590820312e-05
Smoothing and Normalization: 5.888938903808594e-05
Fit RMSE: 0.034584179766703846
parafac Test Against Ground Truth
Smoothing and Normalization: 8.296966552734375e-05
Fit RMSE: 0.03446018290895256
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999997, 1.0]
Centering time: 0.008001565933227539
PCA fit: 0.27884578704833984
PCA Transform: 0.001554727554321289
total iterations: 200
TLDA fit: 2.7924253940582275
Whitened factor: 
[[-0.83430836  0.86248125]
 [ 0.55129808 -0.50608902]]
PCA Reverse Transform: 5.435943603515625e-05
decenter with old strategy:
[0.00447263 0.00065121]
Smoothing and Normalization: 5.8650970458984375e-05
Fit RMSE: 0.04815480311170343
 Test Against Ground Truth
M1: 0.0016260147094726562
M2: 6.665769815444946
[[ 1.00000000e+00 -2.63677968e-16]
 [-1.11022302e-16  1.00000000e+00]]
W: 0.00409388542175293
Whiten X: 0.00591278076171875
Whiten M1: 2.2649765014648438e-05
Parafac M3: 0.05685615539550781
Parafac Decomposition: 12.786757230758667
Unwhitening parafac factors: 1.5735626220703125e-05
Initialization
[[-0.99701108 -0.07725874]
 [-0.07725874  0.99701108]]
SGD Calc: 21.564332246780396
Unwhitening factors: 1.4781951904296875e-05
Smoothing and Normalization: 4.2438507080078125e-05
Fit RMSE: 0.048013542628973865
parafac Test Against Ground Truth
Smoothing and Normalization: 6.151199340820312e-05
Fit RMSE: 0.04899432069368547
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.02456974983215332
PCA fit: 0.735846996307373
PCA Transform: 0.0013031959533691406
total iterations: 200
TLDA fit: 2.867560386657715
Whitened factor: 
[[ 0.89225546 -0.90428462]
 [-0.45153095  0.42693012]]
PCA Reverse Transform: 8.940696716308594e-05
decenter with old strategy:
[-1.96256983e-05  2.19531937e-04]
Smoothing and Normalization: 6.866455078125e-05
Fit RMSE: 0.03360891972358228
 Test Against Ground Truth
M1: 0.0045719146728515625
M2: 19.897196054458618
[[1.00000000e+00 1.17961196e-16]
 [1.63064007e-16 1.00000000e+00]]
W: 0.014272928237915039
Whiten X: 0.016739606857299805
Whiten M1: 3.2901763916015625e-05
Parafac M3: 0.057929039001464844
Parafac Decomposition: 12.494054794311523
Unwhitening parafac factors: 1.9788742065429688e-05
Initialization
[[-0.10316249 -0.99466452]
 [-0.99466452  0.10316249]]
SGD Calc: 21.921989917755127
Unwhitening factors: 1.9550323486328125e-05
Smoothing and Normalization: 6.318092346191406e-05
Fit RMSE: 0.03364129957562725
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010204315185546875
Fit RMSE: 0.03444733510070115
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999997]
Centering time: 0.008074283599853516
PCA fit: 0.28203749656677246
PCA Transform: 0.0007061958312988281
total iterations: 200
TLDA fit: 2.767786741256714
Whitened factor: 
[[ 0.91631002 -0.93225778]
 [-0.40046966  0.36179472]]
PCA Reverse Transform: 5.412101745605469e-05
decenter with old strategy:
[0.01133324 0.00077313]
Smoothing and Normalization: 6.0558319091796875e-05
Fit RMSE: 0.04640108550811444
 Test Against Ground Truth
M1: 0.0015969276428222656
M2: 6.7096803188323975
[[ 1.00000000e+00  1.97758476e-16]
 [-5.37764278e-17  1.00000000e+00]]
W: 0.004710197448730469
Whiten X: 0.0058248043060302734
Whiten M1: 2.0503997802734375e-05
Parafac M3: 0.05742621421813965
Parafac Decomposition: 12.663031816482544
Unwhitening parafac factors: 2.002716064453125e-05
Initialization
[[-0.94438779 -0.32883386]
 [-0.32883386  0.94438779]]
SGD Calc: 21.60483455657959
Unwhitening factors: 1.5020370483398438e-05
Smoothing and Normalization: 4.4345855712890625e-05
Fit RMSE: 0.04735289601722244
parafac Test Against Ground Truth
Smoothing and Normalization: 6.532669067382812e-05
Fit RMSE: 0.04638399308321393
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999989, 1.0000000000000002]
Centering time: 0.023319005966186523
PCA fit: 0.6947362422943115
PCA Transform: 0.001318216323852539
total iterations: 200
TLDA fit: 2.8232319355010986
Whitened factor: 
[[ 0.82543562 -0.84290663]
 [-0.56449627  0.53805986]]
PCA Reverse Transform: 7.605552673339844e-05
decenter with old strategy:
[1.44070797e-04 4.93180218e-05]
Smoothing and Normalization: 7.677078247070312e-05
Fit RMSE: 0.03335385723107622
 Test Against Ground Truth
M1: 0.00458836555480957
M2: 20.044631481170654
[[ 1.00000000e+00 -4.16333634e-17]
 [ 9.71445147e-17  1.00000000e+00]]
W: 0.012148141860961914
Whiten X: 0.01662158966064453
Whiten M1: 3.504753112792969e-05
Parafac M3: 0.05697822570800781
Parafac Decomposition: 12.579599857330322
Unwhitening parafac factors: 2.7894973754882812e-05
Initialization
[[-0.7501759   0.66123832]
 [ 0.66123832  0.7501759 ]]
SGD Calc: 21.491685152053833
Unwhitening factors: 1.6450881958007812e-05
Smoothing and Normalization: 6.079673767089844e-05
Fit RMSE: 0.03335281730414064
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010156631469726562
Fit RMSE: 0.03467787680930564
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999997]
Centering time: 0.007937192916870117
PCA fit: 0.27124857902526855
PCA Transform: 0.0007174015045166016
total iterations: 192
TLDA fit: 2.7487051486968994
Whitened factor: 
[[ 0.67158246 -0.69893594]
 [-0.74092982  0.71518428]]
PCA Reverse Transform: 5.650520324707031e-05
decenter with old strategy:
[0.00336864 0.02852175]
Smoothing and Normalization: 5.888938903808594e-05
Fit RMSE: 0.04865392979689381
 Test Against Ground Truth
M1: 0.0015215873718261719
M2: 6.882788181304932
[[1.00000000e+00 3.60822483e-16]
 [1.94289029e-16 1.00000000e+00]]
W: 0.004723787307739258
Whiten X: 0.008070707321166992
Whiten M1: 2.8848648071289062e-05
Parafac M3: 0.057248830795288086
Parafac Decomposition: 13.079514265060425
Unwhitening parafac factors: 1.6689300537109375e-05
Initialization
[[-0.99752133 -0.0703648 ]
 [-0.0703648   0.99752133]]
SGD Calc: 21.414247512817383
Unwhitening factors: 1.3828277587890625e-05
Smoothing and Normalization: 4.076957702636719e-05
Fit RMSE: 0.048848393787807524
parafac Test Against Ground Truth
Smoothing and Normalization: 6.985664367675781e-05
Fit RMSE: 0.053501785475752336
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 0.9999999999999994]
Centering time: 0.021224260330200195
PCA fit: 0.7424125671386719
PCA Transform: 0.002717256546020508
total iterations: 200
TLDA fit: 2.767958641052246
Whitened factor: 
[[ 0.88905013 -0.90817995]
 [-0.45780986  0.41857995]]
PCA Reverse Transform: 7.843971252441406e-05
decenter with old strategy:
[0.0003896  0.00284458]
Smoothing and Normalization: 6.985664367675781e-05
Fit RMSE: 0.03459781438632916
 Test Against Ground Truth
M1: 0.004458189010620117
M2: 20.30422854423523
[[ 1.00000000e+00  2.77555756e-17]
 [-1.24900090e-16  1.00000000e+00]]
W: 0.012622833251953125
Whiten X: 0.016722440719604492
Whiten M1: 4.5299530029296875e-05
Parafac M3: 0.05768013000488281
Parafac Decomposition: 12.424261331558228
Unwhitening parafac factors: 1.7642974853515625e-05
Initialization
[[-0.69330836  0.72064105]
 [ 0.72064105  0.69330836]]
SGD Calc: 21.44044518470764
Unwhitening factors: 1.9311904907226562e-05
Smoothing and Normalization: 5.817413330078125e-05
Fit RMSE: 0.035480313106597744
parafac Test Against Ground Truth
Smoothing and Normalization: 9.918212890625e-05
Fit RMSE: 0.03504897645188191
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 0.9999999999999998]
Centering time: 0.00784921646118164
PCA fit: 0.3050711154937744
PCA Transform: 0.0007219314575195312
total iterations: 200
TLDA fit: 2.7804148197174072
Whitened factor: 
[[ 0.81272725 -0.83382845]
 [-0.58264433  0.55202366]]
PCA Reverse Transform: 5.2928924560546875e-05
decenter with old strategy:
[0.00350655 0.62927213]
Smoothing and Normalization: 5.984306335449219e-05
Fit RMSE: 0.04715763725927817
 Test Against Ground Truth
M1: 0.0015485286712646484
M2: 6.932594060897827
[[ 1.00000000e+00 -1.38777878e-17]
 [ 1.24900090e-16  1.00000000e+00]]
W: 0.004791975021362305
Whiten X: 0.008277416229248047
Whiten M1: 3.170967102050781e-05
Parafac M3: 0.05841517448425293
Parafac Decomposition: 12.888899326324463
Unwhitening parafac factors: 2.0503997802734375e-05
Initialization
[[-0.98952244 -0.14437916]
 [-0.14437916  0.98952244]]
SGD Calc: 22.098095178604126
Unwhitening factors: 1.4781951904296875e-05
Smoothing and Normalization: 5.5789947509765625e-05
Fit RMSE: 0.04781571349821492
parafac Test Against Ground Truth
Smoothing and Normalization: 6.437301635742188e-05
Fit RMSE: 0.04903934920364993
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000004]
Centering time: 0.021610021591186523
PCA fit: 0.6739399433135986
PCA Transform: 0.0013227462768554688
total iterations: 200
TLDA fit: 2.7733023166656494
Whitened factor: 
[[-0.12987188  0.05450073]
 [-0.99153078  0.99851373]]
PCA Reverse Transform: 8.988380432128906e-05
decenter with old strategy:
[ 6.56683848e-10 -6.27042956e-10]
Smoothing and Normalization: 7.009506225585938e-05
Fit RMSE: 0.03685754044811924
 Test Against Ground Truth
M1: 0.004509687423706055
M2: 20.440285444259644
[[ 1.00000000e+00 -1.31838984e-16]
 [-1.17961196e-16  1.00000000e+00]]
W: 0.012411355972290039
Whiten X: 0.017098188400268555
Whiten M1: 3.4332275390625e-05
Parafac M3: 0.05800986289978027
Parafac Decomposition: 12.953933954238892
Unwhitening parafac factors: 2.0742416381835938e-05
Initialization
[[-0.21259246 -0.97714095]
 [-0.97714095  0.21259246]]
SGD Calc: 21.43742799758911
Unwhitening factors: 1.7642974853515625e-05
Smoothing and Normalization: 6.413459777832031e-05
Fit RMSE: 0.03336903889020141
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010156631469726562
Fit RMSE: 0.03417510946333548
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.999999999999999, 1.0]
Centering time: 0.008077383041381836
PCA fit: 0.2506265640258789
PCA Transform: 0.0007567405700683594
total iterations: 134
TLDA fit: 1.8531923294067383
Whitened factor: 
[[-0.01810216  0.11217588]
 [ 0.99983614 -0.99368837]]
PCA Reverse Transform: 5.626678466796875e-05
decenter with old strategy:
[0.04234319 0.0341318 ]
Smoothing and Normalization: 5.8650970458984375e-05
Fit RMSE: 0.05080566650970615
 Test Against Ground Truth
M1: 0.001867532730102539
M2: 6.734184503555298
[[1.00000000e+00 1.38777878e-16]
 [1.80411242e-16 1.00000000e+00]]
W: 0.0046253204345703125
Whiten X: 0.008087158203125
Whiten M1: 3.0040740966796875e-05
Parafac M3: 0.05809831619262695
Parafac Decomposition: 12.576669454574585
Unwhitening parafac factors: 1.621246337890625e-05
Initialization
[[-0.92095497 -0.38966901]
 [-0.38966901  0.92095497]]
SGD Calc: 21.62558603286743
Unwhitening factors: 1.52587890625e-05
Smoothing and Normalization: 5.626678466796875e-05
Fit RMSE: 0.04712910466290079
parafac Test Against Ground Truth
Smoothing and Normalization: 7.104873657226562e-05
Fit RMSE: 0.045920831606078565
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999994]
Centering time: 0.022069692611694336
PCA fit: 0.6525967121124268
PCA Transform: 0.0013275146484375
total iterations: 200
TLDA fit: 2.8317599296569824
Whitened factor: 
[[ 0.88280693 -0.90162147]
 [-0.46973602  0.43252599]]
PCA Reverse Transform: 7.271766662597656e-05
decenter with old strategy:
[ 2.48695545e-10 -2.26294088e-10]
Smoothing and Normalization: 6.842613220214844e-05
Fit RMSE: 0.03345432681140252
 Test Against Ground Truth
M1: 0.004560232162475586
M2: 19.830267667770386
[[ 1.00000000e+00 -1.17961196e-16]
 [-1.21430643e-16  1.00000000e+00]]
W: 0.012287378311157227
Whiten X: 0.01687479019165039
Whiten M1: 3.647804260253906e-05
Parafac M3: 0.05733776092529297
Parafac Decomposition: 12.614491939544678
Unwhitening parafac factors: 2.0265579223632812e-05
Initialization
[[-0.26673979 -0.96376858]
 [-0.96376858  0.26673979]]
SGD Calc: 22.23482632637024
Unwhitening factors: 1.7881393432617188e-05
Smoothing and Normalization: 5.841255187988281e-05
Fit RMSE: 0.034796073180540524
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0001068115234375
Fit RMSE: 0.03340228962375299
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999994]
Centering time: 0.008092641830444336
PCA fit: 0.2814643383026123
PCA Transform: 0.0006959438323974609
total iterations: 200
TLDA fit: 2.7426717281341553
Whitened factor: 
[[-0.92322115  0.89482087]
 [-0.38426905  0.44642537]]
PCA Reverse Transform: 5.53131103515625e-05
decenter with old strategy:
[0.00762303 0.00065053]
Smoothing and Normalization: 5.8650970458984375e-05
Fit RMSE: 0.04683067641348011
 Test Against Ground Truth
M1: 0.0014846324920654297
M2: 6.726550340652466
[[1.00000000e+00 2.96637714e-16]
 [3.55618313e-16 1.00000000e+00]]
W: 0.004635810852050781
Whiten X: 0.008019685745239258
Whiten M1: 2.9325485229492188e-05
Parafac M3: 0.05782961845397949
Parafac Decomposition: 12.977581262588501
Unwhitening parafac factors: 1.8835067749023438e-05
Initialization
[[-0.02265144  0.99974342]
 [ 0.99974342  0.02265144]]
SGD Calc: 22.84231662750244
Unwhitening factors: 1.3828277587890625e-05
Smoothing and Normalization: 4.076957702636719e-05
Fit RMSE: 0.04726991814301145
parafac Test Against Ground Truth
Smoothing and Normalization: 8.130073547363281e-05
Fit RMSE: 0.048637888687484476
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999997, 0.9999999999999996]
Centering time: 0.021061420440673828
PCA fit: 0.6565470695495605
PCA Transform: 0.0013186931610107422
total iterations: 200
TLDA fit: 2.77487850189209
Whitened factor: 
[[ 0.80542364 -0.82745946]
 [-0.59269955  0.56152546]]
PCA Reverse Transform: 8.940696716308594e-05
decenter with old strategy:
[0.00531077 0.03439229]
Smoothing and Normalization: 6.842613220214844e-05
Fit RMSE: 0.03332589349454682
 Test Against Ground Truth
M1: 0.004438877105712891
M2: 19.83659291267395
[[ 1.00000000e+00 -1.52655666e-16]
 [-2.15105711e-16  1.00000000e+00]]
W: 0.012539386749267578
Whiten X: 0.01640462875366211
Whiten M1: 3.361701965332031e-05
Parafac M3: 0.058159589767456055
Parafac Decomposition: 13.4655442237854
Unwhitening parafac factors: 2.002716064453125e-05
Initialization
[[-0.9896831  0.1432737]
 [ 0.1432737  0.9896831]]
SGD Calc: 21.34519338607788
Unwhitening factors: 1.71661376953125e-05
Smoothing and Normalization: 6.222724914550781e-05
Fit RMSE: 0.033277531408437026
parafac Test Against Ground Truth
Smoothing and Normalization: 8.416175842285156e-05
Fit RMSE: 0.03478385752564006
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.00792384147644043
PCA fit: 0.3004145622253418
PCA Transform: 0.0007140636444091797
total iterations: 200
TLDA fit: 2.7993595600128174
Whitened factor: 
[[-0.89805845  0.87756454]
 [-0.43987615  0.47945852]]
PCA Reverse Transform: 5.3882598876953125e-05
decenter with old strategy:
[0.05182309 0.00533864]
Smoothing and Normalization: 7.271766662597656e-05
Fit RMSE: 0.04715950044100724
 Test Against Ground Truth
M1: 0.0015163421630859375
M2: 6.785006761550903
[[1.00000000e+00 2.22044605e-16]
 [1.38777878e-16 1.00000000e+00]]
W: 0.004664182662963867
Whiten X: 0.008154869079589844
Whiten M1: 3.075599670410156e-05
Parafac M3: 0.056946754455566406
Parafac Decomposition: 12.538418292999268
Unwhitening parafac factors: 1.5735626220703125e-05
Initialization
[[-0.96599519 -0.25856005]
 [-0.25856005  0.96599519]]
SGD Calc: 21.40271258354187
Unwhitening factors: 1.3828277587890625e-05
Smoothing and Normalization: 5.4836273193359375e-05
Fit RMSE: 0.049048832572867344
parafac Test Against Ground Truth
Smoothing and Normalization: 6.628036499023438e-05
Fit RMSE: 0.04700527710869908
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999993, 0.9999999999999987]
Centering time: 0.021270275115966797
PCA fit: 0.6648578643798828
PCA Transform: 0.0013358592987060547
total iterations: 194
TLDA fit: 2.655883312225342
Whitened factor: 
[[ 0.02151156  0.05262978]
 [ 0.9997686  -0.99861409]]
PCA Reverse Transform: 8.940696716308594e-05
decenter with old strategy:
[0.00023861 0.00034044]
Smoothing and Normalization: 7.152557373046875e-05
Fit RMSE: 0.03655300551512555
 Test Against Ground Truth
M1: 0.004652261734008789
M2: 19.889479398727417
[[1.00000000e+00 1.24900090e-16]
 [1.04083409e-16 1.00000000e+00]]
W: 0.013050556182861328
Whiten X: 0.016620874404907227
Whiten M1: 3.457069396972656e-05
Parafac M3: 0.05868816375732422
Parafac Decomposition: 12.612426280975342
Unwhitening parafac factors: 2.0742416381835938e-05
Initialization
[[-0.61834575 -0.78590619]
 [-0.78590619  0.61834575]]
SGD Calc: 21.57415008544922
Unwhitening factors: 1.7404556274414062e-05
Smoothing and Normalization: 5.6743621826171875e-05
Fit RMSE: 0.033850382438097404
parafac Test Against Ground Truth
Smoothing and Normalization: 8.344650268554688e-05
Fit RMSE: 0.03346735337824935
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999996, 1.0000000000000018]
Centering time: 0.007969141006469727
PCA fit: 0.3146960735321045
PCA Transform: 0.0007560253143310547
total iterations: 198
TLDA fit: 2.7644429206848145
Whitened factor: 
[[-0.96411729  0.94329135]
 [-0.26547664  0.33196601]]
PCA Reverse Transform: 5.269050598144531e-05
decenter with old strategy:
[0.00165326 0.00331502]
Smoothing and Normalization: 6.103515625e-05
Fit RMSE: 0.04850493513642224
 Test Against Ground Truth
M1: 0.0016026496887207031
M2: 6.797122955322266
[[1.00000000e+00 7.63278329e-17]
 [3.12250226e-17 1.00000000e+00]]
W: 0.004611015319824219
Whiten X: 0.008009672164916992
Whiten M1: 3.0994415283203125e-05
Parafac M3: 0.05829000473022461
Parafac Decomposition: 12.535698413848877
Unwhitening parafac factors: 1.0728836059570312e-05
Initialization
[[-0.20174227 -0.97943864]
 [-0.97943864  0.20174227]]
SGD Calc: 21.976343870162964
Unwhitening factors: 1.239776611328125e-05
Smoothing and Normalization: 4.00543212890625e-05
Fit RMSE: 0.048590723803289486
parafac Test Against Ground Truth
Smoothing and Normalization: 6.437301635742188e-05
Fit RMSE: 0.04831200875002482
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999999, 1.0000000000000002]
Centering time: 0.020925045013427734
PCA fit: 0.7063567638397217
PCA Transform: 0.0013082027435302734
total iterations: 200
TLDA fit: 2.8987998962402344
Whitened factor: 
[[ 0.05443079  0.03065152]
 [-0.99851755  0.99953013]]
PCA Reverse Transform: 7.605552673339844e-05
decenter with old strategy:
[ 2.32921452e-10 -2.50442469e-10]
Smoothing and Normalization: 6.723403930664062e-05
Fit RMSE: 0.03672668717970534
 Test Against Ground Truth
M1: 0.004461050033569336
M2: 19.755483150482178
[[ 1.00000000e+00  1.38777878e-17]
 [-6.93889390e-17  1.00000000e+00]]
W: 0.014027118682861328
Whiten X: 0.016447067260742188
Whiten M1: 3.314018249511719e-05
Parafac M3: 0.05731606483459473
Parafac Decomposition: 12.642734289169312
Unwhitening parafac factors: 2.002716064453125e-05
Initialization
[[-0.95033489  0.31122917]
 [ 0.31122917  0.95033489]]
SGD Calc: 21.382917881011963
Unwhitening factors: 1.8358230590820312e-05
Smoothing and Normalization: 6.341934204101562e-05
Fit RMSE: 0.03374419872646328
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010657310485839844
Fit RMSE: 0.03477363838438861
 Test Against Ground Truth
Done!
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000004]
Centering time: 0.010184049606323242
PCA fit: 0.2611863613128662
PCA Transform: 0.0008816719055175781
total iterations: 200
TLDA fit: 2.793745994567871
Whitened factor: 
[[ 0.85818077 -0.88572373]
 [-0.5133476   0.46421274]]
PCA Reverse Transform: 6.818771362304688e-05
decenter with old strategy:
[-1.65839906e-09  1.70564093e-09]
Smoothing and Normalization: 5.817413330078125e-05
Fit RMSE: 0.05559882342395432
 Test Against Ground Truth
M1: 0.0016129016876220703
M2: 6.869493007659912
[[1.00000000e+00 2.60425362e-16]
 [2.61157198e-16 1.00000000e+00]]
W: 0.004284858703613281
Whiten X: 0.008818387985229492
Whiten M1: 2.8371810913085938e-05
Parafac M3: 0.05557847023010254
Parafac Decomposition: 13.081509828567505
Unwhitening parafac factors: 1.4543533325195312e-05
Initialization
[[-0.65734985 -0.75358555]
 [-0.75358555  0.65734985]]
SGD Calc: 21.872451782226562
Unwhitening factors: 1.4781951904296875e-05
Smoothing and Normalization: 4.172325134277344e-05
Fit RMSE: 0.04878929823464653
parafac Test Against Ground Truth
Smoothing and Normalization: 6.29425048828125e-05
Fit RMSE: 0.052505099754717635
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0000000000000004]
Centering time: 0.023435592651367188
PCA fit: 0.6905443668365479
PCA Transform: 0.0013468265533447266
total iterations: 200
TLDA fit: 2.82088303565979
Whitened factor: 
[[-0.95742845  0.94367719]
 [-0.28867069  0.33086759]]
PCA Reverse Transform: 8.58306884765625e-05
decenter with old strategy:
[-1.91033084e-09  1.88318242e-09]
Smoothing and Normalization: 6.985664367675781e-05
Fit RMSE: 0.040441850982305946
 Test Against Ground Truth
M1: 0.004547595977783203
M2: 19.887452602386475
[[ 1.00000000e+00 -4.85722573e-16]
 [-4.54497551e-16  1.00000000e+00]]
W: 0.013956785202026367
Whiten X: 0.016945600509643555
Whiten M1: 3.409385681152344e-05
Parafac M3: 0.05531024932861328
Parafac Decomposition: 12.909369468688965
Unwhitening parafac factors: 1.9788742065429688e-05
Initialization
[[-0.34293185 -0.93936028]
 [-0.93936028  0.34293185]]
SGD Calc: 21.496050357818604
Unwhitening factors: 1.7404556274414062e-05
Smoothing and Normalization: 5.340576171875e-05
Fit RMSE: 0.04025286040252527
parafac Test Against Ground Truth
Smoothing and Normalization: 8.0108642578125e-05
Fit RMSE: 0.03868861587299964
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999989]
Centering time: 0.03426718711853027
PCA fit: 1.2377898693084717
PCA Transform: 0.001836538314819336
total iterations: 200
TLDA fit: 2.840968370437622
Whitened factor: 
[[ 0.85669127 -0.88450733]
 [-0.51582949  0.46652629]]
PCA Reverse Transform: 0.00010776519775390625
decenter with old strategy:
[ 1.08638604e-09 -1.10595860e-09]
Smoothing and Normalization: 7.724761962890625e-05
Fit RMSE: 0.03327674854773665
 Test Against Ground Truth
M1: 0.006308317184448242
M2: 49.60551714897156
[[1.00000000e+00 9.86623977e-17]
 [8.32667268e-17 1.00000000e+00]]
W: 0.02893853187561035
Whiten X: 0.02416205406188965
Whiten M1: 3.981590270996094e-05
Parafac M3: 0.0558011531829834
Parafac Decomposition: 13.036566019058228
Unwhitening parafac factors: 2.4318695068359375e-05
Initialization
[[-0.74598297  0.66596502]
 [ 0.66596502  0.74598297]]
SGD Calc: 21.39545774459839
Unwhitening factors: 2.2411346435546875e-05
Smoothing and Normalization: 7.081031799316406e-05
Fit RMSE: 0.02984625398763304
parafac Test Against Ground Truth
Smoothing and Normalization: 9.72747802734375e-05
Fit RMSE: 0.03306684669257242
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000002, 0.9999999999999996]
Centering time: 0.008116483688354492
PCA fit: 0.25823020935058594
PCA Transform: 0.0006856918334960938
total iterations: 200
TLDA fit: 2.8138296604156494
Whitened factor: 
[[ 0.73430604 -0.75724296]
 [-0.67881856  0.65313329]]
PCA Reverse Transform: 5.602836608886719e-05
decenter with old strategy:
[-1.57223666e-09  1.61152045e-09]
Smoothing and Normalization: 5.8650970458984375e-05
Fit RMSE: 0.057666437760795136
 Test Against Ground Truth
M1: 0.001607656478881836
M2: 6.934086084365845
[[1.00000000e+00 4.64905892e-16]
 [4.92661467e-16 1.00000000e+00]]
W: 0.004111528396606445
Whiten X: 0.008263826370239258
Whiten M1: 3.075599670410156e-05
Parafac M3: 0.05570340156555176
Parafac Decomposition: 12.940302610397339
Unwhitening parafac factors: 1.621246337890625e-05
Initialization
[[-0.09570489 -0.99540975]
 [-0.99540975  0.09570489]]
SGD Calc: 21.42196226119995
Unwhitening factors: 1.3589859008789062e-05
Smoothing and Normalization: 3.790855407714844e-05
Fit RMSE: 0.05720652270283534
parafac Test Against Ground Truth
Smoothing and Normalization: 6.318092346191406e-05
Fit RMSE: 0.0571329480483218
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999999, 0.9999999999999999]
Centering time: 0.02482914924621582
PCA fit: 0.6662948131561279
PCA Transform: 0.001312255859375
total iterations: 153
TLDA fit: 2.1791648864746094
Whitened factor: 
[[ 0.01260003  0.07975539]
 [ 0.99992062 -0.99681446]]
PCA Reverse Transform: 0.00010585784912109375
decenter with old strategy:
[-1.27372829e-09  1.36417875e-09]
Smoothing and Normalization: 6.842613220214844e-05
Fit RMSE: 0.04182510836072855
 Test Against Ground Truth
M1: 0.004499197006225586
M2: 19.94818663597107
[[1.00000000e+00 6.86869181e-16]
 [6.87004707e-16 1.00000000e+00]]
W: 0.014442920684814453
Whiten X: 0.016873598098754883
Whiten M1: 3.4809112548828125e-05
Parafac M3: 0.056092262268066406
Parafac Decomposition: 13.000781774520874
Unwhitening parafac factors: 2.002716064453125e-05
Initialization
[[-0.45492588 -0.8905293 ]
 [-0.8905293   0.45492588]]
SGD Calc: 21.705540895462036
Unwhitening factors: 1.9788742065429688e-05
Smoothing and Normalization: 5.555152893066406e-05
Fit RMSE: 0.03592771782895755
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010180473327636719
Fit RMSE: 0.039496987950454955
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0000000000000002]
Centering time: 0.034018754959106445
PCA fit: 1.2207441329956055
PCA Transform: 0.002262592315673828
total iterations: 180
TLDA fit: 2.5222551822662354
Whitened factor: 
[[-0.93444021  0.91008813]
 [-0.35612005  0.41441476]]
PCA Reverse Transform: 0.00010919570922851562
decenter with old strategy:
[ 1.12289561e-09 -1.09667775e-09]
Smoothing and Normalization: 7.748603820800781e-05
Fit RMSE: 0.033136103352917806
 Test Against Ground Truth
M1: 0.006325244903564453
M2: 49.37393021583557
[[ 1.00000000e+00 -1.67400815e-16]
 [-1.63064007e-16  1.00000000e+00]]
W: 0.028578519821166992
Whiten X: 0.02409648895263672
Whiten M1: 3.790855407714844e-05
Parafac M3: 0.05607175827026367
Parafac Decomposition: 12.8612539768219
Unwhitening parafac factors: 2.574920654296875e-05
Initialization
[[-0.99201265 -0.12613841]
 [-0.12613841  0.99201265]]
SGD Calc: 21.34904432296753
Unwhitening factors: 2.1696090698242188e-05
Smoothing and Normalization: 7.05718994140625e-05
Fit RMSE: 0.0292184839649314
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00011444091796875
Fit RMSE: 0.031309313336277386
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 0.9999999999999999]
Centering time: 0.008036136627197266
PCA fit: 0.293712854385376
PCA Transform: 0.0007119178771972656
total iterations: 200
TLDA fit: 2.823317527770996
Whitened factor: 
[[ 0.1437634  -0.18621237]
 [-0.98961209  0.98250952]]
PCA Reverse Transform: 5.14984130859375e-05
decenter with old strategy:
[2.00885519e-04 3.78887843e-06]
Smoothing and Normalization: 5.8650970458984375e-05
Fit RMSE: 0.05926403525513521
 Test Against Ground Truth
M1: 0.0015265941619873047
M2: 6.9506261348724365
[[ 1.00000000e+00 -1.66533454e-16]
 [-1.90819582e-16  1.00000000e+00]]
W: 0.004162788391113281
Whiten X: 0.00798344612121582
Whiten M1: 3.0994415283203125e-05
Parafac M3: 0.05546832084655762
Parafac Decomposition: 12.821593046188354
Unwhitening parafac factors: 1.4781951904296875e-05
Initialization
[[-0.26036725  0.96550966]
 [ 0.96550966  0.26036725]]
SGD Calc: 21.731048345565796
Unwhitening factors: 1.430511474609375e-05
Smoothing and Normalization: 4.029273986816406e-05
Fit RMSE: 0.05093672321084036
parafac Test Against Ground Truth
Smoothing and Normalization: 6.151199340820312e-05
Fit RMSE: 0.05553669129662227
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000009]
Centering time: 0.021116971969604492
PCA fit: 0.6937000751495361
PCA Transform: 0.0017707347869873047
total iterations: 200
TLDA fit: 2.936042308807373
Whitened factor: 
[[ 0.14119115 -0.0618601 ]
 [ 0.98998235 -0.99808483]]
PCA Reverse Transform: 8.559226989746094e-05
decenter with old strategy:
[-1.18467581e-11 -9.68812121e-11]
Smoothing and Normalization: 7.104873657226562e-05
Fit RMSE: 0.04233601778257396
 Test Against Ground Truth
M1: 0.0044019222259521484
M2: 20.087214708328247
[[1.00000000e+00 1.15630162e-16]
 [1.14654380e-16 1.00000000e+00]]
W: 0.01298213005065918
Whiten X: 0.016576290130615234
Whiten M1: 3.3855438232421875e-05
Parafac M3: 0.055960893630981445
Parafac Decomposition: 12.98385739326477
Unwhitening parafac factors: 2.0742416381835938e-05
Initialization
[[-0.08403432 -0.99646286]
 [-0.99646286  0.08403432]]
SGD Calc: 21.463999271392822
Unwhitening factors: 1.9311904907226562e-05
Smoothing and Normalization: 5.793571472167969e-05
Fit RMSE: 0.037154663613532035
parafac Test Against Ground Truth
Smoothing and Normalization: 8.0108642578125e-05
Fit RMSE: 0.04015659396448187
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000013, 1.0]
Centering time: 0.03503727912902832
PCA fit: 1.232170581817627
PCA Transform: 0.0017855167388916016
total iterations: 200
TLDA fit: 2.898468494415283
Whitened factor: 
[[-0.94530547  0.92256722]
 [-0.3261864   0.38583639]]
PCA Reverse Transform: 8.893013000488281e-05
decenter with old strategy:
[ 1.10581947e-09 -1.05819729e-09]
Smoothing and Normalization: 7.748603820800781e-05
Fit RMSE: 0.032897392164253476
 Test Against Ground Truth
M1: 0.006192922592163086
M2: 49.56350803375244
[[ 1.00000000e+00 -7.28583860e-17]
 [-7.45931095e-17  1.00000000e+00]]
W: 0.030542850494384766
Whiten X: 0.023708343505859375
Whiten M1: 3.886222839355469e-05
Parafac M3: 0.055590152740478516
Parafac Decomposition: 12.940559387207031
Unwhitening parafac factors: 2.3603439331054688e-05
Initialization
[[-0.86266376  0.50577785]
 [ 0.50577785  0.86266376]]
SGD Calc: 21.536441326141357
Unwhitening factors: 2.0265579223632812e-05
Smoothing and Normalization: 7.009506225585938e-05
Fit RMSE: 0.029149461584530297
parafac Test Against Ground Truth
Smoothing and Normalization: 9.72747802734375e-05
Fit RMSE: 0.03196855701073209
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999999]
Centering time: 0.008116006851196289
PCA fit: 0.3128979206085205
PCA Transform: 0.0007071495056152344
total iterations: 200
TLDA fit: 2.7908520698547363
Whitened factor: 
[[-0.9385182   0.92101691]
 [-0.34522976  0.38952259]]
PCA Reverse Transform: 5.316734313964844e-05
decenter with old strategy:
[-4.06388845e-10  2.66409658e-10]
Smoothing and Normalization: 5.7697296142578125e-05
Fit RMSE: 0.057076043446488985
 Test Against Ground Truth
M1: 0.0024862289428710938
M2: 6.948794603347778
[[1.00000000e+00 6.07153217e-17]
 [2.60208521e-17 1.00000000e+00]]
W: 0.004153728485107422
Whiten X: 0.008538246154785156
Whiten M1: 3.0040740966796875e-05
Parafac M3: 0.0558476448059082
Parafac Decomposition: 12.931329011917114
Unwhitening parafac factors: 1.8358230590820312e-05
Initialization
[[-0.16275488 -0.98666653]
 [-0.98666653  0.16275488]]
SGD Calc: 21.572036027908325
Unwhitening factors: 1.3589859008789062e-05
Smoothing and Normalization: 3.886222839355469e-05
Fit RMSE: 0.05714487112202348
parafac Test Against Ground Truth
Smoothing and Normalization: 7.653236389160156e-05
Fit RMSE: 0.05680201006377638
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000002]
Centering time: 0.021137237548828125
PCA fit: 0.6593301296234131
PCA Transform: 0.0013167858123779297
total iterations: 200
TLDA fit: 2.7762937545776367
Whitened factor: 
[[ 0.86197651 -0.88930067]
 [-0.50694822  0.457323  ]]
PCA Reverse Transform: 8.821487426757812e-05
decenter with old strategy:
[-6.50103587e-11 -3.07228155e-11]
Smoothing and Normalization: 6.699562072753906e-05
Fit RMSE: 0.04028772822381718
 Test Against Ground Truth
M1: 0.0046269893646240234
M2: 19.976504802703857
[[ 1.00000000e+00 -4.77048956e-16]
 [-4.82253126e-16  1.00000000e+00]]
W: 0.01233983039855957
Whiten X: 0.01666116714477539
Whiten M1: 3.4809112548828125e-05
Parafac M3: 0.05468273162841797
Parafac Decomposition: 12.998754262924194
Unwhitening parafac factors: 2.0265579223632812e-05
Initialization
[[-0.71149264 -0.70269355]
 [-0.70269355  0.71149264]]
SGD Calc: 21.56408953666687
Unwhitening factors: 1.5974044799804688e-05
Smoothing and Normalization: 5.364418029785156e-05
Fit RMSE: 0.0400364959804611
parafac Test Against Ground Truth
Smoothing and Normalization: 8.153915405273438e-05
Fit RMSE: 0.04001036937738782
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.000000000000001]
Centering time: 0.03348493576049805
PCA fit: 1.2342798709869385
PCA Transform: 0.0018489360809326172
total iterations: 158
TLDA fit: 2.284058094024658
Whitened factor: 
[[-0.90532053  0.8738028 ]
 [-0.42472902  0.48628045]]
PCA Reverse Transform: 0.0001087188720703125
decenter with old strategy:
[ 9.28192904e-10 -8.93405741e-10]
Smoothing and Normalization: 7.677078247070312e-05
Fit RMSE: 0.03294607827925141
 Test Against Ground Truth
M1: 0.006776332855224609
M2: 49.0720489025116
[[ 1.00000000e+00 -1.38777878e-17]
 [-3.46944695e-17  1.00000000e+00]]
W: 0.031051158905029297
Whiten X: 0.023673534393310547
Whiten M1: 3.695487976074219e-05
Parafac M3: 0.0550382137298584
Parafac Decomposition: 13.105683088302612
Unwhitening parafac factors: 2.47955322265625e-05
Initialization
[[-0.98540774  0.1702104 ]
 [ 0.1702104   0.98540774]]
SGD Calc: 21.55745577812195
Unwhitening factors: 2.4080276489257812e-05
Smoothing and Normalization: 8.630752563476562e-05
Fit RMSE: 0.032818490848005345
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010538101196289062
Fit RMSE: 0.03158534411214122
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.007978677749633789
PCA fit: 0.2608828544616699
PCA Transform: 0.0006890296936035156
total iterations: 145
TLDA fit: 2.0810508728027344
Whitened factor: 
[[ 0.094339   -0.00634903]
 [ 0.99554013 -0.99997984]]
PCA Reverse Transform: 5.435943603515625e-05
decenter with old strategy:
[-2.45685933e-11 -1.40063977e-10]
Smoothing and Normalization: 5.745887756347656e-05
Fit RMSE: 0.059351176196083474
 Test Against Ground Truth
M1: 0.0015308856964111328
M2: 6.837459325790405
[[1.00000000e+00 2.23779328e-16]
 [2.18575158e-16 1.00000000e+00]]
W: 0.0047702789306640625
Whiten X: 0.008172273635864258
Whiten M1: 2.8371810913085938e-05
Parafac M3: 0.055455684661865234
Parafac Decomposition: 13.005481243133545
Unwhitening parafac factors: 1.9073486328125e-05
Initialization
[[-0.19591147 -0.98062159]
 [-0.98062159  0.19591147]]
SGD Calc: 22.147990942001343
Unwhitening factors: 1.52587890625e-05
Smoothing and Normalization: 4.220008850097656e-05
Fit RMSE: 0.049440976134044995
parafac Test Against Ground Truth
Smoothing and Normalization: 6.437301635742188e-05
Fit RMSE: 0.05345835835193713
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.021075010299682617
PCA fit: 0.6868669986724854
PCA Transform: 0.001302480697631836
total iterations: 200
TLDA fit: 2.879455327987671
Whitened factor: 
[[ 0.83253256 -0.8594092 ]
 [-0.55397611  0.5112884 ]]
PCA Reverse Transform: 8.535385131835938e-05
decenter with old strategy:
[ 1.53112232e-09 -1.53283744e-09]
Smoothing and Normalization: 6.67572021484375e-05
Fit RMSE: 0.03961698897925754
 Test Against Ground Truth
M1: 0.004462003707885742
M2: 19.84768033027649
[[ 1.00000000e+00 -4.01425854e-17]
 [-3.97969960e-17  1.00000000e+00]]
W: 0.012959718704223633
Whiten X: 0.016607999801635742
Whiten M1: 3.457069396972656e-05
Parafac M3: 0.0561833381652832
Parafac Decomposition: 13.092724800109863
Unwhitening parafac factors: 2.0742416381835938e-05
Initialization
[[-0.95371045 -0.30072642]
 [-0.30072642  0.95371045]]
SGD Calc: 21.834253311157227
Unwhitening factors: 1.9550323486328125e-05
Smoothing and Normalization: 5.507469177246094e-05
Fit RMSE: 0.03934877419571028
parafac Test Against Ground Truth
Smoothing and Normalization: 8.511543273925781e-05
Fit RMSE: 0.03933738039587049
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999992, 1.0000000000000009]
Centering time: 0.033628225326538086
PCA fit: 1.2393779754638672
PCA Transform: 0.001817941665649414
total iterations: 162
TLDA fit: 2.2768874168395996
Whitened factor: 
[[-0.04503128 -0.03755153]
 [-0.99898558  0.99929469]]
PCA Reverse Transform: 0.00011920928955078125
decenter with old strategy:
[ 6.50747915e-11 -1.61442458e-10]
Smoothing and Normalization: 8.0108642578125e-05
Fit RMSE: 0.03473843233847032
 Test Against Ground Truth
M1: 0.00626683235168457
M2: 49.459989070892334
[[ 1.00000000e+00 -1.51354623e-16]
 [-1.59594560e-16  1.00000000e+00]]
W: 0.030864238739013672
Whiten X: 0.02353811264038086
Whiten M1: 3.814697265625e-05
Parafac M3: 0.055782318115234375
Parafac Decomposition: 13.162439346313477
Unwhitening parafac factors: 2.4318695068359375e-05
Initialization
[[-0.79022171 -0.61282106]
 [-0.61282106  0.79022171]]
SGD Calc: 21.648197174072266
Unwhitening factors: 2.7894973754882812e-05
Smoothing and Normalization: 0.00010085105895996094
Fit RMSE: 0.03318984726827667
parafac Test Against Ground Truth
Smoothing and Normalization: 9.703636169433594e-05
Fit RMSE: 0.03313744313839319
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.007966279983520508
PCA fit: 0.24446868896484375
PCA Transform: 0.0026671886444091797
total iterations: 200
TLDA fit: 2.774317741394043
Whitened factor: 
[[-0.94979501  0.92909708]
 [-0.31287287  0.36983593]]
PCA Reverse Transform: 7.176399230957031e-05
decenter with old strategy:
[-1.35410453e-09  1.19445621e-09]
Smoothing and Normalization: 5.745887756347656e-05
Fit RMSE: 0.055840037711675665
 Test Against Ground Truth
M1: 0.0015864372253417969
M2: 6.6869683265686035
[[ 1.00000000e+00 -7.12320827e-17]
 [-7.11236625e-17  1.00000000e+00]]
W: 0.014049768447875977
Whiten X: 0.010215282440185547
Whiten M1: 3.5762786865234375e-05
Parafac M3: 0.0707242488861084
Parafac Decomposition: 13.27958869934082
Unwhitening parafac factors: 1.6450881958007812e-05
Initialization
[[-0.63382189 -0.77347903]
 [-0.77347903  0.63382189]]
SGD Calc: 21.96963858604431
Unwhitening factors: 1.4543533325195312e-05
Smoothing and Normalization: 4.553794860839844e-05
Fit RMSE: 0.05003784021529619
parafac Test Against Ground Truth
Smoothing and Normalization: 6.127357482910156e-05
Fit RMSE: 0.05539465246117262
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000004]
Centering time: 0.021592378616333008
PCA fit: 0.6677913665771484
PCA Transform: 0.0013315677642822266
total iterations: 200
TLDA fit: 2.810879707336426
Whitened factor: 
[[ 0.8333582  -0.8594457 ]
 [-0.5527333   0.51122704]]
PCA Reverse Transform: 6.937980651855469e-05
decenter with old strategy:
[ 7.03199426e-10 -7.64705284e-10]
Smoothing and Normalization: 6.842613220214844e-05
Fit RMSE: 0.03948416349620005
 Test Against Ground Truth
M1: 0.004616975784301758
M2: 20.340919256210327
[[ 1.00000000e+00 -5.01443505e-16]
 [-5.00359303e-16  1.00000000e+00]]
W: 0.013067007064819336
Whiten X: 0.016946077346801758
Whiten M1: 3.504753112792969e-05
Parafac M3: 0.056208133697509766
Parafac Decomposition: 12.893192768096924
Unwhitening parafac factors: 2.09808349609375e-05
Initialization
[[-0.27322546 -0.96195002]
 [-0.96195002  0.27322546]]
SGD Calc: 22.028462409973145
Unwhitening factors: 1.9550323486328125e-05
Smoothing and Normalization: 5.269050598144531e-05
Fit RMSE: 0.039234358705510196
parafac Test Against Ground Truth
Smoothing and Normalization: 7.891654968261719e-05
Fit RMSE: 0.039220263307318155
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0000000000000007]
Centering time: 0.034436941146850586
PCA fit: 1.2278859615325928
PCA Transform: 0.0019168853759765625
total iterations: 200
TLDA fit: 2.795506238937378
Whitened factor: 
[[ 0.31598255 -0.33058598]
 [-0.948765    0.94377588]]
PCA Reverse Transform: 0.00010657310485839844
decenter with old strategy:
[ 7.38754475e-10 -7.18451668e-10]
Smoothing and Normalization: 7.772445678710938e-05
Fit RMSE: 0.0336412369044978
 Test Against Ground Truth
M1: 0.006372928619384766
M2: 50.94144344329834
[[ 1.00000000e+00 -1.41190228e-16]
 [-1.41407068e-16  1.00000000e+00]]
W: 0.02967691421508789
Whiten X: 0.024314403533935547
Whiten M1: 3.743171691894531e-05
Parafac M3: 0.056177616119384766
Parafac Decomposition: 13.36282229423523
Unwhitening parafac factors: 2.7894973754882812e-05
Initialization
[[-0.03394383  0.99942374]
 [ 0.99942374  0.03394383]]
SGD Calc: 22.00709295272827
Unwhitening factors: 2.193450927734375e-05
Smoothing and Normalization: 7.295608520507812e-05
Fit RMSE: 0.03182036840170857
parafac Test Against Ground Truth
Smoothing and Normalization: 9.870529174804688e-05
Fit RMSE: 0.03239528407392279
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0]
Centering time: 0.008188486099243164
PCA fit: 0.29180169105529785
PCA Transform: 0.0007169246673583984
total iterations: 147
TLDA fit: 2.069425106048584
Whitened factor: 
[[-0.86918395  0.84122952]
 [-0.49448889  0.54067818]]
PCA Reverse Transform: 5.5789947509765625e-05
decenter with old strategy:
[ 1.96398450e-09 -1.90391206e-09]
Smoothing and Normalization: 6.008148193359375e-05
Fit RMSE: 0.0570350342881183
 Test Against Ground Truth
M1: 0.0016391277313232422
M2: 6.93736457824707
[[1.000000e+00 4.290188e-16]
 [4.290188e-16 1.000000e+00]]
W: 0.0045549869537353516
Whiten X: 0.00815439224243164
Whiten M1: 2.8133392333984375e-05
Parafac M3: 0.05606269836425781
Parafac Decomposition: 13.227140426635742
Unwhitening parafac factors: 1.621246337890625e-05
Initialization
[[-0.4532119  -0.89140281]
 [-0.89140281  0.4532119 ]]
SGD Calc: 21.948338508605957
Unwhitening factors: 1.4066696166992188e-05
Smoothing and Normalization: 3.910064697265625e-05
Fit RMSE: 0.05223396125768741
parafac Test Against Ground Truth
Smoothing and Normalization: 6.151199340820312e-05
Fit RMSE: 0.05666597808586784
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.021452665328979492
PCA fit: 0.6830658912658691
PCA Transform: 0.00140380859375
total iterations: 140
TLDA fit: 2.0156638622283936
Whitened factor: 
[[-0.1063817   0.1950638 ]
 [ 0.99432537 -0.98079056]]
PCA Reverse Transform: 9.1552734375e-05
decenter with old strategy:
[ 1.09673488e-10 -2.43859006e-10]
Smoothing and Normalization: 7.05718994140625e-05
Fit RMSE: 0.04238525728889176
 Test Against Ground Truth
M1: 0.004487276077270508
M2: 20.178277730941772
[[ 1.00000000e+00 -5.03937170e-16]
 [-5.03720329e-16  1.00000000e+00]]
W: 0.012855768203735352
Whiten X: 0.016913890838623047
Whiten M1: 3.6716461181640625e-05
Parafac M3: 0.05617165565490723
Parafac Decomposition: 13.359025955200195
Unwhitening parafac factors: 2.0742416381835938e-05
Initialization
[[-0.68916007 -0.72460914]
 [-0.72460914  0.68916007]]
SGD Calc: 21.55602526664734
Unwhitening factors: 1.71661376953125e-05
Smoothing and Normalization: 5.626678466796875e-05
Fit RMSE: 0.03771110033654623
parafac Test Against Ground Truth
Smoothing and Normalization: 8.177757263183594e-05
Fit RMSE: 0.04032662009686169
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0]
Centering time: 0.035443782806396484
PCA fit: 1.2429966926574707
PCA Transform: 0.00177001953125
total iterations: 55
TLDA fit: 0.8080611228942871
Whitened factor: 
[[-0.99796242  0.9997896 ]
 [ 0.06380446 -0.02051238]]
PCA Reverse Transform: 0.00010585784912109375
decenter with old strategy:
[ 1.31763868e-09 -1.25401730e-09]
Smoothing and Normalization: 7.700920104980469e-05
Fit RMSE: 0.032830579982289984
 Test Against Ground Truth
M1: 0.006460666656494141
M2: 50.81045866012573
[[1.00000000e+00 3.20110691e-16]
 [3.19751549e-16 1.00000000e+00]]
W: 0.029090404510498047
Whiten X: 0.024168968200683594
Whiten M1: 3.695487976074219e-05
Parafac M3: 0.05566096305847168
Parafac Decomposition: 13.101592063903809
Unwhitening parafac factors: 2.5272369384765625e-05
Initialization
[[-0.66064137  0.75070166]
 [ 0.75070166  0.66064137]]
SGD Calc: 21.987360954284668
Unwhitening factors: 2.1457672119140625e-05
Smoothing and Normalization: 7.295608520507812e-05
Fit RMSE: 0.02976022043606679
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010991096496582031
Fit RMSE: 0.03185367615373651
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000002]
Centering time: 0.008089780807495117
PCA fit: 0.28458333015441895
PCA Transform: 0.0007150173187255859
total iterations: 114
TLDA fit: 1.5737431049346924
Whitened factor: 
[[-0.0364722  -0.04800643]
 [-0.99933467  0.99884703]]
PCA Reverse Transform: 5.5789947509765625e-05
decenter with old strategy:
[1.91871611e-05 1.86814285e-04]
Smoothing and Normalization: 5.936622619628906e-05
Fit RMSE: 0.0597515492797718
 Test Against Ground Truth
M1: 0.0015726089477539062
M2: 6.819967985153198
[[ 1.00000000e+00 -2.94902991e-16]
 [-2.96637714e-16  1.00000000e+00]]
W: 0.004953145980834961
Whiten X: 0.008209466934204102
Whiten M1: 3.1948089599609375e-05
Parafac M3: 0.05587124824523926
Parafac Decomposition: 13.488058805465698
Unwhitening parafac factors: 1.6689300537109375e-05
Initialization
[[-0.56233867 -0.82690702]
 [-0.82690702  0.56233867]]
SGD Calc: 22.305092096328735
Unwhitening factors: 1.8835067749023438e-05
Smoothing and Normalization: 4.744529724121094e-05
Fit RMSE: 0.04968738775740668
parafac Test Against Ground Truth
Smoothing and Normalization: 4.6253204345703125e-05
Fit RMSE: 0.05665780353316794
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000002, 0.9999999999999996]
Centering time: 0.021157026290893555
PCA fit: 0.7169628143310547
PCA Transform: 0.00131988525390625
total iterations: 200
TLDA fit: 3.0460309982299805
Whitened factor: 
[[ 6.93479197e-02 -6.18164654e-04]
 [ 9.97592535e-01 -9.99999809e-01]]
PCA Reverse Transform: 0.00011682510375976562
decenter with old strategy:
[-4.73568538e-10  4.04737384e-10]
Smoothing and Normalization: 7.867813110351562e-05
Fit RMSE: 0.04186194693366661
 Test Against Ground Truth
M1: 0.00443720817565918
M2: 20.054314851760864
[[ 1.00000000e+00 -1.83880688e-16]
 [-2.77555756e-16  1.00000000e+00]]
W: 0.012459993362426758
Whiten X: 0.01659989356994629
Whiten M1: 5.817413330078125e-05
Parafac M3: 0.05611681938171387
Parafac Decomposition: 12.820299625396729
Unwhitening parafac factors: 2.193450927734375e-05
Initialization
[[-0.99191118 -0.12693385]
 [-0.12693385  0.99191118]]
SGD Calc: 22.295140027999878
Unwhitening factors: 2.0265579223632812e-05
Smoothing and Normalization: 6.580352783203125e-05
Fit RMSE: 0.03947246599213712
parafac Test Against Ground Truth
Smoothing and Normalization: 9.512901306152344e-05
Fit RMSE: 0.03814170178863787
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000002, 1.0000000000000002]
Centering time: 0.03365945816040039
PCA fit: 1.2671549320220947
PCA Transform: 0.001783132553100586
total iterations: 200
TLDA fit: 2.7815706729888916
Whitened factor: 
[[-0.98554318  0.9770464 ]
 [-0.16942445  0.2130266 ]]
PCA Reverse Transform: 0.00011229515075683594
decenter with old strategy:
[-8.19260249e-10  7.52621183e-10]
Smoothing and Normalization: 7.748603820800781e-05
Fit RMSE: 0.03332440414416078
 Test Against Ground Truth
M1: 0.0061838626861572266
M2: 49.88238978385925
[[ 1.00000000e+00 -6.22332047e-17]
 [-6.23416249e-17  1.00000000e+00]]
W: 0.03157520294189453
Whiten X: 0.023849010467529297
Whiten M1: 4.00543212890625e-05
Parafac M3: 0.05617928504943848
Parafac Decomposition: 13.110205888748169
Unwhitening parafac factors: 4.0531158447265625e-05
Initialization
[[-0.92594999  0.37764615]
 [ 0.37764615  0.92594999]]
SGD Calc: 22.878427028656006
Unwhitening factors: 2.384185791015625e-05
Smoothing and Normalization: 9.083747863769531e-05
Fit RMSE: 0.03050848954501113
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00012564659118652344
Fit RMSE: 0.03251850320501447
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999999, 1.0000000000000004]
Centering time: 0.00788569450378418
PCA fit: 0.27904796600341797
PCA Transform: 0.0006837844848632812
total iterations: 162
TLDA fit: 2.2765731811523438
Whitened factor: 
[[-0.89546597  0.86799926]
 [-0.44512998  0.49656549]]
PCA Reverse Transform: 5.435943603515625e-05
decenter with old strategy:
[-2.93241038e-09  2.98963539e-09]
Smoothing and Normalization: 5.841255187988281e-05
Fit RMSE: 0.05646451754006887
 Test Against Ground Truth
M1: 0.0016469955444335938
M2: 6.799858093261719
[[ 1.00000000e+00 -2.88180937e-16]
 [-2.87964097e-16  1.00000000e+00]]
W: 0.004956722259521484
Whiten X: 0.008278131484985352
Whiten M1: 3.0279159545898438e-05
Parafac M3: 0.05578279495239258
Parafac Decomposition: 12.95321774482727
Unwhitening parafac factors: 1.7404556274414062e-05
Initialization
[[-0.96714645  0.25421989]
 [ 0.25421989  0.96714645]]
SGD Calc: 22.32286238670349
Unwhitening factors: 1.5020370483398438e-05
Smoothing and Normalization: 3.8623809814453125e-05
Fit RMSE: 0.053694410176033554
parafac Test Against Ground Truth
Smoothing and Normalization: 6.389617919921875e-05
Fit RMSE: 0.056066684521931254
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999997, 0.9999999999999991]
Centering time: 0.02083611488342285
PCA fit: 0.6945507526397705
PCA Transform: 0.001316070556640625
total iterations: 200
TLDA fit: 2.8167405128479004
Whitened factor: 
[[-0.9612151   0.94729219]
 [-0.27579981  0.32037089]]
PCA Reverse Transform: 7.271766662597656e-05
decenter with old strategy:
[-2.45203473e-09  2.47770161e-09]
Smoothing and Normalization: 7.653236389160156e-05
Fit RMSE: 0.040386534618190126
 Test Against Ground Truth
M1: 0.0044710636138916016
M2: 19.947563886642456
[[ 1.00000000e+00 -2.85904113e-16]
 [-2.86012533e-16  1.00000000e+00]]
W: 0.012633323669433594
Whiten X: 0.016407012939453125
Whiten M1: 3.552436828613281e-05
Parafac M3: 0.05603384971618652
Parafac Decomposition: 12.788915634155273
Unwhitening parafac factors: 2.3603439331054688e-05
Initialization
[[-0.54776643 -0.8366313 ]
 [-0.8366313   0.54776643]]
SGD Calc: 21.77262234687805
Unwhitening factors: 2.288818359375e-05
Smoothing and Normalization: 5.817413330078125e-05
Fit RMSE: 0.036834344512310255
parafac Test Against Ground Truth
Smoothing and Normalization: 8.225440979003906e-05
Fit RMSE: 0.03880137104400952
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0]
Centering time: 0.033217668533325195
PCA fit: 1.2222459316253662
PCA Transform: 0.0017902851104736328
total iterations: 200
TLDA fit: 2.823284149169922
Whitened factor: 
[[ 0.69636822 -0.71281825]
 [-0.71768468  0.70134881]]
PCA Reverse Transform: 9.036064147949219e-05
decenter with old strategy:
[ 4.04528256e-10 -3.64042845e-10]
Smoothing and Normalization: 7.677078247070312e-05
Fit RMSE: 0.033506316724949844
 Test Against Ground Truth
M1: 0.006255388259887695
M2: 49.47376346588135
[[ 1.00000000e+00 -7.11236625e-17]
 [-7.37257477e-17  1.00000000e+00]]
W: 0.03174614906311035
Whiten X: 0.023463726043701172
Whiten M1: 3.719329833984375e-05
Parafac M3: 0.05527329444885254
Parafac Decomposition: 12.954411268234253
Unwhitening parafac factors: 2.3365020751953125e-05
Initialization
[[-0.98385044 -0.17899249]
 [-0.17899249  0.98385044]]
SGD Calc: 21.58712387084961
Unwhitening factors: 2.1219253540039062e-05
Smoothing and Normalization: 6.914138793945312e-05
Fit RMSE: 0.03160989519552358
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00012159347534179688
Fit RMSE: 0.033144696550757864
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999999, 1.0000000000000004]
Centering time: 0.008209466934204102
PCA fit: 0.2939436435699463
PCA Transform: 0.0007033348083496094
total iterations: 200
TLDA fit: 2.9300336837768555
Whitened factor: 
[[-0.91966403  0.8938132 ]
 [-0.39270609  0.44843947]]
PCA Reverse Transform: 5.340576171875e-05
decenter with old strategy:
[-6.02947420e-10  4.77959426e-10]
Smoothing and Normalization: 4.6253204345703125e-05
Fit RMSE: 0.054977175976379036
 Test Against Ground Truth
M1: 0.0015375614166259766
M2: 6.719588279724121
[[ 1.00000000e+00 -2.30718222e-16]
 [-1.96023753e-16  1.00000000e+00]]
W: 0.005010366439819336
Whiten X: 0.008065938949584961
Whiten M1: 3.147125244140625e-05
Parafac M3: 0.05607938766479492
Parafac Decomposition: 12.833154439926147
Unwhitening parafac factors: 1.5735626220703125e-05
Initialization
[[-0.9972773   0.07374272]
 [ 0.07374272  0.9972773 ]]
SGD Calc: 21.325319290161133
Unwhitening factors: 1.2874603271484375e-05
Smoothing and Normalization: 3.6716461181640625e-05
Fit RMSE: 0.05551250401145191
parafac Test Against Ground Truth
Smoothing and Normalization: 7.939338684082031e-05
Fit RMSE: 0.054499448522615915
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999993]
Centering time: 0.020763635635375977
PCA fit: 0.7354497909545898
PCA Transform: 0.0013065338134765625
total iterations: 200
TLDA fit: 2.7909958362579346
Whitened factor: 
[[-0.85232571  0.82053971]
 [-0.52301135  0.57158952]]
PCA Reverse Transform: 8.869171142578125e-05
decenter with old strategy:
[-1.35571272e-09  1.34629567e-09]
Smoothing and Normalization: 5.507469177246094e-05
Fit RMSE: 0.041373787629868035
 Test Against Ground Truth
M1: 0.00446772575378418
M2: 19.99475598335266
[[1.00000000e+00 2.79507320e-16]
 [2.80374682e-16 1.00000000e+00]]
W: 0.012594461441040039
Whiten X: 0.016693830490112305
Whiten M1: 3.409385681152344e-05
Parafac M3: 0.05554342269897461
Parafac Decomposition: 13.177140235900879
Unwhitening parafac factors: 2.3126602172851562e-05
Initialization
[[-0.81032417 -0.58598186]
 [-0.58598186  0.81032417]]
SGD Calc: 21.731403827667236
Unwhitening factors: 1.7881393432617188e-05
Smoothing and Normalization: 5.53131103515625e-05
Fit RMSE: 0.03790829708700585
parafac Test Against Ground Truth
Smoothing and Normalization: 9.703636169433594e-05
Fit RMSE: 0.039287532816343285
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999991, 1.0000000000000002]
Centering time: 0.03352928161621094
PCA fit: 1.2296979427337646
PCA Transform: 0.0017933845520019531
total iterations: 141
TLDA fit: 2.016169309616089
Whitened factor: 
[[-0.09978792  0.18865058]
 [ 0.99500873 -0.98204428]]
PCA Reverse Transform: 0.0001087188720703125
decenter with old strategy:
[-2.98701664e-09  3.21013123e-09]
Smoothing and Normalization: 7.62939453125e-05
Fit RMSE: 0.03475341505827504
 Test Against Ground Truth
M1: 0.006246328353881836
M2: 49.46859073638916
[[1.00000000e+00 3.45318392e-16]
 [3.45467470e-16 1.00000000e+00]]
W: 0.03008866310119629
Whiten X: 0.02362823486328125
Whiten M1: 3.814697265625e-05
Parafac M3: 0.055960655212402344
Parafac Decomposition: 13.156076669692993
Unwhitening parafac factors: 2.5033950805664062e-05
Initialization
[[-0.19390538 -0.98102024]
 [-0.98102024  0.19390538]]
SGD Calc: 21.531729459762573
Unwhitening factors: 2.2172927856445312e-05
Smoothing and Normalization: 7.271766662597656e-05
Fit RMSE: 0.031468030112090165
parafac Test Against Ground Truth
Smoothing and Normalization: 9.322166442871094e-05
Fit RMSE: 0.03311972137513026
 Test Against Ground Truth
Done!
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999996]
Centering time: 0.009328126907348633
PCA fit: 0.2507603168487549
PCA Transform: 0.0008909702301025391
total iterations: 188
TLDA fit: 2.7320947647094727
Whitened factor: 
[[-0.92866413  0.90376862]
 [-0.37092173  0.42802136]]
PCA Reverse Transform: 5.030632019042969e-05
decenter with old strategy:
[3.43623349e-05 1.66153885e-04]
Smoothing and Normalization: 5.91278076171875e-05
Fit RMSE: 0.048321075763002555
 Test Against Ground Truth
M1: 0.0015151500701904297
M2: 6.6632819175720215
[[ 1.00000000e+00 -1.40512602e-16]
 [-8.93382590e-17  1.00000000e+00]]
W: 0.004327058792114258
Whiten X: 0.008664846420288086
Whiten M1: 2.7418136596679688e-05
Parafac M3: 0.05710911750793457
Parafac Decomposition: 13.067499160766602
Unwhitening parafac factors: 1.7881393432617188e-05
Initialization
[[-0.7187588  -0.69525951]
 [-0.69525951  0.7187588 ]]
SGD Calc: 22.308350324630737
Unwhitening factors: 1.3828277587890625e-05
Smoothing and Normalization: 3.981590270996094e-05
Fit RMSE: 0.04850672142712739
parafac Test Against Ground Truth
Smoothing and Normalization: 7.033348083496094e-05
Fit RMSE: 0.04812153675760448
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 1.0000000000000009]
Centering time: 0.020378828048706055
PCA fit: 0.6642529964447021
PCA Transform: 0.0017108917236328125
total iterations: 200
TLDA fit: 2.741389751434326
Whitened factor: 
[[ 0.91843504 -0.92884508]
 [-0.39557185  0.37046837]]
PCA Reverse Transform: 9.846687316894531e-05
decenter with old strategy:
[0.00497608 0.00303601]
Smoothing and Normalization: 6.914138793945312e-05
Fit RMSE: 0.03290644283808741
 Test Against Ground Truth
M1: 0.004315376281738281
M2: 19.667308568954468
[[ 1.00000000e+00 -2.26815094e-16]
 [-2.76254714e-16  1.00000000e+00]]
W: 0.012937307357788086
Whiten X: 0.015617609024047852
Whiten M1: 3.4809112548828125e-05
Parafac M3: 0.05809783935546875
Parafac Decomposition: 13.219634532928467
Unwhitening parafac factors: 2.0742416381835938e-05
Initialization
[[-0.9759532   0.21798014]
 [ 0.21798014  0.9759532 ]]
SGD Calc: 22.39842438697815
Unwhitening factors: 2.002716064453125e-05
Smoothing and Normalization: 6.246566772460938e-05
Fit RMSE: 0.03296168738365026
parafac Test Against Ground Truth
Smoothing and Normalization: 9.775161743164062e-05
Fit RMSE: 0.03373868918945133
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999998, 0.9999999999999982]
Centering time: 0.03288984298706055
PCA fit: 1.120826005935669
PCA Transform: 0.0017938613891601562
total iterations: 123
TLDA fit: 1.7762815952301025
Whitened factor: 
[[-0.88177864  0.83728958]
 [-0.47166347  0.54675969]]
PCA Reverse Transform: 0.0001163482666015625
decenter with old strategy:
[0.00017633 0.0002189 ]
Smoothing and Normalization: 7.843971252441406e-05
Fit RMSE: 0.027316079067881863
 Test Against Ground Truth
M1: 0.006066560745239258
M2: 48.22539520263672
[[ 1.00000000e+00 -4.72712147e-16]
 [-4.60135402e-16  1.00000000e+00]]
W: 0.028187990188598633
Whiten X: 0.023879051208496094
Whiten M1: 3.147125244140625e-05
Parafac M3: 0.05722999572753906
Parafac Decomposition: 13.18398118019104
Unwhitening parafac factors: 2.5272369384765625e-05
Initialization
[[-0.38985616 -0.92087577]
 [-0.92087577  0.38985616]]
SGD Calc: 22.246347427368164
Unwhitening factors: 1.8835067749023438e-05
Smoothing and Normalization: 8.034706115722656e-05
Fit RMSE: 0.02533090147814007
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010466575622558594
Fit RMSE: 0.027555471169695072
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 1.0000000000000002]
Centering time: 0.008022308349609375
PCA fit: 0.2958710193634033
PCA Transform: 0.0007171630859375
total iterations: 200
TLDA fit: 2.8502981662750244
Whitened factor: 
[[ 0.87035176 -0.89729398]
 [-0.49243052  0.44143347]]
PCA Reverse Transform: 5.245208740234375e-05
decenter with old strategy:
[0.00010013 0.00111781]
Smoothing and Normalization: 5.984306335449219e-05
Fit RMSE: 0.04684706848108521
 Test Against Ground Truth
M1: 0.0014710426330566406
M2: 6.668140411376953
[[ 1.00000000e+00  2.77555756e-17]
 [-4.16333634e-17  1.00000000e+00]]
W: 0.005340576171875
Whiten X: 0.008299827575683594
Whiten M1: 2.6464462280273438e-05
Parafac M3: 0.05768752098083496
Parafac Decomposition: 13.05542540550232
Unwhitening parafac factors: 1.52587890625e-05
Initialization
[[-0.99545362 -0.09524748]
 [-0.09524748  0.99545362]]
SGD Calc: 22.138460159301758
Unwhitening factors: 1.2636184692382812e-05
Smoothing and Normalization: 3.933906555175781e-05
Fit RMSE: 0.04749761807579516
parafac Test Against Ground Truth
Smoothing and Normalization: 6.580352783203125e-05
Fit RMSE: 0.046807431518303855
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000002, 0.9999999999999996]
Centering time: 0.02016925811767578
PCA fit: 0.686887264251709
PCA Transform: 0.0013070106506347656
total iterations: 186
TLDA fit: 2.629429578781128
Whitened factor: 
[[-0.94364004  0.91588356]
 [-0.33097352  0.40144403]]
PCA Reverse Transform: 0.00010395050048828125
decenter with old strategy:
[0.00101998 0.0002059 ]
Smoothing and Normalization: 5.507469177246094e-05
Fit RMSE: 0.034060810153988874
 Test Against Ground Truth
M1: 0.004359006881713867
M2: 20.297840356826782
[[1.00000000e+00 7.49400542e-16]
 [6.10622664e-16 1.00000000e+00]]
W: 0.01315164566040039
Whiten X: 0.015533924102783203
Whiten M1: 3.123283386230469e-05
Parafac M3: 0.05776500701904297
Parafac Decomposition: 13.377164125442505
Unwhitening parafac factors: 2.6464462280273438e-05
Initialization
[[-0.44342484  0.89631156]
 [ 0.89631156  0.44342484]]
SGD Calc: 22.29532480239868
Unwhitening factors: 2.0265579223632812e-05
Smoothing and Normalization: 5.7220458984375e-05
Fit RMSE: 0.03516812085215518
parafac Test Against Ground Truth
Smoothing and Normalization: 8.654594421386719e-05
Fit RMSE: 0.03388914822020044
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000002, 1.0000000000000004]
Centering time: 0.032871246337890625
PCA fit: 1.1077558994293213
PCA Transform: 0.0019259452819824219
total iterations: 159
TLDA fit: 2.2292404174804688
Whitened factor: 
[[-0.13633824  0.25607363]
 [ 0.99066235 -0.96665728]]
PCA Reverse Transform: 8.916854858398438e-05
decenter with old strategy:
[-4.69682902e-10  5.05146282e-10]
Smoothing and Normalization: 7.843971252441406e-05
Fit RMSE: 0.028702310079987005
 Test Against Ground Truth
M1: 0.006216526031494141
M2: 49.08426809310913
[[1.00000000e+00 2.42861287e-16]
 [1.75207071e-16 1.00000000e+00]]
W: 0.02944207191467285
Whiten X: 0.025482177734375
Whiten M1: 3.147125244140625e-05
Parafac M3: 0.056910037994384766
Parafac Decomposition: 13.210394144058228
Unwhitening parafac factors: 2.3365020751953125e-05
Initialization
[[-0.99839293 -0.05667068]
 [-0.05667068  0.99839293]]
SGD Calc: 22.372068405151367
Unwhitening factors: 1.9550323486328125e-05
Smoothing and Normalization: 7.915496826171875e-05
Fit RMSE: 0.026494867321416943
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0001201629638671875
Fit RMSE: 0.027457480386021218
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999996, 0.9999999999999996]
Centering time: 0.008028745651245117
PCA fit: 0.29682421684265137
PCA Transform: 0.0007112026214599609
total iterations: 200
TLDA fit: 2.8832218647003174
Whitened factor: 
[[ 0.56288941 -0.56545401]
 [-0.82653222  0.82477982]]
PCA Reverse Transform: 5.1975250244140625e-05
decenter with old strategy:
[1.00063266e-04 9.95707597e-05]
Smoothing and Normalization: 5.936622619628906e-05
Fit RMSE: 0.04914693858407448
 Test Against Ground Truth
M1: 0.0015411376953125
M2: 6.7033257484436035
[[1.00000000e+00 2.49800181e-16]
 [3.26128013e-16 1.00000000e+00]]
W: 0.004963874816894531
Whiten X: 0.007836580276489258
Whiten M1: 3.0517578125e-05
Parafac M3: 0.05817675590515137
Parafac Decomposition: 12.990984201431274
Unwhitening parafac factors: 1.5020370483398438e-05
Initialization
[[-0.89911431  0.43771391]
 [ 0.43771391  0.89911431]]
SGD Calc: 22.240967273712158
Unwhitening factors: 1.3828277587890625e-05
Smoothing and Normalization: 4.6253204345703125e-05
Fit RMSE: 0.044630088285510676
parafac Test Against Ground Truth
Smoothing and Normalization: 8.344650268554688e-05
Fit RMSE: 0.04816437913446127
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000002, 0.9999999999999993]
Centering time: 0.020154714584350586
PCA fit: 0.6612927913665771
PCA Transform: 0.001331329345703125
total iterations: 154
TLDA fit: 2.1802752017974854
Whitened factor: 
[[-0.86149088  0.81661904]
 [-0.50777305  0.57717704]]
PCA Reverse Transform: 9.202957153320312e-05
decenter with old strategy:
[0.05359753 0.01007938]
Smoothing and Normalization: 7.009506225585938e-05
Fit RMSE: 0.033356407150270405
 Test Against Ground Truth
M1: 0.004311800003051758
M2: 19.797731637954712
[[ 1.00000000e+00 -2.81025203e-16]
 [-3.01841885e-16  1.00000000e+00]]
W: 0.01355433464050293
Whiten X: 0.015434026718139648
Whiten M1: 3.170967102050781e-05
Parafac M3: 0.057088375091552734
Parafac Decomposition: 12.971571445465088
Unwhitening parafac factors: 1.811981201171875e-05
Initialization
[[-0.99599549  0.0894035 ]
 [ 0.0894035   0.99599549]]
SGD Calc: 22.093637228012085
Unwhitening factors: 1.621246337890625e-05
Smoothing and Normalization: 5.6743621826171875e-05
Fit RMSE: 0.033790014412904294
parafac Test Against Ground Truth
Smoothing and Normalization: 6.580352783203125e-05
Fit RMSE: 0.03310555880096996
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000018]
Centering time: 0.03286266326904297
PCA fit: 1.1242105960845947
PCA Transform: 0.001775503158569336
total iterations: 200
TLDA fit: 2.820748805999756
Whitened factor: 
[[-0.43452202  0.4922471 ]
 [ 0.90066121 -0.87045551]]
PCA Reverse Transform: 8.845329284667969e-05
decenter with old strategy:
[ 5.50305776e-10 -5.54144032e-10]
Smoothing and Normalization: 7.700920104980469e-05
Fit RMSE: 0.02825968354568682
 Test Against Ground Truth
M1: 0.0060040950775146484
M2: 48.22520732879639
[[1.00000000e+00 2.56739074e-16]
 [2.28983499e-16 1.00000000e+00]]
W: 0.028493404388427734
Whiten X: 0.023878097534179688
Whiten M1: 3.4332275390625e-05
Parafac M3: 0.0570988655090332
Parafac Decomposition: 12.900430679321289
Unwhitening parafac factors: 2.193450927734375e-05
Initialization
[[-0.21511378 -0.97658899]
 [-0.97658899  0.21511378]]
SGD Calc: 22.216950178146362
Unwhitening factors: 2.3126602172851562e-05
Smoothing and Normalization: 7.891654968261719e-05
Fit RMSE: 0.028131470802027787
parafac Test Against Ground Truth
Smoothing and Normalization: 9.965896606445312e-05
Fit RMSE: 0.027178171489101767
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0]
Centering time: 0.00777745246887207
PCA fit: 0.280087947845459
PCA Transform: 0.000701904296875
total iterations: 200
TLDA fit: 2.8552064895629883
Whitened factor: 
[[ 0.87020996 -0.89571538]
 [-0.49268105  0.44462789]]
PCA Reverse Transform: 5.4836273193359375e-05
decenter with old strategy:
[0.00154422 0.01748192]
Smoothing and Normalization: 7.295608520507812e-05
Fit RMSE: 0.04763206126709219
 Test Against Ground Truth
M1: 0.0014884471893310547
M2: 6.7436628341674805
[[1.00000000e+00 6.93889390e-18]
 [2.77555756e-17 1.00000000e+00]]
W: 0.004837751388549805
Whiten X: 0.008126020431518555
Whiten M1: 2.956390380859375e-05
Parafac M3: 0.05745053291320801
Parafac Decomposition: 12.980706453323364
Unwhitening parafac factors: 1.5735626220703125e-05
Initialization
[[-0.80244837 -0.59672155]
 [-0.59672155  0.80244837]]
SGD Calc: 22.198358058929443
Unwhitening factors: 1.7404556274414062e-05
Smoothing and Normalization: 4.482269287109375e-05
Fit RMSE: 0.047647101691116495
parafac Test Against Ground Truth
Smoothing and Normalization: 4.76837158203125e-05
Fit RMSE: 0.048125573096325445
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000002, 1.0000000000000018]
Centering time: 0.020662784576416016
PCA fit: 0.6896297931671143
PCA Transform: 0.0012984275817871094
total iterations: 147
TLDA fit: 2.138401746749878
Whitened factor: 
[[-0.92307951  0.8901914 ]
 [-0.38460918  0.45558673]]
PCA Reverse Transform: 9.489059448242188e-05
decenter with old strategy:
[ 4.45227257e-10 -4.24229048e-10]
Smoothing and Normalization: 5.7220458984375e-05
Fit RMSE: 0.03390169134511016
 Test Against Ground Truth
M1: 0.004343986511230469
M2: 19.991263151168823
[[ 1.00000000e+00 -1.32706346e-16]
 [-1.38777878e-16  1.00000000e+00]]
W: 0.012599706649780273
Whiten X: 0.016050338745117188
Whiten M1: 3.218650817871094e-05
Parafac M3: 0.05767703056335449
Parafac Decomposition: 13.062405347824097
Unwhitening parafac factors: 1.9550323486328125e-05
Initialization
[[-0.25849199 -0.9660134 ]
 [-0.9660134   0.25849199]]
SGD Calc: 22.198322296142578
Unwhitening factors: 1.8596649169921875e-05
Smoothing and Normalization: 6.341934204101562e-05
Fit RMSE: 0.03371780227998378
parafac Test Against Ground Truth
Smoothing and Normalization: 8.511543273925781e-05
Fit RMSE: 0.034628017354139354
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000013, 1.0000000000000002]
Centering time: 0.033434152603149414
PCA fit: 1.1479153633117676
PCA Transform: 0.003810882568359375
total iterations: 190
TLDA fit: 2.749868869781494
Whitened factor: 
[[-0.82490723  0.86849106]
 [ 0.56526813 -0.49570484]]
PCA Reverse Transform: 0.0001068115234375
decenter with old strategy:
[ 4.84993732e-04 -7.98257902e-05]
Smoothing and Normalization: 7.82012939453125e-05
Fit RMSE: 0.02736664950558836
 Test Against Ground Truth
M1: 0.006239414215087891
M2: 48.68220019340515
[[1.00000000e+00 3.46944695e-17]
 [1.63064007e-16 1.00000000e+00]]
W: 0.032010555267333984
Whiten X: 0.024314403533935547
Whiten M1: 3.4332275390625e-05
Parafac M3: 0.05721712112426758
Parafac Decomposition: 13.100449323654175
Unwhitening parafac factors: 2.5272369384765625e-05
Initialization
[[-0.46392889 -0.88587244]
 [-0.88587244  0.46392889]]
SGD Calc: 22.13670325279236
Unwhitening factors: 2.1457672119140625e-05
Smoothing and Normalization: 9.72747802734375e-05
Fit RMSE: 0.027126222676368654
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010371208190917969
Fit RMSE: 0.02791905800473789
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999992, 0.9999999999999997]
Centering time: 0.008095264434814453
PCA fit: 0.3149425983428955
PCA Transform: 0.0006783008575439453
total iterations: 200
TLDA fit: 2.799767255783081
Whitened factor: 
[[ 0.79008107 -0.80145672]
 [-0.61300237  0.59805278]]
PCA Reverse Transform: 5.459785461425781e-05
decenter with old strategy:
[ 2.95063599e-10 -3.14742752e-10]
Smoothing and Normalization: 6.008148193359375e-05
Fit RMSE: 0.04702647145872362
 Test Against Ground Truth
M1: 0.0017423629760742188
M2: 6.908085584640503
[[ 1.00000000e+00 -8.32667268e-17]
 [-1.52655666e-16  1.00000000e+00]]
W: 0.005101442337036133
Whiten X: 0.008065223693847656
Whiten M1: 2.8848648071289062e-05
Parafac M3: 0.056854963302612305
Parafac Decomposition: 13.514520168304443
Unwhitening parafac factors: 1.5735626220703125e-05
Initialization
[[-0.34712129 -0.93782025]
 [-0.93782025  0.34712129]]
SGD Calc: 22.76835799217224
Unwhitening factors: 1.5974044799804688e-05
Smoothing and Normalization: 4.0531158447265625e-05
Fit RMSE: 0.047154350982311924
parafac Test Against Ground Truth
Smoothing and Normalization: 7.009506225585938e-05
Fit RMSE: 0.048430531222534406
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999987, 1.0000000000000009]
Centering time: 0.020777225494384766
PCA fit: 0.6589150428771973
PCA Transform: 0.0013206005096435547
total iterations: 162
TLDA fit: 2.422111988067627
Whitened factor: 
[[-0.90248444  0.86265264]
 [-0.43072245  0.50579681]]
PCA Reverse Transform: 0.00012230873107910156
decenter with old strategy:
[5.87130060e-05 1.30384264e-04]
Smoothing and Normalization: 7.891654968261719e-05
Fit RMSE: 0.032551245089112564
 Test Against Ground Truth
M1: 0.004426002502441406
M2: 20.03038787841797
[[1.00000000e+00 3.33066907e-16]
 [5.03503489e-16 1.00000000e+00]]
W: 0.01308894157409668
Whiten X: 0.01610398292541504
Whiten M1: 3.457069396972656e-05
Parafac M3: 0.057066917419433594
Parafac Decomposition: 13.655615329742432
Unwhitening parafac factors: 2.09808349609375e-05
Initialization
[[-0.99610124 -0.08821744]
 [-0.08821744  0.99610124]]
SGD Calc: 22.965999841690063
Unwhitening factors: 2.2411346435546875e-05
Smoothing and Normalization: 7.605552673339844e-05
Fit RMSE: 0.03229761749022586
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010013580322265625
Fit RMSE: 0.033547316887396685
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000002, 1.0]
Centering time: 0.033620357513427734
PCA fit: 1.1932616233825684
PCA Transform: 0.0018000602722167969
total iterations: 192
TLDA fit: 2.7215735912323
Whitened factor: 
[[ 0.18291568 -0.0685014 ]
 [-0.98312861  0.99765102]]
PCA Reverse Transform: 0.00010609626770019531
decenter with old strategy:
[-1.90613785e-10  1.36816914e-10]
Smoothing and Normalization: 8.273124694824219e-05
Fit RMSE: 0.0292057077301826
 Test Against Ground Truth
M1: 0.006150722503662109
M2: 49.006683349609375
[[1.00000000e+00 5.55111512e-17]
 [1.11022302e-16 1.00000000e+00]]
W: 0.03198862075805664
Whiten X: 0.02446126937866211
Whiten M1: 3.314018249511719e-05
Parafac M3: 0.05725216865539551
Parafac Decomposition: 13.010466575622559
Unwhitening parafac factors: 3.647804260253906e-05
Initialization
[[-0.89701397 -0.44200218]
 [-0.44200218  0.89701397]]
SGD Calc: 22.86020541191101
Unwhitening factors: 1.9788742065429688e-05
Smoothing and Normalization: 7.915496826171875e-05
Fit RMSE: 0.026972210377747895
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010132789611816406
Fit RMSE: 0.027884444876516135
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000013, 0.9999999999999998]
Centering time: 0.007936477661132812
PCA fit: 0.2758216857910156
PCA Transform: 0.000843048095703125
total iterations: 200
TLDA fit: 2.7996420860290527
Whitened factor: 
[[ 0.82210793 -0.84112766]
 [-0.56933167  0.54083663]]
PCA Reverse Transform: 5.4836273193359375e-05
decenter with old strategy:
[0.01185412 0.11748729]
Smoothing and Normalization: 4.76837158203125e-05
Fit RMSE: 0.04689842788601916
 Test Against Ground Truth
M1: 0.0017366409301757812
M2: 6.892928600311279
[[ 1.00000000e+00 -2.77555756e-17]
 [-2.08166817e-17  1.00000000e+00]]
W: 0.00501561164855957
Whiten X: 0.008193254470825195
Whiten M1: 3.123283386230469e-05
Parafac M3: 0.05788540840148926
Parafac Decomposition: 13.40075159072876
Unwhitening parafac factors: 1.9311904907226562e-05
Initialization
[[-0.97214797 -0.23436791]
 [-0.23436791  0.97214797]]
SGD Calc: 22.445443153381348
Unwhitening factors: 1.52587890625e-05
Smoothing and Normalization: 6.365776062011719e-05
Fit RMSE: 0.04694023950429903
parafac Test Against Ground Truth
Smoothing and Normalization: 7.200241088867188e-05
Fit RMSE: 0.04793486144096554
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000007, 1.0000000000000002]
Centering time: 0.02083301544189453
PCA fit: 0.6548867225646973
PCA Transform: 0.0012924671173095703
total iterations: 137
TLDA fit: 1.9666485786437988
Whitened factor: 
[[-0.2107498   0.11554798]
 [-0.97754004  0.9933019 ]]
PCA Reverse Transform: 9.226799011230469e-05
decenter with old strategy:
[ 5.87856403e-10 -5.61671183e-10]
Smoothing and Normalization: 6.937980651855469e-05
Fit RMSE: 0.036646481697581036
 Test Against Ground Truth
M1: 0.004471302032470703
M2: 20.485734462738037
[[1.00000000e+00 2.67147415e-16]
 [3.01841885e-16 1.00000000e+00]]
W: 0.013468742370605469
Whiten X: 0.015896081924438477
Whiten M1: 3.4332275390625e-05
Parafac M3: 0.057250261306762695
Parafac Decomposition: 13.311090230941772
Unwhitening parafac factors: 1.9311904907226562e-05
Initialization
[[-0.90188429  0.43197769]
 [ 0.43197769  0.90188429]]
SGD Calc: 22.841858625411987
Unwhitening factors: 1.9788742065429688e-05
Smoothing and Normalization: 7.104873657226562e-05
Fit RMSE: 0.03494518270457867
parafac Test Against Ground Truth
Smoothing and Normalization: 8.511543273925781e-05
Fit RMSE: 0.033843622748585725
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0]
Centering time: 0.03371143341064453
PCA fit: 1.1106336116790771
PCA Transform: 0.0018045902252197266
total iterations: 186
TLDA fit: 2.6095619201660156
Whitened factor: 
[[-0.50359001  0.43326161]
 [-0.86394277  0.90126821]]
PCA Reverse Transform: 0.00010204315185546875
decenter with old strategy:
[-2.44624981e-11 -3.99855719e-12]
Smoothing and Normalization: 7.843971252441406e-05
Fit RMSE: 0.028559203194735015
 Test Against Ground Truth
M1: 0.006150245666503906
M2: 49.13527727127075
[[1.00000000e+00 1.52655666e-16]
 [2.42861287e-16 1.00000000e+00]]
W: 0.030560731887817383
Whiten X: 0.02512502670288086
Whiten M1: 3.147125244140625e-05
Parafac M3: 0.05719923973083496
Parafac Decomposition: 12.901864290237427
Unwhitening parafac factors: 2.2649765014648438e-05
Initialization
[[-0.35060606  0.93652303]
 [ 0.93652303  0.35060606]]
SGD Calc: 22.445791244506836
Unwhitening factors: 2.0503997802734375e-05
Smoothing and Normalization: 7.557868957519531e-05
Fit RMSE: 0.02849847172835379
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010156631469726562
Fit RMSE: 0.028020400395184518
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0000000000000002]
Centering time: 0.00792074203491211
PCA fit: 0.305370569229126
PCA Transform: 0.0007157325744628906
total iterations: 195
TLDA fit: 2.8013226985931396
Whitened factor: 
[[-0.92364195  0.89296302]
 [-0.3832565   0.45013002]]
PCA Reverse Transform: 7.939338684082031e-05
decenter with old strategy:
[0.00990411 0.00239529]
Smoothing and Normalization: 6.031990051269531e-05
Fit RMSE: 0.048608547883276294
 Test Against Ground Truth
M1: 0.0014925003051757812
M2: 6.83087420463562
[[1.00000000e+00 2.08166817e-16]
 [1.87350135e-16 1.00000000e+00]]
W: 0.004986286163330078
Whiten X: 0.007965087890625
Whiten M1: 2.956390380859375e-05
Parafac M3: 0.058298587799072266
Parafac Decomposition: 13.225629568099976
Unwhitening parafac factors: 1.5020370483398438e-05
Initialization
[[-0.91196095 -0.41027702]
 [-0.41027702  0.91196095]]
SGD Calc: 22.41436004638672
Unwhitening factors: 1.3828277587890625e-05
Smoothing and Normalization: 4.1961669921875e-05
Fit RMSE: 0.04849079249194767
parafac Test Against Ground Truth
Smoothing and Normalization: 6.341934204101562e-05
Fit RMSE: 0.04955116747978595
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000007, 1.000000000000001]
Centering time: 0.02044963836669922
PCA fit: 0.6330904960632324
PCA Transform: 0.0013415813446044922
total iterations: 152
TLDA fit: 2.160828113555908
Whitened factor: 
[[-0.94089239  0.91033016]
 [-0.33870563  0.41388283]]
PCA Reverse Transform: 7.867813110351562e-05
decenter with old strategy:
[0.00048995 0.00226462]
Smoothing and Normalization: 6.914138793945312e-05
Fit RMSE: 0.033892167846089546
 Test Against Ground Truth
M1: 0.0044558048248291016
M2: 20.246524333953857
[[ 1.00000000e+00 -3.29597460e-16]
 [-2.29850861e-16  1.00000000e+00]]
W: 0.012906074523925781
Whiten X: 0.015868425369262695
Whiten M1: 3.3855438232421875e-05
Parafac M3: 0.057253122329711914
Parafac Decomposition: 12.986719369888306
Unwhitening parafac factors: 1.811981201171875e-05
Initialization
[[-0.69764819 -0.71644051]
 [-0.71644051  0.69764819]]
SGD Calc: 22.66430640220642
Unwhitening factors: 1.7881393432617188e-05
Smoothing and Normalization: 6.365776062011719e-05
Fit RMSE: 0.03365297573576493
parafac Test Against Ground Truth
Smoothing and Normalization: 8.463859558105469e-05
Fit RMSE: 0.034087841989580474
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999976, 0.9999999999999992]
Centering time: 0.0334475040435791
PCA fit: 1.1562895774841309
PCA Transform: 0.0018508434295654297
total iterations: 200
TLDA fit: 2.820528745651245
Whitened factor: 
[[-0.64048289  0.59750822]
 [-0.76797244  0.80186279]]
PCA Reverse Transform: 0.0001049041748046875
decenter with old strategy:
[0.00084304 0.00585766]
Smoothing and Normalization: 7.867813110351562e-05
Fit RMSE: 0.027886789963025658
 Test Against Ground Truth
M1: 0.006067514419555664
M2: 49.760345220565796
[[ 1.00000000e+00 -1.35308431e-16]
 [-1.07552856e-16  1.00000000e+00]]
W: 0.031909942626953125
Whiten X: 0.024322032928466797
Whiten M1: 3.647804260253906e-05
Parafac M3: 0.057146310806274414
Parafac Decomposition: 12.99145245552063
Unwhitening parafac factors: 2.2649765014648438e-05
Initialization
[[-0.98845552  0.15151133]
 [ 0.15151133  0.98845552]]
SGD Calc: 22.775777101516724
Unwhitening factors: 2.3365020751953125e-05
Smoothing and Normalization: 9.202957153320312e-05
Fit RMSE: 0.027249417808761348
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00012159347534179688
Fit RMSE: 0.02787555148440124
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000013, 1.0000000000000009]
Centering time: 0.007951736450195312
PCA fit: 0.29010844230651855
PCA Transform: 0.0007178783416748047
total iterations: 200
TLDA fit: 2.8239901065826416
Whitened factor: 
[[-0.92795543  0.90226296]
 [-0.37269118  0.43118621]]
PCA Reverse Transform: 5.507469177246094e-05
decenter with old strategy:
[-6.16909014e-11  7.04449981e-12]
Smoothing and Normalization: 6.079673767089844e-05
Fit RMSE: 0.04644977717231222
 Test Against Ground Truth
M1: 0.001689910888671875
M2: 6.823019504547119
[[1.00000000e+00 5.34294831e-16]
 [3.88578059e-16 1.00000000e+00]]
W: 0.004755258560180664
Whiten X: 0.008133649826049805
Whiten M1: 2.956390380859375e-05
Parafac M3: 0.056853532791137695
Parafac Decomposition: 13.031581401824951
Unwhitening parafac factors: 1.5974044799804688e-05
Initialization
[[-0.73235214 -0.68092609]
 [-0.68092609  0.73235214]]
SGD Calc: 22.342735528945923
Unwhitening factors: 1.1444091796875e-05
Smoothing and Normalization: 3.981590270996094e-05
Fit RMSE: 0.047262065446047566
parafac Test Against Ground Truth
Smoothing and Normalization: 6.508827209472656e-05
Fit RMSE: 0.04638393318782336
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000018, 1.0000000000000007]
Centering time: 0.02065300941467285
PCA fit: 0.6691455841064453
PCA Transform: 0.0012989044189453125
total iterations: 103
TLDA fit: 1.4967105388641357
Whitened factor: 
[[-0.90790435  0.86957199]
 [-0.41917739  0.49380619]]
PCA Reverse Transform: 9.560585021972656e-05
decenter with old strategy:
[0.33961857 0.0934998 ]
Smoothing and Normalization: 6.985664367675781e-05
Fit RMSE: 0.03386868616273508
 Test Against Ground Truth
M1: 0.00438380241394043
M2: 20.2648503780365
[[1.00000000e+00 1.70870262e-16]
 [1.24466409e-16 1.00000000e+00]]
W: 0.013848304748535156
Whiten X: 0.015744447708129883
Whiten M1: 3.528594970703125e-05
Parafac M3: 0.05887413024902344
Parafac Decomposition: 13.213210821151733
Unwhitening parafac factors: 2.1457672119140625e-05
Initialization
[[-0.32706881 -0.94500053]
 [-0.94500053  0.32706881]]
SGD Calc: 22.745432138442993
Unwhitening factors: 1.6927719116210938e-05
Smoothing and Normalization: 5.91278076171875e-05
Fit RMSE: 0.03379356122668459
parafac Test Against Ground Truth
Smoothing and Normalization: 8.58306884765625e-05
Fit RMSE: 0.0349726077392598
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999991]
Centering time: 0.033533573150634766
PCA fit: 1.120478630065918
PCA Transform: 0.001965045928955078
total iterations: 165
TLDA fit: 2.31791090965271
Whitened factor: 
[[ 0.20928882 -0.10695814]
 [-0.97785387  0.99426352]]
PCA Reverse Transform: 0.00010824203491210938
decenter with old strategy:
[0.02658676 0.04160327]
Smoothing and Normalization: 9.059906005859375e-05
Fit RMSE: 0.029047956674897943
 Test Against Ground Truth
M1: 0.0060880184173583984
M2: 49.440239667892456
[[ 1.00000000e+00 -2.42861287e-16]
 [-2.42861287e-16  1.00000000e+00]]
W: 0.028919696807861328
Whiten X: 0.024398088455200195
Whiten M1: 3.504753112792969e-05
Parafac M3: 0.05714893341064453
Parafac Decomposition: 13.234667778015137
Unwhitening parafac factors: 2.3603439331054688e-05
Initialization
[[-0.07934149  0.99684749]
 [ 0.99684749  0.07934149]]
SGD Calc: 22.431065559387207
Unwhitening factors: 2.5033950805664062e-05
Smoothing and Normalization: 9.083747863769531e-05
Fit RMSE: 0.027734439433293286
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010442733764648438
Fit RMSE: 0.02765947403469249
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.000000000000001, 0.999999999999999]
Centering time: 0.007927417755126953
PCA fit: 0.2884061336517334
PCA Transform: 0.00069427490234375
total iterations: 155
TLDA fit: 2.16333270072937
Whitened factor: 
[[-0.91449977  0.8821081 ]
 [-0.40458642  0.47104702]]
PCA Reverse Transform: 5.364418029785156e-05
decenter with old strategy:
[0.07459676 0.00980322]
Smoothing and Normalization: 6.008148193359375e-05
Fit RMSE: 0.04736417767272579
 Test Against Ground Truth
M1: 0.0015778541564941406
M2: 6.934256315231323
[[ 1.00000000e+00 -3.46944695e-16]
 [-2.77555756e-16  1.00000000e+00]]
W: 0.004797458648681641
Whiten X: 0.008127212524414062
Whiten M1: 2.956390380859375e-05
Parafac M3: 0.05710315704345703
Parafac Decomposition: 12.820844411849976
Unwhitening parafac factors: 1.52587890625e-05
Initialization
[[-0.48566857 -0.87414303]
 [-0.87414303  0.48566857]]
SGD Calc: 22.44375729560852
Unwhitening factors: 1.239776611328125e-05
Smoothing and Normalization: 4.315376281738281e-05
Fit RMSE: 0.04269737116568699
parafac Test Against Ground Truth
Smoothing and Normalization: 7.987022399902344e-05
Fit RMSE: 0.0480802329540626
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000013, 1.000000000000001]
Centering time: 0.02082657814025879
PCA fit: 0.6592528820037842
PCA Transform: 0.0013127326965332031
total iterations: 200
TLDA fit: 2.871363878250122
Whitened factor: 
[[-0.41152711  0.45834272]
 [ 0.91139752 -0.88877554]]
PCA Reverse Transform: 9.632110595703125e-05
decenter with old strategy:
[0.00626812 0.00291776]
Smoothing and Normalization: 6.818771362304688e-05
Fit RMSE: 0.03501776163461275
 Test Against Ground Truth
M1: 0.004393815994262695
M2: 20.025881052017212
[[ 1.00000000e+00 -2.22044605e-16]
 [-3.36536354e-16  1.00000000e+00]]
W: 0.012993097305297852
Whiten X: 0.015754222869873047
Whiten M1: 3.361701965332031e-05
Parafac M3: 0.05706381797790527
Parafac Decomposition: 13.238089323043823
Unwhitening parafac factors: 2.0503997802734375e-05
Initialization
[[-0.98309457 -0.18309851]
 [-0.18309851  0.98309457]]
SGD Calc: 22.179758310317993
Unwhitening factors: 1.5735626220703125e-05
Smoothing and Normalization: 5.6743621826171875e-05
Fit RMSE: 0.03450451204561379
parafac Test Against Ground Truth
Smoothing and Normalization: 8.106231689453125e-05
Fit RMSE: 0.033802595273088994
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999982, 1.0000000000000009]
Centering time: 0.033492088317871094
PCA fit: 1.1146893501281738
PCA Transform: 0.001790761947631836
total iterations: 129
TLDA fit: 1.913088321685791
Whitened factor: 
[[-0.16213568  0.26605241]
 [ 0.98676847 -0.96395857]]
PCA Reverse Transform: 0.00010848045349121094
decenter with old strategy:
[0.00036481 0.00020176]
Smoothing and Normalization: 8.225440979003906e-05
Fit RMSE: 0.029040739290643203
 Test Against Ground Truth
M1: 0.0061266422271728516
M2: 49.07519173622131
[[1.00000000e+00 6.66133815e-16]
 [5.89805982e-16 1.00000000e+00]]
W: 0.030455589294433594
Whiten X: 0.02427816390991211
Whiten M1: 3.8623809814453125e-05
Parafac M3: 0.05739283561706543
Parafac Decomposition: 13.140177965164185
Unwhitening parafac factors: 2.2172927856445312e-05
Initialization
[[-0.90516272 -0.42506522]
 [-0.42506522  0.90516272]]
SGD Calc: 22.469095468521118
Unwhitening factors: 2.2411346435546875e-05
Smoothing and Normalization: 7.724761962890625e-05
Fit RMSE: 0.02724869220900599
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00011801719665527344
Fit RMSE: 0.027825562576069628
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0]
Centering time: 0.007990837097167969
PCA fit: 0.31814098358154297
PCA Transform: 0.0007109642028808594
total iterations: 163
TLDA fit: 2.3253209590911865
Whitened factor: 
[[ 0.10381761 -0.00568205]
 [ 0.99459635 -0.99998386]]
PCA Reverse Transform: 5.626678466796875e-05
decenter with old strategy:
[0.05612679 0.04806534]
Smoothing and Normalization: 5.9604644775390625e-05
Fit RMSE: 0.052347948248576026
 Test Against Ground Truth
M1: 0.0015676021575927734
M2: 6.856892824172974
[[1.00000000e+00 1.28369537e-16]
 [1.63064007e-16 1.00000000e+00]]
W: 0.0047206878662109375
Whiten X: 0.008554220199584961
Whiten M1: 2.8133392333984375e-05
Parafac M3: 0.05733847618103027
Parafac Decomposition: 13.140496253967285
Unwhitening parafac factors: 1.6927719116210938e-05
Initialization
[[-0.64256515 -0.76623106]
 [-0.76623106  0.64256515]]
SGD Calc: 22.424249172210693
Unwhitening factors: 1.52587890625e-05
Smoothing and Normalization: 4.458427429199219e-05
Fit RMSE: 0.047656717176257815
parafac Test Against Ground Truth
Smoothing and Normalization: 6.461143493652344e-05
Fit RMSE: 0.04793993532453579
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0000000000000009]
Centering time: 0.020644664764404297
PCA fit: 0.6667149066925049
PCA Transform: 0.001313924789428711
total iterations: 163
TLDA fit: 2.301748037338257
Whitened factor: 
[[-0.25517022  0.17465954]
 [-0.96689615  0.98462889]]
PCA Reverse Transform: 9.298324584960938e-05
decenter with old strategy:
[0.0006047  0.00094821]
Smoothing and Normalization: 7.152557373046875e-05
Fit RMSE: 0.035213122572154136
 Test Against Ground Truth
M1: 0.004370212554931641
M2: 20.08374524116516
[[1.00000000e+00 9.88792381e-17]
 [1.04083409e-17 1.00000000e+00]]
W: 0.013049840927124023
Whiten X: 0.015808582305908203
Whiten M1: 3.5762786865234375e-05
Parafac M3: 0.057900190353393555
Parafac Decomposition: 13.121178388595581
Unwhitening parafac factors: 1.9311904907226562e-05
Initialization
[[-0.53226009 -0.84658089]
 [-0.84658089  0.53226009]]
SGD Calc: 22.42746329307556
Unwhitening factors: 1.6927719116210938e-05
Smoothing and Normalization: 5.793571472167969e-05
Fit RMSE: 0.033728807186804266
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010275840759277344
Fit RMSE: 0.03260486813540359
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999996]
Centering time: 0.033365726470947266
PCA fit: 1.1340522766113281
PCA Transform: 0.003145456314086914
total iterations: 124
TLDA fit: 1.8113985061645508
Whitened factor: 
[[-0.89798577  0.84838854]
 [-0.4400245   0.52937405]]
PCA Reverse Transform: 0.00010991096496582031
decenter with old strategy:
[ 3.21560167e-10 -2.98613414e-10]
Smoothing and Normalization: 7.867813110351562e-05
Fit RMSE: 0.027769660653839817
 Test Against Ground Truth
M1: 0.006087303161621094
M2: 49.08512234687805
[[ 1.00000000e+00 -1.35308431e-16]
 [-1.70002901e-16  1.00000000e+00]]
W: 0.028063535690307617
Whiten X: 0.024401426315307617
Whiten M1: 3.218650817871094e-05
Parafac M3: 0.05730795860290527
Parafac Decomposition: 13.202307939529419
Unwhitening parafac factors: 2.2411346435546875e-05
Initialization
[[-0.8914972 -0.4530262]
 [-0.4530262  0.8914972]]
SGD Calc: 22.47489094734192
Unwhitening factors: 2.09808349609375e-05
Smoothing and Normalization: 7.510185241699219e-05
Fit RMSE: 0.027741234387435868
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010585784912109375
Fit RMSE: 0.02825340335153994
 Test Against Ground Truth
Done!
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000002, 0.9999999999999994]
M1: 0.002094745635986328
M2: 6.620287656784058
[[ 1.00000000e+00  2.77555756e-17]
 [-2.77555756e-17  1.00000000e+00]]
W: 0.008399248123168945
Whiten X: 0.009177446365356445
Whiten M1: 3.361701965332031e-05
Parafac M3: 0.05657505989074707
Parafac Decomposition: 13.102996826171875
Unwhitening parafac factors: 1.3589859008789062e-05
Initialization
[[-0.20570412  0.97861423]
 [ 0.97861423  0.20570412]]
SGD Calc: 21.65257239341736
Unwhitening factors: 1.2159347534179688e-05
Smoothing and Normalization: 5.245208740234375e-05
Fit RMSE: 0.04871885328707887
parafac Test Against Ground Truth
Smoothing and Normalization: 7.843971252441406e-05
Fit RMSE: 0.04739233639616037
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 1.0000000000000013]
M1: 0.004289150238037109
M2: 19.92413830757141
[[1.00000000e+00 1.38777878e-16]
 [1.66533454e-16 1.00000000e+00]]
W: 0.013687372207641602
Whiten X: 0.01660752296447754
Whiten M1: 3.24249267578125e-05
Parafac M3: 0.055658578872680664
Parafac Decomposition: 12.822356462478638
Unwhitening parafac factors: 2.193450927734375e-05
Initialization
[[-0.66085788 -0.75051107]
 [-0.75051107  0.66085788]]
SGD Calc: 21.164608240127563
Unwhitening factors: 1.9550323486328125e-05
Smoothing and Normalization: 7.462501525878906e-05
Fit RMSE: 0.03550384122220064
parafac Test Against Ground Truth
Smoothing and Normalization: 9.322166442871094e-05
Fit RMSE: 0.034085954359939254
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999996, 0.9999999999999991]
M1: 0.006314992904663086
M2: 49.01897358894348
[[1.00000000e+00 8.67361738e-17]
 [9.36750677e-17 1.00000000e+00]]
W: 0.03086709976196289
Whiten X: 0.02442193031311035
Whiten M1: 6.961822509765625e-05
Parafac M3: 0.05573272705078125
Parafac Decomposition: 12.957013130187988
Unwhitening parafac factors: 2.4318695068359375e-05
Initialization
[[-0.81472365 -0.57984945]
 [-0.57984945  0.81472365]]
SGD Calc: 21.62226152420044
Unwhitening factors: 1.8835067749023438e-05
Smoothing and Normalization: 8.106231689453125e-05
Fit RMSE: 0.026917688642375547
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010442733764648438
Fit RMSE: 0.02768296173038614
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000002, 1.0000000000000002]
M1: 0.0019881725311279297
M2: 6.862964630126953
[[1.00000000e+00 7.63278329e-17]
 [1.73472348e-18 1.00000000e+00]]
W: 0.004356861114501953
Whiten X: 0.005971193313598633
Whiten M1: 2.1457672119140625e-05
Parafac M3: 0.05669236183166504
Parafac Decomposition: 13.197728872299194
Unwhitening parafac factors: 1.6927719116210938e-05
Initialization
[[-0.40920228 -0.91244369]
 [-0.91244369  0.40920228]]
SGD Calc: 21.90539050102234
Unwhitening factors: 1.3113021850585938e-05
Smoothing and Normalization: 4.291534423828125e-05
Fit RMSE: 0.0462330082859196
parafac Test Against Ground Truth
Smoothing and Normalization: 6.67572021484375e-05
Fit RMSE: 0.04666014672082094
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999993, 0.9999999999999999]
M1: 0.004148244857788086
M2: 20.067999839782715
[[1.00000000e+00 3.19189120e-16]
 [3.33066907e-16 1.00000000e+00]]
W: 0.013151407241821289
Whiten X: 0.015737533569335938
Whiten M1: 3.504753112792969e-05
Parafac M3: 0.05637383460998535
Parafac Decomposition: 13.181236743927002
Unwhitening parafac factors: 2.5987625122070312e-05
Initialization
[[-0.7651276  -0.64387868]
 [-0.64387868  0.7651276 ]]
SGD Calc: 21.86521863937378
Unwhitening factors: 1.7881393432617188e-05
Smoothing and Normalization: 6.222724914550781e-05
Fit RMSE: 0.0324890683264363
parafac Test Against Ground Truth
Smoothing and Normalization: 8.273124694824219e-05
Fit RMSE: 0.033501270306556495
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999999, 0.9999999999999989]
[1.0000000000000002, 1.0000000000000004]
M1: 0.005968570709228516
M2: 50.25740885734558
[[1.00000000e+00 1.87350135e-16]
 [1.66533454e-16 1.00000000e+00]]
W: 0.03927111625671387
Whiten X: 0.04828953742980957
Whiten M1: 3.409385681152344e-05
Parafac M3: 0.1120305061340332
Parafac Decomposition: 13.375200986862183
Unwhitening parafac factors: 2.574920654296875e-05
Initialization
[[-0.66907836 -0.74319186]
 [-0.74319186  0.66907836]]
SGD Calc: 22.694137811660767
Unwhitening factors: 2.09808349609375e-05
Smoothing and Normalization: 7.772445678710938e-05
Fit RMSE: 0.028688509400728604
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00011968612670898438
Fit RMSE: 0.02791201238925872
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999994]
M1: 0.002107858657836914
M2: 6.948176622390747
[[ 1.00000000e+00 -2.60208521e-16]
 [-2.26381414e-16  1.00000000e+00]]
W: 0.0049092769622802734
Whiten X: 0.006341695785522461
Whiten M1: 1.5974044799804688e-05
Parafac M3: 0.05614733695983887
Parafac Decomposition: 13.32210397720337
Unwhitening parafac factors: 1.71661376953125e-05
Initialization
[[-0.06976486 -0.99756346]
 [-0.99756346  0.06976486]]
SGD Calc: 22.244535446166992
Unwhitening factors: 1.6450881958007812e-05
Smoothing and Normalization: 5.9604644775390625e-05
Fit RMSE: 0.043946224429524755
parafac Test Against Ground Truth
Smoothing and Normalization: 6.794929504394531e-05
Fit RMSE: 0.049336961726200876
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999992, 0.9999999999999994]
M1: 0.004227638244628906
M2: 20.786434173583984
[[1.00000000e+00 5.27355937e-16]
 [4.30211422e-16 1.00000000e+00]]
W: 0.013221025466918945
Whiten X: 0.015820026397705078
Whiten M1: 2.7179718017578125e-05
Parafac M3: 0.056342124938964844
Parafac Decomposition: 13.358803510665894
Unwhitening parafac factors: 2.2172927856445312e-05
Initialization
[[-0.99984662 -0.01751378]
 [-0.01751378  0.99984662]]
SGD Calc: 22.324243783950806
Unwhitening factors: 2.86102294921875e-05
Smoothing and Normalization: 9.703636169433594e-05
Fit RMSE: 0.03433827676006996
parafac Test Against Ground Truth
Smoothing and Normalization: 8.678436279296875e-05
Fit RMSE: 0.033388080515538245
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0, 1.0000000000000016]
M1: 0.00603485107421875
M2: 51.70406413078308
[[ 1.00000000e+00 -2.77555756e-17]
 [ 4.16333634e-17  1.00000000e+00]]
W: 0.03649187088012695
Whiten X: 0.024738311767578125
Whiten M1: 2.9325485229492188e-05
Parafac M3: 0.05548834800720215
Parafac Decomposition: 13.348637819290161
Unwhitening parafac factors: 2.5987625122070312e-05
Initialization
[[-0.85346408 -0.52115167]
 [-0.52115167  0.85346408]]
SGD Calc: 22.965348720550537
Unwhitening factors: 2.3126602172851562e-05
Smoothing and Normalization: 9.5367431640625e-05
Fit RMSE: 0.027983873584700534
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010418891906738281
Fit RMSE: 0.02690019520773461
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999997, 0.9999999999999993]
M1: 0.0019867420196533203
M2: 6.991909742355347
[[ 1.00000000e+00  1.38777878e-16]
 [-1.11022302e-16  1.00000000e+00]]
W: 0.004726409912109375
Whiten X: 0.006341695785522461
Whiten M1: 1.6689300537109375e-05
Parafac M3: 0.05607771873474121
Parafac Decomposition: 13.341093301773071
Unwhitening parafac factors: 1.7881393432617188e-05
Initialization
[[-0.66722114 -0.74485968]
 [-0.74485968  0.66722114]]
SGD Calc: 22.42234992980957
Unwhitening factors: 1.5735626220703125e-05
Smoothing and Normalization: 6.198883056640625e-05
Fit RMSE: 0.04499117017824031
parafac Test Against Ground Truth
Smoothing and Normalization: 7.295608520507812e-05
Fit RMSE: 0.047817214653453524
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0000000000000013]
M1: 0.004106044769287109
M2: 20.7743182182312
[[1.00000000e+00 3.19189120e-16]
 [1.38777878e-16 1.00000000e+00]]
W: 0.013221979141235352
Whiten X: 0.016022920608520508
Whiten M1: 3.0279159545898438e-05
Parafac M3: 0.05572795867919922
Parafac Decomposition: 13.47484040260315
Unwhitening parafac factors: 2.0265579223632812e-05
Initialization
[[-0.00863975 -0.99996268]
 [-0.99996268  0.00863975]]
SGD Calc: 22.615182638168335
Unwhitening factors: 2.002716064453125e-05
Smoothing and Normalization: 6.4849853515625e-05
Fit RMSE: 0.03321743663846392
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010275840759277344
Fit RMSE: 0.03379872884402751
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0000000000000004]
M1: 0.0059163570404052734
M2: 50.953009605407715
[[ 1.00000000e+00 -2.22044605e-16]
 [-1.11022302e-16  1.00000000e+00]]
W: 0.029186725616455078
Whiten X: 0.02454376220703125
Whiten M1: 2.9802322387695312e-05
Parafac M3: 0.055930376052856445
Parafac Decomposition: 13.41940450668335
Unwhitening parafac factors: 2.6226043701171875e-05
Initialization
[[-0.9145148  -0.40455244]
 [-0.40455244  0.9145148 ]]
SGD Calc: 22.11331057548523
Unwhitening factors: 2.47955322265625e-05
Smoothing and Normalization: 9.751319885253906e-05
Fit RMSE: 0.028355672142996082
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00012922286987304688
Fit RMSE: 0.027126813192913338
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999997]
M1: 0.0020918846130371094
M2: 6.95712947845459
[[ 1.00000000e+00 -4.85722573e-17]
 [ 1.73472348e-17  1.00000000e+00]]
W: 0.005076885223388672
Whiten X: 0.0062596797943115234
Whiten M1: 1.6927719116210938e-05
Parafac M3: 0.05593085289001465
Parafac Decomposition: 13.617581605911255
Unwhitening parafac factors: 1.6450881958007812e-05
Initialization
[[-0.88417058 -0.4671642 ]
 [-0.4671642   0.88417058]]
SGD Calc: 22.542688846588135
Unwhitening factors: 1.8835067749023438e-05
Smoothing and Normalization: 5.6743621826171875e-05
Fit RMSE: 0.04970063866754953
parafac Test Against Ground Truth
Smoothing and Normalization: 7.414817810058594e-05
Fit RMSE: 0.04809011333320747
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999999, 1.0000000000000004]
M1: 0.004057168960571289
M2: 21.72863745689392
[[ 1.00000000e+00 -8.32667268e-17]
 [ 5.55111512e-17  1.00000000e+00]]
W: 0.013736724853515625
Whiten X: 0.0160980224609375
Whiten M1: 3.266334533691406e-05
Parafac M3: 0.05611681938171387
Parafac Decomposition: 13.455389738082886
Unwhitening parafac factors: 2.3603439331054688e-05
Initialization
[[-0.65436041 -0.75618282]
 [-0.75618282  0.65436041]]
SGD Calc: 22.23198175430298
Unwhitening factors: 2.1457672119140625e-05
Smoothing and Normalization: 7.796287536621094e-05
Fit RMSE: 0.03408495167002444
parafac Test Against Ground Truth
Smoothing and Normalization: 8.96453857421875e-05
Fit RMSE: 0.034448107889534015
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999999]
M1: 0.005864620208740234
M2: 50.60896682739258
[[1.00000000e+00 1.35308431e-16]
 [1.38777878e-16 1.00000000e+00]]
W: 0.02897930145263672
Whiten X: 0.024167299270629883
Whiten M1: 2.956390380859375e-05
Parafac M3: 0.056090354919433594
Parafac Decomposition: 13.498632431030273
Unwhitening parafac factors: 2.4080276489257812e-05
Initialization
[[-0.62607451 -0.77976324]
 [-0.77976324  0.62607451]]
SGD Calc: 22.145019054412842
Unwhitening factors: 2.002716064453125e-05
Smoothing and Normalization: 7.724761962890625e-05
Fit RMSE: 0.028893724903519713
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00011873245239257812
Fit RMSE: 0.027701120514824883
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0000000000000002]
M1: 0.0023202896118164062
M2: 7.026928663253784
[[1.00000000e+00 2.48932819e-16]
 [1.32706346e-16 1.00000000e+00]]
W: 0.0047266483306884766
Whiten X: 0.0063915252685546875
Whiten M1: 1.5497207641601562e-05
Parafac M3: 0.056204795837402344
Parafac Decomposition: 13.379297733306885
Unwhitening parafac factors: 1.5497207641601562e-05
Initialization
[[-0.54613344  0.83769819]
 [ 0.83769819  0.54613344]]
SGD Calc: 22.13988471031189
Unwhitening factors: 1.1920928955078125e-05
Smoothing and Normalization: 4.1961669921875e-05
Fit RMSE: 0.04791072070276987
parafac Test Against Ground Truth
Smoothing and Normalization: 6.580352783203125e-05
Fit RMSE: 0.048574770782783994
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999999, 1.0000000000000004]
M1: 0.004126548767089844
M2: 20.346965551376343
[[ 1.00000000e+00 -3.19189120e-16]
 [-2.15105711e-16  1.00000000e+00]]
W: 0.013259172439575195
Whiten X: 0.015797853469848633
Whiten M1: 2.7894973754882812e-05
Parafac M3: 0.05612063407897949
Parafac Decomposition: 13.6044921875
Unwhitening parafac factors: 2.2649765014648438e-05
Initialization
[[-0.40081211  0.91616028]
 [ 0.91616028  0.40081211]]
SGD Calc: 22.122345447540283
Unwhitening factors: 1.9788742065429688e-05
Smoothing and Normalization: 6.031990051269531e-05
Fit RMSE: 0.03445778563673175
parafac Test Against Ground Truth
Smoothing and Normalization: 8.797645568847656e-05
Fit RMSE: 0.03408264941483344
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999996, 1.000000000000002]
M1: 0.005902528762817383
M2: 50.635931730270386
[[1.00000000e+00 3.72965547e-17]
 [4.77048956e-17 1.00000000e+00]]
W: 0.029752492904663086
Whiten X: 0.024226665496826172
Whiten M1: 3.2901763916015625e-05
Parafac M3: 0.056081533432006836
Parafac Decomposition: 13.459303140640259
Unwhitening parafac factors: 2.7894973754882812e-05
Initialization
[[-0.30689497 -0.95174339]
 [-0.95174339  0.30689497]]
SGD Calc: 22.29011082649231
Unwhitening factors: 2.2411346435546875e-05
Smoothing and Normalization: 7.700920104980469e-05
Fit RMSE: 0.029129163029628403
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00012683868408203125
Fit RMSE: 0.030890825430630663
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999997]
M1: 0.0023674964904785156
M2: 6.931257963180542
[[ 1.00000000e+00 -3.05311332e-16]
 [-2.91433544e-16  1.00000000e+00]]
W: 0.004918813705444336
Whiten X: 0.006792783737182617
Whiten M1: 2.0265579223632812e-05
Parafac M3: 0.05633902549743652
Parafac Decomposition: 13.377593278884888
Unwhitening parafac factors: 1.8358230590820312e-05
Initialization
[[-0.85645663 -0.51621899]
 [-0.51621899  0.85645663]]
SGD Calc: 22.22459840774536
Unwhitening factors: 1.4066696166992188e-05
Smoothing and Normalization: 5.7220458984375e-05
Fit RMSE: 0.04955426135284074
parafac Test Against Ground Truth
Smoothing and Normalization: 6.699562072753906e-05
Fit RMSE: 0.04824017620775336
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0000000000000016]
M1: 0.00423884391784668
M2: 20.496419668197632
[[ 1.00000000e+00 -2.08166817e-17]
 [ 3.12250226e-17  1.00000000e+00]]
W: 0.014390230178833008
Whiten X: 0.01578044891357422
Whiten M1: 2.9802322387695312e-05
Parafac M3: 0.056038856506347656
Parafac Decomposition: 13.701466083526611
Unwhitening parafac factors: 2.3603439331054688e-05
Initialization
[[-0.58411821 -0.8116686 ]
 [-0.8116686   0.58411821]]
SGD Calc: 22.26322102546692
Unwhitening factors: 2.0503997802734375e-05
Smoothing and Normalization: 8.058547973632812e-05
Fit RMSE: 0.034409140829427864
parafac Test Against Ground Truth
Smoothing and Normalization: 0.000152587890625
Fit RMSE: 0.0345753685997206
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000002, 0.9999999999999998]
M1: 0.005907297134399414
M2: 51.066107749938965
[[ 1.00000000e+00 -8.95117314e-16]
 [-7.28583860e-16  1.00000000e+00]]
W: 0.03890252113342285
Whiten X: 0.024618864059448242
Whiten M1: 3.0994415283203125e-05
Parafac M3: 0.05566072463989258
Parafac Decomposition: 13.702964067459106
Unwhitening parafac factors: 2.5987625122070312e-05
Initialization
[[-0.58659558 -0.80988001]
 [-0.80988001  0.58659558]]
SGD Calc: 22.270118713378906
Unwhitening factors: 2.3126602172851562e-05
Smoothing and Normalization: 8.344650268554688e-05
Fit RMSE: 0.02735240748338634
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00012063980102539062
Fit RMSE: 0.027893589097898675
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 1.0]
M1: 0.002043485641479492
M2: 7.590717554092407
[[1.00000000e+00 6.93889390e-17]
 [7.63278329e-17 1.00000000e+00]]
W: 0.011162757873535156
Whiten X: 0.009457111358642578
Whiten M1: 2.6702880859375e-05
Parafac M3: 0.056571245193481445
Parafac Decomposition: 13.926844835281372
Unwhitening parafac factors: 1.3113021850585938e-05
Initialization
[[-0.09454634 -0.99552046]
 [-0.99552046  0.09454634]]
SGD Calc: 23.645161628723145
Unwhitening factors: 1.9550323486328125e-05
Smoothing and Normalization: 7.939338684082031e-05
Fit RMSE: 0.04506122463361429
parafac Test Against Ground Truth
Smoothing and Normalization: 8.58306884765625e-05
Fit RMSE: 0.04810235928401416
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000002, 1.0000000000000009]
M1: 0.0042231082916259766
M2: 20.477449417114258
[[ 1.00000000e+00 -1.80411242e-16]
 [-1.52655666e-16  1.00000000e+00]]
W: 0.02322840690612793
Whiten X: 0.03168153762817383
Whiten M1: 2.9087066650390625e-05
Parafac M3: 0.09131050109863281
Parafac Decomposition: 13.74151062965393
Unwhitening parafac factors: 2.7418136596679688e-05
Initialization
[[-0.18800967 -0.98216718]
 [-0.98216718  0.18800967]]
SGD Calc: 22.340275764465332
Unwhitening factors: 2.09808349609375e-05
Smoothing and Normalization: 7.414817810058594e-05
Fit RMSE: 0.0331807796488406
parafac Test Against Ground Truth
Smoothing and Normalization: 8.58306884765625e-05
Fit RMSE: 0.032735654527304536
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0000000000000018]
M1: 0.005982637405395508
M2: 53.955971002578735
[[ 1.00000000e+00 -1.11022302e-16]
 [-1.52655666e-16  1.00000000e+00]]
W: 0.04004621505737305
Whiten X: 0.045583486557006836
Whiten M1: 2.956390380859375e-05
Parafac M3: 0.0859825611114502
Parafac Decomposition: 14.076091527938843
Unwhitening parafac factors: 2.574920654296875e-05
Initialization
[[-0.23607183 -0.97173561]
 [-0.97173561  0.23607183]]
SGD Calc: 24.092811822891235
Unwhitening factors: 2.47955322265625e-05
Smoothing and Normalization: 8.034706115722656e-05
Fit RMSE: 0.02767117994417743
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00011301040649414062
Fit RMSE: 0.027047010243437294
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000007, 0.9999999999999986]
M1: 0.0024080276489257812
M2: 6.952747106552124
[[ 1.00000000e+00 -1.38777878e-17]
 [ 1.38777878e-16  1.00000000e+00]]
W: 0.013576745986938477
Whiten X: 0.012625455856323242
Whiten M1: 2.288818359375e-05
Parafac M3: 0.10394501686096191
Parafac Decomposition: 13.15245532989502
Unwhitening parafac factors: 1.7881393432617188e-05
Initialization
[[-0.18241999 -0.9832207 ]
 [-0.9832207   0.18241999]]
SGD Calc: 22.63583493232727
Unwhitening factors: 1.8835067749023438e-05
Smoothing and Normalization: 5.8650970458984375e-05
Fit RMSE: 0.04875546331483518
parafac Test Against Ground Truth
Smoothing and Normalization: 6.532669067382812e-05
Fit RMSE: 0.047192862540505316
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 1.000000000000002]
M1: 0.0042266845703125
M2: 20.84463858604431
[[1.00000000e+00 3.33066907e-16]
 [1.52655666e-16 1.00000000e+00]]
W: 0.02204728126525879
Whiten X: 0.032236576080322266
Whiten M1: 2.956390380859375e-05
Parafac M3: 0.09167766571044922
Parafac Decomposition: 13.206082582473755
Unwhitening parafac factors: 2.0503997802734375e-05
Initialization
[[-0.39880434  0.91703604]
 [ 0.91703604  0.39880434]]
SGD Calc: 24.01549220085144
Unwhitening factors: 2.2411346435546875e-05
Smoothing and Normalization: 8.344650268554688e-05
Fit RMSE: 0.03243106060413675
parafac Test Against Ground Truth
Smoothing and Normalization: 8.416175842285156e-05
Fit RMSE: 0.03241693102883332
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999969, 0.9999999999999989]
M1: 0.005889177322387695
M2: 50.340752840042114
[[ 1.00000000e+00  2.08166817e-17]
 [-2.77555756e-17  1.00000000e+00]]
W: 0.03836679458618164
Whiten X: 0.02391672134399414
Whiten M1: 2.8133392333984375e-05
Parafac M3: 0.05565690994262695
Parafac Decomposition: 13.074918508529663
Unwhitening parafac factors: 2.4318695068359375e-05
Initialization
[[-0.51384911 -0.85788058]
 [-0.85788058  0.51384911]]
SGD Calc: 22.399656534194946
Unwhitening factors: 2.384185791015625e-05
Smoothing and Normalization: 9.560585021972656e-05
Fit RMSE: 0.027490384777503755
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00011014938354492188
Fit RMSE: 0.02741637776053305
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 1.0000000000000004]
M1: 0.002170085906982422
M2: 7.016758441925049
[[ 1.00000000e+00 -1.36175793e-16]
 [-4.11996826e-17  1.00000000e+00]]
W: 0.005064964294433594
Whiten X: 0.0066945552825927734
Whiten M1: 2.3365020751953125e-05
Parafac M3: 0.05644583702087402
Parafac Decomposition: 13.208902835845947
Unwhitening parafac factors: 1.8835067749023438e-05
Initialization
[[-0.95824346  0.28595362]
 [ 0.28595362  0.95824346]]
SGD Calc: 22.72831416130066
Unwhitening factors: 1.8358230590820312e-05
Smoothing and Normalization: 5.173683166503906e-05
Fit RMSE: 0.04657099774620936
parafac Test Against Ground Truth
Smoothing and Normalization: 8.225440979003906e-05
Fit RMSE: 0.04991981145641251
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999996, 1.0]
M1: 0.004182100296020508
M2: 20.84315013885498
[[1.00000000e+00 2.48932819e-16]
 [2.20309881e-16 1.00000000e+00]]
W: 0.01280665397644043
Whiten X: 0.016692638397216797
Whiten M1: 2.7418136596679688e-05
Parafac M3: 0.055654287338256836
Parafac Decomposition: 13.390077352523804
Unwhitening parafac factors: 1.9073486328125e-05
Initialization
[[-0.75885852 -0.65125552]
 [-0.65125552  0.75885852]]
SGD Calc: 22.29290223121643
Unwhitening factors: 2.09808349609375e-05
Smoothing and Normalization: 6.008148193359375e-05
Fit RMSE: 0.03362571416439636
parafac Test Against Ground Truth
Smoothing and Normalization: 8.344650268554688e-05
Fit RMSE: 0.032652872791541256
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999998]
M1: 0.006072044372558594
M2: 49.98491978645325
[[ 1.00000000e+00 -2.11636264e-16]
 [-1.73472348e-16  1.00000000e+00]]
W: 0.029840946197509766
Whiten X: 0.023922204971313477
Whiten M1: 3.0279159545898438e-05
Parafac M3: 0.055716514587402344
Parafac Decomposition: 13.026257753372192
Unwhitening parafac factors: 2.288818359375e-05
Initialization
[[-0.88403075 -0.46742874]
 [-0.46742874  0.88403075]]
SGD Calc: 22.117711544036865
Unwhitening factors: 2.47955322265625e-05
Smoothing and Normalization: 0.00010037422180175781
Fit RMSE: 0.02529401683525239
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0001049041748046875
Fit RMSE: 0.028674409675872627
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 1.0]
M1: 0.0023698806762695312
M2: 7.0554633140563965
[[ 1.00000000e+00  1.38777878e-17]
 [-4.16333634e-17  1.00000000e+00]]
W: 0.005089998245239258
Whiten X: 0.006721019744873047
Whiten M1: 2.0742416381835938e-05
Parafac M3: 0.05625510215759277
Parafac Decomposition: 13.283546924591064
Unwhitening parafac factors: 2.0503997802734375e-05
Initialization
[[-0.86019883  0.50995879]
 [ 0.50995879  0.86019883]]
SGD Calc: 22.107272624969482
Unwhitening factors: 1.6689300537109375e-05
Smoothing and Normalization: 5.555152893066406e-05
Fit RMSE: 0.049275155179339725
parafac Test Against Ground Truth
Smoothing and Normalization: 6.461143493652344e-05
Fit RMSE: 0.04693786050720105
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000013, 1.0000000000000004]
M1: 0.0041658878326416016
M2: 21.66536259651184
[[1.00000000e+00 1.09287579e-16]
 [1.04083409e-16 1.00000000e+00]]
W: 0.009569406509399414
Whiten X: 0.01724982261657715
Whiten M1: 3.0040740966796875e-05
Parafac M3: 0.05553627014160156
Parafac Decomposition: 13.028447151184082
Unwhitening parafac factors: 1.7881393432617188e-05
Initialization
[[-0.23698157 -0.97151415]
 [-0.97151415  0.23698157]]
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000002, 0.9999999999999998]
M1: 0.002410411834716797
M2: 7.081594467163086
[[1.00000000e+00 2.22044605e-16]
 [1.31838984e-16 1.00000000e+00]]
W: 0.005229473114013672
Whiten X: 0.00676727294921875
Whiten M1: 2.0265579223632812e-05
Parafac M3: 0.056024789810180664
Parafac Decomposition: 13.489257097244263
Unwhitening parafac factors: 1.5020370483398438e-05
Initialization
[[-0.9951956   0.09790665]
 [ 0.09790665  0.9951956 ]]
SGD Calc: 22.57439613342285
Unwhitening factors: 1.5974044799804688e-05
Smoothing and Normalization: 4.267692565917969e-05
Fit RMSE: 0.049464085764396815
parafac Test Against Ground Truth
Smoothing and Normalization: 6.508827209472656e-05
Fit RMSE: 0.04790768106838496
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999987]
M1: 0.0041048526763916016
M2: 21.044142246246338
[[1.00000000e+00 6.17561557e-16]
 [5.06539255e-16 1.00000000e+00]]
W: 0.018550634384155273
Whiten X: 0.016225814819335938
Whiten M1: 3.170967102050781e-05
Parafac M3: 0.05582308769226074
Parafac Decomposition: 13.7230064868927
Unwhitening parafac factors: 2.2649765014648438e-05
Initialization
[[-0.99776781 -0.06677875]
 [-0.06677875  0.99776781]]
SGD Calc: 22.606974363327026
Unwhitening factors: 1.9788742065429688e-05
Smoothing and Normalization: 6.580352783203125e-05
Fit RMSE: 0.03310368014524608
parafac Test Against Ground Truth
Smoothing and Normalization: 8.845329284667969e-05
Fit RMSE: 0.03377113538052864
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0, 0.9999999999999977]
M1: 0.006042957305908203
M2: 51.0427303314209
[[ 1.00000000e+00 -2.42861287e-16]
 [-2.87964097e-16  1.00000000e+00]]
W: 0.03733563423156738
Whiten X: 0.024763107299804688
Whiten M1: 3.314018249511719e-05
Parafac M3: 0.05554366111755371
Parafac Decomposition: 13.543250799179077
Unwhitening parafac factors: 2.6464462280273438e-05
Initialization
[[-0.99810991 -0.06145415]
 [-0.06145415  0.99810991]]
SGD Calc: 22.103004455566406
Unwhitening factors: 2.3603439331054688e-05
Smoothing and Normalization: 9.799003601074219e-05
Fit RMSE: 0.02562993050691018
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010585784912109375
Fit RMSE: 0.028178522727107153
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0]
M1: 0.0023331642150878906
M2: 7.020459890365601
[[ 1.00000000e+00 -2.42861287e-16]
 [-1.59594560e-16  1.00000000e+00]]
W: 0.00507807731628418
Whiten X: 0.006306171417236328
Whiten M1: 1.811981201171875e-05
Parafac M3: 0.05648183822631836
Parafac Decomposition: 13.97081208229065
Unwhitening parafac factors: 1.6450881958007812e-05
Initialization
[[-0.36537776 -0.93085933]
 [-0.93085933  0.36537776]]
SGD Calc: 23.0587375164032
Unwhitening factors: 1.5497207641601562e-05
Smoothing and Normalization: 5.9604644775390625e-05
Fit RMSE: 0.04681784467657908
parafac Test Against Ground Truth
Smoothing and Normalization: 7.033348083496094e-05
Fit RMSE: 0.04774492537471268
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999996, 1.0000000000000009]
M1: 0.004229068756103516
M2: 21.317304134368896
[[ 1.00000000e+00 -6.93889390e-17]
 [-1.45716772e-16  1.00000000e+00]]
W: 0.019364118576049805
Whiten X: 0.0160369873046875
Whiten M1: 3.147125244140625e-05
Parafac M3: 0.05628061294555664
Parafac Decomposition: 13.476046562194824
Unwhitening parafac factors: 2.4557113647460938e-05
Initialization
[[-0.95264435 -0.30408672]
 [-0.30408672  0.95264435]]
SGD Calc: 23.023885488510132
Unwhitening factors: 2.3603439331054688e-05
Smoothing and Normalization: 8.630752563476562e-05
Fit RMSE: 0.03314785378482746
parafac Test Against Ground Truth
Smoothing and Normalization: 8.606910705566406e-05
Fit RMSE: 0.0370202963607949
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999987, 1.0000000000000002]
M1: 0.005933523178100586
M2: 50.81582713127136
[[1.00000000e+00 2.46330734e-16]
 [2.81025203e-16 1.00000000e+00]]
W: 0.038023948669433594
Whiten X: 0.024339914321899414
Whiten M1: 3.075599670410156e-05
Parafac M3: 0.055524349212646484
Parafac Decomposition: 14.037915229797363
Unwhitening parafac factors: 3.838539123535156e-05
Initialization
[[-0.80615547  0.59170378]
 [ 0.59170378  0.80615547]]
SGD Calc: 23.279846668243408
Unwhitening factors: 2.8371810913085938e-05
Smoothing and Normalization: 0.00011277198791503906
Fit RMSE: 0.02750195215752043
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00011920928955078125
Fit RMSE: 0.02810191042619463
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0000000000000013]
M1: 0.002099275588989258
M2: 7.121567726135254
[[1.00000000e+00 4.02455846e-16]
 [2.91433544e-16 1.00000000e+00]]
W: 0.004980325698852539
Whiten X: 0.006332874298095703
Whiten M1: 1.5020370483398438e-05
Parafac M3: 0.05594182014465332
Parafac Decomposition: 14.100095987319946
Unwhitening parafac factors: 1.5735626220703125e-05
Initialization
[[-0.5103384  -0.85997367]
 [-0.85997367  0.5103384 ]]
SGD Calc: 23.20264983177185
Unwhitening factors: 1.5974044799804688e-05
Smoothing and Normalization: 4.57763671875e-05
Fit RMSE: 0.0471102527246561
parafac Test Against Ground Truth
Smoothing and Normalization: 6.532669067382812e-05
Fit RMSE: 0.04685185153253559
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999999, 0.9999999999999997]
M1: 0.0040547847747802734
M2: 22.553478956222534
[[1.00000000e+00 2.08166817e-16]
 [1.52655666e-16 1.00000000e+00]]
W: 0.018912315368652344
Whiten X: 0.01614212989807129
Whiten M1: 3.147125244140625e-05
Parafac M3: 0.05607199668884277
Parafac Decomposition: 14.051192283630371
Unwhitening parafac factors: 2.4080276489257812e-05
Initialization
[[-0.56903284 -0.8223148 ]
 [-0.8223148   0.56903284]]
SGD Calc: 22.510481595993042
Unwhitening factors: 2.002716064453125e-05
Smoothing and Normalization: 6.008148193359375e-05
Fit RMSE: 0.03416624987075463
parafac Test Against Ground Truth
Smoothing and Normalization: 7.343292236328125e-05
Fit RMSE: 0.03337124256100477
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0, 1.0000000000000018]
M1: 0.0062541961669921875
M2: 50.882309675216675
[[ 1.00000000e+00 -2.04697370e-16]
 [-1.11022302e-16  1.00000000e+00]]
W: 0.02951192855834961
Whiten X: 0.02437758445739746
Whiten M1: 3.0517578125e-05
Parafac M3: 0.05594205856323242
Parafac Decomposition: 13.388729572296143
Unwhitening parafac factors: 2.3603439331054688e-05
Initialization
[[-0.99887218 -0.04748009]
 [-0.04748009  0.99887218]]
SGD Calc: 22.404257774353027
Unwhitening factors: 2.3126602172851562e-05
Smoothing and Normalization: 9.775161743164062e-05
Fit RMSE: 0.028088411295405884
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010180473327636719
Fit RMSE: 0.030557468683984998
 Test Against Ground Truth
[[array([0.98909501]), array([2.39514833e-18])], [array([0.99934703]), array([0.9993257])], [array([4.94254595e-17]), array([0.98343582])], [array([0.98281798]), array([-3.68605163e-17])], [array([0.98143582]), array([4.11595157e-17])], [array([0.99915439]), array([0.99908679])], [array([0.99195684]), array([1.28562449e-17])], [array([0.99934399]), array([0.99917386])], [array([0.99869802]), array([0.99854333])], [array([-8.27833992e-18]), array([0.98322769])], [array([0.98445124]), array([2.09846638e-17])], [array([0.99902372]), array([0.99863796])], [array([0.99967019]), array([0.99964381])], [array([-2.10839191e-17]), array([0.98335711])], [array([0.99920929]), array([0.99894425])], [array([-1.62277681e-17]), array([0.98807081])], [array([0.98497166]), array([-2.66829388e-18])], [array([-0.02938503]), array([0.99911857])], [array([0.99962092]), array([0.99967449])], [array([0.98534825]), array([4.99529801e-18])], [array([0.98702841]), array([-4.13090441e-17])], [array([0.99970243]), array([0.99957644])], [array([3.78036837e-18]), array([0.9859813])], [array([-2.55218699e-17]), array([0.98034643])], [array([0.98037758]), array([2.59308958e-17])], [array([-0.04369207]), array([0.99905969])], [array([-3.00770079e-17]), array([0.98939252])], [array([0.99953983]), array([0.99972395])], [array([0.99919732]), array([0.99924041])], [array([0.13869955]), array([0.99863564])]]
Traceback (most recent call last):
  File "generate_tables.py", line 609, in <module>
    main()
  File "generate_tables.py", line 599, in main
    csvwriter.writerow([str(i), str(vocab), str(lr), str(theta), str(acc_parafac[j][0]), str(acc_parafac[j][1]), str(acc_uncentered[j][0]), str(acc_uncentered[j][1]), str(acc_centered[j][0]), str(acc_centered[j][1])])
IndexError: list index out of range
SGD Calc: 22.91983675956726
Unwhitening factors: 1.8358230590820312e-05
Smoothing and Normalization: 8.511543273925781e-05
Fit RMSE: 0.03330600168876202
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010442733764648438
Fit RMSE: 0.03492335459240921
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999991, 0.9999999999999992]
M1: 0.0060842037200927734
M2: 50.516010999679565
[[ 1.00000000e+00 -3.88578059e-16]
 [-2.63677968e-16  1.00000000e+00]]
W: 0.029616117477416992
Whiten X: 0.02391505241394043
Whiten M1: 3.0279159545898438e-05
Parafac M3: 0.05533337593078613
Parafac Decomposition: 13.063055276870728
Unwhitening parafac factors: 2.5033950805664062e-05
Initialization
[[-0.50626586 -0.86237745]
 [-0.86237745  0.50626586]]
SGD Calc: 21.546932697296143
Unwhitening factors: 2.193450927734375e-05
Smoothing and Normalization: 0.00011730194091796875
Fit RMSE: 0.028120229615258643
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00012087821960449219
Fit RMSE: 0.027350256727355547
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999993, 0.9999999999999993]
M1: 0.002299070358276367
M2: 6.9796857833862305
[[ 1.00000000e+00 -6.93889390e-18]
 [ 4.85722573e-17  1.00000000e+00]]
W: 0.005120277404785156
Whiten X: 0.006666421890258789
Whiten M1: 2.002716064453125e-05
Parafac M3: 0.05568265914916992
Parafac Decomposition: 13.172208786010742
Unwhitening parafac factors: 1.7642974853515625e-05
Initialization
[[-0.91956025 -0.39294905]
 [-0.39294905  0.91956025]]
SGD Calc: 22.216443300247192
Unwhitening factors: 1.71661376953125e-05
Smoothing and Normalization: 5.841255187988281e-05
Fit RMSE: 0.04679401914260355
parafac Test Against Ground Truth
Smoothing and Normalization: 7.152557373046875e-05
Fit RMSE: 0.04599083274624246
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999993, 1.0000000000000004]
M1: 0.0040934085845947266
M2: 20.69144034385681
[[1.00000000e+00 9.71445147e-17]
 [2.49800181e-16 1.00000000e+00]]
W: 0.027753114700317383
Whiten X: 0.03261923789978027
Whiten M1: 2.765655517578125e-05
Parafac M3: 0.05712699890136719
Parafac Decomposition: 13.136208772659302
Unwhitening parafac factors: 2.0503997802734375e-05
Initialization
[[-0.54681598 -0.83725282]
 [-0.83725282  0.54681598]]
SGD Calc: 21.914002418518066
Unwhitening factors: 1.9788742065429688e-05
Smoothing and Normalization: 6.341934204101562e-05
Fit RMSE: 0.03280464889983478
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010132789611816406
Fit RMSE: 0.034706315436038575
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.000000000000002, 1.0000000000000004]
M1: 0.006127357482910156
M2: 51.503865003585815
[[1.00000000e+00 4.16333634e-16]
 [3.60822483e-16 1.00000000e+00]]
W: 0.029024124145507812
Whiten X: 0.024040937423706055
Whiten M1: 2.8848648071289062e-05
Parafac M3: 0.05543041229248047
Parafac Decomposition: 13.57754373550415
Unwhitening parafac factors: 2.288818359375e-05
Initialization
[[-0.44660114  0.89473316]
 [ 0.89473316  0.44660114]]
SGD Calc: 22.841145277023315
Unwhitening factors: 2.3365020751953125e-05
Smoothing and Normalization: 0.00010156631469726562
Fit RMSE: 0.027185607671797028
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010466575622558594
Fit RMSE: 0.028897157198697863
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 1.0]
M1: 0.002185821533203125
M2: 7.1028008460998535
[[ 1.00000000e+00 -9.71445147e-17]
 [-1.31838984e-16  1.00000000e+00]]
W: 0.005108833312988281
Whiten X: 0.006719112396240234
Whiten M1: 1.9073486328125e-05
Parafac M3: 0.05570411682128906
Parafac Decomposition: 13.126510858535767
Unwhitening parafac factors: 1.8358230590820312e-05
Initialization
[[-0.84990187 -0.52694099]
 [-0.52694099  0.84990187]]
SGD Calc: 21.725506067276
Unwhitening factors: 1.6927719116210938e-05
Smoothing and Normalization: 4.506111145019531e-05
Fit RMSE: 0.04462052942489577
parafac Test Against Ground Truth
Smoothing and Normalization: 6.890296936035156e-05
Fit RMSE: 0.04973099653412484
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999989, 0.9999999999999997]
M1: 0.004275321960449219
M2: 20.392898321151733
[[ 1.00000000e+00 -5.57279917e-17]
 [ 2.16840434e-17  1.00000000e+00]]
W: 0.017943620681762695
Whiten X: 0.01678323745727539
Whiten M1: 2.6226043701171875e-05
Parafac M3: 0.05535101890563965
Parafac Decomposition: 13.236198425292969
Unwhitening parafac factors: 2.09808349609375e-05
Initialization
[[-0.4126581  -0.91088599]
 [-0.91088599  0.4126581 ]]
SGD Calc: 21.848944425582886
Unwhitening factors: 1.9550323486328125e-05
Smoothing and Normalization: 8.082389831542969e-05
Fit RMSE: 0.03354413619600652
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010657310485839844
Fit RMSE: 0.03517971607239552
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0, 1.0]
M1: 0.006026268005371094
M2: 53.31176471710205
[[ 1.0000000e+00  1.2490009e-16]
 [-6.9388939e-17  1.0000000e+00]]
W: 0.029070138931274414
Whiten X: 0.02385854721069336
Whiten M1: 3.0040740966796875e-05
Parafac M3: 0.05562710762023926
Parafac Decomposition: 13.776641607284546
Unwhitening parafac factors: 2.9802322387695312e-05
Initialization
[[-0.98781136  0.15565575]
 [ 0.15565575  0.98781136]]
SGD Calc: 22.474700450897217
Unwhitening factors: 2.4557113647460938e-05
Smoothing and Normalization: 9.608268737792969e-05
Fit RMSE: 0.02729280138783227
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00012040138244628906
Fit RMSE: 0.028384054110568255
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999998]
M1: 0.0021140575408935547
M2: 7.115682363510132
[[ 1.00000000e+00 -6.24500451e-17]
 [ 2.08166817e-16  1.00000000e+00]]
W: 0.007129669189453125
Whiten X: 0.006394147872924805
Whiten M1: 2.3126602172851562e-05
Parafac M3: 0.056139469146728516
Parafac Decomposition: 13.090748310089111
Unwhitening parafac factors: 1.9073486328125e-05
Initialization
[[-0.93395317 -0.35739542]
 [-0.35739542  0.93395317]]
SGD Calc: 22.24187397956848
Unwhitening factors: 2.193450927734375e-05
Smoothing and Normalization: 6.532669067382812e-05
Fit RMSE: 0.04947317858232315
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0001506805419921875
Fit RMSE: 0.05039798102398093
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000004]
M1: 0.004116058349609375
M2: 20.029587984085083
[[ 1.00000000e+00 -1.35308431e-16]
 [-1.73472348e-17  1.00000000e+00]]
W: 0.012881278991699219
Whiten X: 0.016765594482421875
Whiten M1: 3.24249267578125e-05
Parafac M3: 0.05603671073913574
Parafac Decomposition: 13.1718430519104
Unwhitening parafac factors: 2.002716064453125e-05
Initialization
[[-0.39361534  0.91927524]
 [ 0.91927524  0.39361534]]
SGD Calc: 21.549857139587402
Unwhitening factors: 2.09808349609375e-05
Smoothing and Normalization: 6.0558319091796875e-05
Fit RMSE: 0.03324933263885258
parafac Test Against Ground Truth
Smoothing and Normalization: 8.249282836914062e-05
Fit RMSE: 0.03484364595567681
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999988, 0.9999999999999992]
M1: 0.00590968132019043
M2: 49.08197021484375
[[1.00000000e+00 3.88578059e-16]
 [4.71844785e-16 1.00000000e+00]]
W: 0.03014993667602539
Whiten X: 0.02316141128540039
Whiten M1: 3.62396240234375e-05
Parafac M3: 0.05601668357849121
Parafac Decomposition: 13.13974642753601
Unwhitening parafac factors: 2.3603439331054688e-05
Initialization
[[-0.80061925 -0.59917344]
 [-0.59917344  0.80061925]]
SGD Calc: 20.98244571685791
Unwhitening factors: 1.8835067749023438e-05
Smoothing and Normalization: 8.0108642578125e-05
Fit RMSE: 0.027187323926726074
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010585784912109375
Fit RMSE: 0.02722144701831485
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.000000000000001, 0.9999999999999991]
M1: 0.001950979232788086
M2: 6.801595449447632
[[ 1.00000000e+00 -2.35922393e-16]
 [-1.80411242e-16  1.00000000e+00]]
W: 0.004935503005981445
Whiten X: 0.005879640579223633
Whiten M1: 2.1696090698242188e-05
Parafac M3: 0.05637407302856445
Parafac Decomposition: 12.911375045776367
Unwhitening parafac factors: 1.7642974853515625e-05
Initialization
[[-0.99909222  0.0425998 ]
 [ 0.0425998   0.99909222]]
SGD Calc: 20.80827236175537
Unwhitening factors: 1.52587890625e-05
Smoothing and Normalization: 4.267692565917969e-05
Fit RMSE: 0.048956166397949606
parafac Test Against Ground Truth
Smoothing and Normalization: 8.249282836914062e-05
Fit RMSE: 0.048375077657591234
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 1.0000000000000009]
M1: 0.004105806350708008
M2: 19.722158432006836
[[1.00000000e+00 4.16333634e-17]
 [1.66533454e-16 1.00000000e+00]]
W: 0.013021707534790039
Whiten X: 0.01638031005859375
Whiten M1: 3.504753112792969e-05
Parafac M3: 0.056047677993774414
Parafac Decomposition: 12.729225635528564
Unwhitening parafac factors: 2.0265579223632812e-05
Initialization
[[-0.97122866 -0.23814887]
 [-0.23814887  0.97122866]]
SGD Calc: 20.771398544311523
Unwhitening factors: 1.7404556274414062e-05
Smoothing and Normalization: 6.246566772460938e-05
Fit RMSE: 0.03380038751014864
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010538101196289062
Fit RMSE: 0.03387407779025489
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999998, 0.9999999999999997]
M1: 0.005905628204345703
M2: 48.62910199165344
[[1.00000000e+00 7.63278329e-17]
 [9.71445147e-17 1.00000000e+00]]
W: 0.030274152755737305
Whiten X: 0.023074626922607422
Whiten M1: 3.528594970703125e-05
Parafac M3: 0.05561494827270508
Parafac Decomposition: 12.675641536712646
Unwhitening parafac factors: 2.4318695068359375e-05
Initialization
[[-0.94328704 -0.33197824]
 [-0.33197824  0.94328704]]
SGD Calc: 20.72732138633728
Unwhitening factors: 2.1457672119140625e-05
Smoothing and Normalization: 8.082389831542969e-05
Fit RMSE: 0.026905929655716896
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010538101196289062
Fit RMSE: 0.026966539331692636
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999992]
M1: 0.0021114349365234375
M2: 6.925172567367554
[[ 1.00000000e+00 -5.55111512e-17]
 [-2.08166817e-17  1.00000000e+00]]
W: 0.004772186279296875
Whiten X: 0.005735158920288086
Whiten M1: 2.2172927856445312e-05
Parafac M3: 0.055614471435546875
Parafac Decomposition: 12.619122743606567
Unwhitening parafac factors: 1.5974044799804688e-05
Initialization
[[-0.88657795  0.46257922]
 [ 0.46257922  0.88657795]]
SGD Calc: 20.880050897598267
Unwhitening factors: 1.4066696166992188e-05
Smoothing and Normalization: 4.220008850097656e-05
Fit RMSE: 0.048042398388771966
parafac Test Against Ground Truth
Smoothing and Normalization: 6.556510925292969e-05
Fit RMSE: 0.050933769645459745
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0000000000000018]
M1: 0.0041751861572265625
M2: 19.83133363723755
[[ 1.00000000e+00 -2.60208521e-16]
 [-1.29236899e-16  1.00000000e+00]]
W: 0.012729883193969727
Whiten X: 0.01653122901916504
Whiten M1: 3.600120544433594e-05
Parafac M3: 0.05556082725524902
Parafac Decomposition: 12.64456558227539
Unwhitening parafac factors: 1.9788742065429688e-05
Initialization
[[-0.538296   -0.84275585]
 [-0.84275585  0.538296  ]]
SGD Calc: 20.94731640815735
Unwhitening factors: 1.9788742065429688e-05
Smoothing and Normalization: 6.031990051269531e-05
Fit RMSE: 0.03320354130826131
parafac Test Against Ground Truth
Smoothing and Normalization: 8.893013000488281e-05
Fit RMSE: 0.033103641403714086
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999974, 0.9999999999999983]
M1: 0.005980730056762695
M2: 48.71933078765869
[[ 1.00000000e+00 -2.22044605e-16]
 [-1.66533454e-16  1.00000000e+00]]
W: 0.029452800750732422
Whiten X: 0.023454904556274414
Whiten M1: 3.1948089599609375e-05
Parafac M3: 0.055768728256225586
Parafac Decomposition: 12.838621139526367
Unwhitening parafac factors: 1.9073486328125e-05
Initialization
[[-0.373946   -0.92745048]
 [-0.92745048  0.373946  ]]
SGD Calc: 20.98277449607849
Unwhitening factors: 1.9788742065429688e-05
Smoothing and Normalization: 7.271766662597656e-05
Fit RMSE: 0.028450581640073625
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010395050048828125
Fit RMSE: 0.027790027892216573
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999996, 1.0]
M1: 0.0020575523376464844
M2: 6.882238149642944
[[ 1.00000000e+00  3.46944695e-17]
 [-1.38777878e-17  1.00000000e+00]]
W: 0.00503849983215332
Whiten X: 0.005750179290771484
Whiten M1: 2.002716064453125e-05
Parafac M3: 0.05553579330444336
Parafac Decomposition: 12.716735124588013
Unwhitening parafac factors: 2.0503997802734375e-05
Initialization
[[-0.68075246 -0.73251354]
 [-0.73251354  0.68075246]]
SGD Calc: 21.583895683288574
Unwhitening factors: 1.33514404296875e-05
Smoothing and Normalization: 3.933906555175781e-05
Fit RMSE: 0.04959920460687307
parafac Test Against Ground Truth
Smoothing and Normalization: 5.1021575927734375e-05
Fit RMSE: 0.047115219972730465
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999986, 1.0000000000000002]
M1: 0.004138469696044922
M2: 19.970479488372803
[[1.00000000e+00 5.13478149e-16]
 [5.20417043e-16 1.00000000e+00]]
W: 0.012350320816040039
Whiten X: 0.016470670700073242
Whiten M1: 3.5762786865234375e-05
Parafac M3: 0.05536389350891113
Parafac Decomposition: 12.728874921798706
Unwhitening parafac factors: 2.002716064453125e-05
Initialization
[[-0.89144156 -0.45313568]
 [-0.45313568  0.89144156]]
SGD Calc: 21.34173846244812
Unwhitening factors: 1.811981201171875e-05
Smoothing and Normalization: 6.29425048828125e-05
Fit RMSE: 0.030202729737781504
parafac Test Against Ground Truth
Smoothing and Normalization: 8.463859558105469e-05
Fit RMSE: 0.03418953553822174
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000013, 1.0000000000000009]
M1: 0.005954265594482422
M2: 48.79478406906128
[[1.00000000e+00 4.16333634e-16]
 [3.74700271e-16 1.00000000e+00]]
W: 0.029139995574951172
Whiten X: 0.023363113403320312
Whiten M1: 3.5762786865234375e-05
Parafac M3: 0.056140899658203125
Parafac Decomposition: 12.802285432815552
Unwhitening parafac factors: 3.314018249511719e-05
Initialization
[[-0.35207129 -0.93597319]
 [-0.93597319  0.35207129]]
SGD Calc: 21.07561182975769
Unwhitening factors: 2.47955322265625e-05
Smoothing and Normalization: 8.654594421386719e-05
Fit RMSE: 0.02778941493973344
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010442733764648438
Fit RMSE: 0.026784862786122438
 Test Against Ground Truth
[[array([0.9355971]), array([-4.55400068e-18])], [array([0.99897083]), array([0.99923727])], [array([0.99898274]), array([0.99882955])], [array([0.99960636]), array([0.99950002])], [array([0.99916041]), array([0.99913724])], [array([0.99095216]), array([0.98926568])], [array([0.99965063]), array([0.99984688])], [array([0.99923173]), array([0.99919694])], [array([-6.40564215e-18]), array([0.93099432])], [array([0.99979223]), array([0.99971881])], [array([0.92721193]), array([1.91718491e-18])], [array([0.99889125]), array([0.99897699])], [array([0.9996441]), array([0.99957779])], [array([0.93483395]), array([-4.46934793e-17])], [array([2.35869448e-18]), array([0.94892316])], [array([-1.36448678e-17]), array([0.91799784])], [array([0.93882409]), array([-1.48014238e-17])], [array([4.09264988e-17]), array([0.94130065])], [array([9.17895419e-18]), array([0.93873273])], [array([0.95193525]), array([-1.70278093e-17])], [array([0.98991357]), array([0.98808933])], [array([0.99976544]), array([0.99972223])], [array([0.98122606]), array([0.98730875])], [array([0.98568361]), array([0.98694742])], [array([-3.25786852e-17]), array([0.94472104])], [array([0.98620789]), array([0.98394185])], [array([0.99897624]), array([0.99879311])], [array([0.99969697]), array([0.99956783])], [array([-1.99879333e-17]), array([0.92625808])], [array([0.9988045]), array([0.99880334])]]
Traceback (most recent call last):
  File "generate_tables.py", line 609, in <module>
    main()
  File "generate_tables.py", line 599, in main
    csvwriter.writerow([str(i), str(vocab), str(lr), str(theta), str(acc_parafac[j][0]), str(acc_parafac[j][1]), str(acc_uncentered[j][0]), str(acc_uncentered[j][1]), str(acc_centered[j][0]), str(acc_centered[j][1])])
IndexError: list index out of range
new version
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000002]
M1: 0.002210855484008789
M2: 6.710667133331299
[[1.00000000e+00 1.24900090e-16]
 [9.71445147e-17 1.00000000e+00]]
W: 0.005189180374145508
Whiten X: 0.009921789169311523
Whiten M1: 3.504753112792969e-05
Parafac M3: 0.05834770202636719
Parafac Decomposition: 12.658123970031738
Unwhitening parafac factors: 1.0967254638671875e-05
Initialization
[[-0.8369542   0.54727294]
 [ 0.54727294  0.8369542 ]]
SGD Calc: 23.89223003387451
Unwhitening factors: 1.4543533325195312e-05
Smoothing and Normalization: 5.14984130859375e-05
Fit RMSE: 0.04822153723566104
parafac Test Against Ground Truth
Smoothing and Normalization: 5.817413330078125e-05
Fit RMSE: 0.04713480568087965
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0]
M1: 0.00451207160949707
M2: 19.84970784187317
[[1.00000000e+00 1.24900090e-16]
 [1.94289029e-16 1.00000000e+00]]
W: 0.014020681381225586
Whiten X: 0.016729354858398438
Whiten M1: 3.600120544433594e-05
Parafac M3: 0.05750560760498047
Parafac Decomposition: 12.83958101272583
Unwhitening parafac factors: 2.0742416381835938e-05
Initialization
[[-0.71352412  0.70063066]
 [ 0.70063066  0.71352412]]
SGD Calc: 84.09448289871216
Unwhitening factors: 1.9788742065429688e-05
Smoothing and Normalization: 6.604194641113281e-05
Fit RMSE: 0.03330127709891975
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010967254638671875
Fit RMSE: 0.035173125298813196
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999996, 1.0000000000000007]
M1: 0.006339311599731445
M2: 49.25886845588684
[[ 1.00000000e+00 -3.74700271e-16]
 [-2.91433544e-16  1.00000000e+00]]
W: 0.030087709426879883
Whiten X: 0.023639202117919922
Whiten M1: 3.647804260253906e-05
Parafac M3: 0.05824017524719238
Parafac Decomposition: 12.658342361450195
Unwhitening parafac factors: 2.288818359375e-05
Initialization
[[-0.99368365 -0.11221766]
 [-0.11221766  0.99368365]]
SGD Calc: 81.67106866836548
Unwhitening factors: 2.288818359375e-05
Smoothing and Normalization: 8.535385131835938e-05
Fit RMSE: 0.02767711612078506
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00012040138244628906
Fit RMSE: 0.029111378881737184
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999987, 1.0000000000000002]
M1: 0.0022020339965820312
M2: 6.752421140670776
[[1.00000000e+00 9.02056208e-17]
 [2.77555756e-17 1.00000000e+00]]
W: 0.004027843475341797
Whiten X: 0.005957841873168945
Whiten M1: 1.8596649169921875e-05
Parafac M3: 0.057904720306396484
Parafac Decomposition: 12.849990129470825
Unwhitening parafac factors: 1.621246337890625e-05
Initialization
[[-0.99684383  0.07938755]
 [ 0.07938755  0.99684383]]
SGD Calc: 21.944812297821045
Unwhitening factors: 1.6689300537109375e-05
Smoothing and Normalization: 4.9114227294921875e-05
Fit RMSE: 0.04895837758849863
parafac Test Against Ground Truth
Smoothing and Normalization: 6.699562072753906e-05
Fit RMSE: 0.04701152558392551
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999991, 0.9999999999999996]
M1: 0.004476308822631836
M2: 19.902621507644653
[[ 1.00000000e+00 -1.38777878e-16]
 [-1.24900090e-16  1.00000000e+00]]
W: 0.013052940368652344
Whiten X: 0.016581296920776367
Whiten M1: 3.719329833984375e-05
Parafac M3: 0.0576627254486084
Parafac Decomposition: 12.658376693725586
Unwhitening parafac factors: 2.0265579223632812e-05
Initialization
[[-0.23482711 -0.97203715]
 [-0.97203715  0.23482711]]
SGD Calc: 21.40244483947754
Unwhitening factors: 1.7881393432617188e-05
Smoothing and Normalization: 5.7697296142578125e-05
Fit RMSE: 0.03317245927793892
parafac Test Against Ground Truth
Smoothing and Normalization: 8.654594421386719e-05
Fit RMSE: 0.03251533096401753
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999998]
M1: 0.006183147430419922
M2: 49.42074251174927
[[ 1.00000000e+00 -5.03611909e-17]
 [ 5.56737816e-17  1.00000000e+00]]
W: 0.0294034481048584
Whiten X: 0.023463726043701172
Whiten M1: 3.719329833984375e-05
Parafac M3: 0.05823826789855957
Parafac Decomposition: 12.627026557922363
Unwhitening parafac factors: 2.3365020751953125e-05
Initialization
[[-0.49833695 -0.86698344]
 [-0.86698344  0.49833695]]
SGD Calc: 117.27617335319519
Unwhitening factors: 2.8133392333984375e-05
Smoothing and Normalization: 9.894371032714844e-05
Fit RMSE: 0.026923542921433886
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010418891906738281
Fit RMSE: 0.028233304350550804
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999991, 1.0]
M1: 0.0022537708282470703
M2: 6.872545480728149
[[ 1.00000000e+00 -3.53883589e-16]
 [-4.05925293e-16  1.00000000e+00]]
W: 0.004030704498291016
Whiten X: 0.005766630172729492
Whiten M1: 1.9550323486328125e-05
Parafac M3: 0.057895660400390625
Parafac Decomposition: 12.695722579956055
Unwhitening parafac factors: 1.5020370483398438e-05
Initialization
[[-0.74466568 -0.66743766]
 [-0.66743766  0.74466568]]
SGD Calc: 129.49599027633667
Unwhitening factors: 1.5497207641601562e-05
Smoothing and Normalization: 4.291534423828125e-05
Fit RMSE: 0.04646921652095188
parafac Test Against Ground Truth
Smoothing and Normalization: 8.0108642578125e-05
Fit RMSE: 0.04894036941846495
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000016, 1.0000000000000018]
M1: 0.004755496978759766
M2: 19.884737014770508
[[1.0000000e+00 4.0939474e-16]
 [4.0939474e-16 1.0000000e+00]]
W: 0.013033390045166016
Whiten X: 0.016867637634277344
Whiten M1: 3.24249267578125e-05
Parafac M3: 0.05771780014038086
Parafac Decomposition: 13.325522184371948
Unwhitening parafac factors: 2.1696090698242188e-05
Initialization
[[-0.96393609 -0.26613383]
 [-0.26613383  0.96393609]]
SGD Calc: 22.278640031814575
Unwhitening factors: 1.8835067749023438e-05
Smoothing and Normalization: 6.0558319091796875e-05
Fit RMSE: 0.03499314081952364
parafac Test Against Ground Truth
Smoothing and Normalization: 8.535385131835938e-05
Fit RMSE: 0.033312667523878674
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0]
M1: 0.006269931793212891
M2: 49.20860147476196
[[1.00000000e+00 2.67147415e-16]
 [8.67361738e-17 1.00000000e+00]]
W: 0.0278928279876709
Whiten X: 0.023760080337524414
Whiten M1: 3.5762786865234375e-05
Parafac M3: 0.057604312896728516
Parafac Decomposition: 13.091656684875488
Unwhitening parafac factors: 2.6226043701171875e-05
Initialization
[[-0.21073655 -0.97754289]
 [-0.97754289  0.21073655]]
SGD Calc: 22.289794445037842
Unwhitening factors: 1.9788742065429688e-05
Smoothing and Normalization: 7.367134094238281e-05
Fit RMSE: 0.028609641789600862
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010323524475097656
Fit RMSE: 0.027474203219143328
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000002, 0.9999999999999996]
M1: 0.0023956298828125
M2: 6.963540315628052
[[1.00000000e+00 3.46944695e-17]
 [1.04083409e-17 1.00000000e+00]]
W: 0.004060506820678711
Whiten X: 0.0057985782623291016
Whiten M1: 2.3126602172851562e-05
Parafac M3: 0.05802154541015625
Parafac Decomposition: 13.257794857025146
Unwhitening parafac factors: 1.6927719116210938e-05
Initialization
[[-0.15721867 -0.98756382]
 [-0.98756382  0.15721867]]
SGD Calc: 99.70453643798828
Unwhitening factors: 1.4781951904296875e-05
Smoothing and Normalization: 4.363059997558594e-05
Fit RMSE: 0.04707062588114617
parafac Test Against Ground Truth
Smoothing and Normalization: 8.130073547363281e-05
Fit RMSE: 0.048792890314516744
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000002]
M1: 0.004477977752685547
M2: 19.818290948867798
[[ 1.00000000e+00  5.55111512e-17]
 [-1.24900090e-16  1.00000000e+00]]
W: 0.014297008514404297
Whiten X: 0.016511917114257812
Whiten M1: 3.6716461181640625e-05
Parafac M3: 0.05774855613708496
Parafac Decomposition: 12.628207206726074
Unwhitening parafac factors: 2.2411346435546875e-05
Initialization
[[-0.62810429 -0.77812917]
 [-0.77812917  0.62810429]]
SGD Calc: 21.681184768676758
Unwhitening factors: 2.3603439331054688e-05
Smoothing and Normalization: 7.152557373046875e-05
Fit RMSE: 0.03503647127992878
parafac Test Against Ground Truth
Smoothing and Normalization: 9.012222290039062e-05
Fit RMSE: 0.033524538587683235
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000013, 1.0000000000000013]
M1: 0.006288051605224609
M2: 49.583648920059204
[[ 1.00000000e+00 -4.57966998e-16]
 [-4.85722573e-16  1.00000000e+00]]
W: 0.02741265296936035
Whiten X: 0.023543834686279297
Whiten M1: 3.814697265625e-05
Parafac M3: 0.05794715881347656
Parafac Decomposition: 13.295398950576782
Unwhitening parafac factors: 2.5033950805664062e-05
Initialization
[[-0.83897764 -0.54416589]
 [-0.54416589  0.83897764]]
SGD Calc: 42.72970938682556
Unwhitening factors: 2.3126602172851562e-05
Smoothing and Normalization: 8.082389831542969e-05
Fit RMSE: 0.027298898890612138
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010895729064941406
Fit RMSE: 0.027317366297141037
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 0.9999999999999998]
M1: 0.0022444725036621094
M2: 6.7215070724487305
[[ 1.00000000e+00 -1.73472348e-16]
 [-4.85722573e-17  1.00000000e+00]]
W: 0.004241943359375
Whiten X: 0.00574183464050293
Whiten M1: 2.09808349609375e-05
Parafac M3: 0.05776691436767578
Parafac Decomposition: 12.644491910934448
Unwhitening parafac factors: 1.71661376953125e-05
Initialization
[[-0.25896368  0.96588706]
 [ 0.96588706  0.25896368]]
SGD Calc: 99.53991961479187
Unwhitening factors: 1.5497207641601562e-05
Smoothing and Normalization: 4.1484832763671875e-05
Fit RMSE: 0.04791051004323614
parafac Test Against Ground Truth
Smoothing and Normalization: 5.221366882324219e-05
Fit RMSE: 0.05004023503194912
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0]
M1: 0.00446772575378418
M2: 19.855597257614136
[[ 1.00000000e+00 -3.87710697e-16]
 [-4.01588485e-16  1.00000000e+00]]
W: 0.013047933578491211
Whiten X: 0.0164492130279541
Whiten M1: 3.409385681152344e-05
Parafac M3: 0.05768585205078125
Parafac Decomposition: 12.58826732635498
Unwhitening parafac factors: 1.9788742065429688e-05
Initialization
[[-0.51130671 -0.8593983 ]
 [-0.8593983   0.51130671]]
SGD Calc: 57.71681332588196
Unwhitening factors: 2.0265579223632812e-05
Smoothing and Normalization: 6.461143493652344e-05
Fit RMSE: 0.031013487448528273
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010251998901367188
Fit RMSE: 0.03370977281578271
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999973]
M1: 0.006308078765869141
M2: 49.269057512283325
[[1.00000000e+00 1.52655666e-16]
 [1.87350135e-16 1.00000000e+00]]
W: 0.02748894691467285
Whiten X: 0.02352309226989746
Whiten M1: 3.790855407714844e-05
Parafac M3: 0.05856895446777344
Parafac Decomposition: 13.198285579681396
Unwhitening parafac factors: 1.9311904907226562e-05
Initialization
[[-0.99810987  0.06145472]
 [ 0.06145472  0.99810987]]
SGD Calc: 105.61052393913269
Unwhitening factors: 2.8133392333984375e-05
Smoothing and Normalization: 9.942054748535156e-05
Fit RMSE: 0.027444776576565737
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00012922286987304688
Fit RMSE: 0.02906664425320598
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0]
M1: 0.0020208358764648438
M2: 6.8328142166137695
[[ 1.00000000e+00 -5.96744876e-16]
 [-7.84095011e-16  1.00000000e+00]]
W: 0.004274845123291016
Whiten X: 0.005795717239379883
Whiten M1: 2.9325485229492188e-05
Parafac M3: 0.05825614929199219
Parafac Decomposition: 12.730515241622925
Unwhitening parafac factors: 1.7642974853515625e-05
Initialization
[[-0.82745535 -0.56153152]
 [-0.56153152  0.82745535]]
SGD Calc: 117.54411458969116
Unwhitening factors: 1.6689300537109375e-05
Smoothing and Normalization: 4.3392181396484375e-05
Fit RMSE: 0.04165244674435553
parafac Test Against Ground Truth
Smoothing and Normalization: 6.67572021484375e-05
Fit RMSE: 0.04777565740130082
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.000000000000001, 1.000000000000001]
M1: 0.004484415054321289
M2: 19.955836534500122
[[ 1.00000000e+00  5.55111512e-17]
 [-6.93889390e-18  1.00000000e+00]]
W: 0.013077259063720703
Whiten X: 0.016647815704345703
Whiten M1: 3.504753112792969e-05
Parafac M3: 0.05767655372619629
Parafac Decomposition: 12.617368936538696
Unwhitening parafac factors: 2.1696090698242188e-05
Initialization
[[-0.44903426 -0.89351454]
 [-0.89351454  0.44903426]]
SGD Calc: 43.02804231643677
Unwhitening factors: 1.7881393432617188e-05
Smoothing and Normalization: 6.0558319091796875e-05
Fit RMSE: 0.03256934343291572
parafac Test Against Ground Truth
Smoothing and Normalization: 9.560585021972656e-05
Fit RMSE: 0.03252262333149601
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000007, 1.0000000000000004]
M1: 0.006270408630371094
M2: 49.50765132904053
[[ 1.00000000e+00  4.16333634e-17]
 [-3.81639165e-17  1.00000000e+00]]
W: 0.027983427047729492
Whiten X: 0.023751258850097656
Whiten M1: 3.814697265625e-05
Parafac M3: 0.05803871154785156
Parafac Decomposition: 12.615082025527954
Unwhitening parafac factors: 2.4080276489257812e-05
Initialization
[[-0.4157416  -0.90948278]
 [-0.90948278  0.4157416 ]]
SGD Calc: 21.42982053756714
Unwhitening factors: 2.3126602172851562e-05
Smoothing and Normalization: 7.462501525878906e-05
Fit RMSE: 0.02829635886675291
parafac Test Against Ground Truth
Smoothing and Normalization: 9.942054748535156e-05
Fit RMSE: 0.02751039911112949
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999999, 1.0000000000000004]
M1: 0.0021791458129882812
M2: 6.885883569717407
[[ 1.00000000e+00 -8.32667268e-17]
 [ 1.52655666e-16  1.00000000e+00]]
W: 0.004317522048950195
Whiten X: 0.005826234817504883
Whiten M1: 2.0742416381835938e-05
Parafac M3: 0.057602643966674805
Parafac Decomposition: 12.751823902130127
Unwhitening parafac factors: 1.621246337890625e-05
Initialization
[[-0.99881006  0.04876946]
 [ 0.04876946  0.99881006]]
SGD Calc: 96.6383810043335
Unwhitening factors: 1.5497207641601562e-05
Smoothing and Normalization: 4.410743713378906e-05
Fit RMSE: 0.048276106751917784
parafac Test Against Ground Truth
Smoothing and Normalization: 6.318092346191406e-05
Fit RMSE: 0.050073595721448164
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999994]
M1: 0.004453897476196289
M2: 19.889485597610474
[[ 1.00000000e+00 -3.19189120e-16]
 [-1.56125113e-16  1.00000000e+00]]
W: 0.0125579833984375
Whiten X: 0.01654982566833496
Whiten M1: 3.695487976074219e-05
Parafac M3: 0.05764460563659668
Parafac Decomposition: 12.590879201889038
Unwhitening parafac factors: 2.0742416381835938e-05
Initialization
[[-0.77316526 -0.63420461]
 [-0.63420461  0.77316526]]
SGD Calc: 109.2699863910675
Unwhitening factors: 2.0265579223632812e-05
Smoothing and Normalization: 6.127357482910156e-05
Fit RMSE: 0.0333317927197605
parafac Test Against Ground Truth
Smoothing and Normalization: 8.654594421386719e-05
Fit RMSE: 0.03477939467210547
 Test Against Ground Truth
Traceback (most recent call last):
  File "generate_tables.py", line 610, in <module>
    main()
  File "generate_tables.py", line 531, in main
    create_data(vocab=vocab, seed=seed_arr[j])
IndexError: list index out of range
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
0         every traitor conservative value question whet...
1         suspect fda withhold authorization plasma covi...
2         dianne offer condolence prayer president famil...
3         last twitter censored suggesting child far le ...
4         german study month found child actually slow t...
                                ...                        
541941    year ago long islander teddy roosevelt nobel p...
541942    honest negotiation potus dems border security ...
541943    incredible represent hardworking long islander...
541944    rep johnson comparing pres trump rep sherman f...
541945                        wishing loved happy safe best
Name: tweet, Length: 541946, dtype: object
Traceback (most recent call last):
  File "coherence_comparison.py", line 533, in <module>
    main()
  File "coherence_comparison.py", line 401, in main
    texts, vocab = get_congress_data()
  File "coherence_comparison.py", line 133, in get_congress_data
    pickle.dump(tweets, open("data/preprocessed_congress_tweets.obj", wb))
NameError: name 'wb' is not defined
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
0         every traitor conservative value question whet...
1         suspect fda withhold authorization plasma covi...
2         dianne offer condolence prayer president famil...
3         last twitter censored suggesting child far le ...
4         german study month found child actually slow t...
                                ...                        
541941    year ago long islander teddy roosevelt nobel p...
541942    honest negotiation potus dems border security ...
541943    incredible represent hardworking long islander...
541944    rep johnson comparing pres trump rep sherman f...
541945                        wishing loved happy safe best
Name: tweet, Length: 541946, dtype: object
Traceback (most recent call last):
  File "coherence_comparison.py", line 533, in <module>
    main()
  File "coherence_comparison.py", line 401, in main
    texts, vocab = get_congress_data()
  File "coherence_comparison.py", line 135, in get_congress_data
    vectors = vectorizer.fit_transform(tweets).toarray()
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/sklearn/feature_extraction/text.py", line 1221, in fit_transform
    X, self.stop_words_ = self._limit_features(X, vocabulary,
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/sklearn/feature_extraction/text.py", line 1092, in _limit_features
    raise ValueError("After pruning, no terms remain. Try a lower"
ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
0         every traitor conservative value question whet...
1         suspect fda withhold authorization plasma covi...
2         dianne offer condolence prayer president famil...
3         last twitter censored suggesting child far le ...
4         german study month found child actually slow t...
                                ...                        
541941    year ago long islander teddy roosevelt nobel p...
541942    honest negotiation potus dems border security ...
541943    incredible represent hardworking long islander...
541944    rep johnson comparing pres trump rep sherman f...
541945                        wishing loved happy safe best
Name: tweet, Length: 541946, dtype: object
Traceback (most recent call last):
  File "coherence_comparison.py", line 533, in <module>
    main()
  File "coherence_comparison.py", line 401, in main
    texts, vocab = get_congress_data()
  File "coherence_comparison.py", line 135, in get_congress_data
    vectors = vectorizer.fit_transform(tweets).toarray()
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/sklearn/feature_extraction/text.py", line 1221, in fit_transform
    X, self.stop_words_ = self._limit_features(X, vocabulary,
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/sklearn/feature_extraction/text.py", line 1092, in _limit_features
    raise ValueError("After pruning, no terms remain. Try a lower"
ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
0         every traitor conservative value question whet...
1         suspect fda withhold authorization plasma covi...
2         dianne offer condolence prayer president famil...
3         last twitter censored suggesting child far le ...
4         german study month found child actually slow t...
                                ...                        
541941    year ago long islander teddy roosevelt nobel p...
541942    honest negotiation potus dems border security ...
541943    incredible represent hardworking long islander...
541944    rep johnson comparing pres trump rep sherman f...
541945                        wishing loved happy safe best
Name: tweet, Length: 541946, dtype: object
got data
(10, 128)
Centering time: 8.368492126464844e-05
Traceback (most recent call last):
  File "coherence_comparison.py", line 533, in <module>
    main()
  File "coherence_comparison.py", line 429, in main
    res, factors_tlda = gen_fit_0_20(texts, num_tops = n_tops, alpha_0 = alpha_0, n_iter_train = 10001)
  File "coherence_comparison.py", line 351, in gen_fit_0_20
    pca.fit(x_cent)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/pca.py", line 24, in fit
    self.pca.fit(X*tl.sqrt(self.alpha_0+1))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/sklearn/decomposition/_incremental_pca.py", line 214, in fit
    self.partial_fit(X_batch, check_input=False)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/sklearn/decomposition/_incremental_pca.py", line 261, in partial_fit
    raise ValueError("n_components=%r must be less or equal to "
ValueError: n_components=20 must be less or equal to the batch number of samples 10.
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
got data
(541946, 6022)
Centering time: 19.55328607559204
PCA fit: 1364.881026983261
numpy
{'dtype': dtype('float64')}
PCA Transform: 2.3421480655670166
{'dtype': dtype('float64')}
total iterations: 39
numpy
TLDA fit: 64.48846864700317
PCA Reverse Transform: 0.0008533000946044922
Decentering: 4.294081687927246
decenter with old strategy:
[0.00142469 0.000871   0.00101063 0.00067421 0.00075067 0.00085748
 0.00103902 0.0009717  0.00085007 0.00088499 0.00093184 0.00046446
 0.00112778 0.00040523 0.00149194 0.00092276 0.00093038 0.00080157
 0.0009658  0.00111912]
Smoothing and Normalization: 0.0005173683166503906
numpy
[[1.17943975e-04 7.06617722e-05 8.70332585e-05 ... 6.57871851e-05
  7.86451488e-05 9.29436648e-05]
 [1.80147894e-05 1.47482049e-05 2.07375747e-05 ... 1.73138986e-05
  1.60185219e-05 1.88365149e-05]
 [1.39842701e-04 1.17475086e-04 9.47095489e-05 ... 9.15432727e-05
  1.08810359e-04 1.15696408e-04]
 ...
 [4.83008197e-05 4.33424368e-05 5.02973755e-05 ... 4.78152623e-05
  4.39479455e-05 4.58599017e-05]
 [5.52666027e-05 6.23151809e-05 7.35767546e-05 ... 8.34024334e-05
  5.95396810e-05 6.26450770e-05]
 [1.81414264e-05 1.27626880e-05 1.50844730e-05 ... 1.32656239e-05
  1.53897243e-05 1.57128084e-05]]
[('centering', 19.55328607559204), ('PCA fit', 1364.881026983261), ('PCA transform', 2.3421480655670166), ('TLDA fit', 64.48846864700317), ('unwhiten factors', 0.0008533000946044922), (' decentering', 4.294081687927246), (' smoothing and normalization', 0.0005173683166503906)]
numpy
[array(['year', 'day', 'last', 'fight', 'commun', 'life', 'presid',
       'famili', 'health', 'nation', 'protect', 'get', 'woman', 'act',
       'first', 'right', 'pass', 'million', 'everi', 'job'], dtype='<U16'), array(['busi', 'small', 'famili', 'health', 'care', 'get', 'commun',
       'provid', 'nation', 'act', 'child', 'access', 'million', 'presid',
       'keep', 'trump', 'year', 'woman', 'relief', 'fund'], dtype='<U16'), array(['famili', 'get', 'commun', 'protect', 'act', 'keep', 'trump',
       'child', 'life', 'state', 'mani', 'servic', 'year', 'presid',
       'nation', 'happi', 'fight', 'busi', 'offic', 'pass'], dtype='<U16'), array(['provid', 'care', 'veteran', 'fund', 'servic', 'health', 'protect',
       'commun', 'get', 'famili', 'nation', 'presid', 'act', 'feder',
       'access', 'relief', 'busi', 'state', 'keep', 'includ'],
      dtype='<U16'), array(['health', 'care', 'protect', 'commun', 'get', 'public', 'nation',
       'state', 'presid', 'keep', 'act', 'busi', 'trump', 'day', 'servic',
       'provid', 'million', 'fund', 'veteran', 'first'], dtype='<U16'), array(['busi', 'small', 'protect', 'state', 'commun', 'feder', 'get',
       'local', 'nation', 'presid', 'fund', 'day', 'keep', 'offic',
       'year', 'program', 'fight', 'act', 'trump', 'call'], dtype='<U16'), array(['protect', 'act', 'get', 'commun', 'nation', 'health', 'fight',
       'presid', 'right', 'trump', 'provid', 'pass', 'care', 'famili',
       'busi', 'year', 'worker', 'million', 'legisl', 'life'],
      dtype='<U16'), array(['health', 'famili', 'care', 'child', 'get', 'commun', 'act',
       'nation', 'provid', 'access', 'right', 'presid', 'million', 'year',
       'trump', 'woman', 'fight', 'secur', 'mani', 'life'], dtype='<U16'), array(['secur', 'health', 'fund', 'nation', 'border', 'protect', 'presid',
       'care', 'commun', 'get', 'day', 'right', 'last', 'provid', 'biden',
       'democrat', 'year', 'everi', 'crisi', 'woman'], dtype='<U16'), array(['protect', 'offic', 'commun', 'call', 'act', 'get', 'nation',
       'keep', 'life', 'presid', 'trump', 'feder', 'fight', 'servic',
       'pleas', 'woman', 'polic', 'right', 'veteran', 'day'], dtype='<U16'), array(['state', 'famili', 'health', 'fund', 'get', 'presid', 'nation',
       'local', 'commun', 'care', 'provid', 'year', 'day', 'busi', 'unit',
       'feder', 'right', 'mani', 'first', 'everi'], dtype='<U16'), array(['provid', 'student', 'school', 'fund', 'care', 'servic', 'get',
       'protect', 'commun', 'health', 'nation', 'keep', 'learn', 'famili',
       'presid', 'child', 'program', 'act', 'access', 'call'],
      dtype='<U16'), array(['right', 'famili', 'vote', 'woman', 'fight', 'nation', 'get',
       'presid', 'commun', 'protect', 'year', 'everi', 'act', 'mani',
       'secur', 'provid', 'pass', 'feder', 'trump', 'call'], dtype='<U16'), array(['job', 'million', 'keep', 'worker', 'protect', 'get', 'presid',
       'feder', 'commun', 'health', 'fight', 'famili', 'tax', 'nation',
       'fund', 'busi', 'creat', 'act', 'trump', 'last'], dtype='<U16'), array(['woman', 'veteran', 'servic', 'day', 'nation', 'commun', 'life',
       'everi', 'protect', 'serv', 'get', 'year', 'health', 'right',
       'famili', 'act', 'care', 'offic', 'state', 'presid'], dtype='<U16'), array(['famili', 'nation', 'secur', 'get', 'presid', 'commun', 'fund',
       'child', 'border', 'day', 'provid', 'year', 'last', 'tax', 'life',
       'trump', 'keep', 'mani', 'million', 'biden'], dtype='<U16'), array(['state', 'protect', 'feder', 'presid', 'commun', 'nation', 'get',
       'local', 'day', 'offic', 'fund', 'unit', 'year', 'first', 'right',
       'fight', 'servic', 'act', 'call', 'everi'], dtype='<U16'), array(['offic', 'feder', 'call', 'state', 'famili', 'commun', 'get',
       'health', 'nation', 'presid', 'day', 'keep', 'servic', 'year',
       'pleas', 'fund', 'local', 'visit', 'everi', 'care'], dtype='<U16'), array(['health', 'care', 'right', 'protect', 'act', 'fight', 'state',
       'commun', 'woman', 'get', 'public', 'access', 'vote', 'nation',
       'presid', 'busi', 'year', 'everi', 'worker', 'call'], dtype='<U16'), array(['year', 'school', 'student', 'day', 'nation', 'last', 'get',
       'commun', 'protect', 'presid', 'right', 'life', 'famili', 'fight',
       'health', 'act', 'first', 'everi', 'high', 'public'], dtype='<U16')] <_io.TextIOWrapper name='results/tlda_topics_congress.txt' mode='w' encoding='UTF-8'>
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
got data
(541946, 6022)
Centering time: 18.20383596420288
PCA fit: 1234.443926334381
numpy
{'dtype': dtype('float64')}
PCA Transform: 2.363334894180298
{'dtype': dtype('float64')}
total iterations: 19
numpy
TLDA fit: 30.923814296722412
PCA Reverse Transform: 0.0009143352508544922
Decentering: 4.536555290222168
decenter with old strategy:
[0.00111482 0.00081396 0.00069213 0.00114752 0.00118481 0.00105123
 0.00087981 0.00073657 0.00069896 0.00124407 0.00059031 0.00062651
 0.00079483 0.0011252  0.00065204 0.00070777 0.00116739 0.00132552
 0.00072406 0.00102547]
Smoothing and Normalization: 0.0005161762237548828
numpy
[[9.72248337e-05 6.74552483e-05 5.39806629e-05 ... 1.21788970e-04
  5.81147133e-05 8.52785712e-05]
 [2.04502670e-05 1.60166043e-05 9.48924417e-06 ... 2.44076236e-05
  1.25273686e-05 1.54793298e-05]
 [4.58539447e-05 1.01874187e-04 2.02979631e-04 ... 7.25376302e-05
  1.99478946e-04 1.44836156e-04]
 ...
 [4.56574759e-05 5.10483110e-05 4.62323155e-05 ... 5.29100796e-05
  5.03888077e-05 4.08716315e-05]
 [1.02524680e-04 8.35756784e-05 2.67632263e-05 ... 9.95327214e-05
  3.37142579e-05 4.45520584e-05]
 [1.82765181e-05 1.66461141e-05 1.32526549e-05 ... 2.44214594e-05
  1.25357837e-05 1.46269455e-05]]
[('centering', 18.20383596420288), ('PCA fit', 1234.443926334381), ('PCA transform', 2.363334894180298), ('TLDA fit', 30.923814296722412), ('unwhiten factors', 0.0009143352508544922), (' decentering', 4.536555290222168), (' smoothing and normalization', 0.0005161762237548828)]
numpy
[array(['busi', 'small', 'health', 'protect', 'commun', 'get', 'famili',
       'act', 'woman', 'state', 'local', 'nation', 'right', 'care',
       'fight', 'mani', 'day', 'call', 'servic', 'hear'], dtype='<U16'), array(['offic', 'feder', 'call', 'nation', 'commun', 'get', 'presid',
       'health', 'protect', 'keep', 'servic', 'day', 'act', 'pleas',
       'woman', 'year', 'visit', 'right', 'polic', 'student'],
      dtype='<U16'), array(['presid', 'trump', 'secur', 'fund', 'nation', 'border', 'million',
       'biden', 'protect', 'get', 'provid', 'commun', 'year', 'job',
       'last', 'famili', 'feder', 'day', 'democrat', 'keep'], dtype='<U16'), array(['nation', 'right', 'student', 'school', 'year', 'get', 'presid',
       'commun', 'health', 'fight', 'fund', 'state', 'act', 'vote',
       'provid', 'mani', 'woman', 'day', 'everi', 'trump'], dtype='<U16'), array(['nation', 'right', 'presid', 'year', 'commun', 'student', 'school',
       'health', 'provid', 'state', 'day', 'woman', 'trump', 'mani',
       'fund', 'act', 'fight', 'last', 'servic', 'celebr'], dtype='<U16'), array(['right', 'famili', 'protect', 'woman', 'vote', 'fight', 'offic',
       'commun', 'act', 'get', 'feder', 'health', 'everi', 'presid',
       'care', 'call', 'state', 'mani', 'year', 'access'], dtype='<U16'), array(['nation', 'school', 'student', 'get', 'health', 'commun', 'presid',
       'fund', 'day', 'year', 'provid', 'keep', 'public', 'servic',
       'trump', 'state', 'learn', 'act', 'busi', 'last'], dtype='<U16'), array(['protect', 'get', 'famili', 'health', 'commun', 'act', 'offic',
       'care', 'presid', 'keep', 'busi', 'feder', 'life', 'trump',
       'worker', 'fight', 'million', 'small', 'pass', 'state'],
      dtype='<U16'), array(['school', 'student', 'protect', 'year', 'get', 'famili', 'commun',
       'provid', 'learn', 'fund', 'health', 'right', 'child', 'high',
       'offic', 'presid', 'act', 'care', 'visit', 'day'], dtype='<U16'), array(['health', 'commun', 'care', 'get', 'famili', 'protect', 'act',
       'state', 'nation', 'public', 'right', 'fight', 'servic', 'call',
       'life', 'veteran', 'woman', 'day', 'mani', 'join'], dtype='<U16'), array(['health', 'care', 'presid', 'trump', 'protect', 'busi', 'million',
       'nation', 'get', 'provid', 'commun', 'act', 'fund', 'access',
       'small', 'keep', 'famili', 'public', 'fight', 'worker'],
      dtype='<U16'), array(['busi', 'presid', 'small', 'trump', 'protect', 'fund', 'million',
       'nation', 'get', 'job', 'provid', 'commun', 'keep', 'secur',
       'year', 'famili', 'biden', 'program', 'feder', 'act'], dtype='<U16'), array(['famili', 'protect', 'offic', 'feder', 'commun', 'call', 'health',
       'get', 'care', 'act', 'life', 'keep', 'presid', 'trump', 'woman',
       'day', 'veteran', 'million', 'busi', 'fight'], dtype='<U16'), array(['nation', 'health', 'commun', 'get', 'secur', 'presid', 'protect',
       'act', 'famili', 'fight', 'woman', 'state', 'keep', 'day', 'trump',
       'veteran', 'busi', 'life', 'countri', 'servic'], dtype='<U16'), array(['presid', 'job', 'million', 'trump', 'keep', 'year', 'get',
       'worker', 'protect', 'commun', 'act', 'famili', 'fight', 'nation',
       'care', 'creat', 'tax', 'feder', 'health', 'provid'], dtype='<U16'), array(['get', 'protect', 'famili', 'commun', 'health', 'back', 'act',
       'vaccin', 'presid', 'care', 'busi', 'keep', 'feder', 'trump',
       'offic', 'fight', 'call', 'state', 'day', 'nation'], dtype='<U16'), array(['secur', 'health', 'nation', 'commun', 'fund', 'protect', 'border',
       'get', 'day', 'busi', 'famili', 'small', 'everi', 'call', 'join',
       'right', 'woman', 'life', 'last', 'mani'], dtype='<U16'), array(['commun', 'get', 'famili', 'protect', 'nation', 'state', 'right',
       'day', 'life', 'act', 'mani', 'call', 'happi', 'offic', 'servic',
       'woman', 'join', 'student', 'celebr', 'year'], dtype='<U16'), array(['presid', 'trump', 'million', 'get', 'protect', 'job', 'commun',
       'nation', 'famili', 'keep', 'year', 'provid', 'act', 'biden',
       'fight', 'right', 'state', 'feder', 'offic', 'care'], dtype='<U16'), array(['famili', 'protect', 'child', 'get', 'commun', 'care', 'health',
       'provid', 'presid', 'act', 'state', 'mani', 'trump', 'fight',
       'nation', 'year', 'pass', 'million', 'right', 'access'],
      dtype='<U16')] <_io.TextIOWrapper name='results/tlda_topics_congress.txt' mode='w' encoding='UTF-8'>
-3.179665645330702
new version
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999993, 1.0000000000000004]
Traceback (most recent call last):
  File "generate_tables.py", line 611, in <module>
    main()
  File "generate_tables.py", line 532, in main
    create_data(vocab=vocab, seed=seed_arr[j])
  File "generate_tables.py", line 69, in create_data
    x, mu, _, alpha_0 = test_util.get_mu(num_tops, vocab, num_tweets, density, seed)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/test_util.py", line 66, in get_mu
    if w[0][k] == 1:
KeyboardInterrupt
new version
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0000000000000004]
M1: 0.0018239021301269531
M2: 7.48681902885437
[[1.00000000e+00 1.66533454e-16]
 [2.35922393e-16 1.00000000e+00]]
W: 0.01114034652709961
Whiten X: 0.009364128112792969
Whiten M1: 2.9087066650390625e-05
Parafac M3: 0.05773305892944336
Parafac Decomposition: 13.2690589427948
Unwhitening parafac factors: 1.0728836059570312e-05
Initialization
[[-0.7187588  -0.69525951]
 [-0.69525951  0.7187588 ]]
SGD Calc: 43.926459312438965
Unwhitening factors: 1.6450881958007812e-05
Smoothing and Normalization: 5.507469177246094e-05
Fit RMSE: 0.04739440815326108
parafac Test Against Ground Truth
Smoothing and Normalization: 6.651878356933594e-05
Fit RMSE: 0.047514243634077725
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000018, 0.999999999999999]
M1: 0.0043179988861083984
M2: 20.213670015335083
[[ 1.00000000e+00 -5.98479599e-17]
 [ 2.30718222e-16  1.00000000e+00]]
W: 0.01421499252319336
Whiten X: 0.01657271385192871
Whiten M1: 4.887580871582031e-05
Parafac M3: 0.057854413986206055
Parafac Decomposition: 12.60683298110962
Unwhitening parafac factors: 2.2172927856445312e-05
Initialization
[[-0.9759532   0.21798014]
 [ 0.21798014  0.9759532 ]]
SGD Calc: 98.33648324012756
Unwhitening factors: 2.0503997802734375e-05
Smoothing and Normalization: 6.580352783203125e-05
Fit RMSE: 0.03390211576183285
parafac Test Against Ground Truth
Smoothing and Normalization: 8.654594421386719e-05
Fit RMSE: 0.03546220889215629
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999996, 0.9999999999999991]
M1: 0.0060863494873046875
M2: 49.091614961624146
[[ 1.00000000e+00 -8.08381140e-16]
 [-8.25728375e-16  1.00000000e+00]]
W: 0.029097557067871094
Whiten X: 0.024248600006103516
Whiten M1: 3.838539123535156e-05
Parafac M3: 0.05760335922241211
Parafac Decomposition: 12.643373250961304
Unwhitening parafac factors: 2.3603439331054688e-05
Initialization
[[-0.38985616 -0.92087577]
 [-0.92087577  0.38985616]]
SGD Calc: 108.68959641456604
Unwhitening factors: 2.4318695068359375e-05
Smoothing and Normalization: 8.392333984375e-05
Fit RMSE: 0.02757350263318154
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00011777877807617188
Fit RMSE: 0.02869369681117227
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000007, 1.0]
M1: 0.002054452896118164
M2: 6.888672113418579
[[1.00000000e+00 1.92554306e-16]
 [8.32667268e-17 1.00000000e+00]]
W: 0.0040667057037353516
Whiten X: 0.005834341049194336
Whiten M1: 2.4080276489257812e-05
Parafac M3: 0.05770754814147949
Parafac Decomposition: 12.535536289215088
Unwhitening parafac factors: 1.6927719116210938e-05
Initialization
[[-0.99545362 -0.09524748]
 [-0.09524748  0.99545362]]
SGD Calc: 21.743094205856323
Unwhitening factors: 1.2874603271484375e-05
Smoothing and Normalization: 4.00543212890625e-05
Fit RMSE: 0.04805524975855112
parafac Test Against Ground Truth
Smoothing and Normalization: 6.604194641113281e-05
Fit RMSE: 0.047061919353476776
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0]
M1: 0.0043070316314697266
M2: 19.987218141555786
[[ 1.00000000e+00 -3.63424568e-16]
 [-3.88578059e-16  1.00000000e+00]]
W: 0.013268709182739258
Whiten X: 0.016101360321044922
Whiten M1: 3.409385681152344e-05
Parafac M3: 0.05742287635803223
Parafac Decomposition: 12.496381282806396
Unwhitening parafac factors: 2.0742416381835938e-05
Initialization
[[-0.44342484  0.89631156]
 [ 0.89631156  0.44342484]]
SGD Calc: 22.205681085586548
Unwhitening factors: 1.7642974853515625e-05
Smoothing and Normalization: 6.031990051269531e-05
Fit RMSE: 0.03468852217590469
parafac Test Against Ground Truth
Smoothing and Normalization: 8.320808410644531e-05
Fit RMSE: 0.032874971854429676
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000018, 1.0000000000000022]
M1: 0.0060541629791259766
M2: 49.35731911659241
[[ 1.00000000e+00 -1.66533454e-16]
 [-1.94289029e-16  1.00000000e+00]]
W: 0.02698349952697754
Whiten X: 0.024252891540527344
Whiten M1: 4.124641418457031e-05
Parafac M3: 0.05728554725646973
Parafac Decomposition: 12.697001218795776
Unwhitening parafac factors: 2.2649765014648438e-05
Initialization
[[-0.99839293 -0.05667068]
 [-0.05667068  0.99839293]]
SGD Calc: 22.017603397369385
Unwhitening factors: 1.9311904907226562e-05
Smoothing and Normalization: 7.367134094238281e-05
Fit RMSE: 0.02758883324038383
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0001068115234375
Fit RMSE: 0.027393156460751723
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999998]
M1: 0.002061605453491211
M2: 6.934650182723999
[[1.00000000e+00 1.38777878e-17]
 [6.93889390e-18 1.00000000e+00]]
W: 0.004084110260009766
Whiten X: 0.005707979202270508
Whiten M1: 2.3365020751953125e-05
Parafac M3: 0.05759549140930176
Parafac Decomposition: 12.555307626724243
Unwhitening parafac factors: 1.5735626220703125e-05
Initialization
[[-0.89911431  0.43771391]
 [ 0.43771391  0.89911431]]
SGD Calc: 112.64339017868042
Unwhitening factors: 1.5974044799804688e-05
Smoothing and Normalization: 4.4345855712890625e-05
Fit RMSE: 0.046808659373854124
parafac Test Against Ground Truth
Smoothing and Normalization: 6.461143493652344e-05
Fit RMSE: 0.0498694166001545
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0000000000000007]
M1: 0.004423856735229492
M2: 19.984715938568115
[[1.00000000e+00 2.01227923e-16]
 [1.24900090e-16 1.00000000e+00]]
W: 0.013179779052734375
Whiten X: 0.015769243240356445
Whiten M1: 3.4809112548828125e-05
Parafac M3: 0.05797123908996582
Parafac Decomposition: 12.708466529846191
Unwhitening parafac factors: 1.9073486328125e-05
Initialization
[[-0.99599549  0.0894035 ]
 [ 0.0894035   0.99599549]]
SGD Calc: 21.964548587799072
Unwhitening factors: 1.52587890625e-05
Smoothing and Normalization: 5.841255187988281e-05
Fit RMSE: 0.03342459751236393
parafac Test Against Ground Truth
Smoothing and Normalization: 8.630752563476562e-05
Fit RMSE: 0.03270558912580655
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999988]
M1: 0.0061206817626953125
M2: 49.361578941345215
[[ 1.00000000e+00 -2.67147415e-16]
 [-2.65412692e-16  1.00000000e+00]]
W: 0.02803802490234375
Whiten X: 0.024443864822387695
Whiten M1: 4.458427429199219e-05
Parafac M3: 0.05788254737854004
Parafac Decomposition: 12.671351432800293
Unwhitening parafac factors: 2.5510787963867188e-05
Initialization
[[-0.21511378 -0.97658899]
 [-0.97658899  0.21511378]]
SGD Calc: 22.208189010620117
Unwhitening factors: 2.3365020751953125e-05
Smoothing and Normalization: 7.724761962890625e-05
Fit RMSE: 0.027999491125468356
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010609626770019531
Fit RMSE: 0.027334877435091397
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999998, 1.0000000000000004]
M1: 0.002058744430541992
M2: 6.763479232788086
[[ 1.00000000e+00 -2.77555756e-16]
 [-3.46944695e-16  1.00000000e+00]]
W: 0.009573221206665039
Whiten X: 0.005750894546508789
Whiten M1: 2.1219253540039062e-05
Parafac M3: 0.058612823486328125
Parafac Decomposition: 12.551005840301514
Unwhitening parafac factors: 1.621246337890625e-05
Initialization
[[-0.80244837 -0.59672155]
 [-0.59672155  0.80244837]]
SGD Calc: 21.79913830757141
Unwhitening factors: 1.3113021850585938e-05
Smoothing and Normalization: 4.172325134277344e-05
Fit RMSE: 0.048281757696919295
parafac Test Against Ground Truth
Smoothing and Normalization: 6.389617919921875e-05
Fit RMSE: 0.047570399940859376
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 0.9999999999999994]
M1: 0.004372596740722656
M2: 20.05535316467285
[[ 1.00000000e+00 -1.66533454e-16]
 [-4.85722573e-17  1.00000000e+00]]
W: 0.013189315795898438
Whiten X: 0.01575016975402832
Whiten M1: 3.552436828613281e-05
Parafac M3: 0.057942867279052734
Parafac Decomposition: 12.532832860946655
Unwhitening parafac factors: 2.0503997802734375e-05
Initialization
[[-0.25849199 -0.9660134 ]
 [-0.9660134   0.25849199]]
SGD Calc: 22.096094369888306
Unwhitening factors: 1.7881393432617188e-05
Smoothing and Normalization: 5.9604644775390625e-05
Fit RMSE: 0.034964060310742666
parafac Test Against Ground Truth
Smoothing and Normalization: 8.392333984375e-05
Fit RMSE: 0.03368233690415425
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999994, 1.0000000000000009]
M1: 0.0060710906982421875
M2: 49.30290699005127
[[ 1.00000000e+00 -1.31838984e-16]
 [-3.26128013e-16  1.00000000e+00]]
W: 0.028508424758911133
Whiten X: 0.024341106414794922
Whiten M1: 4.029273986816406e-05
Parafac M3: 0.05797076225280762
Parafac Decomposition: 12.579663515090942
Unwhitening parafac factors: 2.3365020751953125e-05
Initialization
[[-0.46392889 -0.88587244]
 [-0.88587244  0.46392889]]
SGD Calc: 21.995630741119385
Unwhitening factors: 2.288818359375e-05
Smoothing and Normalization: 7.748603820800781e-05
Fit RMSE: 0.027617120353172913
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010228157043457031
Fit RMSE: 0.026928541960908898
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0, 0.9999999999999998]
M1: 0.0020821094512939453
M2: 6.75398588180542
[[1.00000000e+00 4.85722573e-17]
 [4.85722573e-17 1.00000000e+00]]
W: 0.004125118255615234
Whiten X: 0.00572657585144043
Whiten M1: 2.1457672119140625e-05
Parafac M3: 0.05777883529663086
Parafac Decomposition: 12.546350240707397
Unwhitening parafac factors: 1.621246337890625e-05
Initialization
[[-0.34712129 -0.93782025]
 [-0.93782025  0.34712129]]
SGD Calc: 22.01378560066223
Unwhitening factors: 1.33514404296875e-05
Smoothing and Normalization: 4.0531158447265625e-05
Fit RMSE: 0.04869321092446215
parafac Test Against Ground Truth
Smoothing and Normalization: 6.628036499023438e-05
Fit RMSE: 0.046672838166704256
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999993, 0.9999999999999999]
M1: 0.004359006881713867
M2: 20.095433473587036
[[ 1.00000000e+00 -8.67361738e-18]
 [ 8.67361738e-17  1.00000000e+00]]
W: 0.013257741928100586
Whiten X: 0.0157928466796875
Whiten M1: 3.528594970703125e-05
Parafac M3: 0.05797839164733887
Parafac Decomposition: 12.46706247329712
Unwhitening parafac factors: 1.8596649169921875e-05
Initialization
[[-0.99610124 -0.08821744]
 [-0.08821744  0.99610124]]
SGD Calc: 97.18175435066223
Unwhitening factors: 2.0503997802734375e-05
Smoothing and Normalization: 8.177757263183594e-05
Fit RMSE: 0.030509045408685268
parafac Test Against Ground Truth
Smoothing and Normalization: 8.392333984375e-05
Fit RMSE: 0.034888205385604866
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999992, 1.0]
M1: 0.006075859069824219
M2: 48.958383083343506
[[ 1.00000000e+00  9.71445147e-17]
 [-1.31838984e-16  1.00000000e+00]]
W: 0.027745485305786133
Whiten X: 0.023880958557128906
Whiten M1: 3.3855438232421875e-05
Parafac M3: 0.05741596221923828
Parafac Decomposition: 12.587412357330322
Unwhitening parafac factors: 2.3603439331054688e-05
Initialization
[[-0.89701397 -0.44200218]
 [-0.44200218  0.89701397]]
SGD Calc: 21.953631162643433
Unwhitening factors: 1.8596649169921875e-05
Smoothing and Normalization: 7.486343383789062e-05
Fit RMSE: 0.02939279142516404
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010466575622558594
Fit RMSE: 0.02839455205362256
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000002, 1.0000000000000002]
M1: 0.0020482540130615234
M2: 6.732572555541992
[[ 1.00000000e+00 -3.05311332e-16]
 [-3.81639165e-16  1.00000000e+00]]
W: 0.0046770572662353516
Whiten X: 0.0061740875244140625
Whiten M1: 2.0742416381835938e-05
Parafac M3: 0.0576784610748291
Parafac Decomposition: 12.609359502792358
Unwhitening parafac factors: 1.6927719116210938e-05
Initialization
[[-0.97214797 -0.23436791]
 [-0.23436791  0.97214797]]
SGD Calc: 21.870382070541382
Unwhitening factors: 1.4543533325195312e-05
Smoothing and Normalization: 5.4836273193359375e-05
Fit RMSE: 0.048582174872485624
parafac Test Against Ground Truth
Smoothing and Normalization: 6.628036499023438e-05
Fit RMSE: 0.04754704647174602
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[0.9999999999999998, 0.9999999999999998]
M1: 0.0042650699615478516
M2: 19.960690021514893
[[ 1.00000000e+00 -4.16333634e-17]
 [ 2.77555756e-17  1.00000000e+00]]
W: 0.013361215591430664
Whiten X: 0.01587843894958496
Whiten M1: 3.4809112548828125e-05
Parafac M3: 0.057102203369140625
Parafac Decomposition: 12.567424774169922
Unwhitening parafac factors: 1.9550323486328125e-05
Initialization
[[-0.90188429  0.43197769]
 [ 0.43197769  0.90188429]]
SGD Calc: 96.17565321922302
Unwhitening factors: 2.0503997802734375e-05
Smoothing and Normalization: 6.437301635742188e-05
Fit RMSE: 0.03380498411835058
parafac Test Against Ground Truth
Smoothing and Normalization: 8.606910705566406e-05
Fit RMSE: 0.035432619848460904
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999984]
M1: 0.005998373031616211
M2: 49.092573165893555
[[ 1.00000000e+00 -2.28983499e-16]
 [-2.04697370e-16  1.00000000e+00]]
W: 0.031015634536743164
Whiten X: 0.024318456649780273
Whiten M1: 4.029273986816406e-05
Parafac M3: 0.05785989761352539
Parafac Decomposition: 12.619698524475098
Unwhitening parafac factors: 1.7642974853515625e-05
Initialization
[[-0.35060606  0.93652303]
 [ 0.93652303  0.35060606]]
SGD Calc: 23.994290590286255
Unwhitening factors: 1.8596649169921875e-05
Smoothing and Normalization: 7.367134094238281e-05
Fit RMSE: 0.028386008016724813
parafac Test Against Ground Truth
Smoothing and Normalization: 8.344650268554688e-05
Fit RMSE: 0.027188732863717823
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[1.0000000000000004, 1.0000000000000004]
M1: 0.0019805431365966797
M2: 6.766292095184326
[[1.0000000e+00 6.9388939e-17]
 [1.2490009e-16 1.0000000e+00]]
W: 0.0062618255615234375
Whiten X: 0.005719184875488281
Whiten M1: 2.3365020751953125e-05
Parafac M3: 0.05760931968688965
Parafac Decomposition: 12.607333183288574
Unwhitening parafac factors: 1.0967254638671875e-05
Initialization
[[-0.91196095 -0.41027702]
 [-0.41027702  0.91196095]]
SGD Calc: 41.833611249923706
Unwhitening factors: 1.71661376953125e-05
Smoothing and Normalization: 4.553794860839844e-05
Fit RMSE: 0.047018343963995325
parafac Test Against Ground Truth
Smoothing and Normalization: 6.842613220214844e-05
Fit RMSE: 0.04713868303251004
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000009, 0.999999999999999]
M1: 0.004431247711181641
M2: 19.943482637405396
[[ 1.00000000e+00 -7.97972799e-17]
 [-3.46944695e-17  1.00000000e+00]]
W: 0.012630224227905273
Whiten X: 0.015857696533203125
Whiten M1: 3.528594970703125e-05
Parafac M3: 0.05768895149230957
Parafac Decomposition: 12.652460813522339
Unwhitening parafac factors: 1.9311904907226562e-05
Initialization
[[-0.69764819 -0.71644051]
 [-0.71644051  0.69764819]]
SGD Calc: 21.952648401260376
Unwhitening factors: 1.7404556274414062e-05
Smoothing and Normalization: 5.8650970458984375e-05
Fit RMSE: 0.03407071451886489
parafac Test Against Ground Truth
Smoothing and Normalization: 8.392333984375e-05
Fit RMSE: 0.03334629784820334
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999991, 1.0000000000000004]
M1: 0.006036043167114258
M2: 49.226630449295044
[[ 1.00000000e+00 -2.77555756e-16]
 [-2.98372438e-16  1.00000000e+00]]
W: 0.02939295768737793
Whiten X: 0.024364471435546875
Whiten M1: 6.628036499023438e-05
Parafac M3: 0.058303117752075195
Parafac Decomposition: 12.579440593719482
Unwhitening parafac factors: 2.288818359375e-05
Initialization
[[-0.98845552  0.15151133]
 [ 0.15151133  0.98845552]]
SGD Calc: 21.770702362060547
Unwhitening factors: 2.09808349609375e-05
Smoothing and Normalization: 9.1552734375e-05
Fit RMSE: 0.028043446233260827
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010538101196289062
Fit RMSE: 0.02777413935722829
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999996, 0.9999999999999991]
M1: 0.0020232200622558594
M2: 6.878357648849487
[[ 1.00000000e+00 -2.98372438e-16]
 [-3.15719673e-16  1.00000000e+00]]
W: 0.005107879638671875
Whiten X: 0.0059931278228759766
Whiten M1: 1.9311904907226562e-05
Parafac M3: 0.05794215202331543
Parafac Decomposition: 12.983328580856323
Unwhitening parafac factors: 1.5497207641601562e-05
Initialization
[[-0.73235214 -0.68092609]
 [-0.68092609  0.73235214]]
SGD Calc: 21.82870388031006
Unwhitening factors: 1.2874603271484375e-05
Smoothing and Normalization: 4.1484832763671875e-05
Fit RMSE: 0.04866396865570723
parafac Test Against Ground Truth
Smoothing and Normalization: 5.1975250244140625e-05
Fit RMSE: 0.04735612857955201
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0000000000000004, 0.9999999999999996]
M1: 0.00442194938659668
M2: 20.23950982093811
[[ 1.00000000e+00 -1.52655666e-16]
 [ 4.16333634e-17  1.00000000e+00]]
W: 0.013264894485473633
Whiten X: 0.015772342681884766
Whiten M1: 3.2901763916015625e-05
Parafac M3: 0.057471275329589844
Parafac Decomposition: 12.63850998878479
Unwhitening parafac factors: 1.9550323486328125e-05
Initialization
[[-0.32706881 -0.94500053]
 [-0.94500053  0.32706881]]
SGD Calc: 21.795357704162598
Unwhitening factors: 1.5974044799804688e-05
Smoothing and Normalization: 5.793571472167969e-05
Fit RMSE: 0.03378315052860236
parafac Test Against Ground Truth
Smoothing and Normalization: 8.559226989746094e-05
Fit RMSE: 0.03315605578379832
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000018, 1.0000000000000022]
M1: 0.006092548370361328
M2: 49.307283878326416
[[ 1.00000000e+00 -2.08166817e-16]
 [-6.93889390e-17  1.00000000e+00]]
W: 0.0281527042388916
Whiten X: 0.024343013763427734
Whiten M1: 3.361701965332031e-05
Parafac M3: 0.057677268981933594
Parafac Decomposition: 12.61910343170166
Unwhitening parafac factors: 2.384185791015625e-05
Initialization
[[-0.07934149  0.99684749]
 [ 0.99684749  0.07934149]]
SGD Calc: 110.36525988578796
Unwhitening factors: 2.4080276489257812e-05
Smoothing and Normalization: 7.534027099609375e-05
Fit RMSE: 0.028163684953841
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00011229515075683594
Fit RMSE: 0.029529299521331837
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999996, 1.0000000000000002]
M1: 0.0020575523376464844
M2: 6.8501763343811035
[[1.00000000e+00 1.94289029e-16]
 [1.73472348e-16 1.00000000e+00]]
W: 0.00474095344543457
Whiten X: 0.005950927734375
Whiten M1: 1.8596649169921875e-05
Parafac M3: 0.05757641792297363
Parafac Decomposition: 12.53925633430481
Unwhitening parafac factors: 1.5497207641601562e-05
Initialization
[[-0.48566857 -0.87414303]
 [-0.87414303  0.48566857]]
SGD Calc: 22.15100932121277
Unwhitening factors: 1.4066696166992188e-05
Smoothing and Normalization: 4.0531158447265625e-05
Fit RMSE: 0.04907640476273123
parafac Test Against Ground Truth
Smoothing and Normalization: 6.365776062011719e-05
Fit RMSE: 0.04758445228102219
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.0, 0.9999999999999987]
M1: 0.004432201385498047
M2: 19.93601417541504
[[ 1.00000000e+00 -1.02348685e-16]
 [-6.24500451e-17  1.00000000e+00]]
W: 0.013335466384887695
Whiten X: 0.016011714935302734
Whiten M1: 3.147125244140625e-05
Parafac M3: 0.057639360427856445
Parafac Decomposition: 12.526473760604858
Unwhitening parafac factors: 2.193450927734375e-05
Initialization
[[-0.98309457 -0.18309851]
 [-0.18309851  0.98309457]]
SGD Calc: 71.15992569923401
Unwhitening factors: 2.0503997802734375e-05
Smoothing and Normalization: 6.413459777832031e-05
Fit RMSE: 0.03096526912436784
parafac Test Against Ground Truth
Smoothing and Normalization: 8.511543273925781e-05
Fit RMSE: 0.03476606865083632
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[1.0000000000000009, 1.0000000000000009]
M1: 0.0060558319091796875
M2: 49.24493980407715
[[ 1.00000000e+00 -3.64291930e-17]
 [ 7.45931095e-17  1.00000000e+00]]
W: 0.027430057525634766
Whiten X: 0.024309158325195312
Whiten M1: 3.4809112548828125e-05
Parafac M3: 0.057530879974365234
Parafac Decomposition: 12.643672227859497
Unwhitening parafac factors: 2.4318695068359375e-05
Initialization
[[-0.90516272 -0.42506522]
 [-0.42506522  0.90516272]]
SGD Calc: 47.901530742645264
Unwhitening factors: 2.2172927856445312e-05
Smoothing and Normalization: 8.106231689453125e-05
Fit RMSE: 0.027087305409338687
parafac Test Against Ground Truth
Smoothing and Normalization: 0.0001068115234375
Fit RMSE: 0.027148627215860078
 Test Against Ground Truth
Vocab: 500
num_tweets: 10000
density: 15
[0.9999999999999999, 1.0]
M1: 0.002052783966064453
M2: 6.920288324356079
[[ 1.00000000e+00 -6.93889390e-17]
 [-2.22044605e-16  1.00000000e+00]]
W: 0.004689693450927734
Whiten X: 0.0057179927825927734
Whiten M1: 2.2649765014648438e-05
Parafac M3: 0.05774855613708496
Parafac Decomposition: 12.531170845031738
Unwhitening parafac factors: 1.621246337890625e-05
Initialization
[[-0.64256515 -0.76623106]
 [-0.76623106  0.64256515]]
SGD Calc: 139.78399229049683
Unwhitening factors: 1.6450881958007812e-05
Smoothing and Normalization: 4.38690185546875e-05
Fit RMSE: 0.04556614112728683
parafac Test Against Ground Truth
Smoothing and Normalization: 6.651878356933594e-05
Fit RMSE: 0.04818908101450019
 Test Against Ground Truth
Vocab: 1000
num_tweets: 10000
density: 15
[1.000000000000001, 1.0]
M1: 0.0043637752532958984
M2: 19.9909565448761
[[ 1.00000000e+00 -7.28583860e-17]
 [-1.66533454e-16  1.00000000e+00]]
W: 0.013329029083251953
Whiten X: 0.015773296356201172
Whiten M1: 3.409385681152344e-05
Parafac M3: 0.05807781219482422
Parafac Decomposition: 12.639643669128418
Unwhitening parafac factors: 2.1219253540039062e-05
Initialization
[[-0.53226009 -0.84658089]
 [-0.84658089  0.53226009]]
SGD Calc: 43.84074902534485
Unwhitening factors: 1.7881393432617188e-05
Smoothing and Normalization: 6.222724914550781e-05
Fit RMSE: 0.03371696238673467
parafac Test Against Ground Truth
Smoothing and Normalization: 9.965896606445312e-05
Fit RMSE: 0.033728513386989935
 Test Against Ground Truth
Vocab: 1500
num_tweets: 10000
density: 15
[0.9999999999999999, 0.9999999999999998]
M1: 0.006052494049072266
M2: 49.17557692527771
[[1.00000000e+00 3.80554963e-17]
 [1.81820704e-16 1.00000000e+00]]
W: 0.028734683990478516
Whiten X: 0.024331331253051758
Whiten M1: 3.814697265625e-05
Parafac M3: 0.05741071701049805
Parafac Decomposition: 12.639801025390625
Unwhitening parafac factors: 2.3603439331054688e-05
Initialization
[[-0.8914972 -0.4530262]
 [-0.4530262  0.8914972]]
SGD Calc: 140.2700424194336
Unwhitening factors: 2.3126602172851562e-05
Smoothing and Normalization: 8.153915405273438e-05
Fit RMSE: 0.026717197576967028
parafac Test Against Ground Truth
Smoothing and Normalization: 0.00010132789611816406
Fit RMSE: 0.028301010338202787
 Test Against Ground Truth
[[array([0.98856475]), array([0.99100995])], [array([6.98749044e-18]), array([0.9490741])], [array([0.93887601]), array([2.47063662e-17])], [array([0.99966866]), array([0.99968415])], [array([0.99922596]), array([0.99921241])], [array([0.99879955]), array([0.99920041])], [array([-1.92586695e-17]), array([0.96133217])], [array([0.99918196]), array([0.99926862])], [array([0.99896333]), array([0.99910398])], [array([0.99931394]), array([0.99961561])], [array([0.99954589]), array([0.99932448])], [array([0.99865582]), array([0.99862668])], [array([0.99970087]), array([0.99943816])], [array([1.08157529e-17]), array([0.93415658])], [array([0.99893197]), array([0.99896578])], [array([0.99960929]), array([0.99961555])], [array([-4.90059758e-18]), array([0.95590735])], [array([0.99878768]), array([0.99851132])], [array([0.98779807]), array([0.98847128])], [array([0.99898824]), array([0.99938102])], [array([0.99888584]), array([0.99907907])], [array([0.99969891]), array([0.9996833])], [array([0.99910673]), array([0.99940929])], [array([0.9551906]), array([4.77046457e-17])], [array([0.99971298]), array([0.99957894])], [array([-4.94496372e-17]), array([0.9372232])], [array([0.98906533]), array([0.99101604])], [array([0.94649538]), array([7.06131172e-18])], [array([0.98384952]), array([0.98108659])], [array([-8.93420614e-18]), array([0.94413513])]]
Done!
Traceback (most recent call last):
  File "coherence_comparison.py", line 29, in <module>
    from version0_20.tlda_final_validation import TLDA
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/tlda_final_validation.py", line 5, in <module>
    from   cumulant_gradient import cumulant_gradient
ModuleNotFoundError: No module named 'cumulant_gradient'
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.015285491943359
PCA fit: 171.83358192443848
numpy
PCA Transform: 3.831059455871582
{'dtype': dtype('float32')}
total iterations: 11
numpy
TLDA fit: 36.987719774246216
PCA Reverse Transform: 0.004875659942626953
Decentering: 0.7440812587738037
decenter with old strategy:
[0.00321915 0.00321915 0.00321915 0.00321915 0.00321915 0.00321915
 0.00321915 0.00321915 0.00321915 0.00321915 0.00321915 0.00321915
 0.00339517 0.00339517 0.00339517 0.00339517 0.00334846 0.00334846
 0.00334846 0.00334846]
Smoothing and Normalization: 0.00015115737915039062
numpy
[[0.00078267 0.00078267 0.00078267 ... 0.00079676 0.00079676 0.00079676]
 [0.00272372 0.00272372 0.00272372 ... 0.00099505 0.00099505 0.00099505]
 [0.00111046 0.00111046 0.00111046 ... 0.00103227 0.00103227 0.00103227]
 ...
 [0.00080782 0.00080782 0.00080782 ... 0.00071341 0.00071341 0.00071341]
 [0.00071663 0.00071663 0.00071663 ... 0.00070062 0.00070062 0.00070062]
 [0.00026455 0.00026455 0.00026455 ... 0.00029033 0.00029033 0.00029033]]
[('centering', 4.015285491943359), ('PCA fit', 171.83358192443848), ('PCA transform', 3.831059455871582), ('TLDA fit', 36.987719774246216), ('unwhiten factors', 0.004875659942626953), (' decentering', 0.7440812587738037), (' smoothing and normalization', 0.00015115737915039062)]
numpy
Traceback (most recent call last):
  File "coherence_comparison.py", line 568, in <module>
    main()
  File "coherence_comparison.py", line 482, in main
    vocab = np.asarray(vocab)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py", line 1678, in __array__
    raise TypeError(
TypeError: Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU array, consider using cupy.asarray(...)
To explicitly construct a host array, consider using .to_array()
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.072798013687134
PCA fit: 175.2822880744934
numpy
PCA Transform: 3.945246696472168
{'dtype': dtype('float32')}
total iterations: 12
numpy
TLDA fit: 5.269637107849121
PCA Reverse Transform: 0.0004639625549316406
Decentering: 0.7911074161529541
decenter with old strategy:
[0.00320808 0.00324651 0.00327429 0.003646   0.003646   0.00332443
 0.00333428 0.00302588 0.00327429 0.00371479 0.00314175 0.00314175
 0.00346337 0.00333428 0.00320808 0.00309251 0.00340704 0.00346337
 0.00309251 0.00309251]
Smoothing and Normalization: 0.0001804828643798828
numpy
[[0.0007975  0.00077953 0.00078379 ... 0.00088351 0.00077044 0.00077044]
 [0.00195364 0.00287405 0.00183922 ... 0.00051974 0.00116876 0.00116876]
 [0.00111341 0.00139763 0.00123614 ... 0.00130534 0.00092483 0.00092483]
 ...
 [0.0007775  0.00075426 0.00078657 ... 0.00078871 0.00065314 0.00065314]
 [0.00050797 0.00061674 0.00065001 ... 0.00053184 0.00054303 0.00054303]
 [0.00029198 0.00027459 0.00032257 ... 0.00029121 0.00042898 0.00042898]]
[('centering', 4.072798013687134), ('PCA fit', 175.2822880744934), ('PCA transform', 3.945246696472168), ('TLDA fit', 5.269637107849121), ('unwhiten factors', 0.0004639625549316406), (' decentering', 0.7911074161529541), (' smoothing and normalization', 0.0001804828643798828)]
numpy
Traceback (most recent call last):
  File "coherence_comparison.py", line 568, in <module>
    main()
  File "coherence_comparison.py", line 482, in main
    vocab = np.asarray(vocab)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py", line 1678, in __array__
    raise TypeError(
TypeError: Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU array, consider using cupy.asarray(...)
To explicitly construct a host array, consider using .to_array()
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 570, in <module>
    main()
  File "coherence_comparison.py", line 428, in main
    vocab = cp.asnumpy(vectorizer.get_feature_names())
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cupy/__init__.py", line 775, in asnumpy
    elif hasattr(a, "__cuda_array_interface__"):
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py", line 3869, in __cuda_array_interface__
    return self._column.__cuda_array_interface__
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/column/column.py", line 1030, in __cuda_array_interface__
    raise NotImplementedError(
NotImplementedError: dtype object is not yet supported via `__cuda_array_interface__`
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 572, in <module>
    main()
  File "coherence_comparison.py", line 431, in main
    texts_raw = pickle.load(open("data/preprocessed_metoo_tweets.obj", "rb"))
EOFError: Ran out of input
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
                                              tweets
0  #womensmarch plant a seed for futur initi norm...
1  congratul to the women march today we must go ...
2                     #womensmarch @ trafalgar squar
3  jasmin frye amp bri curri both from virgina ar...
4  @indivisibleteam @leoweekli at #womensmarch sf...
Traceback (most recent call last):
  File "coherence_comparison.py", line 577, in <module>
    main()
  File "coherence_comparison.py", line 429, in main
    texts_raw = [stem(removeStopwords(tokenize(regexchars(cleanLine(str(line)))))) for line in texts_raw]
  File "coherence_comparison.py", line 429, in <listcomp>
    texts_raw = [stem(removeStopwords(tokenize(regexchars(cleanLine(str(line)))))) for line in texts_raw]
KeyboardInterrupt
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 577, in <module>
    main()
  File "coherence_comparison.py", line 425, in main
    df = pd.read_csv("data/MeTooMonthCleaned/twitter_per_month_201701.csv", header=0, names=["tweets"], dtype = str)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/pandas/io/parsers.py", line 610, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/pandas/io/parsers.py", line 468, in _read
    return parser.read(nrows)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/pandas/io/parsers.py", line 1057, in read
    index, columns, col_dict = self._engine.read(nrows)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/pandas/io/parsers.py", line 2036, in read
    data = self._reader.read(nrows)
  File "pandas/_libs/parsers.pyx", line 756, in pandas._libs.parsers.TextReader.read
  File "pandas/_libs/parsers.pyx", line 771, in pandas._libs.parsers.TextReader._read_low_memory
  File "pandas/_libs/parsers.pyx", line 827, in pandas._libs.parsers.TextReader._read_rows
  File "pandas/_libs/parsers.pyx", line 814, in pandas._libs.parsers.TextReader._tokenize_rows
  File "pandas/_libs/parsers.pyx", line 1951, in pandas._libs.parsers.raise_parser_error
pandas.errors.ParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
                                              tweets
0  #womensmarch plant a seed for futur initi norm...
1  congratul to the women march today we must go ...
2                     #womensmarch @ trafalgar squar
3  jasmin frye amp bri curri both from virgina ar...
4  @indivisibleteam @leoweekli at #womensmarch sf...
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 580, in <module>
    main()
  File "coherence_comparison.py", line 441, in main
    texts_lemmatized = pickle.load(open("data/countvecs_metoo_lemmatized.obj", 'wb'))
io.UnsupportedOperation: read
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 580, in <module>
    main()
  File "coherence_comparison.py", line 441, in main
    texts_lemmatized = pickle.load(open("data/countvecs_metoo_lemmatized.obj", 'rb'))
EOFError: Ran out of input
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
                                              tweets
0  #womensmarch plant a seed for futur initi norm...
1  congratul to the women march today we must go ...
2                     #womensmarch @ trafalgar squar
3  jasmin frye amp bri curri both from virgina ar...
4  @indivisibleteam @leoweekli at #womensmarch sf...
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 582, in <module>
    main()
  File "coherence_comparison.py", line 434, in main
    vocab = cp.asnumpy(vectorizer.get_feature_names())
NameError: name 'vectorizer' is not defined
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 582, in <module>
    main()
  File "coherence_comparison.py", line 434, in main
    vocab = cp.asnumpy(vectorizer.get_feature_names())
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cupy/__init__.py", line 775, in asnumpy
    elif hasattr(a, "__cuda_array_interface__"):
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py", line 3869, in __cuda_array_interface__
    return self._column.__cuda_array_interface__
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/column/column.py", line 1030, in __cuda_array_interface__
    raise NotImplementedError(
NotImplementedError: dtype object is not yet supported via `__cuda_array_interface__`
starting
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
('gensim LDA', 624.430757522583)
Traceback (most recent call last):
  File "coherence_comparison.py", line 582, in <module>
    main()
  File "coherence_comparison.py", line 471, in main
    temp_file = datapath("data/gensim_metoo_model")
NameError: name 'datapath' is not defined
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.366981267929077
PCA fit: 196.4431221485138
numpy
PCA Transform: 4.365019798278809
{'dtype': dtype('float32')}
total iterations: 200
numpy
TLDA fit: 125.87183904647827
PCA Reverse Transform: 0.008216142654418945
Decentering: 0.8586628437042236
decenter with old strategy:
[0.00331517 0.00303388 0.00312364 0.00398886 0.00321652 0.00308239
 0.00303388 0.00411752 0.00376066 0.0030709  0.00411752 0.00298956
 0.00327819 0.00331517 0.00308239 0.0030709  0.00307786 0.00327819
 0.0031298  0.00319583 0.00351803 0.00307743 0.00345355 0.00331517
 0.00351803 0.00411752 0.0032474  0.00308239 0.00312364 0.0032164 ]
Smoothing and Normalization: 0.0002353191375732422
numpy
[[0.00053044 0.00050447 0.00053094 ... 0.00052674 0.00053094 0.00052829]
 [0.00166255 0.00165076 0.00233986 ... 0.001309   0.00233986 0.00093036]
 [0.00070171 0.00074574 0.00078665 ... 0.00069882 0.00078665 0.00068096]
 ...
 [0.00047573 0.00059685 0.00061645 ... 0.00050872 0.00061645 0.00042343]
 [0.00043071 0.00043751 0.00054964 ... 0.00039556 0.00054964 0.00034003]
 [0.00020508 0.00021618 0.00016911 ... 0.00020645 0.00016911 0.0002105 ]]
[('centering', 4.366981267929077), ('PCA fit', 196.4431221485138), ('PCA transform', 4.365019798278809), ('TLDA fit', 125.87183904647827), ('unwhiten factors', 0.008216142654418945), (' decentering', 0.8586628437042236), (' smoothing and normalization', 0.0002353191375732422)]
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.310445070266724
PCA fit: 196.21167969703674
numpy
PCA Transform: 4.136113405227661
{'dtype': dtype('float32')}
total iterations: 14
numpy
TLDA fit: 6.565389633178711
PCA Reverse Transform: 0.00046706199645996094
Decentering: 0.8469324111938477
decenter with old strategy:
[0.00376459 0.0033257  0.00336448 0.00309859 0.00330229 0.00312228
 0.00375141 0.00330229 0.00312228 0.00312228 0.00321403 0.00312228
 0.00312228 0.00330229 0.00316886 0.00312562 0.00443521 0.00299888
 0.00312228 0.003235  ]
Smoothing and Normalization: 0.00018405914306640625
numpy
[[0.00062894 0.00051358 0.00055217 ... 0.00052354 0.00053587 0.00054121]
 [0.00118792 0.00104259 0.00107556 ... 0.00100854 0.00161979 0.00097188]
 [0.00067964 0.00065762 0.00086729 ... 0.00067624 0.00085039 0.00068998]
 ...
 [0.00049544 0.00047006 0.00049742 ... 0.00048399 0.00054688 0.00043984]
 [0.00032917 0.00033418 0.00035104 ... 0.00038473 0.0004593  0.00058677]
 [0.00020594 0.00021774 0.00017041 ... 0.00021322 0.00016797 0.0002106 ]]
[('centering', 4.310445070266724), ('PCA fit', 196.21167969703674), ('PCA transform', 4.136113405227661), ('TLDA fit', 6.565389633178711), ('unwhiten factors', 0.00046706199645996094), (' decentering', 0.8469324111938477), (' smoothing and normalization', 0.00018405914306640625)]
  File "coherence_comparison.py", line 466
    passes=10,
    ^
SyntaxError: invalid syntax
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 585, in <module>
    main()
  File "coherence_comparison.py", line 462, in main
    lda_model = models.ldamulticore.LdaMulticore(corpus=corpus,
TypeError: __init__() got an unexpected keyword argument 'chunk_size'
starting
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
('gensim LDA', 1157.7618517875671)
Traceback (most recent call last):
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/gensim/utils.py", line 763, in save
    _pickle.dump(self, fname_or_handle, protocol=pickle_protocol)
TypeError: file must have a 'write' attribute

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "coherence_comparison.py", line 585, in <module>
    main()
  File "coherence_comparison.py", line 474, in main
    lda_model.save(temp_file)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/gensim/models/ldamodel.py", line 1595, in save
    self.state.save(utils.smart_extension(fname, '.state'), *args, **kwargs)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/gensim/utils.py", line 766, in save
    self._smart_save(fname_or_handle, separately, sep_limit, ignore, pickle_protocol=pickle_protocol)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/gensim/utils.py", line 610, in _smart_save
    pickle(self, fname, protocol=pickle_protocol)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/gensim/utils.py", line 1441, in pickle
    with open(fname, 'wb') as fout:  # 'b' for binary, needed on Windows
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/smart_open/smart_open_lib.py", line 188, in open
    fobj = _shortcut_open(
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/smart_open/smart_open_lib.py", line 361, in _shortcut_open
    return _builtin_open(local_path, mode, buffering=buffering, **open_kwargs)
FileNotFoundError: [Errno 2] No such file or directory: '/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/gensim/test/test_data/data/gensim_metoo_model.state'
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.456366062164307
PCA fit: 177.61651587486267
numpy
PCA Transform: 4.2198076248168945
{'dtype': dtype('float32')}
total iterations: 200
numpy
TLDA fit: 92.04925680160522
PCA Reverse Transform: 0.00618743896484375
Decentering: 0.8722636699676514
decenter with old strategy:
[0.00327491 0.00320435 0.00320435 0.00320435 0.00317981 0.00325546
 0.00417385 0.00322345 0.00314842 0.00388995 0.00327491 0.00384703
 0.00320435 0.00320435 0.00325546 0.00320435 0.00315769 0.00321017
 0.0030493  0.00320435]
Smoothing and Normalization: 0.0002143383026123047
numpy
(20, 1714)
Traceback (most recent call last):
  File "coherence_comparison.py", line 587, in <module>
    main()
  File "coherence_comparison.py", line 501, in main
    vocab = np.asarray(vocab)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py", line 1678, in __array__
    raise TypeError(
TypeError: Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU array, consider using cupy.asarray(...)
To explicitly construct a host array, consider using .to_array()
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.449879884719849
PCA fit: 165.46786212921143
numpy
PCA Transform: 4.136975288391113
{'dtype': dtype('float32')}
total iterations: 34
numpy
TLDA fit: 15.499260425567627
PCA Reverse Transform: 0.00048422813415527344
Decentering: 0.8392715454101562
decenter with old strategy:
[0.00330874 0.00308752 0.00421885 0.00411146 0.00323448 0.00421885
 0.00308755 0.00341208 0.00341208 0.00317745 0.00317745 0.00323448
 0.00322953 0.00322953 0.00321594 0.00298231 0.00323448 0.00298231
 0.00325046 0.0032082 ]
Smoothing and Normalization: 0.0001938343048095703
numpy
(20, 1714)
-7.60634613421716
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.301898002624512
PCA fit: 154.6352243423462
numpy
PCA Transform: 4.050484657287598
{'dtype': dtype('float32')}
total iterations: 11
numpy
TLDA fit: 4.702479839324951
PCA Reverse Transform: 0.0004429817199707031
Decentering: 0.7940421104431152
decenter with old strategy:
[0.00341971 0.0040279  0.00305294 0.00305294 0.00296396 0.00396397
 0.00323409 0.0040279  0.0040279  0.00297959 0.00324954 0.00333474
 0.00341971 0.00324954 0.00297959 0.00341971 0.00324954 0.00324954
 0.00448702 0.00312834]
Smoothing and Normalization: 0.00017333030700683594
numpy
(20, 1714)
-7.137149515872936
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.2767040729522705
PCA fit: 135.6094925403595
numpy
PCA Transform: 4.052855730056763
{'dtype': dtype('float32')}
total iterations: 11
numpy
TLDA fit: 4.774925470352173
PCA Reverse Transform: 0.0004677772521972656
Decentering: 0.8205044269561768
decenter with old strategy:
[0.00321516 0.00414812 0.00332744 0.00314645 0.00410229 0.00285567
 0.00323374 0.00410229 0.00323374 0.00344533 0.00402648 0.00323374
 0.00332672 0.00344533 0.00453058 0.00306608 0.00344533 0.00410229
 0.00321371 0.00321371]
Smoothing and Normalization: 0.0001735687255859375
numpy
(20, 1714)
-6.308819607165363
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.339581727981567
PCA fit: 130.94852232933044
numpy
PCA Transform: 4.220030069351196
{'dtype': dtype('float32')}
total iterations: 11
numpy
TLDA fit: 4.696486473083496
PCA Reverse Transform: 0.0007016658782958984
Decentering: 0.8714101314544678
decenter with old strategy:
[0.00322611 0.00344667 0.00277302 0.00330373 0.00320866 0.00298157
 0.00321514 0.00413016 0.00322611 0.00413016 0.00344667 0.00411008
 0.00316534 0.00330373 0.00321514 0.0036465  0.00323609 0.00322611
 0.00453456 0.0039872 ]
Smoothing and Normalization: 0.00017261505126953125
numpy
(20, 1714)
-6.83549164043597
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.407552242279053
PCA fit: 132.53206324577332
numpy
PCA Transform: 4.259575128555298
{'dtype': dtype('float32')}
total iterations: 14
numpy
TLDA fit: 6.097167491912842
PCA Reverse Transform: 0.0005061626434326172
Decentering: 0.8764593601226807
decenter with old strategy:
[0.00344985 0.00399508 0.00331164 0.00322969 0.00324894 0.00320869
 0.00399508 0.00324894 0.00344985 0.00320869 0.00324894 0.00316838
 0.00321545 0.0041065  0.00322969 0.00451761 0.00324894 0.00344985
 0.00322969 0.00315726]
Smoothing and Normalization: 0.0001404285430908203
numpy
(20, 1714)
-6.498687184962064
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.309466361999512
PCA fit: 128.84122610092163
numpy
PCA Transform: 4.076329708099365
{'dtype': dtype('float32')}
total iterations: 119
numpy
TLDA fit: 55.52168083190918
PCA Reverse Transform: 0.00047659873962402344
Decentering: 0.8042194843292236
decenter with old strategy:
[0.00314829 0.00344756 0.00399234 0.00450909 0.00399234 0.00314829
 0.00324858 0.00320442 0.00323256 0.00344756 0.00314829 0.00323256
 0.00330162 0.00324858 0.00324858 0.00344756 0.00322969 0.00364622
 0.00323256 0.00320042]
Smoothing and Normalization: 0.0001761913299560547
numpy
(20, 1714)
-6.439431481254576
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.341917991638184
PCA fit: 132.76436114311218
numpy
PCA Transform: 4.133180618286133
{'dtype': dtype('float32')}
total iterations: 200
numpy
TLDA fit: 92.10053253173828
PCA Reverse Transform: 0.0020394325256347656
Decentering: 0.8465578556060791
decenter with old strategy:
[0.00321746 0.00327215 0.00331674 0.00311919 0.00344957 0.00326753
 0.00344526 0.00289221 0.00325345 0.00323384 0.00390068 0.00318701
 0.00323145 0.00406991 0.00313854 0.00360015 0.0032518  0.00323522
 0.00456578 0.00460141]
Smoothing and Normalization: 0.000164031982421875
numpy
(20, 1714)
-6.3430944403257765
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Centering time: 4.4259092807769775
PCA fit: 131.93625497817993
numpy
PCA Transform: 4.196850538253784
{'dtype': dtype('float32')}
total iterations: 200
numpy
TLDA fit: 67.56486392021179
PCA Reverse Transform: 0.0010044574737548828
Decentering: 0.8521068096160889
decenter with old strategy:
[0.00228577 0.00361179 0.00259962 0.00359531 0.00395597 0.00437199
 0.00312266 0.003812   0.0038296  0.00322793 0.00346279 0.0033145
 0.00352904 0.00318998 0.00410571 0.00385253 0.00363831 0.00353683
 0.00268343 0.00355781]
Smoothing and Normalization: 0.0002009868621826172
numpy
(20, 1714)
-7.108636239501936
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 590, in <module>
    main()
  File "coherence_comparison.py", line 426, in main
    texts, vocab = get_metoo_data()
  File "coherence_comparison.py", line 157, in get_metoo_data
    vectorizer = pickle.load(open("data/Meena_testing/countvec.obj", "rb"))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/abc.py", line 166, in host_deserialize
    @classmethod
KeyboardInterrupt
starting
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
('gensim LDA', 613.8320708274841)
('gensim LDA', 613.8320708274841)
-10.27267457861488
starting
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
('gensim LDA', 1193.520837545395)
('gensim LDA', 1193.520837545395)
-10.264622133369024
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
Traceback (most recent call last):
  File "coherence_comparison.py", line 33, in <module>
    from version0_20.preprocess_efficient import cleanLine, regexchars, tokenize, removeStopwords, stem
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/preprocess_efficient.py", line 27, in <module>
    import dask.dataframe as dd
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/dataframe/__init__.py", line 3, in <module>
    from . import backends, dispatch, rolling
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/dataframe/backends.py", line 19, in <module>
    from .core import DataFrame, Index, Scalar, Series, _Frame
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/dataframe/core.py", line 26, in <module>
    from .. import array as da
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/array/__init__.py", line 3, in <module>
    from . import backends, fft, lib, linalg, ma, overlap, random
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/array/fft.py", line 14, in <module>
    from .creation import arange as _arange
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/array/creation.py", line 27, in <module>
    from .ufunc import greater_equal, rint
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/array/ufunc.py", line 215, in <module>
    square = ufunc(np.square)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/array/ufunc.py", line 109, in __init__
    derived_from(np)(self)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/utils.py", line 757, in wrapper
    method.__doc__ = _derived_from(
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/utils.py", line 726, in _derived_from
    doc = extra_titles(doc)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/utils.py", line 619, in extra_titles
    titles = {
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/utils.py", line 622, in <dictcomp>
    if lines[i + 1].strip() and all(c == "-" for c in lines[i + 1].strip())
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/dask/utils.py", line 622, in <genexpr>
    if lines[i + 1].strip() and all(c == "-" for c in lines[i + 1].strip())
KeyboardInterrupt
starting
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
('gensim LDA', 1189.2340593338013)
('gensim LDA', 1189.2340593338013)
-10.219692223297116
starting
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
Traceback (most recent call last):
  File "coherence_comparison.py", line 30, in <module>
    from version0_20.pca import PCA
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/pca.py", line 3, in <module>
    import cuml
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cuml/__init__.py", line 78, in <module>
    from cuml.svm import SVC
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cuml/svm/__init__.py", line 16, in <module>
    from cuml.svm.svc import SVC
  File "cuml/svm/svc.pyx", line 47, in init cuml.svm.svc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cuml/multiclass/__init__.py", line 16, in <module>
    from cuml.multiclass.multiclass import OneVsOneClassifier
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cuml/multiclass/multiclass.py", line 17, in <module>
    import sklearn.multiclass
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/sklearn/multiclass.py", line 138, in <module>
    class OneVsRestClassifier(MultiOutputMixin, ClassifierMixin,
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/sklearn/multiclass.py", line 249, in OneVsRestClassifier
    def __init__(self, estimator, *, n_jobs=None):
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/sklearn/utils/validation.py", line 78, in _deprecate_positional_args
    return _inner_deprecate_positional_args(func)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/sklearn/utils/validation.py", line 49, in _inner_deprecate_positional_args
    sig = signature(f)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/inspect.py", line 3105, in signature
    return Signature.from_callable(obj, follow_wrapped=follow_wrapped)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/inspect.py", line 2854, in from_callable
    return _signature_from_callable(obj, sigcls=cls,
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/inspect.py", line 2304, in _signature_from_callable
    return _signature_from_function(sigcls, obj,
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/inspect.py", line 2197, in _signature_from_function
    parameters.append(Parameter(name, annotation=annotation,
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/inspect.py", line 2491, in __init__
    self._kind = _ParameterKind(kind)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/enum.py", line 339, in __call__
    return cls.__new__(cls, value)
KeyboardInterrupt
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 591, in <module>
    main()
  File "coherence_comparison.py", line 426, in main
    texts, vocab = get_metoo_data()
  File "coherence_comparison.py", line 156, in get_metoo_data
    vectors = pickle.load(open("data/Meena_testing/x_mat/twitter_per_month_201701_split_1.obj", 'rb'))
FileNotFoundError: [Errno 2] No such file or directory: 'data/Meena_testing/x_mat/twitter_per_month_201701_split_1.obj'
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 591, in <module>
    main()
  File "coherence_comparison.py", line 426, in main
    texts, vocab = get_metoo_data()
  File "coherence_comparison.py", line 156, in get_metoo_data
    vectors = pickle.load(open("data/Meena_testing/x_mat/twitter_per_month_201701_split_1.obj", 'rb'))
FileNotFoundError: [Errno 2] No such file or directory: 'data/Meena_testing/x_mat/twitter_per_month_201701_split_1.obj'
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 591, in <module>
    main()
  File "coherence_comparison.py", line 443, in main
    texts_lemmatized.append([w for w in text.split(' ') if w.lower() in vocab])
NameError: name 'texts_lemmatized' is not defined
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
Traceback (most recent call last):
  File "coherence_comparison.py", line 591, in <module>
    main()
  File "coherence_comparison.py", line 443, in main
    texts_lemmatized.append([w for w in text.split(' ') if w.lower() in vocab])
AttributeError: 'cupy._core.core.ndarray' object has no attribute 'split'
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
0           abl
1         abort
2       absolut
3           abt
4           aca
         ...   
1709         yo
1710       york
1711      young
1712     youtub
1713       zero
Name: token, Length: 1714, dtype: object
Traceback (most recent call last):
  File "coherence_comparison.py", line 592, in <module>
    main()
  File "coherence_comparison.py", line 444, in main
    texts_lemmatized.append([w for w in text.split(' ') if w.lower() in vocab])
AttributeError: 'numpy.ndarray' object has no attribute 'split'
starting
0           abl
1         abort
2       absolut
3           abt
4           aca
         ...   
1709         yo
1710       york
1711      young
1712     youtub
1713       zero
Name: token, Length: 1714, dtype: object
                                              tweets
0  #womensmarch plant a seed for futur initi norm...
1  congratul to the women march today we must go ...
2                     #womensmarch @ trafalgar squar
3  jasmin frye amp bri curri both from virgina ar...
4  @indivisibleteam @leoweekli at #womensmarch sf...
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
('gensim LDA', 582.9479422569275)
('gensim LDA', 582.9479422569275)
-10.170606746644347
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
0           abl
1         abort
2       absolut
3           abt
4           aca
         ...   
1709         yo
1710       york
1711      young
1712     youtub
1713       zero
Name: token, Length: 1714, dtype: object
Centering time: 4.346883058547974
Traceback (most recent call last):
  File "coherence_comparison.py", line 592, in <module>
    main()
  File "coherence_comparison.py", line 482, in main
    res, factors_tlda = gen_fit_0_20(texts, num_tops = n_tops, alpha_0 = alpha_0, n_iter_train = 10001)
  File "coherence_comparison.py", line 386, in gen_fit_0_20
    x_whit = pca.transform(x_cent)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/pca.py", line 48, in transform
    return tl.dot(X, (self.projection_weights_ / tl.sqrt(self.whitening_weights_)[None, :]))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/__init__.py", line 161, in inner
    return _get_backend_method(name)(*args, **kwargs)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/numpy_backend.py", line 38, in dot
    return a.dot(b)
ValueError: shapes (1192442,1720) and (1714,20) not aligned: 1720 (dim 1) != 1714 (dim 0)
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
0           abl
1         abort
2       absolut
3           abt
4           aca
         ...   
1709         yo
1710       york
1711      young
1712     youtub
1713       zero
Name: token, Length: 1714, dtype: object
Centering time: 4.4371466636657715
Traceback (most recent call last):
  File "coherence_comparison.py", line 592, in <module>
    main()
  File "coherence_comparison.py", line 482, in main
    res, factors_tlda = gen_fit_0_20(texts, num_tops = n_tops, alpha_0 = alpha_0, n_iter_train = 10001)
  File "coherence_comparison.py", line 386, in gen_fit_0_20
    x_whit = pca.transform(x_cent)
  File "/home/danny/Dropbox/TLDA-Dev/version0_20/pca.py", line 48, in transform
    return tl.dot(X, (self.projection_weights_ / tl.sqrt(self.whitening_weights_)[None, :]))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/__init__.py", line 161, in inner
    return _get_backend_method(name)(*args, **kwargs)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/numpy_backend.py", line 38, in dot
    return a.dot(b)
ValueError: shapes (1192442,1720) and (1714,20) not aligned: 1720 (dim 1) != 1714 (dim 0)
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
0           abl
1         abort
2       absolut
3           abt
4           aca
         ...   
1709         yo
1710       york
1711      young
1712     youtub
1713       zero
Name: token, Length: 1714, dtype: object
Centering time: 4.422138452529907
PCA fit: 135.23408579826355
PCA Transform: 4.130000114440918
{'dtype': dtype('float32')}
total iterations: 57
numpy
TLDA fit: 11.056969165802002
PCA Reverse Transform: 0.0005364418029785156
Decentering: 0.8205325603485107
decenter with old strategy:
[0.00320308 0.00323062 0.00366124 0.00313442 0.00329852 0.00323062
 0.00313442 0.00324978 0.00320308 0.00451535 0.0040392  0.00320308
 0.00324978 0.00324978 0.00323062 0.00321012 0.00461149 0.00343596
 0.00410547 0.00410547]
Smoothing and Normalization: 0.00018286705017089844
numpy
(20, 1720)
[('centering', 4.422138452529907), ('PCA fit', 135.23408579826355), ('PCA transform', 4.130000114440918), ('TLDA fit', 11.056969165802002), ('unwhiten factors', 0.0005364418029785156), (' decentering', 0.8205325603485107), (' smoothing and normalization', 0.00018286705017089844)]
-6.226641524414981
[nltk_data] Downloading package stopwords to /home/danny/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
starting
0           abl
1         abort
2       absolut
3           abt
4           aca
         ...   
1709         yo
1710       york
1711      young
1712     youtub
1713       zero
Name: token, Length: 1714, dtype: object
Traceback (most recent call last):
  File "coherence_comparison.py", line 592, in <module>
    main()
  File "coherence_comparison.py", line 482, in main
    res, factors_tlda = gen_fit_0_20(texts, num_tops = n_tops, alpha_0 = alpha_0, n_iter_train = 10001)
  File "coherence_comparison.py", line 365, in gen_fit_0_20
    x_cent = tl.tensor(x - tl.mean(x, axis=0))
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/__init__.py", line 161, in inner
    return _get_backend_method(name)(*args, **kwargs)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/tensorly/backend/cupy_backend.py", line 24, in tensor
    return cp.array(data, dtype=dtype)
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/cupy/_creation/from_data.py", line 41, in array
    return _core.array(obj, dtype, copy, order, subok, ndmin)
  File "cupy/_core/core.pyx", line 2120, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2137, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 394, in cupy._core.core.ndarray.astype
  File "cupy/_core/core.pyx", line 453, in cupy._core.core.ndarray.astype
  File "cupy/_core/core.pyx", line 164, in cupy._core.core.ndarray.__init__
  File "cupy/cuda/memory.pyx", line 735, in cupy.cuda.memory.alloc
  File "/home/danny/anaconda3/envs/rapids-21.08/lib/python3.8/site-packages/rmm/rmm.py", line 212, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "rmm/_lib/device_buffer.pyx", line 84, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: CUDA error at: /home/danny/anaconda3/envs/rapids-21.08/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
